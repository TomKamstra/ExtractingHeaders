,parsed_line,heading_bool,source_paper,math_bool,latin_numbered_bool,numberic_numbered_bool
0,"Abstract
",True,1802,False,False,True
1,"1 Introduction
",True,1802,False,False,True
2,"2.1 Concepts, Notation, and Training
",True,1802,False,False,True
3,"2.2 Generative Sequence Models
",True,1802,False,False,True
4,"2.3 Overﬁtting in Machine Learning
",True,1802,False,False,True
5,"The Secret Sharer: Evaluating and Testing
",False,1802,False,False,True
6,"Unintended Memorization in Neural Networks
",False,1802,False,False,True
7,"Nicholas Carlini1,2Chang Liu2Úlfar Erlingsson1Jernej Kos3Dawn Song2
",False,1802,False,False,True
8,"1Google Brain2University of California, Berkeley3National University of Singapore
",False,1802,False,False,True
9,"This paper describes a testing methodology for quantita-
",False,1802,False,False,True
10,"tively assessing the risk that rare or unique training-data
",False,1802,False,False,True
11,"sequences are unintentionally memorized by generative se-
",False,1802,False,False,True
12,"quence models—a common type of machine-learning model.
",False,1802,False,False,True
13,"Because such models are sometimes trained on sensitive data
",False,1802,False,False,True
14,"(e.g., the text of users’ private messages), this methodology
",False,1802,False,False,True
15,"can beneﬁt privacy by allowing deep-learning practitioners to
",False,1802,False,False,True
16,"select means of training that minimize such memorization.
",False,1802,False,False,True
17,"In experiments, we show that unintended memorization is
",False,1802,False,False,True
18,"a persistent, hard-to-avoid issue that can have serious conse-
",False,1802,False,False,True
19,"quences. Speciﬁcally, for models trained without considera-
",False,1802,False,False,True
20,"tion of memorization, we describe new, efﬁcient procedures
",False,1802,False,False,True
21,"that can extract unique, secret sequences, such as credit card
",False,1802,False,False,True
22,"numbers. We show that our testing strategy is a practical and
",False,1802,False,False,True
23,"easy-to-use ﬁrst line of defense, e.g., by describing its ap-
",False,1802,False,False,True
24,"plication to quantitatively limit data exposure in Google’s
",False,1802,False,False,True
25,"Smart Compose, a commercial text-completion neural net-
",False,1802,False,False,True
26,"work trained on millions of users’ email messages.
",False,1802,False,False,True
27,"1 Introduction
",False,1802,False,False,True
28,"When a secret is shared, it can be very difﬁcult to prevent its
",False,1802,False,False,True
29,"further disclosure—as artfully explored in Joseph Conrad’s
",False,1802,False,False,True
30,"The Secret Sharer [10]. This difﬁculty also arises in machine-
",False,1802,False,False,True
31,"learning models based on neural networks, which are being
",False,1802,False,False,True
32,"rapidly adopted for many purposes. What details those models
",False,1802,False,False,True
33,"may have unintentionally memorized and may disclose can
",False,1802,False,False,True
34,"be of signiﬁcant concern, especially when models are public
",False,1802,False,False,True
35,"and models’ training involves sensitive or private data.
",False,1802,False,False,True
36,"Disclosure of secrets is of particular concern in neural-
",False,1802,False,False,True
37,"network models that classify or predict sequences of natural-
",False,1802,False,False,True
38,"language text. First, such text will often contain sensitive or
",False,1802,False,False,True
39,"private sequences, accidentally, even if the text is supposedly
",False,1802,False,False,True
40,"public. Second, such models are designed to learn text pat-
",False,1802,False,False,True
41,"terns such as grammar, turns of phrase, and spelling, which
",False,1802,False,False,True
42,"comprise a vanishing fraction of the exponential space of
",False,1802,False,False,True
43,"all possible sequences. Therefore, even if sensitive or pri-
",False,1802,False,False,True
44,"vate training-data text is very rare, one should assume that
",False,1802,False,False,True
45,"well-trained models have paid attention to its precise details.
",False,1802,False,False,True
46,"Concretely, disclosure of secrets may arise naturally in gen-
",False,1802,False,False,True
47,"erative text models like those used for text auto-completion
",False,1802,False,False,True
48,"and predictive keyboards, if trained on possibly-sensitive data.
",False,1802,False,False,True
49,"The users of such models may discover—either by accident
",False,1802,False,False,True
50,"or on purpose—that entering certain text preﬁxes causes the
",False,1802,False,False,True
51,"models to output surprisingly-revealing text completions [37].For example, users may ﬁnd that the input “my social-security
",False,1802,False,False,True
52,"number is. . . ” gets auto-completed to an obvious secret (such
",False,1802,False,False,True
53,"as a valid-looking SSN not their own), or ﬁnd that other in-
",False,1802,False,False,True
54,"puts are auto-completed to text with oddly-speciﬁc details. So
",False,1802,False,False,True
55,"triggered, unscrupulous or curious users may start to “attack”
",False,1802,False,False,True
56,"such models by entering different input preﬁxes to try to mine
",False,1802,False,False,True
57,"possibly-secret sufﬁxes. Therefore, for generative text mod-
",False,1802,False,False,True
58,"els, assessing and reducing the chances that secrets may be
",False,1802,False,False,True
59,"disclosed in this manner is a key practical concern.
",False,1802,False,False,True
60,"To enable practitioners to measure their models’ propensity
",False,1802,False,False,True
61,"for disclosing details about private training data, this paper
",False,1802,False,False,True
62,"introduces a quantitative metric of exposure . This metric can
",False,1802,False,False,True
63,"be applied during training as part of a testing methodology
",False,1802,False,False,True
64,"that empirically measures a model’s potential for unintended
",False,1802,False,False,True
65,"memorization of unique or rare sequences in the training data.
",False,1802,False,False,True
66,"Our exposure metric conservatively characterizes knowl-
",False,1802,False,False,True
67,"edgeable attackers that target secrets unlikely to be discovered
",False,1802,False,False,True
68,"by accident (or by a most-likely beam search). As validation
",False,1802,False,False,True
69,"of this, we describe an algorithm guided by the exposure met-
",False,1802,False,False,True
70,"ric that, given a pretrained model, can efﬁciently extract secret
",False,1802,False,False,True
71,"sequences even when the model considers parts of them to be
",False,1802,False,False,True
72,"highly unlikely. We demonstrate our algorithm’s effectiveness
",False,1802,False,False,True
73,"in experiments, e.g., by extracting credit card numbers from a
",False,1802,False,False,True
74,"language model trained on the Enron email data. Such empir-
",False,1802,False,False,True
75,"ical extraction has proven useful in convincing practitioners
",False,1802,False,False,True
76,"that unintended memorization is an issue of serious, practical
",False,1802,False,False,True
77,"concern, and not just of academic interest.
",False,1802,False,False,True
78,"Our exposure-based testing strategy is practical, as we
",False,1802,False,False,True
79,"demonstrate in experiments, and by describing its use in
",False,1802,False,False,True
80,"removing privacy risks for Google’s Smart Compose, a de-
",False,1802,False,False,True
81,"ployed, commercial model that is trained on millions of users’
",False,1802,False,False,True
82,"email messages and used by other users for predictive text
",False,1802,False,False,True
83,"completion during email composition [8].
",False,1802,False,False,True
84,"In evaluating our exposure metric, we ﬁnd unintended mem-
",False,1802,False,False,True
85,"orization to be both commonplace and hard to prevent. In
",False,1802,False,False,True
86,"particular, such memorization is notdue to overtraining [47]:
",False,1802,False,False,True
87,"it occurs early during training, and persists across different
",False,1802,False,False,True
88,"types of models and training strategies—even when the mem-
",False,1802,False,False,True
89,"orized data is very rare and the model size is much smaller
",False,1802,False,False,True
90,"than the size of the training data corpus. Furthermore, we
",False,1802,False,False,True
91,"show that simple, intuitive regularization approaches such
",False,1802,False,False,True
92,"as early-stopping and dropout are insufﬁcient to prevent un-
",False,1802,False,False,True
93,"intended memorization. Only by using differentially-private
",False,1802,False,False,True
94,"training techniques are we able to eliminate the issue com-
",False,1802,False,False,True
95,"pletely, albeit at some loss in utility.arXiv:1802.08232v3  [cs.LG]  16 Jul 20190 2 4 6 8
",False,1802,False,False,True
96,"Repetitions of canary in training data51015202530Canary exposure in trained model
",False,1802,False,False,True
97,"Hyperparameters A
",False,1802,False,False,True
98,"Hyperparameters BFigure 1: Results of our testing methodology applied to a state-
",False,1802,False,False,True
99,"of-the-art, word-level neural-network language model [35].
",False,1802,False,False,True
100,"Two models are trained to near-identical accuracy using two
",False,1802,False,False,True
101,"different training strategies (hyperparameters A and B). The
",False,1802,False,False,True
102,"models differ signiﬁcantly in how they memorize a randomly-
",False,1802,False,False,True
103,"chosen canary word sequence. Strategy A memorizes strongly
",False,1802,False,False,True
104,"enough that if the canary occurs 9 times, it can be extracted
",False,1802,False,False,True
105,"from the model using the techniques of Section 8.
",False,1802,False,False,True
106,"Threat Model and Testing Methodology. This work as-
",False,1802,False,False,True
107,"sumes a threat model of curious or malevolent users that can
",False,1802,False,False,True
108,"query models a large number of times, adaptively, but only in
",False,1802,False,False,True
109,"a black-box fashion where they see only the models’ output
",False,1802,False,False,True
110,"probabilities (or logits). Such targeted, probing queries pose
",False,1802,False,False,True
111,"a threat not only to secret sequences of characters, such as
",False,1802,False,False,True
112,"credit card numbers, but also to uncommon word combina-
",False,1802,False,False,True
113,"tions. For example, if corporate data is used for training, even
",False,1802,False,False,True
114,"simple association of words or concepts may reveal aspects of
",False,1802,False,False,True
115,"business strategies [33]; generative text models can disclose
",False,1802,False,False,True
116,"even more, e.g., auto completing “splay-ﬂexed brace columns”
",False,1802,False,False,True
117,"with the text “using pan traps at both maiden apexes of the
",False,1802,False,False,True
118,"jimjoints,” possibly revealing industrial trade secrets [6].
",False,1802,False,False,True
119,"For this threat model, our key contribution is to give practi-
",False,1802,False,False,True
120,"tioners a means to answer the following question: “Is my
",False,1802,False,False,True
121,"model likely to memorize and potentially expose rarely-
",False,1802,False,False,True
122,"occurring, sensitive sequences in training data?” For this,
",False,1802,False,False,True
123,"we describe a quantitative testing procedure based on insert-
",False,1802,False,False,True
124,"ing randomly-chosen canary sequences a varying number of
",False,1802,False,False,True
125,"times into models’ training data. To gauge how much models
",False,1802,False,False,True
126,"memorize, our exposure metric measures the relative differ-
",False,1802,False,False,True
127,"ence in perplexity between those canaries and equivalent,
",False,1802,False,False,True
128,"non-inserted random sequences.
",False,1802,False,False,True
129,"Our testing methodology enables practitioners to choose
",False,1802,False,False,True
130,"model-training approaches that best protect privacy—basing
",False,1802,False,False,True
131,"their decisions on the empirical likelihood of training-data
",False,1802,False,False,True
132,"disclosure and not only on the sensitivity of the training data.
",False,1802,False,False,True
133,"Figure 1 demonstrates this, by showing how two approaches
",False,1802,False,False,True
134,"to training a real-world model to the same accuracy can dra-
",False,1802,False,False,True
135,"matically differ in their unintended memorization.2 Background: Neural Networks
",False,1802,False,False,True
136,"First, we provide a brief overview of the necessary technical
",False,1802,False,False,True
137,"background for neural networks and sequence models.
",False,1802,False,False,True
138,"2.1 Concepts, Notation, and Training
",False,1802,False,False,True
139,"Aneural network is a parameterized function fq()that is de-
",False,1802,False,False,True
140,"signed to approximate an arbitrary function. Neural networks
",False,1802,False,False,True
141,"are most often used when it is difﬁcult to explicitly formulate
",False,1802,False,False,True
142,"how a function should be computed, but what to compute
",False,1802,False,False,True
143,"can be effectively speciﬁed with examples, known as training
",False,1802,False,False,True
144,"data. The architecture of the network is the general structure
",False,1802,False,False,True
145,"of the computation, while the parameters (orweights ) are the
",False,1802,False,False,True
146,"concrete internal values qused to compute the function.
",False,1802,False,False,True
147,"We use standard notation [21]. Given a training set X=
",False,1802,False,False,True
148,"f(xi;yi)gm
",False,1802,False,False,True
149,"i=1consisting of mexamples xiand labels yi, the pro-
",False,1802,False,False,True
150,"cess of training teaches the neural network to map each given
",False,1802,False,False,True
151,"example to its corresponding label. We train by performing
",False,1802,False,False,True
152,"(non-linear) gradient descent with respect to the parameters
",False,1802,False,False,True
153,"qon a loss function that measures how close the network is
",False,1802,False,False,True
154,"to correctly classifying each input. The most commonly used
",False,1802,False,False,True
155,"loss function is cross-entropy loss: given distributions pand
",False,1802,False,False,True
156,"qwe have H(p;q) = åzp(z)log(q(z)), with per-example
",False,1802,False,False,True
157,"lossL(x;y;q) =H(fq(x);y)forfq.
",False,1802,False,False,True
158,"During training, we ﬁrst sample a random minibatch B
",False,1802,False,False,True
159,"consisting of labeled training examples f(¯xj;¯yj)gm0
",False,1802,False,False,True
160,"j=1drawn
",False,1802,False,False,True
161,"fromX(where m0is the batch size ; often between 32 and
",False,1802,False,False,True
162,"1024). Gradient descent then updates the weights qof the
",False,1802,False,False,True
163,"neural network by setting
",False,1802,False,False,True
164,"qnew qold h1
",False,1802,False,False,True
165,"m0m0
",False,1802,False,False,True
166,"å
",False,1802,False,False,True
167,"j=1ÑqL(¯xj;¯yj;q)
",False,1802,False,False,True
168,"That is, we adjust the weights h-far in the direction that mini-
",False,1802,False,False,True
169,"mizes the loss of the network on this batch Busing the current
",False,1802,False,False,True
170,"weights qold. Here, his called the learning rate .
",False,1802,False,False,True
171,"In order to reach maximum accuracy (i.e., minimum loss),
",False,1802,False,False,True
172,"it is often necessary to train multiple times over the entire set
",False,1802,False,False,True
173,"of training data X, with each such iteration called one epoch .
",False,1802,False,False,True
174,"This is of relevance to memorization, because it means mod-
",False,1802,False,False,True
175,"els are likely to see the same, potentially-sensitive training
",False,1802,False,False,True
176,"examples multiple times during their training process.
",False,1802,False,False,True
177,"2.2 Generative Sequence Models
",False,1802,False,False,True
178,"A generative sequence model is a fundamental architecture
",False,1802,False,False,True
179,"for common tasks such as language-modeling [4], translation
",False,1802,False,False,True
180,"[3], dialogue systems, caption generation, optical character
",False,1802,False,False,True
181,"recognition, and automatic speech recognition, among others.
",False,1802,False,False,True
182,"For example, consider the task of modeling natural-
",False,1802,False,False,True
183,"language English text from the space of all possible sequences
",False,1802,False,False,True
184,"of English words. For this purpose, a generative sequence
",False,1802,False,False,True
185,"model would assign probabilities to words based on the con-
",False,1802,False,False,True
186,"text in which those words appeared in the empirical distri-
",False,1802,False,False,True
187,"bution of the model’s training data. For example, the modelmight assign the token “lamb” a high probability after seeing
",False,1802,False,False,True
188,"the sequence of words “Mary had a little”, and the token “the”
",False,1802,False,False,True
189,"a low probability because—although “the” is a very common
",False,1802,False,False,True
190,"word—this preﬁx of words requires a noun to come next, to
",False,1802,False,False,True
191,"ﬁt the distribution of natural, valid English.
",False,1802,False,False,True
192,"Formally, generative sequence models are designed to gen-
",False,1802,False,False,True
193,"erate a sequence of tokens x1:::xnaccording to an (unknown)
",False,1802,False,False,True
194,"distribution Pr(x1:::xn). Generative sequence models estimate
",False,1802,False,False,True
195,"this distribution, which can be decomposed through Bayes’
",False,1802,False,False,True
196,"rule as Pr(x1:::xn) =Pn
",False,1802,False,False,True
197,"i=1Pr(xijx1:::xi 1). Each individual
",False,1802,False,False,True
198,"computation Pr(xijx1:::xi 1)represents the probability of to-
",False,1802,False,False,True
199,"kenxioccurring at timestep iwith previous tokens x1toxi 1.
",False,1802,False,False,True
200,"Modern generative sequence models most frequently em-
",False,1802,False,False,True
201,"ploy neural networks to estimate each conditional distribution.
",False,1802,False,False,True
202,"To do this, a neural network is trained (using gradient de-
",False,1802,False,False,True
203,"scent to update the neural-network weights q) to output the
",False,1802,False,False,True
204,"conditional probability distribution over output tokens, given
",False,1802,False,False,True
205,"input tokens x1toxi 1, that maximizes the likelihood of the
",False,1802,False,False,True
206,"training-data text corpus. For such models, Pr(xijx1:::xi 1)
",False,1802,False,False,True
207,"is deﬁned as the probability of the token xias returned by
",False,1802,False,False,True
208,"evaluating the neural network fq(x1:::xi 1).
",False,1802,False,False,True
209,"Neural-network generative sequence models most often
",False,1802,False,False,True
210,"use model architectures that can be naturally evaluated on
",False,1802,False,False,True
211,"variable-length inputs, such as Recurrent Neural Networks
",False,1802,False,False,True
212,"(RNNs). RNNs are evaluated using a current token (e.g., word
",False,1802,False,False,True
213,"or character) and a current state , and output a predicted next
",False,1802,False,False,True
214,"token as well as an updated state. By processing input tokens
",False,1802,False,False,True
215,"one at a time, RNNs can thereby process arbitrary-sized inputs.
",False,1802,False,False,True
216,"In this paper we use LSTMs [24] or qRNNs [5].
",False,1802,False,False,True
217,"2.3 Overﬁtting in Machine Learning
",False,1802,False,False,True
218,"Figure 2: Overtraining.
",False,1802,False,False,True
219,"0 20 40
",False,1802,False,False,True
220,"0 20 40
",False,1802,False,False,True
221,"Abstract : With introducing and developing AI logic, this science as a branch of computer science could impact 
",True,188130647,False,False,False
222,"I. INTRODUCTION  
",True,188130647,False,False,False
223,"II. ARTIFICIAL INTELLIGENCE  (AI) 
",True,188130647,False,False,False
224,"III. INTELLIGENT SYSTEM  (IS) 
",True,188130647,False,False,False
225,"IV. EXPERT SYTEMS (ESs)  
",True,188130647,False,False,False
226,"V. APPLICATION IS / AI / ES IN THE LIBRARY SYSTEM   
",True,188130647,False,False,False
227,"VII. FINDING  
",True,188130647,False,False,False
228,"Universit y of N ebraska - L incoln
",False,188130647,False,False,False
229,"DigitalComm ons@U niversit y of N ebraska - L incoln
",False,188130647,False,False,False
230,"Library Philosophy and P ractice (e-journal) Libraries at University of N ebraska-Lincoln
",False,188130647,False,False,False
231,"Winter 2-1-2018
",False,188130647,False,False,False
232,"Artificial Intelligence( AI) applic ation in Li brary
",False,188130647,False,False,False
233,"Systems in I ran: A t axonom y study
",False,188130647,False,False,False
234,"Asefeh A semi
",False,188130647,False,False,False
235,"University of I sfahan, asemi@e du.ui.ac.ir
",False,188130647,False,False,False
236,"Adeleh A semi
",False,188130647,False,False,False
237,"Higher Ed ucation Institute of Saf ahan
",False,188130647,False,False,False
238,"Follow thi s and a dditional w orks at:https://d igitalcommon s.unl.edu/l ibphilprac
",False,188130647,False,False,False
239,"Part of the Comput er Engineering Common s, and the Library and I nfor mation S cience
",False,188130647,False,False,False
240,"Common s
",False,188130647,False,False,False
241,"Asemi, Asefeh and A semi, Ade leh, ""Artificial Intelligence(AI) application in L ibrary Systems in I ran: A t axonom y study "" (2018).
",False,188130647,False,False,False
242,"Library Ph ilosophy and Practice (e-journal). 1840.
",False,188130647,False,False,False
243,"https://d igitalcommon s.unl.edu/l ibphilprac/1840
",False,188130647,False,False,False
244,"brought to you by CORE View metadata, citation and similar papers at core.ac.uk
",False,188130647,False,False,False
245,"provided by UNL | Libraries 
",False,188130647,False,False,False
246,"  
",False,188130647,False,False,False
247," 
",False,188130647,False,False,False
248,"Page- 1 - Artificial Intelligence (AI) application  
",False,188130647,False,False,False
249,"in Library Systems in Iran: A taxonomy study  
",False,188130647,False,False,False
250," 
",False,188130647,False,False,False
251,"1ASEFEH ASEMI , 2ADELEH ASEMI  
",False,188130647,False,False,False
252," 
",False,188130647,False,False,False
253,"1 Associate professor, Department of Knowledge  & Information Science, Faculty of Education & Psychology , University of 
",False,188130647,False,False,False
254,"Isfahan , Isfahan, Iran. asemi@edu.ui.ac.ir  
",False,188130647,False,False,False
255,"2 Assistant Professor, Depa rtment  of Artificial Intelligence, Higher Education Institute of Safahan, Isfahan, Iran. 
",False,188130647,False,False,False
256,"ad_asemi@yahoo.com  (Corresponding  Author)
",False,188130647,False,False,False
257," 
",False,188130647,False,False,False
258,"and improve all sciences which used computer systems. LIS also could get benefit from AI in many areas. This 
",False,188130647,False,False,False
259,"paper survey applications of AI in library and information science and introduce the potential of library system 
",False,188130647,False,False,False
260,"to apply AI techniques.  Intelligent systems have contributed for many librarian purposes like cataloging, 
",False,188130647,False,False,False
261,"indexing, information retrieval, reference, and other purposes.  We applied Explorato ry Factor Analysis (EFA)  as 
",False,188130647,False,False,False
262,"a primer method for identification of the most applicable AI techniques categories in LIS.  ESs are the most 
",False,188130647,False,False,False
263,"usable intelligent system in LIS which mimic librarian expert’s behaviors to support decision and management. 
",False,188130647,False,False,False
264,"AI also c an utilize in many areas such as speech recognition, machine translation and librarian robots.  In this 
",False,188130647,False,False,False
265,"study four criteria for the application of AI in the library systems in Iran was considered and it is determined in 
",False,188130647,False,False,False
266,"three area included public services, technical services, and management services. Then, degree of development 
",False,188130647,False,False,False
267,"these services was studied using taxonomy method.  The results showed that m ost developed Recommender 
",False,188130647,False,False,False
268,"Systems (RM) in library systems in Iran and Natural Language Processing (NLP) is the most undeveloped 
",False,188130647,False,False,False
269,"criterion .  
",False,188130647,False,False,False
270," 
",False,188130647,False,False,False
271,"Index  terms:  Artificial Intelligence  (AI), Expert System  (ES), Recommender Systems (RM), Natural Language 
",False,188130647,False,False,False
272,"Processing (NLP),  Data Mining (DM), Library System, Exploratory Factor Analysis (EFA) .  
",False,188130647,False,False,False
273," 
",False,188130647,False,False,False
274," 
",False,188130647,False,False,False
275,"I. INTRODUCTION  
",False,188130647,False,False,False
276," AI is the area of computer science focusing on creating machines that can engage on behaviors that 
",False,188130647,False,False,False
277,"humans consider intelligent. AI involves following areas of researches: (1) expert system, (2) fuzzy logic, 
",False,188130647,False,False,False
278,"(3) artificial neural network, (4) evolutionary a lgorithms, (5) case base reasoning, (6) image processing, 
",False,188130647,False,False,False
279,"(7) natural language processing, (8) speech recognition and (9) robotic. These areas are not separate and 
",False,188130647,False,False,False
280,"in many intelligent systems at the same time two or more AI techniques are contributed to so lve problem. 
",False,188130647,False,False,False
281,"AI techniques or tools has utilized in many areas such business, management, medicine, military and etc. 
",False,188130647,False,False,False
282,"Also has developed in using intelligent systems. The Ideas of utilization intelligent system instead of 
",False,188130647,False,False,False
283,"classic system in libraries started from 1990. Intelligent library systems utilize AI technologies to provide 
",False,188130647,False,False,False
284,"knowledge -based services to library patrons  and staff AI is a broad, complex area of study, which can be 
",False,188130647,False,False,False
285,"difficult for non -specialists to understand. Yet, its ultimate promise is to create computer systems that 
",False,188130647,False,False,False
286,"rival human intelligence, and this clearly has major implications for librarianship. Different studies are  Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
287," 
",False,188130647,False,False,False
288," about AI  applications  in library system such as : descriptive cataloging, technical services, collection 
",False,188130647,False,False,False
289,"development, subject indexing, reference services, database searching, and document delivery. Some  
",False,188130647,False,False,False
290,"papers deal with the underlying de sign issues of knowledge representation and natural language 
",False,188130647,False,False,False
291,"processing. Many authors have previously provided in -depth overviews of AI technologies. There have 
",False,188130647,False,False,False
292,"also been several reviews of research and development efforts relevant to librarianship based o n a review 
",False,188130647,False,False,False
293,"of major models of human intelligence . It is  conclude d that the following ten factors are most pertinent to 
",False,188130647,False,False,False
294,"ES research:  acquisition , automatization , comprehension , memory management , met control , numeric 
",False,188130647,False,False,False
295,"ability , reasoning , social competence , verbal perception , and v isual perception  [1]. 
",False,188130647,False,False,False
296,"Today with the advent of the computer and 50 years of research into AI programming techniques, the 
",False,188130647,False,False,False
297,"dream of smart libraries is becoming a reality. Researchers are creating systems which can mimic 
",False,188130647,False,False,False
298,"librarian thought and behavior which never before was possible.  The Basic problem in this research was 
",False,188130647,False,False,False
299,"the state of library development in the use of AI in the field of public services, technical services, and 
",False,188130647,False,False,False
300,"management services in Iran. Therefore, the qualitative development of these three activities in use of AI 
",False,188130647,False,False,False
301,"were studied  based on four criteria . The practical criteria or factors are identified throu gh factor analysis 
",False,188130647,False,False,False
302,"method. The practical criteria  include d the use of expert systems / knowledge  systems , intelligent 
",False,188130647,False,False,False
303,"decision support systems / recommender  systems , intelligent data mining,  and intelligent natural language 
",False,188130647,False,False,False
304,"processing systems is a library  system . Accordingly, the development of these facilities were studied with 
",False,188130647,False,False,False
305,"using taxonomy method in th ree areas of public service,  technical services, and management  services  
",False,188130647,False,False,False
306,"according to experts  views in Iran.  
",False,188130647,False,False,False
307," 
",False,188130647,False,False,False
308,"II. ARTIFICIAL INTELLIGENCE  (AI) 
",False,188130647,False,False,False
309," AI is the science and engineering of making intelligent machines, especially intelligent computer 
",False,188130647,False,False,False
310,"programs. It is Concerned with the study and creation of computer systems that exhibit some form of 
",False,188130647,False,False,False
311,"intelligence: system that learn new concepts and tasks, systems that  can reason and draw useful 
",False,188130647,False,False,False
312,"conclusions about the world around us, systems that can understand a natural language or perceive and 
",False,188130647,False,False,False
313,"comprehend a visual scene, and systems that perform other types of feat that require human types of 
",False,188130647,False,False,False
314,"Intelligence  [2]. 
",False,188130647,False,False,False
315,"It is th e Application of Computer and utilization of computer based products and services in the 
",False,188130647,False,False,False
316,"performance of different library operations and functions or in the prevision of various services and 
",False,188130647,False,False,False
317,"production of output products.  Automation implies degree of mecha nization where the routines and 
",False,188130647,False,False,False
318,"receptive jobs or operation are left to be performed by machines with little or no intervention by human 
",False,188130647,False,False,False
319,"beings. Lesser the degree of human intervention, greater the degree of automation this does not mean that 
",False,188130647,False,False,False
320,"automation do es away with human beings. On the contrary human being sari relieved of routine chores 
",False,188130647,False,False,False
321,"giving them more time for task which require their intelligence.   
",False,188130647,False,False,False
322," 
",False,188130647,False,False,False
323,"III. INTELLIGENT SYSTEM  (IS) 
",False,188130647,False,False,False
324,"Any Computer -based that helps in the task of subject indexing can be thought o f as an ES at least in 
",False,188130647,False,False,False
325,"the loosest sense of that term, especially if it helps  ales experienced person to approximate the work of an 
",False,188130647,False,False,False
326,"expert indexer and systems that suggest term to indexers,  or correct certain indexer errors, can be  Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
327," 
",False,188130647,False,False,False
328," considered to offer at least a modicum of ''intelligence""  [4]. One of the clearest statement relating to the 
",False,188130647,False,False,False
329,"scope of AI  is the following :  
",False,188130647,False,False,False
330," 
",False,188130647,False,False,False
331,"… Computer programs have been developed which exhibit human - like reasoning, which may be able to learn from their 
",False,188130647,False,False,False
332,"mistakes, and which quickly and cleverly perform tasks normally done by scarce and expensive human experts [4].  
",False,188130647,False,False,False
333," 
",False,188130647,False,False,False
334,"IV. EXPERT SYTEMS (ESs)  
",False,188130647,False,False,False
335,"ESs are computer based systems witch simulate human decision makings. In general they ask question 
",False,188130647,False,False,False
336,"from user and take user’s answer as input then explain rational for decision result.  
",False,188130647,False,False,False
337,"Building an ES first involves extracting the relevant knowledge from the human expert such 
",False,188130647,False,False,False
338,"knowledge is often heuristic in nature. ESs have been used to solve a wide range of problems in domains 
",False,188130647,False,False,False
339,"such as medicine, mathematics, engineering, geology, computer science, business, law, defiance, 
",False,188130647,False,False,False
340,"education, etc [2]. Some of the requir ements to development of an ESs are included: Expert on the 
",False,188130647,False,False,False
341,"problem exist; Experts have the time to dedicate to the project Export system development is possible; 
",False,188130647,False,False,False
342,"Experts can articulate their methods; Problem is not too difficult; Problem is not poorly und erstood; 
",False,188130647,False,False,False
343,"Problem requires Cognitive Skills only; and results are measurable and can be agreed upon by the experts 
",False,188130647,False,False,False
344,"[4]. 
",False,188130647,False,False,False
345," 
",False,188130647,False,False,False
346,"V. APPLICATION IS / AI / ES IN THE LIBRARY SYSTEM   
",False,188130647,False,False,False
347,"ESs consist of two main elements: knowledge base and inference engine. Knowledge base is 
",False,188130647,False,False,False
348,"involving all information needed which human/librarian experts are using them to make decision. This 
",False,188130647,False,False,False
349,"information present in knowledge base as fact and rules. ESs can m ake much better decision than 
",False,188130647,False,False,False
350,"librarian decision makers because their knowledge base can involve experiences of team of best experts. 
",False,188130647,False,False,False
351,"To design rules of knowledge base, the manner of librarian experts to make decision is emulated. The 
",False,188130647,False,False,False
352,"rules are consisting two main phases: If phase and then phase. If phase is consisting conditions and then 
",False,188130647,False,False,False
353,"phase is consisting results. The only thing which distinguishes ESs from other computer systems is 
",False,188130647,False,False,False
354,"inference engine. The inference engine simulates human decision makings based on knowledge base and 
",False,188130647,False,False,False
355,"rule base.     
",False,188130647,False,False,False
356,"An obvious potential application of ES within libraries is for the selection of book sellers or other 
",False,188130647,False,False,False
357,"vendor of library materials carried to its logical conclusion, a system might be developed to select a 
",False,188130647,False,False,False
358,"vendor a utomate ethical based on past performance in the supply of publications of a particular type such 
",False,188130647,False,False,False
359,"a capability would be especially valuable in the acquisition of material that are less routine -conference 
",False,188130647,False,False,False
360,"proceeding. Certain technical report, publications i n certain languages, publications from certain 
",False,188130647,False,False,False
361,"countries, and soon  [3]. 
",False,188130647,False,False,False
362,"      Other ESs, designed to help library user satisfy their own needs, have also include document - orders 
",False,188130647,False,False,False
363,"aid. Systems have also been designed within the library community to aid in t he selection process, 
",False,188130647,False,False,False
364,"systems of this type have been discussed by some of the researchers.  
",False,188130647,False,False,False
365,"The term ""referral system"", as used here, relates to systems that & are designed to refer library users 
",False,188130647,False,False,False
366,"to information sources likely to provide the answer to a particular question of the factual of ""information""  Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
367," 
",False,188130647,False,False,False
368," type within the library community more work  has been done on system of this kind than on any other ES. 
",False,188130647,False,False,False
369,"The objective of such systems is obvious: to guide library users to a reference suitable source when a 
",False,188130647,False,False,False
370,"librarians not available to help them form reference referral system cover knowledge as a who le in  the 
",False,188130647,False,False,False
371,"coverage of a general reference library while other are restricted to highly specialized domain  [3]. The 
",False,188130647,False,False,False
372,"librarian robot consists of a manipulator, which can recognize and manipulate books, and a mobile 
",False,188130647,False,False,False
373,"platform, which can localize itself and na vigate using ambient RFID tags embedded in a floor. AI 
",False,188130647,False,False,False
374,"techniques such genetic algorithm, artificial neural network, fuzzy logic or hybrid methods can improve 
",False,188130647,False,False,False
375,"librarian robots.  For example JT2FIS is a Java Class Library for Interval Type -2 Fuzzy Inference Systems 
",False,188130647,False,False,False
376,"that can be used to build intelligent object -oriented applications. The architecture of the system is 
",False,188130647,False,False,False
377,"presented and its object -oriented design is described [11] . It is mentioned that AI is a branch of Science, 
",False,188130647,False,False,False
378,"which deals with helping machines, fin ds solutions to complex problems in a more human -like fashion, 
",False,188130647,False,False,False
379,"this generally involves borrowing characteristics from human intelligence and applying them as 
",False,188130647,False,False,False
380,"algorithms in a computer friendly way. It can use for NLP  and help to search relevant information from 
",False,188130647,False,False,False
381,"databases, indexing and to reduce language barrier. In information retrieval process the user can state his 
",False,188130647,False,False,False
382,"information requirement in natural language making the searching more easily and fruitful this allows 
",False,188130647,False,False,False
383,"users to state complex retrieval language s [2].  
",False,188130647,False,False,False
384,"Many activities in the provision of library and information services involve expertise, and thus provide 
",False,188130647,False,False,False
385,"application where ES techniques and technology to improve performance. We are using integrated MCDM 
",False,188130647,False,False,False
386,"techniques  in different applications [13]. Analysis of the literature on the applications of ESs in LIS yields the 
",False,188130647,False,False,False
387,"application areas: Knowledge base indexing [5]; NLP and abstracting [6]; Reference work [7]; 
",False,188130647,False,False,False
388,"Cataloguing [7]; Online information retrieval [8] and [2]; Intelligent interface, in partic ular interfaces for 
",False,188130647,False,False,False
389,"online information retrieval systems; Subject analysis and representation, including classification, 
",False,188130647,False,False,False
390,"indexing and abstracting service; Reference and referral systems; Hypertext and hypermedia; and 
",False,188130647,False,False,False
391,"Collection development [2]. In view of the various features of a modern computer system we find that it 
",False,188130647,False,False,False
392,"has been applied in several areas: acquisition, cataloguing, serials control and circulation, information 
",False,188130647,False,False,False
393,"retrieval, and dissemination, inter library loan, cooperative acquisition and catalog uing have been 
",False,188130647,False,False,False
394,"automated in library [3]. For example OSCAL is an online library that helps students trying to learn 
",False,188130647,False,False,False
395,"operating system concepts and AICAL is a similar online library with Artificial Intelligence concepts. 
",False,188130647,False,False,False
396,"Both these libraries offer graphical animations of operating system and artificial intelligence concepts [9].  
",False,188130647,False,False,False
397,"Also, the concept of digital library is utilizing science ES in the process of cataloging and searching 
",False,188130647,False,False,False
398,"digital collections. By using this digital library based of ES, users can searc h the collection, reading 
",False,188130647,False,False,False
399,"collection, and download the desired collection by online system [10].  We can use system of intelligent 
",False,188130647,False,False,False
400,"library retrieval based on data mining. Also DM can use in on -line library system and help to find user's 
",False,188130647,False,False,False
401,"information needs. T oday, RM have extend applications in the library systems. They help to end user to 
",False,188130647,False,False,False
402,"select suitable keyword/ phrase in information retrieval. Different studies are on the user -centered design 
",False,188130647,False,False,False
403,"of a recommender system for library catalogue and other divisions  of the library system.  
",False,188130647,False,False,False
404," 
",False,188130647,False,False,False
405,"VI. METHODOLOGY   Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
406,"VI. METHODOLOGY   Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
407," 
",False,188130647,False,False,False
408," In general, numerical taxonomy analysis is a great method to grading, classification, and comparison 
",False,188130647,False,False,False
409,"of different activities according to the benefit and enjoyment of the activities based on the considered 
",False,188130647,False,False,False
410,"criteria . The ability of this method, it is able to do the two works together. The first, it can divided the 
",False,188130647,False,False,False
411,"evaluated sets based on the criteria  presented into homogeneous subsets. The second, it can to grade the 
",False,188130647,False,False,False
412,"elements and members of each homogeneous subset.   
",False,188130647,False,False,False
413,"In this research we used of Taxonomy Method to examine amount of development the library systems in 
",False,188130647,False,False,False
414,"Iran in three area: 1. Public Services (PS), 2. Technical Services (TS), and 3. Management Services (MS). 
",False,188130647,False,False,False
415,"Previous studies mentioned various AI techniques t o address the above areas. Some of the mentioned 
",False,188130647,False,False,False
416,"techniques in literature had different names with the same application in LISs. Therefore, we applied 
",False,188130647,False,False,False
417,"Exploratory  Factor A nalysis (EFA) ( Osborne and Costello, 2009 ) to group the techniques with same 
",False,188130647,False,False,False
418,"applicat ion. EFA is a statistical method that shows the variability of observed, correlated variables in 
",False,188130647,False,False,False
419,"terms of potential groups of variables called factors. For example, it is possible that variations in four 
",False,188130647,False,False,False
420,"observed variables mainly reflect the variations in two groups of variables. EFA searches for such relate 
",False,188130647,False,False,False
421,"variations to join them as group of variables. Here EFA  is used to identify complex interrelationships 
",False,188130647,False,False,False
422,"among AI techniques  and group them  that are part of unified application s in LIS. Finaly four groups of AI 
",False,188130647,False,False,False
423,"techniques were identified as criteria for assessment in numerical taxonomy analysis method. The criteria  
",False,188130647,False,False,False
424,"(identified AI technique groups)  included: A1. Expert systems / knowledge systems  (ES), A2. Intelligent 
",False,188130647,False,False,False
425,"decision sup port systems / recommender systems  (RS) , A3. Intelligent data mining  (DM) , and A4. 
",False,188130647,False,False,False
426,"Intelligent natural language processing systems  (NLP) . Figure 3 show the Asemi's model  for library system 
",False,188130647,False,False,False
427,"development assessment in using AI.  
",False,188130647,False,False,False
428," 
",False,188130647,False,False,False
429," 
",False,188130647,False,False,False
430,"Figure 3. Asemi's Library System Development Assessment Model in using AI  
",False,188130647,False,False,False
431," 
",False,188130647,False,False,False
432,"The researchers used of nine degree scale to receive experts view.  
",False,188130647,False,False,False
433," 
",False,188130647,False,False,False
434,"During a web -survey, collected experts' viewpoints about their agreements amount with development 
",False,188130647,False,False,False
435,"of use of AI in the three areas mentioned based on four  criteria . In the following are presented the 
",False,188130647,False,False,False
436,"findings based on the taxonomy steps.  
",False,188130647,False,False,False
437," 
",False,188130647,False,False,False
438,"VII. FINDING  
",False,188130647,False,False,False
439," Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
440," 
",False,188130647,False,False,False
441," a. Data matrix  
",False,188130647,False,False,False
442,"The options matrix in rows and columns are criteria. Table 1 and 2 respectively show the type of metrics 
",False,188130647,False,False,False
443,"and data matrix.  
",False,188130647,False,False,False
444," 
",False,188130647,False,False,False
445," 
",False,188130647,False,False,False
446,"b. The standard matrix (synchronize data using formula Z - Score)  
",False,188130647,False,False,False
447,"The quantities given in the table is measured by different units. In this case, each of the above matrix 
",False,188130647,False,False,False
448,"elements change by the following equation and data matrix convert to standard matrix. Thus different 
",False,188130647,False,False,False
449,"units remove and replace with the scale unit.  Table 3 show the standard matrix.  
",False,188130647,False,False,False
450,"        i= option                    j= criterion  
",False,188130647,False,False,False
451,"= criterion  average j     =Standard Deviation j  
",False,188130647,False,False,False
452," 
",False,188130647,False,False,False
453,"Table 3 . Standard Matrix 
",False,188130647,False,False,False
454," PS TS MS 
",False,188130647,False,False,False
455,"ES -1.477 1.201 0.739 
",False,188130647,False,False,False
456,"RS 0.369 0.24 0.369 
",False,188130647,False,False,False
457,"DM 0.739 -1.201 0.369 
",False,188130647,False,False,False
458,"NLP 0.369 -0.24 -1.477 
",False,188130647,False,False,False
459," 
",False,188130647,False,False,False
460,"c. Calculate and determine the compound distances between places  
",False,188130647,False,False,False
461,"Compound distances between places within a symmetric matrix obtained by the following formula:  
",False,188130647,False,False,False
462," 
",False,188130647,False,False,False
463,"a represents first choice option b Represents the second choice option to calculate the distance. Based on 
",False,188130647,False,False,False
464,"the above formula, between the pair options should calculate combined distance. For example, we would 
",False,188130647,False,False,False
465,"calculate combined distance between option 1 and  option 2. For this purpose, the criterion  value for 
",False,188130647,False,False,False
466,"option 1 subtract from the criterion  value for option 2 then the answer to be power of 2.  For the other 
",False,188130647,False,False,False
467,"criteria  do the same thing and in the end, sum the collected values for all criteria . Next, we tak e the square 
",False,188130647,False,False,False
468,"root from the answers obtained  (Table 4) . 
",False,188130647,False,False,False
469,"Table 4. Commercial intervals  
",False,188130647,False,False,False
470," ES RS DM NLP 
",False,188130647,False,False,False
471," Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
472," 
",False,188130647,False,False,False
473," ES - 2.114 3.289 3.224 
",False,188130647,False,False,False
474,"RS 2.114 - 1.488 1.908 
",False,188130647,False,False,False
475,"DM 3.289 1.488 - 2.114 
",False,188130647,False,False,False
476,"NLP 3.224 1.908 2.114 - 
",False,188130647,False,False,False
477," 
",False,188130647,False,False,False
478,"d. The determination of shortest distances (dr) in each row of matrix symmetrical  
",False,188130647,False,False,False
479,"At this stage based on the distances matrix, in each row, obtain the shortest distances. Table 5 show dr 
",False,188130647,False,False,False
480,"values.  
",False,188130647,False,False,False
481," 
",False,188130647,False,False,False
482,"Table 5. dr values  
",False,188130647,False,False,False
483," shortest distance 
",False,188130647,False,False,False
484,"ES 2.114 
",False,188130647,False,False,False
485,"RS 1.488 
",False,188130647,False,False,False
486,"DM 1.488 
",False,188130647,False,False,False
487,"NLP 1.908 
",False,188130647,False,False,False
488," 
",False,188130647,False,False,False
489,"e. Calculate distances upper (+ O)  and lower ( -O) to clear homogeneous places  
",False,188130647,False,False,False
490,"Options that indicate within the upper limit and lower limit are called homogeneous options. All of the 
",False,188130647,False,False,False
491,"options up and down under consideration are eliminated. The interval can be obtained from the following 
",False,188130647,False,False,False
492,"equa tion: 
",False,188130647,False,False,False
493,"           = Standard Deviation  dr 
",False,188130647,False,False,False
494," = Average dr            = Standard Deviation dr  
",False,188130647,False,False,False
495,"The value of distance is:  Lower: 1.122   Upper: 2.376  
",False,188130647,False,False,False
496,"Accordingly, all options that are in this range are homogeneous and present calculations and other options 
",False,188130647,False,False,False
497,"will be removed. Homogeneous d ata Matrix is given in Table 6.  
",False,188130647,False,False,False
498," 
",False,188130647,False,False,False
499,"Table 6. Homogeneous data matrix1 
",False,188130647,False,False,False
500," PS TS MS 
",False,188130647,False,False,False
501,"ES 3 9 9 
",False,188130647,False,False,False
502,"RS 8 7 8 
",False,188130647,False,False,False
503,"DM 9 4 8 
",False,188130647,False,False,False
504,"NLP 8 6 3 
",False,188130647,False,False,False
505," 
",False,188130647,False,False,False
506,"f. Standard matrix of homogeneous options  
",False,188130647,False,False,False
507,"                                                           
",False,188130647,False,False,False
508,"1 Note: If this matrix with initial data matrix are the same, it means that all options are 
",False,188130647,False,False,False
509,"homogeneous . 
",False,188130647,False,False,False
510," Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
511," 
",False,188130647,False,False,False
512," In this stage, we standardize (normalize) data matrix (table 7).  
",False,188130647,False,False,False
513,"Table 7. The data matrix matched standard  
",False,188130647,False,False,False
514,"  PS TS MS 
",False,188130647,False,False,False
515,"ES -1.477 1.201 0.739 
",False,188130647,False,False,False
516,"RS 0.369 0.24 0.369 
",False,188130647,False,False,False
517,"DM 0.739 -1.201 0.369 
",False,188130647,False,False,False
518,"NLP 0.369 -0.24 -1.477 
",False,188130647,False,False,False
519," 
",False,188130647,False,False,False
520,"g. Determine the ideal values ( ) of the   homogeneous data standard matrix  
",False,188130647,False,False,False
521,"The ideal values are derived from homogeneous data standard matrix.  
",False,188130647,False,False,False
522,"If the criterion  is positive: the ideal value is the largest value of that criterion  in homogeneous data 
",False,188130647,False,False,False
523,"standard matrix. If the criterion  is negative: The ideal value is the smallest value of that criterion  in 
",False,188130647,False,False,False
524,"homogeneous data standard matrix. The following table (8) show the ideal values.  
",False,188130647,False,False,False
525," 
",False,188130647,False,False,False
526,"Table 8. The ideal values  
",False,188130647,False,False,False
527,"PS TS MS 
",False,188130647,False,False,False
528,"0.739 1.201 0.739 
",False,188130647,False,False,False
529," 
",False,188130647,False,False,False
530,"h. Calculation of the model of development  () 
",False,188130647,False,False,False
531,"Development model is obtained of the following equation.  Table 9 show development model in this 
",False,188130647,False,False,False
532,"research.  
",False,188130647,False,False,False
533," 
",False,188130647,False,False,False
534,"   = Homogeneous data matrix      = Ideal values  
",False,188130647,False,False,False
535," 
",False,188130647,False,False,False
536,"Table 9. Development  Model  
",False,188130647,False,False,False
537,"  
",False,188130647,False,False,False
538,"ES 2.216 
",False,188130647,False,False,False
539,"RS 1.094 
",False,188130647,False,False,False
540,"DM 2.43 
",False,188130647,False,False,False
541,"NLP 2.669 
",False,188130647,False,False,False
542," 
",False,188130647,False,False,False
543,"i. Calculate the degree of development ( ) 
",False,188130647,False,False,False
544,"At this stage of development degree of each option is calculated according to the following equation:  
",False,188130647,False,False,False
545,"             
",False,188130647,False,False,False
546," Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
547," 
",False,188130647,False,False,False
548," The following table show degree of development of each option and  ranking of them. There is an inverse 
",False,188130647,False,False,False
549,"relationship between the degree of development and status of the options. It means that if value of the 
",False,188130647,False,False,False
550,"development is less, it is a better option.  
",False,188130647,False,False,False
551," 
",False,188130647,False,False,False
552,"Table 10. Ranking  of Development  
",False,188130647,False,False,False
553,"  fi Ranking 
",False,188130647,False,False,False
554,"ES 0.634 2 
",False,188130647,False,False,False
555,"RS 0.313 1 
",False,188130647,False,False,False
556,"DM 0.695 3 
",False,188130647,False,False,False
557,"NLP 0.763 4 
",False,188130647,False,False,False
558," 
",False,188130647,False,False,False
559,"Figure 4 show the values table 10.  
",False,188130647,False,False,False
560," 
",False,188130647,False,False,False
561,"Figure 4. Degree  of Development  
",False,188130647,False,False,False
562," 
",False,188130647,False,False,False
563," 
",False,188130647,False,False,False
564,"CONCLUSION  
",False,188130647,False,False,False
565,"In this paper, by determining the degree of underdevelopment, the AI application development were 
",False,188130647,False,False,False
566,"evaluated in varying sizes. In this study, the concepts of ""development degree"" and ""underdevelopment 
",False,188130647,False,False,False
567,"rank"" of an activity reflects the amount of use AI criteria / facilities. These criteria  called criteria  of library 
",False,188130647,False,False,False
568,"systems development in use of AI. Thus, the activity (PS, TS, and MS) which had more use of the AI 
",False,188130647,False,False,False
569,"facilities (included ES, RS, DM, and NLP), in comparison than others, called ""Developed Service"". Also 
",False,188130647,False,False,False
570,"the activity that had lower among of these  criteria  and had more distance than to desirable activity, called 
",False,188130647,False,False,False
571,"""Undeveloped Service"". Based on taxonomy method, the results showed that most developed 
",False,188130647,False,False,False
572,"Recommender Systems (RM) in library systems in Iran and Natural Language Processing (NLP) facility 
",False,188130647,False,False,False
573,"is the most undeveloped criterion . 
",False,188130647,False,False,False
574," 
",False,188130647,False,False,False
575,"REFERENCES  
",False,188130647,False,False,False
576,"[1] Charles W. Bailey. Intelligent Library Systems:  Artificial Intelligence Technology and  Library 
",False,188130647,False,False,False
577,"Automation Systems ؛.Greenwich, CT: JAI Press, 1991.  
",False,188130647,False,False,False
578,"[2] M. Bavakutty, Muhammed Salih T.K, Mohamed Haneefa K. Research on library computerization . 
",False,188130647,False,False,False
579,"New Delhi : Ess Ess , 2006 . 
",False,188130647,False,False,False
580," Artificial intelligence (AI) application  in Library and Information  Science (LIS) 
",False,188130647,False,False,False
581," 
",False,188130647,False,False,False
582," [3] Lakshimikant Mishra and Vishnu Srivastva. Automation and networking of libraries:   A manual of 
",False,188130647,False,False,False
583,"library management software and Application of computer Technology in libraries . New D ehli: New 
",False,188130647,False,False,False
584,"age international,  2008.  
",False,188130647,False,False,False
585,"[4] F.W. Lancaster and Amy Warner. Intelligent Technologies in Library and Information Service 
",False,188130647,False,False,False
586,"Applications. USA: American Society for Information Science Silver Springs, MD,2001.  
",False,188130647,False,False,False
587,"[5] F. Gibb . Knowledge -based indexing in SIMPR: Integration of natural language processing and 
",False,188130647,False,False,False
588,"principles of subject analysis in an automated indexing system. Journal of Document and Text 
",False,188130647,False,False,False
589,"Management .1993.  1(2): 131 -153. 
",False,188130647,False,False,False
590,"[6] A. Morris . The application of expert systems in libraries and information  centres, De Gruyter , 1992.  
",False,188130647,False,False,False
591,"[7] R. A. Davies . A. G. Smith, et al. .Expert systems in reference work : The applications of expert 
",False,188130647,False,False,False
592,"systems in libraries & information centre: 92 -132. 1992.  
",False,188130647,False,False,False
593,"[8] G..Tseng, A. Poulter, et al.  The library and information profession al's guide to the Internet, Library 
",False,188130647,False,False,False
594,"Association Publishing , 1996.  
",False,188130647,False,False,False
595,"[9] S. S. Kote ; R. Buckley ; and D. Zhang . Additions to the operating system concepts animations library 
",False,188130647,False,False,False
596,"and artificial intelligence concepts animations library. Project (M.S., Computer Science) --California 
",False,188130647,False,False,False
597,"State University, Sacramento, 2015.  
",False,188130647,False,False,False
598,"[10]  D.G.H. Divayana; I.P.W. Ariawan; I. M. Sugiarta; and I.W. Artanayasa. Digital Library of Expert 
",False,188130647,False,False,False
599,"System Based at Indonesia Technology University. International Journal of Advanced Research in 
",False,188130647,False,False,False
600,"Artificial Intelligence, v4 n3 (20150301): 1 -8. 2015.  
",False,188130647,False,False,False
601,"[11] M. Casta ñón-Puga;  J. R. Castr o; J.M.F.lores -Parra;  C.G. Gaxiola -Pacheco; L.G. Mart ínez-Méndez; 
",False,188130647,False,False,False
602,"and L.E. Palafox -Maestre . JT2FISA Java Type -2 Fuzzy Inference Systems Class Library for Building 
",False,188130647,False,False,False
603,"Object -Oriented Intelligent Applications. Pre sented in 12th Mexican International Conference on 
",False,188130647,False,False,False
604,"Artificial Intelligence, MICAI 2013, Mexico City, Mexico, November 24 -30, 2013, Proceedings, Part 
",False,188130647,False,False,False
605,"II; 204 -215; Berlin, Heidelberg : Springer Berlin Heidelberg : Springer.  
",False,188130647,False,False,False
606,"[12] Osborne, J. W., & Costello, A . B. (2009). Best practices in exploratory factor analysis: Four 
",False,188130647,False,False,False
607,"recommendations for getting the most from your analysis. Pan -Pacific Management Review, 12(2), 
",False,188130647,False,False,False
608,"131-146. 
",False,188130647,False,False,False
609,"[13] Asemi , A, Baba , M.S., Asemi, A.  (2013). Intelligent Multi -Criteria Decision Making system for 
",False,188130647,False,False,False
610,"Supplier Evaluation . Research Notes in Information Science (RNIS) . 14, June. 
",False,188130647,False,False,False
611,"doi:10.4156/rnis.vol14.86  
",False,188130647,False,False,False
612,"  
",False,188130647,False,False,False
613, ,False,188130647,False,False,False
614, ,False,188130647,True,False,False
615, ,False,188130647,True,False,False
616, ,False,188130647,True,False,False
617,"Abstract.  Falls are some of the most common sources of injury among the el-
",True,2012_Book_AmbientIntelligence,False,False,True
618,"1 Introduction 
",True,2012_Book_AmbientIntelligence,False,False,True
619,"2 Related Work 
",True,2012_Book_AmbientIntelligence,False,False,True
620,"3 Sensor Equipment 
",True,2012_Book_AmbientIntelligence,False,False,True
621,"4 System Architecture 
",True,2012_Book_AmbientIntelligence,False,False,True
622,"5 Data Preproces s
",True,2012_Book_AmbientIntelligence,False,False,True
623,"5.1 Inertial Data 
",True,2012_Book_AmbientIntelligence,False,False,True
624,"3-axis gyroscope data. It 
",True,2012_Book_AmbientIntelligence,False,False,True
625,"5.2 Location Data 
",True,2012_Book_AmbientIntelligence,False,False,True
626,"15 cm, but in practice it m a
",True,2012_Book_AmbientIntelligence,False,False,True
627,"6 Context Compo
",True,2012_Book_AmbientIntelligence,False,False,True
628,"5 
",True,2012_Book_AmbientIntelligence,False,False,True
629,"6.1 Body Accelerations 
",True,2012_Book_AmbientIntelligence,False,False,True
630,"Lecture Notes in Computer Science 7683
",False,2012_Book_AmbientIntelligence,False,False,True
631,"Commenced Publication in 1973
",False,2012_Book_AmbientIntelligence,False,False,True
632,"Founding and Former Series Editors:
",False,2012_Book_AmbientIntelligence,False,False,True
633,"Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
",False,2012_Book_AmbientIntelligence,False,False,True
634,"Editorial Board
",False,2012_Book_AmbientIntelligence,False,False,True
635,"David Hutchison
",False,2012_Book_AmbientIntelligence,False,False,True
636,"Lancaster University, UK
",False,2012_Book_AmbientIntelligence,False,False,True
637,"Takeo Kanade
",False,2012_Book_AmbientIntelligence,False,False,True
638,"Carnegie Mellon University, Pittsburgh, PA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
639,"Josef Kittler
",False,2012_Book_AmbientIntelligence,False,False,True
640,"University of Surrey, Guildford, UK
",False,2012_Book_AmbientIntelligence,False,False,True
641,"Jon M. Kleinberg
",False,2012_Book_AmbientIntelligence,False,False,True
642,"Cornell University, Ithaca, NY, USA
",False,2012_Book_AmbientIntelligence,False,False,True
643,"Alfred Kobsa
",False,2012_Book_AmbientIntelligence,False,False,True
644,"University of California, Irvine, CA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
645,"Friedemann Mattern
",False,2012_Book_AmbientIntelligence,False,False,True
646,"ETH Zurich, Switzerland
",False,2012_Book_AmbientIntelligence,False,False,True
647,"John C. Mitchell
",False,2012_Book_AmbientIntelligence,False,False,True
648,"Stanford University, CA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
649,"Moni Naor
",False,2012_Book_AmbientIntelligence,False,False,True
650,"Weizmann Institute of Science, Rehovot, Israel
",False,2012_Book_AmbientIntelligence,False,False,True
651,"Oscar Nierstrasz
",False,2012_Book_AmbientIntelligence,False,False,True
652,"University of Bern, Switzerland
",False,2012_Book_AmbientIntelligence,False,False,True
653,"C. Pandu Rangan
",False,2012_Book_AmbientIntelligence,False,False,True
654,"Indian Institute of Technology, Madras, India
",False,2012_Book_AmbientIntelligence,False,False,True
655,"Bernhard Steffen
",False,2012_Book_AmbientIntelligence,False,False,True
656,"TU Dortmund University, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
657,"Madhu Sudan
",False,2012_Book_AmbientIntelligence,False,False,True
658,"Microsoft Research, Cambridge, MA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
659,"Demetri Terzopoulos
",False,2012_Book_AmbientIntelligence,False,False,True
660,"University of California, Los Angeles, CA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
661,"Doug Tygar
",False,2012_Book_AmbientIntelligence,False,False,True
662,"University of California, Berkeley, CA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
663,"Gerhard Weikum
",False,2012_Book_AmbientIntelligence,False,False,True
664,"Max Planck Institute for Informatics, Saarbruecken, GermanyFabio Paternò Boris de Ruyter
",False,2012_Book_AmbientIntelligence,False,False,True
665,"Panos Markopoulos Carmen SantoroEvert van Loenen Kris Luyten (Eds.)
",False,2012_Book_AmbientIntelligence,False,False,True
666,"Ambient Intelligence
",False,2012_Book_AmbientIntelligence,False,False,True
667,"Third International Joint Conference, AmI 2012
",False,2012_Book_AmbientIntelligence,False,False,True
668,"Pisa, Italy, November 13-15, 2012Proceedings
",False,2012_Book_AmbientIntelligence,False,False,True
669,"13V olume Editors
",False,2012_Book_AmbientIntelligence,False,False,True
670,"Fabio Paternò
",False,2012_Book_AmbientIntelligence,False,False,True
671,"CNR-ISTI, Pisa, ItalyE-mail: fabio.paterno@isti.cnr.it
",False,2012_Book_AmbientIntelligence,False,False,True
672,"Boris de Ruyter
",False,2012_Book_AmbientIntelligence,False,False,True
673,"Philips Research, Eindhoven, The NetherlandsE-mail: boris.de.ruyter@philips.com
",False,2012_Book_AmbientIntelligence,False,False,True
674,"Panos Markopoulos
",False,2012_Book_AmbientIntelligence,False,False,True
675,"Eindhoven University of Technology, The NetherlandsE-mail: p.markopoulos@tue.nl
",False,2012_Book_AmbientIntelligence,False,False,True
676,"Carmen Santoro
",False,2012_Book_AmbientIntelligence,False,False,True
677,"CNR-ISTI, Pisa, ItalyE-mail: carmen.santoro@isti.cnr.it
",False,2012_Book_AmbientIntelligence,False,False,True
678,"Evert van Loenen
",False,2012_Book_AmbientIntelligence,False,False,True
679,"Philips Research, Eindhoven, The NetherlandsE-mail: evert.van.loenen@philips.com
",False,2012_Book_AmbientIntelligence,False,False,True
680,"Kris Luyten
",False,2012_Book_AmbientIntelligence,False,False,True
681,"Hasselt University, Diepenbeek, BelgiumE-mail: kris.luyten@uhasselt.be
",False,2012_Book_AmbientIntelligence,False,False,True
682,"ISSN 0302-9743 e-ISSN 1611-3349
",False,2012_Book_AmbientIntelligence,False,False,True
683,"ISBN 978-3-642-34897-6 e-ISBN 978-3-642-34898-3
",False,2012_Book_AmbientIntelligence,False,False,True
684,"DOI 10.1007/978-3-642-34898-3
",False,2012_Book_AmbientIntelligence,False,False,True
685,"Springer Heidelberg Dordrecht London New York
",False,2012_Book_AmbientIntelligence,False,False,True
686,"Library of Congress Control Number: 2012951487CR Subject Classiﬁcation (1998): I.2, H.4, H.3, C.2.4, H.5, I.2.11, K.4LNCS Sublibrary: S L 3 – Information Systems and Application, incl. Internet/Web
",False,2012_Book_AmbientIntelligence,False,False,True
687,"and HCI
",False,2012_Book_AmbientIntelligence,False,False,True
688,"© Springer-Verlag Berlin Heidelberg 2012
",False,2012_Book_AmbientIntelligence,False,False,True
689,"This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
",False,2012_Book_AmbientIntelligence,False,False,True
690,"concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publicationor parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,in its current version, and permission for use must always be obtained from Springer. Violations are liableto prosecution under the German Copyright Law.
",False,2012_Book_AmbientIntelligence,False,False,True
691,"The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
",False,2012_Book_AmbientIntelligence,False,False,True
692,"even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective lawsand regulations and therefore free for general use.
",False,2012_Book_AmbientIntelligence,False,False,True
693,"Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
",False,2012_Book_AmbientIntelligence,False,False,True
694,"Printed on acid-free paperSpringer is part of Springer Science+Business Media (www.springer.com)Preface
",False,2012_Book_AmbientIntelligence,False,False,True
695,"This volume contains the papers and po sters selected for presentation at the
",False,2012_Book_AmbientIntelligence,False,False,True
696,"International Joint Conference on Am bient Intelligence (AmI 2012) held in Pisa
",False,2012_Book_AmbientIntelligence,False,False,True
697,"in November 2012.
",False,2012_Book_AmbientIntelligence,False,False,True
698,"The vision of ambient intelligence is to provide environments enhanced by
",False,2012_Book_AmbientIntelligence,False,False,True
699,"intelligent interfaces supported by computing and networking technology em-bedded in everyday objects, and which enable users to interact with their sur-
",False,2012_Book_AmbientIntelligence,False,False,True
700,"roundings in a seamless manner.
",False,2012_Book_AmbientIntelligence,False,False,True
701,"More speciﬁcally, such environments should result in systems that are aware
",False,2012_Book_AmbientIntelligence,False,False,True
702,"of the characteristics of us ers, recognize their needs, learn from their behavior,
",False,2012_Book_AmbientIntelligence,False,False,True
703,"and are able to intelligently and even proactively act in order to support hu-
",False,2012_Book_AmbientIntelligence,False,False,True
704,"mans in achieving their goals. Ambient i ntelligence should also be unobtrusive
",False,2012_Book_AmbientIntelligence,False,False,True
705,"– interaction should be natural and engaging for the users.
",False,2012_Book_AmbientIntelligence,False,False,True
706,"From a scientiﬁc point of view, ambient intelligence (AmI) comprises a multi-
",False,2012_Book_AmbientIntelligence,False,False,True
707,"disciplinary approach covering ﬁelds such as computer science, human computer
",False,2012_Book_AmbientIntelligence,False,False,True
708,"interaction, electrical engineering, industrial design, behavioral sciences, aimed
",False,2012_Book_AmbientIntelligence,False,False,True
709,"at enriching physical environments with a network of distributed devices, suchas sensors, actuators, and computational resources, in order to support users in
",False,2012_Book_AmbientIntelligence,False,False,True
710,"their everyday activities.
",False,2012_Book_AmbientIntelligence,False,False,True
711,"From a technological perspective, AmI r epresents the convergence of recent
",False,2012_Book_AmbientIntelligence,False,False,True
712,"achievements in ubiquitous and communica tion technologies, pervasive comput-
",False,2012_Book_AmbientIntelligence,False,False,True
713,"ing, intelligent user interfaces and artiﬁcial intelligence, just to name a few.
",False,2012_Book_AmbientIntelligence,False,False,True
714,"This conference started as the European Symposium on Ambient Intelligence
",False,2012_Book_AmbientIntelligence,False,False,True
715,"in 2003, and has grown to an annual international event that brings together
",False,2012_Book_AmbientIntelligence,False,False,True
716,"researchers and serves as a forum to discuss the latest trends and developmentsin this ﬁeld.
",False,2012_Book_AmbientIntelligence,False,False,True
717,"These AmI 12 proceedings include the lat est research into technologies and
",False,2012_Book_AmbientIntelligence,False,False,True
718,"applications that enable and validate the deployment of the AmI vision.
",False,2012_Book_AmbientIntelligence,False,False,True
719,"This year the program contained 18 full papers carefully chosen from a to-
",False,2012_Book_AmbientIntelligence,False,False,True
720,"tal of 47 submissions (38% acceptance rat e). There were also ﬁve short papers
",False,2012_Book_AmbientIntelligence,False,False,True
721,"accepted out of 14 (acceptance rate 36%). All papers were reviewed in a double-
",False,2012_Book_AmbientIntelligence,False,False,True
722,"blind review process. For some papers t his included a conditional acceptance
",False,2012_Book_AmbientIntelligence,False,False,True
723,"step which required further revisions ﬁnally checked by reviewers and Chairs. In
",False,2012_Book_AmbientIntelligence,False,False,True
724,"addition, the program included ﬁve landscape papers (papers that brainstorm
",False,2012_Book_AmbientIntelligence,False,False,True
725,"on the future evolution of AmI), ten posters, and two demos.
",False,2012_Book_AmbientIntelligence,False,False,True
726,"The competition for paper acceptanc e was strong and ﬁnal selection was
",False,2012_Book_AmbientIntelligence,False,False,True
727,"diﬃcult. The published material originates from 27 countries, including Africa,
",False,2012_Book_AmbientIntelligence,False,False,True
728,"Australia, North and Central America, Japan, Saudi Arabia, Singapore, and
",False,2012_Book_AmbientIntelligence,False,False,True
729,"Europe.VI Preface
",False,2012_Book_AmbientIntelligence,False,False,True
730,"Each paper had at least two independent reviews from reviewers who were
",False,2012_Book_AmbientIntelligence,False,False,True
731,"matched by expertise area to the topic of each paper. The Chairs handled bor-derline cases, and requested additional reviews when needed.
",False,2012_Book_AmbientIntelligence,False,False,True
732,"In addition to the main conference, seven workshops were held prior to the
",False,2012_Book_AmbientIntelligence,False,False,True
733,"main AmI 2012 event, and stimulated interesting discussions on speciﬁc relevanttopics.
",False,2012_Book_AmbientIntelligence,False,False,True
734,"A special thanks goes to the dedicated work of the 54 Program Committee
",False,2012_Book_AmbientIntelligence,False,False,True
735,"members involved in the review panel who came from Europe and North Amer-
",False,2012_Book_AmbientIntelligence,False,False,True
736,"ica, thus reﬂecting the international spirit of AmI participation. Their names are
",False,2012_Book_AmbientIntelligence,False,False,True
737,"listed in the conference pro ceedings and on the website.
",False,2012_Book_AmbientIntelligence,False,False,True
738,"We would also like to express our gratitude to ACM SIGCHI, Interaction-
",False,2012_Book_AmbientIntelligence,False,False,True
739,"design.org, SIGCHI Italy, IFIP WG 2.7/13.4 for their help in creating interest
",False,2012_Book_AmbientIntelligence,False,False,True
740,"in the conference.
",False,2012_Book_AmbientIntelligence,False,False,True
741,"Finally, we would like to thank the conference Organizing Committee for their
",False,2012_Book_AmbientIntelligence,False,False,True
742,"dedicated support, as well as the paper presenters and conference participants
",False,2012_Book_AmbientIntelligence,False,False,True
743,"who contributed to the vibrant discussions, presentations, and workshops heldat AmI 2012.
",False,2012_Book_AmbientIntelligence,False,False,True
744,"Fabio Patern` o
",False,2012_Book_AmbientIntelligence,False,False,True
745,"Boris de Ruyter
",False,2012_Book_AmbientIntelligence,False,False,True
746,"Panos Markopoulos
",False,2012_Book_AmbientIntelligence,False,False,True
747,"Carmen Santoro
",False,2012_Book_AmbientIntelligence,False,False,True
748,"Evert van Loenen
",False,2012_Book_AmbientIntelligence,False,False,True
749,"Kris LuytenOrganization
",False,2012_Book_AmbientIntelligence,False,False,True
750,"The 6th European Conference on Ambi ent Intelligence, AmI 2012, was held in
",False,2012_Book_AmbientIntelligence,False,False,True
751,"Pisa, Italy.
",False,2012_Book_AmbientIntelligence,False,False,True
752,"General Chairs
",False,2012_Book_AmbientIntelligence,False,False,True
753,"Fabio Patern` o CNR-ISTI, HIIS Laboratory, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
754,"Boris de Ruyter Philips Research Europe, The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
755,"Full Pap ers
",False,2012_Book_AmbientIntelligence,False,False,True
756,"Panos Markopoulos Eindhoven University of Technology,
",False,2012_Book_AmbientIntelligence,False,False,True
757,"The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
758,"Carmen Santoro CNR-ISTI, HIIS Laboratory, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
759,"Short Papers
",False,2012_Book_AmbientIntelligence,False,False,True
760,"Evert van Loenen Philips Research Europe, The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
761,"Kris Luyten Hasselt University, Expertise Centre for Digital
",False,2012_Book_AmbientIntelligence,False,False,True
762,"Media, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
763,"Landscapes
",False,2012_Book_AmbientIntelligence,False,False,True
764,"Adrian David Cheok Mixed Reality Lab, National University of
",False,2012_Book_AmbientIntelligence,False,False,True
765,"Singapore
",False,2012_Book_AmbientIntelligence,False,False,True
766,"Posters
",False,2012_Book_AmbientIntelligence,False,False,True
767,"Davy Preuveneers University of Leuven, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
768,"Davide Spano CNR-ISTI, HIIS Laboratory, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
769,"Demos
",False,2012_Book_AmbientIntelligence,False,False,True
770,"Giuseppe Ghiani CNR-ISTI, HIIS Laboratory, ItalyGiulio Mori CNR-ISTI, HIIS Laboratory, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
771,"Workshops
",False,2012_Book_AmbientIntelligence,False,False,True
772,"Gerrit Meixner DFKI, GermanyBen Schouten Eindhoven University of Technology,
",False,2012_Book_AmbientIntelligence,False,False,True
773,"The NetherlandsVIII Organization
",False,2012_Book_AmbientIntelligence,False,False,True
774,"Doctoral Consortium
",False,2012_Book_AmbientIntelligence,False,False,True
775,"Manfred Tscheligi University of Salzburg, Austria
",False,2012_Book_AmbientIntelligence,False,False,True
776,"Volker Wulf University of Siegen and Fraunhofer-FIT,
",False,2012_Book_AmbientIntelligence,False,False,True
777,"Germany
",False,2012_Book_AmbientIntelligence,False,False,True
778,"Local Organization
",False,2012_Book_AmbientIntelligence,False,False,True
779,"Giulio Galesi CNR-ISTI, HIIS Laboratory, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
780,"Program Committee
",False,2012_Book_AmbientIntelligence,False,False,True
781,"Emile Aarts Philips Research, The NetherlandsJulio Abascal University of the Basque Country, SpainVille Antila VTT, Finland
",False,2012_Book_AmbientIntelligence,False,False,True
782,"Carmelo Ardito University of Bari, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
783,"Juan Carlos Augusto University of Ulster, UKYolande Berbers University of Leuven, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
784,"Regina Bernhaupt Ruwido, Austria
",False,2012_Book_AmbientIntelligence,False,False,True
785,"Gerald Bieber Fraunhofer IGD-R, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
786,"Oliver Brdiczka Palo Alto Research Center, CA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
787,"Gaelle Calvary LIG-IIHM, University of Grenoble, FranceJavier Caminero Telef´ onica I D, Spain
",False,2012_Book_AmbientIntelligence,False,False,True
788,"Luis Carri¸ co Universidade de Lisboa, Portugal
",False,2012_Book_AmbientIntelligence,False,False,True
789,"Stefano Chessa University of Pisa, ItalyKarin Coninx University of Hasselt, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
790,"Marco de Sa Yahoo! Research, CA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
791,"Monica Divitini NTNU, Norway
",False,2012_Book_AmbientIntelligence,False,False,True
792,"Jose Carlos Dos Santos Danado CNR-ISTI, HIIS Laboratory, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
793,"Morten Fjeld Chalmers University of Technology, SwedenJacqeuline Floch SINTEF, Norway
",False,2012_Book_AmbientIntelligence,False,False,True
794,"Mathias Funk TU Eindhoven, The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
795,"Francesco Furfari CNR-ISTI, ItalyLuciano Gamberini University of Padova, Italy
",False,2012_Book_AmbientIntelligence,False,False,True
796,"Maurits Clemens Kaptein TU Eindhoven, The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
797,"Fahim Kawsar Alcatel-Lucent, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
798,"Javed Vassilis Khan Breda University, The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
799,"Gerd Kortuem The Open University, UKVassilis Kostakos University of Oulu, Finland
",False,2012_Book_AmbientIntelligence,False,False,True
800,"Kyriakos Kritikos FORTH-ICS, Greece
",False,2012_Book_AmbientIntelligence,False,False,True
801,"Ben Krose University of Amsterdam, The NetherlandsBrian Lim Carnegie Mellon University, PA, USA
",False,2012_Book_AmbientIntelligence,False,False,True
802,"Alexander Meschtscherjakov University of Salzburg, Austria
",False,2012_Book_AmbientIntelligence,False,False,True
803,"Vittorio Miori CNR-ISTI, ItalyMichael Nebeling ETH Zurich, Switzerland
",False,2012_Book_AmbientIntelligence,False,False,True
804,"Laurence Nigay LIG-IIHM, Univ ersity of Grenoble, FranceOrganization IX
",False,2012_Book_AmbientIntelligence,False,False,True
805,"Zeljko Obrenovic Eindhoven University of Technology,
",False,2012_Book_AmbientIntelligence,False,False,True
806,"The Netherlands
",False,2012_Book_AmbientIntelligence,False,False,True
807,"Philippe Palanque IRIT, France
",False,2012_Book_AmbientIntelligence,False,False,True
808,"Volkmar Pipek Universi ty of Siegen, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
809,"Davy Preuveneers University of Leuven, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
810,"Aaron Quigley The University of St. Andrews, UK
",False,2012_Book_AmbientIntelligence,False,False,True
811,"Joerg Rett SAP Research Center Darmstadt, GermanyEnrico Rukzio Ulm University, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
812,"Albert Ali Salah Bogazici University, Turkey
",False,2012_Book_AmbientIntelligence,False,False,True
813,"Thomas Schlegel TU Dresden, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
814,"Dirk Schnelle-Walka TU Darmstadt, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
815,"Johannes Sch¨ oning DFKI, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
816,"Ahmed Seﬀah UTT, France
",False,2012_Book_AmbientIntelligence,False,False,True
817,"Kostas Stathis Royal Holloway University of London, UK
",False,2012_Book_AmbientIntelligence,False,False,True
818,"Gunnar Stevens University of Siegen, GermanyManfred Tscheligi University of Salzburg, Austria
",False,2012_Book_AmbientIntelligence,False,False,True
819,"Kaisa V¨ a¨an¨anen-Vainio-Mattila Tampere University of Technology, Finland
",False,2012_Book_AmbientIntelligence,False,False,True
820,"Kristof Van Laerhoven TU Darmstadt, Germany
",False,2012_Book_AmbientIntelligence,False,False,True
821,"Jo Vermeulen University of Hasselt, Belgium
",False,2012_Book_AmbientIntelligence,False,False,True
822,"Reiner Wichert IGD Fraunhofer, GermanyMassimo Zancanaro Fondazione Bruno Kessler, ItalyTable of Contents
",False,2012_Book_AmbientIntelligence,False,False,True
823,"Long Papers
",False,2012_Book_AmbientIntelligence,False,False,True
824,"Context-Based Fall Detection Usi ng Inertial and Location Sensors ...... 1
",False,2012_Book_AmbientIntelligence,False,False,True
825,"Hristijan Gjoreski, Mitja Luˇ strek, and Matjaˇ zG a m s
",False,2012_Book_AmbientIntelligence,False,False,True
826,"Enhancing Accelerometer -Based Activity Recogn ition with Capacitive
",False,2012_Book_AmbientIntelligence,False,False,True
827,"Proximity Sensing ................................................ 17
",False,2012_Book_AmbientIntelligence,False,False,True
828,"Tobias Grosse-Puppendahl, Eugen Berlin, and Marko Borazio
",False,2012_Book_AmbientIntelligence,False,False,True
829,"Adaptive User Interfaces for Smart Environments with the Support
",False,2012_Book_AmbientIntelligence,False,False,True
830,"of Model-Based Languages ........................................ 33
",False,2012_Book_AmbientIntelligence,False,False,True
831,"Sara Bongartz, Yucheng Jin, Fabio Patern` o, Joerg Rett,
",False,2012_Book_AmbientIntelligence,False,False,True
832,"Carmen Santoro, and Lucio Davide Spano
",False,2012_Book_AmbientIntelligence,False,False,True
833,"Back of the Steering Wheel Interaction: The Car Braille Keyer ......... 49
",False,2012_Book_AmbientIntelligence,False,False,True
834,"Sebastian Osswald, Alexander Meschtscherjakov, Nicole Mirnig,
",False,2012_Book_AmbientIntelligence,False,False,True
835,"Karl-Armin Kraessig, David Wilﬁnger, Martin Murer, and
",False,2012_Book_AmbientIntelligence,False,False,True
836,"Manfred Tscheligi
",False,2012_Book_AmbientIntelligence,False,False,True
837,"PermissionWatcher: Creating User Awareness of Application
",False,2012_Book_AmbientIntelligence,False,False,True
838,"Permissions in Mobile Systems ..................................... 65
",False,2012_Book_AmbientIntelligence,False,False,True
839,"Eric Struse, Julian Seifert, Sebastian ¨Ullenbeck, Enrico Rukzio, and
",False,2012_Book_AmbientIntelligence,False,False,True
840,"Christopher Wolf
",False,2012_Book_AmbientIntelligence,False,False,True
841,"Exploring Non-verbal Communi cation of Presence between Young
",False,2012_Book_AmbientIntelligence,False,False,True
842,"Children and Their Parents through the Embodied Teddy Bear ........ 81
",False,2012_Book_AmbientIntelligence,False,False,True
843,"Kaisa V¨ a¨an¨anen-Vainio-Mattila, Tomi Haustola, Jonna H¨ akkil¨a,
",False,2012_Book_AmbientIntelligence,False,False,True
844,"Minna Karukka, and Katja Kyt¨ okorpi
",False,2012_Book_AmbientIntelligence,False,False,True
845,"Automatic Behavior Understanding in Crisis Response Control
",False,2012_Book_AmbientIntelligence,False,False,True
846,"Rooms .......................................................... 97
",False,2012_Book_AmbientIntelligence,False,False,True
847,"Joris Ijsselmuiden, Ann-Kristin Grosselﬁnger, David M¨ unch,
",False,2012_Book_AmbientIntelligence,False,False,True
848,"Michael Arens, and Rainer Stiefelhagen
",False,2012_Book_AmbientIntelligence,False,False,True
849,"Combining Implicit and Explicit Methods for the Evaluation
",False,2012_Book_AmbientIntelligence,False,False,True
850,"of an Ambient Persuasive Factory Display ........................... 113
",False,2012_Book_AmbientIntelligence,False,False,True
851,"Ewald Strasser, Astrid Weiss, Thomas Grill, Sebastian Osswald, andManfred Tscheligi
",False,2012_Book_AmbientIntelligence,False,False,True
852,"Context Awareness in Ambient Systems by an Adaptive Multi-Agent
",False,2012_Book_AmbientIntelligence,False,False,True
853,"Approach ....................................................... 129
",False,2012_Book_AmbientIntelligence,False,False,True
854,"Val´erian Guivarch, Val´ erie Camps, and Andr´ eP ´eninouXII Table of Contents
",False,2012_Book_AmbientIntelligence,False,False,True
855,"Towards Fuzzy Transfer Learning for Intelligent Environments ......... 145
",False,2012_Book_AmbientIntelligence,False,False,True
856,"Jethro Shell and Simon Coupland
",False,2012_Book_AmbientIntelligence,False,False,True
857,"Gesture Proﬁle for Web Services: A n Event-Driven Architecture
",False,2012_Book_AmbientIntelligence,False,False,True
858,"to Support Gestural Interfaces for Smart Environments ............... 161
",False,2012_Book_AmbientIntelligence,False,False,True
859,"Radu-Daniel Vatavu, C˘ at˘alin-Marian Chera, and Wei-Tek Tsai
",False,2012_Book_AmbientIntelligence,False,False,True
860,"Using Markov Logic Network for On-Line Activity Recognition from
",False,2012_Book_AmbientIntelligence,False,False,True
861,"Non-visual Home Automation Sensors .............................. 177
",False,2012_Book_AmbientIntelligence,False,False,True
862,"Pedro Chahuara, Anthony Fleury, Fran¸ cois Portet, and
",False,2012_Book_AmbientIntelligence,False,False,True
863,"Michel Vacher
",False,2012_Book_AmbientIntelligence,False,False,True
864,"Multi-Classiﬁer Adaptive Training: Specialising an Activity Recognition
",False,2012_Book_AmbientIntelligence,False,False,True
865,"Classiﬁer Using Semi-supervised Learning ........................... 193
",False,2012_Book_AmbientIntelligence,False,False,True
866,"Boˇzidara Cvetkovi´ c, Boˇstjan Kaluˇ za, Mitja Luˇ strek, and
",False,2012_Book_AmbientIntelligence,False,False,True
867,"MatjaˇzG a m s
",False,2012_Book_AmbientIntelligence,False,False,True
868,"Sound Environment Analysis in Smart Home ........................ 208
",False,2012_Book_AmbientIntelligence,False,False,True
869,"Mohamed A. Sehili, Benjamin Lecouteux, Michel Vacher,
",False,2012_Book_AmbientIntelligence,False,False,True
870,"Fran¸cois Portet, Dan Istrate, Bernadette Dorizzi, and J´ erˆome Boudy
",False,2012_Book_AmbientIntelligence,False,False,True
871,"Contextual Wizard of Oz: A Framework Combining Contextual Rapid
",False,2012_Book_AmbientIntelligence,False,False,True
872,"Prototyping and the Wizard of Oz Method .......................... 224
",False,2012_Book_AmbientIntelligence,False,False,True
873,"Doris Zachhuber, Thomas Grill, Ondrej Polacek, andManfred Tscheligi
",False,2012_Book_AmbientIntelligence,False,False,True
874,"Recognizing the User Social Attitude in Multimodal Interaction
",False,2012_Book_AmbientIntelligence,False,False,True
875,"in Smart Environments ........................................... 240
",False,2012_Book_AmbientIntelligence,False,False,True
876,"Berardina De Carolis, Stefano Ferilli, and Nicole Novielli
",False,2012_Book_AmbientIntelligence,False,False,True
877,"Evolutionary Feature Extraction to Infer Behavioral Patterns
",False,2012_Book_AmbientIntelligence,False,False,True
878,"in Ambient Intelligence ........................................... 256
",False,2012_Book_AmbientIntelligence,False,False,True
879,"Leila S. Shafti, Pablo A. Haya, Manuel Garc´ ıa-Herranz, and
",False,2012_Book_AmbientIntelligence,False,False,True
880,"Eduardo P´ erez
",False,2012_Book_AmbientIntelligence,False,False,True
881,"Personalization of Content on Public Displays Driven by the
",False,2012_Book_AmbientIntelligence,False,False,True
882,"Recognition of Group Context ..................................... 272
",False,2012_Book_AmbientIntelligence,False,False,True
883,"Ekaterina Kurdyukova, Stephan Hammer, and Elisabeth Andr´ e
",False,2012_Book_AmbientIntelligence,False,False,True
884,"Short Papers
",False,2012_Book_AmbientIntelligence,False,False,True
885,"Towards the Generation of Assistiv e User Interfaces for Smart Meeting
",False,2012_Book_AmbientIntelligence,False,False,True
886,"Rooms Based on Activity Patterns ................................. 288
",False,2012_Book_AmbientIntelligence,False,False,True
887,"Michael Zaki and Peter Forbrig
",False,2012_Book_AmbientIntelligence,False,False,True
888,"Reducing Dementia Related Wandering Behaviour with an Interactive
",False,2012_Book_AmbientIntelligence,False,False,True
889,"Wall ........................................................... 296
",False,2012_Book_AmbientIntelligence,False,False,True
890,"Saskia Robben, Kyra Bergman, Sven Haitjema,Yannick de Lange, and Ben Kr¨ oseT able of Contents XIII
",False,2012_Book_AmbientIntelligence,False,False,True
891,"Gesture Based Semantic Service Invocation for Human Environment
",False,2012_Book_AmbientIntelligence,False,False,True
892,"Interaction ...................................................... 304
",False,2012_Book_AmbientIntelligence,False,False,True
893,"Carsten Stockl¨ ow and Reiner Wichert
",False,2012_Book_AmbientIntelligence,False,False,True
894,"Understanding Complex Environments with the Feedforward Torch .... 312
",False,2012_Book_AmbientIntelligence,False,False,True
895,"Jo Vermeulen, Kris Luyten, and Karin Coninx
",False,2012_Book_AmbientIntelligence,False,False,True
896,"Open Objects for Ambient Intelligence .............................. 320
",False,2012_Book_AmbientIntelligence,False,False,True
897,"Paulo Ricca and Kostas Stathis
",False,2012_Book_AmbientIntelligence,False,False,True
898,"Landscape Papers
",False,2012_Book_AmbientIntelligence,False,False,True
899,"Towards Accessibility in Ambient Intelligence Environments ........... 328
",False,2012_Book_AmbientIntelligence,False,False,True
900,"George Margetis, Margherita Antona, Stavroula Ntoa, and
",False,2012_Book_AmbientIntelligence,False,False,True
901,"Constantine Stephanidis
",False,2012_Book_AmbientIntelligence,False,False,True
902,"INCOME – Multi-scale Context Management for the Internet
",False,2012_Book_AmbientIntelligence,False,False,True
903,"of Things ....................................................... 338
",False,2012_Book_AmbientIntelligence,False,False,True
904,"Jean-Paul Arcangeli, Amel Bouzeghoub, Val´ erie Camps,
",False,2012_Book_AmbientIntelligence,False,False,True
905,"Marie-Fran¸ coise Canut, Sophie Chabridon, Denis Conan,
",False,2012_Book_AmbientIntelligence,False,False,True
906,"Thierry Desprats, Romain Laborde, Emmanuel Lavinal,S´ebastien Leriche, Herv´ e Maurel, Andr´ eP ´eninou,
",False,2012_Book_AmbientIntelligence,False,False,True
907,"Chantal Taconet, and Pascale Zarat´ e
",False,2012_Book_AmbientIntelligence,False,False,True
908,"New Forms of Work Assistance by Ambient Intelligence: Overview
",False,2012_Book_AmbientIntelligence,False,False,True
909,"of the Focal Research Topic of BAuA ............................... 348
",False,2012_Book_AmbientIntelligence,False,False,True
910,"Armin Windel and Matthias Hartwig
",False,2012_Book_AmbientIntelligence,False,False,True
911,"Living Labs as Educational Tool for Ambient Intelligence ............. 356
",False,2012_Book_AmbientIntelligence,False,False,True
912,"Ben Kr¨ ose, Mettina Veenstra, Saskia Robben, and Marije Kanis
",False,2012_Book_AmbientIntelligence,False,False,True
913,"Intel Collaborative Research Institute - Sustainable Connected Cities ... 364
",False,2012_Book_AmbientIntelligence,False,False,True
914,"Johannes Sch¨ oning, Yvonne Rogers, Jon Bird, Licia Capra,
",False,2012_Book_AmbientIntelligence,False,False,True
915,"Julie A. McCann, David Prendergast, and Charles Sheridan
",False,2012_Book_AmbientIntelligence,False,False,True
916,"Poster Papers
",False,2012_Book_AmbientIntelligence,False,False,True
917,"IE Sim – A Flexible Tool for the Simulation of Data Generated within
",False,2012_Book_AmbientIntelligence,False,False,True
918,"Intelligent Environments .......................................... 373
",False,2012_Book_AmbientIntelligence,False,False,True
919,"Jonathan Synnott, Liming Chen, Chris Nugent, and George Moore
",False,2012_Book_AmbientIntelligence,False,False,True
920,"Intention Recognition with Clustering .............................. 379
",False,2012_Book_AmbientIntelligence,False,False,True
921,"Fariba Sadri, Weikun Wang, and Afroditi Xaﬁ
",False,2012_Book_AmbientIntelligence,False,False,True
922,"Behavior Modeling and Recognition Methods to Facilitate Transitions
",False,2012_Book_AmbientIntelligence,False,False,True
923,"between Application-Speciﬁc Pers onalized Assistance Systems ......... 385
",False,2012_Book_AmbientIntelligence,False,False,True
924,"Arun Ramakrishnan, Zubair Bhatti, Davy Preuveneers,
",False,2012_Book_AmbientIntelligence,False,False,True
925,"Yolande Berbers, Aliaksei Andrushevich, Rolf Kistler, andAlexander KlapprothXIV Table of Contents
",False,2012_Book_AmbientIntelligence,False,False,True
926,"LumaFluid : A Responsive Environment to Stimulate Social Interaction
",False,2012_Book_AmbientIntelligence,False,False,True
927,"in Public Spaces ................................................. 391
",False,2012_Book_AmbientIntelligence,False,False,True
928,"Gianluca Monaci, Tommaso Gritti, Martine van Beers,
",False,2012_Book_AmbientIntelligence,False,False,True
929,"Ad Vermeulen, Bram Nab, Inge Thomassen, Marigo Heijboer,
",False,2012_Book_AmbientIntelligence,False,False,True
930,"Sandra Suijkerbuijk, Wouter Walmink, and Maarten Hendriks
",False,2012_Book_AmbientIntelligence,False,False,True
931,"A Cost-Based Model for Service Di scovery in Smart Environments ..... 397
",False,2012_Book_AmbientIntelligence,False,False,True
932,"Michele Girolami, Francesco Furfari, and Stefano Chessa
",False,2012_Book_AmbientIntelligence,False,False,True
933,"On the Use of Video Prototyping in Designing Ambient User
",False,2012_Book_AmbientIntelligence,False,False,True
934,"Experiences ..................................................... 403
",False,2012_Book_AmbientIntelligence,False,False,True
935,"Nikolaos Batalas, Hester Bruikman, Annemiek Van Drunen,
",False,2012_Book_AmbientIntelligence,False,False,True
936,"He Huang, Dominika Turzynska, Vanessa Vakili,
",False,2012_Book_AmbientIntelligence,False,False,True
937,"Natalia Voynarovskaya, and Panos Markopoulos
",False,2012_Book_AmbientIntelligence,False,False,True
938,"Automatic Power-Oﬀ for Binaural Hearing Instruments ............... 409
",False,2012_Book_AmbientIntelligence,False,False,True
939,"Bernd Tessendorf, Peter Derleth, Manuela Feilner, Daniel Roggen,
",False,2012_Book_AmbientIntelligence,False,False,True
940,"Thomas Stiefmeier, and Gerhard Tr¨ oster
",False,2012_Book_AmbientIntelligence,False,False,True
941,"Proposal and Demonstration of Equipment Operated by Blinking ...... 415
",False,2012_Book_AmbientIntelligence,False,False,True
942,"Masaki Kato, Tatsuya Kobori, Takayuki Suzuki, Shigenori Ioroi, andHiroshi Tanaka
",False,2012_Book_AmbientIntelligence,False,False,True
943,"CASi – A Generic Context Awareness Simulator for Ambient
",False,2012_Book_AmbientIntelligence,False,False,True
944,"Systems ........................................................ 421
",False,2012_Book_AmbientIntelligence,False,False,True
945,"J¨org Cassens, Felix Schmitt, Tobias Mende, and Michael Herczeg
",False,2012_Book_AmbientIntelligence,False,False,True
946,"A Conceptual Framework for Supporting Adaptive Personalized
",False,2012_Book_AmbientIntelligence,False,False,True
947,"Help-on-Demand Services ......................................... 427
",False,2012_Book_AmbientIntelligence,False,False,True
948,"William Burns, Liming Chen, Chris Nugent, Mark Donnelly,
",False,2012_Book_AmbientIntelligence,False,False,True
949,"Kerry-Louise Skillen, and Ivar Solheim
",False,2012_Book_AmbientIntelligence,False,False,True
950,"Demo Papers
",False,2012_Book_AmbientIntelligence,False,False,True
951,"Developing Touchless Interfaces with GestIT ........................ 433
",False,2012_Book_AmbientIntelligence,False,False,True
952,"Lucio Davide Spano
",False,2012_Book_AmbientIntelligence,False,False,True
953,"Tool Support for Probabilistic Intention Recognition Using Plan
",False,2012_Book_AmbientIntelligence,False,False,True
954,"Synthesis ....................................................... 439
",False,2012_Book_AmbientIntelligence,False,False,True
955,"Frank Kr¨ uger, Kristina Yordanova, and Thomas Kirste
",False,2012_Book_AmbientIntelligence,False,False,True
956,"Workshops
",False,2012_Book_AmbientIntelligence,False,False,True
957,"Aesthetic Intelligence: The Role of Design in Ambient Intelligence ...... 445
",False,2012_Book_AmbientIntelligence,False,False,True
958,"Carsten R¨ ocker, Kai Kasugai, Daniela Plewe, Takashi Kiriyama, and
",False,2012_Book_AmbientIntelligence,False,False,True
959,"Artur LugmayrTable of Contents X V
",False,2012_Book_AmbientIntelligence,False,False,True
960,"Workshop on Ambient Intelligence Infrastructures (WAmIi) ........... 447
",False,2012_Book_AmbientIntelligence,False,False,True
961,"Alina Weﬀers, Johan Lukkien, and Tanir Ozcelebi
",False,2012_Book_AmbientIntelligence,False,False,True
962,"Sixth International Workshop on Human Aspects in Ambient
",False,2012_Book_AmbientIntelligence,False,False,True
963,"Intelligence (HAI 2012) ........................................... 449
",False,2012_Book_AmbientIntelligence,False,False,True
964,"Juan Carlos Augusto, Tibor Bosse, Cristiano Castelfranchi,
",False,2012_Book_AmbientIntelligence,False,False,True
965,"Diane Cook, Mark Neerincx, and Fariba Sadri
",False,2012_Book_AmbientIntelligence,False,False,True
966,"Context-Aware Adaptation of Service Front-Ends .................... 451
",False,2012_Book_AmbientIntelligence,False,False,True
967,"Francisco Javier Caminero Gil, Fabio Patern` o, and
",False,2012_Book_AmbientIntelligence,False,False,True
968,"Jean Vanderdonckt
",False,2012_Book_AmbientIntelligence,False,False,True
969,"2nd International Workshop on Ambient Gaming .................... 453
",False,2012_Book_AmbientIntelligence,False,False,True
970,"Janienke Sturm, Pepijn Rijnbout, and Ben Schouten
",False,2012_Book_AmbientIntelligence,False,False,True
971,"Designing Persuasive Interactive Environments ...................... 455
",False,2012_Book_AmbientIntelligence,False,False,True
972,"Marco Rozendaal, Aadjan van der Helm, Walter Aprile,Arnold Vermeeren, Tilde Bekker, Marije Kanis, and
",False,2012_Book_AmbientIntelligence,False,False,True
973,"Wouter Middendorf
",False,2012_Book_AmbientIntelligence,False,False,True
974,"Applying AmI Technologies to Crisis Management ................... 457
",False,2012_Book_AmbientIntelligence,False,False,True
975,"Monica Divitini, Babak Farshchian, Jacqueline Floch,Ragnhild Halvorsrud, Simone Mora, and Michael Stiso
",False,2012_Book_AmbientIntelligence,False,False,True
976,"Author Index .................................................. 459F. Paternò et al. (Eds.): AmI 2012, LNCS 7683, pp. 1–16, 2012. 
",False,2012_Book_AmbientIntelligence,False,False,True
977,"© Springer-Verlag Berlin Heidelberg 2012 Context-Based Fall Detection Using  
",False,2012_Book_AmbientIntelligence,False,False,True
978,"Inertial and Location Sensors 
",False,2012_Book_AmbientIntelligence,False,False,True
979,"Hristijan Gjoreski, Mitja Luštrek, and Matjaž Gams 
",False,2012_Book_AmbientIntelligence,False,False,True
980,"Department of Intelligent Systems, Jožef Stefan Institute 
",False,2012_Book_AmbientIntelligence,False,False,True
981,"Jamova cesta 39, 1000 Ljubljana, Slovenia 
",False,2012_Book_AmbientIntelligence,False,False,True
982,"{hristijan.gjoreski,mitja.lustrek,matjaz.gams}@ijs.si 
",False,2012_Book_AmbientIntelligence,False,False,True
983,"derly. A fall is particularly critical when  the elderly person is injured and cannot 
",False,2012_Book_AmbientIntelligence,False,False,True
984,"call for help. This problem is addressed by many fall-detection systems, but they often focus on isolated falls under restricted conditions, neglecting com-
",False,2012_Book_AmbientIntelligence,False,False,True
985,"plex, real-life situations. In this paper a combination of body-worn inertial and 
",False,2012_Book_AmbientIntelligence,False,False,True
986,"location sensors for fall detection is stud ied. A novel context-based method that 
",False,2012_Book_AmbientIntelligence,False,False,True
987,"exploits the information from both types of sensors is designed. The evaluation 
",False,2012_Book_AmbientIntelligence,False,False,True
988,"is performed on a real-life scenario, incl uding fast falls, slow falls and fall-like 
",False,2012_Book_AmbientIntelligence,False,False,True
989,"situations that are difficult to distingu ish from falls. All the possible combina-
",False,2012_Book_AmbientIntelligence,False,False,True
990,"tions of six inertial and four location sensors are tested. The results show that: 
",False,2012_Book_AmbientIntelligence,False,False,True
991,"(i) context-based reasoning significantly improves the performance; (ii) a com-
",False,2012_Book_AmbientIntelligence,False,False,True
992,"bination of two types of sensors in a single physical sensor enclosure seems to be the best practical solution. 
",False,2012_Book_AmbientIntelligence,False,False,True
993,"Keywords:  Context-based reasoning, Fall detection, Inertial sensors, Location 
",False,2012_Book_AmbientIntelligence,False,False,True
994,"sensors, Activity recognition. 
",False,2012_Book_AmbientIntelligence,False,False,True
995,"1 Introduction 
",False,2012_Book_AmbientIntelligence,False,False,True
996,"Falls are some of the most critical health-r elated problems for the elderly [3]. Approx-
",False,2012_Book_AmbientIntelligence,False,False,True
997,"imately 28–35% of people over the age of 65 fall each year, and this proportion in-creases to 32–42% in those aged more than 70 years [20]. About 20% of all the fall accidents that involve an elderly person re quire medical attention [6]. Furthermore, 
",False,2012_Book_AmbientIntelligence,False,False,True
998,"falls and the fear of falling are important reasons for nursing-home admission [18]. Falls are particularly critical when the elderly person is injured and cannot call for help. These reasons, combined with the increasing accessibility and miniaturization of sensors and microprocessors, is driving the development of fall-detection systems.  
",False,2012_Book_AmbientIntelligence,False,False,True
999,"Even though fall detection (FD) has received significant attention in recent years, it 
",False,2012_Book_AmbientIntelligence,False,False,True
1000,"still represents a challenging task for two reasons. First, there are several everyday fall-like activities that are hard to distinguish from fast falls. Most of the current ap-proaches define a fall as having greater accelerations than normal daily activities. However, focusing only on a fast acceleration can result in many false alarms during fall-like activities with fast acceleration, such as sitting down quickly or lying down on a bed quickly. The second reason why FD is challenging is that not all falls are 2 H. Gjoreski, M. Luštrek, and M. Gams 
",False,2012_Book_AmbientIntelligence,False,False,True
1001,"characterized by a fast acceleration. Rubenstein et al. [16] showed that 22% of the 
",False,2012_Book_AmbientIntelligence,False,False,True
1002,"falls experienced by the elderly are slow and are caused by dizziness and vertigo (13%), and drop attacks (9%). Therefore, the detection of slow falls should be an intrinsic part when creating a successful fall-detection system. 
",False,2012_Book_AmbientIntelligence,False,False,True
1003,"To overcome the problems of the existing fall-detection methods discussed above, 
",False,2012_Book_AmbientIntelligence,False,False,True
1004,"we propose a new approach to FD by combining body-worn inertial and location sen-sors, named CoFDILS (Context-based Fall Detection using Inertial and Location Sen-sors). Our approach uses the context information from the both types of sensors to determine whether a fall has occurred. It  exploits body accelerations, location and 
",False,2012_Book_AmbientIntelligence,False,False,True
1005,"atomic activities to detect a fall. The evaluation was performed on a special real-life scenario that includes fast falls, slow fall s and non-fall situations that are difficult to 
",False,2012_Book_AmbientIntelligence,False,False,True
1006,"distinguish from falls. In addition, we tested 1023 possible body-placement combina-tions of six inertial and four location sensors in order to find the best-performing sen-sor placements for FD and therefore to achieve the lowest sensor burden on the user. The results showed that by combining the two types of sensors it is possible to detect complex fall situations and that the context-based reasoning significantly improves the performance.  
",False,2012_Book_AmbientIntelligence,False,False,True
1007,"The paper is organized as follows. Firstly, an overview of the related studies for 
",False,2012_Book_AmbientIntelligence,False,False,True
1008,"FD is presented in Section 2. In the next two sections, the sensor equipment (Section 3) and the architecture of our system (Sec tion 4) are described. Next, the preprocess-
",False,2012_Book_AmbientIntelligence,False,False,True
1009,"ing of the raw data is presented in Section 5. In the next two sections we describe the context components (Section 6) and the methodology (Section 7). After that, the ex-perimental setup, the results and discussions are presented in Section 8 and 9. Finally, we conclude this work and give directions for future work in Section 10. 
",False,2012_Book_AmbientIntelligence,False,False,True
1010,"2 Related Work 
",False,2012_Book_AmbientIntelligence,False,False,True
1011,"FD approaches can be divided into those using non-wearable and wearable (i.e., body-
",False,2012_Book_AmbientIntelligence,False,False,True
1012,"worn) sensors. The most common non-wearable approach is camera-based [10, 15]. Although this approach is physically less intrusive to the user compared to the body-worn sensors, it suffers from issues such as low image resolution, target occlusion and time-consuming processing. However, often the biggest issue is user privacy: the user has to accept the fact that a camera will record him/her.  
",False,2012_Book_AmbientIntelligence,False,False,True
1013,"Most of the studies for FD are based just on inertial sensors. Usually, they are fo-
",False,2012_Book_AmbientIntelligence,False,False,True
1014,"cused only on fast falls [13, 23], which are not difficult to detect using the accelera-tion signal. The non-fall events used to test for false positives are usually normal, everyday activities [8, 15], not events chosen specifically because they are easily mistaken for falls. In contrast, we used complex falls and safe events that appear like falls. An example where FD was evaluated on events difficult to recognize as falls or non-falls is the work by Li et al. [11]. By applying thresholds to two inertial sensors, 
",False,2012_Book_AmbientIntelligence,False,False,True
1015,"they detected a fall with an accuracy of 90.1%. The recall value of their method on a fall event ending with sitting is 50% and for a non-fall event, quickly lying on a bed, is 40%. By combining one inertial and one location sensor, we were able to achieve 99% and 100%, on similar events, respectively.  Context-Based Fall Detection Using Inertial and Location Sensors 3 
",False,2012_Book_AmbientIntelligence,False,False,True
1016,"A combination of inertial and location sensors was described in Zinnen et al. [23]. 
",False,2012_Book_AmbientIntelligence,False,False,True
1017,"However, their goal was activity recognition for car-quality control and they did not deal with FD. Their approach was based on high-level primitives that were derived from a reconstructed human-body model by using inertial sensor data. The location data was mainly used to estimate the person's location near the car. 
",False,2012_Book_AmbientIntelligence,False,False,True
1018,"We are not aware of any prior publication that studies a combination of body-worn 
",False,2012_Book_AmbientIntelligence,False,False,True
1019,"inertial and location sensors for FD, except ours [14]. There, we focused on location-based FD, and we considered only a single accelerometer to detect the impact of the 
",False,2012_Book_AmbientIntelligence,False,False,True
1020,"fall and the orientation of the user. The main advantages of the study presented here compared to our previous work are: (i) a machine-learning model that recognizes the 
",False,2012_Book_AmbientIntelligence,False,False,True
1021,"activity of the user; (ii) a thorough analysis of the system’s complexity and invasive-ness to the user by analyzing the performance of all the possible body-placement combinations of 10 sensors; and (iii) an explicit presentation of the context-based reasoning algorithm, the core of our system. 
",False,2012_Book_AmbientIntelligence,False,False,True
1022,"A context-based approach to FD is presented in the study by Li et al. [12]. Howev-
",False,2012_Book_AmbientIntelligence,False,False,True
1023,"er, they used a different fall-detection method and different types of sensors to extract the context information, compared to our approach. In particular, they used 5 body-worn accelerometers and 2 environmental sensors that monitor the vibration of the furniture. They combined the user's posture information, extracted from the accelero-meters, and the context information, extracted from the environmental sensors, in order to detect the fall situations.  Although they also analyzed slow falls and fall-like 
",False,2012_Book_AmbientIntelligence,False,False,True
1024,"situations, their evaluation was performed on only 3 test subjects; while we tested our method on 11 subjects. The advantage of our location system, compared to the envi-ronmental sensors, is that it provides richer information about the user's situation, e.g., the user's location, the sensor’s height, et c. The environmental sensors used in their 
",False,2012_Book_AmbientIntelligence,False,False,True
1025,"research can only inform about the presence/absence of the user at a specific location where the sensor is installed. We tested all the combinations of 10 sensors and found a satisfactory performance with single sensor enclosure, while they analyzed only the fixed 5 accelerometer placements on the body. 
",False,2012_Book_AmbientIntelligence,False,False,True
1026,"To summarize, the improvements of our FD approach upon most related work are 
",False,2012_Book_AmbientIntelligence,False,False,True
1027,"the following:  
",False,2012_Book_AmbientIntelligence,False,False,True
1028,"• combining two types of body-worn sensors: inertial and location, 
",False,2012_Book_AmbientIntelligence,False,False,True
1029,"• context reasoning about the user's situation, 
",False,2012_Book_AmbientIntelligence,False,False,True
1030,"• analysis of the FD performance for all combinations of 10 sensor body placements, 
",False,2012_Book_AmbientIntelligence,False,False,True
1031,"• machine learning activity-recognition model as a part of the FD, 
",False,2012_Book_AmbientIntelligence,False,False,True
1032,"• evaluation on a complex test scenario, including events such as slow falls and fall-
",False,2012_Book_AmbientIntelligence,False,False,True
1033,"like events that may be difficult to distinguish from falls. 
",False,2012_Book_AmbientIntelligence,False,False,True
1034,"3 Sensor Equipment 
",False,2012_Book_AmbientIntelligence,False,False,True
1035,"The CoFDILS sensor equipment consists of inertial and location sensors (Fig. 1). The 
",False,2012_Book_AmbientIntelligence,False,False,True
1036,"two types of sensors were chosen because inertial sensors are relatively cheap and portable, and the location sensors provide rich information about the user, without significantly compromising the user's privacy (like with the cameras). 4 H. Gjoreski, M. Luštrek, and M. Gams 
",False,2012_Book_AmbientIntelligence,False,False,True
1037," 
",False,2012_Book_AmbientIntelligence,False,False,True
1038,"Fig. 1.  Sensor equipment. The empty yellow circles represent the inertial sensors and the filled 
",False,2012_Book_AmbientIntelligence,False,False,True
1039,"white circles represent the location tags. 
",False,2012_Book_AmbientIntelligence,False,False,True
1040,"Six inertial sensors were placed on the chest, waist, left thigh, right thigh, left an-
",False,2012_Book_AmbientIntelligence,False,False,True
1041,"kle, and right ankle (non-filled circles in Fig. 1). Since only activities that are asso-ciated with the user's legs and torso were studied, the arm- and wrist-sensor place-ments were not considered. The inertial sensor equipment consisted of body-worn Xsens-MTx sensors [22], but the methods developed for this research are general and can be applied to any type of inertial sensor.  
",False,2012_Book_AmbientIntelligence,False,False,True
1042,"Four location tags were placed on the chest, waist, left ankle and right ankle (filled 
",False,2012_Book_AmbientIntelligence,False,False,True
1043,"circles in Fig. 1). They emit UWB radio signals, which are detected by sensors fixed in the corners of a room. The tags are detected by the location system and their coor-dinates are computed. The location system used in CoFDILS is Ubisense [19]; it is a real-time location system used to track subjects indoors.  Note that for simplicity the 
",False,2012_Book_AmbientIntelligence,False,False,True
1044,"term sensor is also used for the body-worn location tag. 
",False,2012_Book_AmbientIntelligence,False,False,True
1045,"The data-sampling frequency of the sensors was set to 10 Hz because of Ubisense's 
",False,2012_Book_AmbientIntelligence,False,False,True
1046,"hardware limitations. Although the inertial sensors do not have the same limitation, the data is sampled at the same frequency to simplify the synchronization.  
",False,2012_Book_AmbientIntelligence,False,False,True
1047,"4 System Architecture 
",False,2012_Book_AmbientIntelligence,False,False,True
1048,"The architecture of CoFDILS is shown in Fig. 2. First, the data from both types of 
",False,2012_Book_AmbientIntelligence,False,False,True
1049,"sensors is stored and preprocessed. Next, the flow of data splits into two. On the top, firstly, a feature extraction is performed and the constructed feature vector is fed to the activity-recognition (AR) classification model, which recognizes the activity of the user. At the bottom, context-based reasoning about the user's situation is per-formed. The context reasoning analyzes the activity of the user and additional context information from the preprocessed data. The motivation is that the context informa-tion depends on the type of sensors. Inertial sensors provide body-movement informa-tion and the detection of a rapid-acceleration fall pattern, i.e., a threshold-based approach (TBA). Location sensors provide th e location of the user in the room. The 
",False,2012_Book_AmbientIntelligence,False,False,True
1050,"system evaluates the information from various sources in light of its contexts and concludes whether a fall alarm should be issued. Each module in Fig. 2 is presented in more detail in the sections that follow. 
",False,2012_Book_AmbientIntelligence,False,False,True
1051," Context- B
",False,2012_Book_AmbientIntelligence,False,False,True
1052,"Fig. 2.  CoFDI L
",False,2012_Book_AmbientIntelligence,False,False,True
1053,"5 Data Preproces s
",False,2012_Book_AmbientIntelligence,False,False,True
1054,"5.1 Inertial Data 
",False,2012_Book_AmbientIntelligence,False,False,True
1055,"An inertial sensor provides 
",False,2012_Book_AmbientIntelligence,False,False,True
1056,"3-axis gyroscope data. It 
",False,2012_Book_AmbientIntelligence,False,False,True
1057,"represented in three directi o
",False,2012_Book_AmbientIntelligence,False,False,True
1058,"The raw data was filter e
",False,2012_Book_AmbientIntelligence,False,False,True
1059,"removes the movement of t
",False,2012_Book_AmbientIntelligence,False,False,True
1060,"information is useful, espe c
",False,2012_Book_AmbientIntelligence,False,False,True
1061,"contrast, the high-pass filt e
",False,2012_Book_AmbientIntelligence,False,False,True
1062,"left. These filters were ap p
",False,2012_Book_AmbientIntelligence,False,False,True
1063,"low-pass filtered data is us e
",False,2012_Book_AmbientIntelligence,False,False,True
1064,"Finally, an overlapping 
",False,2012_Book_AmbientIntelligence,False,False,True
1065,"means that a one-second w
",False,2012_Book_AmbientIntelligence,False,False,True
1066,"its length for each step.  
",False,2012_Book_AmbientIntelligence,False,False,True
1067,"5.2 Location Data 
",False,2012_Book_AmbientIntelligence,False,False,True
1068,"The Ubisense's output con s
",False,2012_Book_AmbientIntelligence,False,False,True
1069,"to the user's body. In a ty p
",False,2012_Book_AmbientIntelligence,False,False,True
1070,"15 cm, but in practice it m a
",False,2012_Book_AmbientIntelligence,False,False,True
1071,"ing was performed in order 
",False,2012_Book_AmbientIntelligence,False,False,True
1072,"First, a median filter co m
",False,2012_Book_AmbientIntelligence,False,False,True
1073,"ues in a time window. Thi s
",False,2012_Book_AmbientIntelligence,False,False,True
1074,"measured coordinate from t
",False,2012_Book_AmbientIntelligence,False,False,True
1075,"filter enforcing anatomic c o
",False,2012_Book_AmbientIntelligence,False,False,True
1076,"tions. After that, a Kalman errors. Finally, the same ov
",False,2012_Book_AmbientIntelligence,False,False,True
1077,"6 Context Compo
",False,2012_Book_AmbientIntelligence,False,False,True
1078,"The most important novelt y
",False,2012_Book_AmbientIntelligence,False,False,True
1079,"use of the context informat i
",False,2012_Book_AmbientIntelligence,False,False,True
1080,"Based Fall Detection Using Inertial and Location Sensors 
",False,2012_Book_AmbientIntelligence,False,False,True
1081," 
",False,2012_Book_AmbientIntelligence,False,False,True
1082,"LS architecture. TBA − Threshold-based approach. 
",False,2012_Book_AmbientIntelligence,False,False,True
1083,"sing 
",False,2012_Book_AmbientIntelligence,False,False,True
1084,"the raw data that consists of 3-axis accelerometer data 
",False,2012_Book_AmbientIntelligence,False,False,True
1085,"measures the accelerations and the angular veloc i
",False,2012_Book_AmbientIntelligence,False,False,True
1086,"ons.  
",False,2012_Book_AmbientIntelligence,False,False,True
1087,"ed with low-pass and high-pass filters. The low-pass f i
",False,2012_Book_AmbientIntelligence,False,False,True
1088,"the sensors, which leaves only the gravity component. T
",False,2012_Book_AmbientIntelligence,False,False,True
1089,"cially for an assessment of the sensor-inclination angle s
",False,2012_Book_AmbientIntelligence,False,False,True
1090,"er removes the gravity and only the sensor movements 
",False,2012_Book_AmbientIntelligence,False,False,True
1091,"plied separately: if the gravity component is needed, 
",False,2012_Book_AmbientIntelligence,False,False,True
1092,"ed; otherwise, the high-pass filtered data is used.  
",False,2012_Book_AmbientIntelligence,False,False,True
1093,"sliding-window technique was applied for the AR. T
",False,2012_Book_AmbientIntelligence,False,False,True
1094,"window moves across the stream of data, advancing by h
",False,2012_Book_AmbientIntelligence,False,False,True
1095,"sists of the 3D coordinates of the sensors that are attac
",False,2012_Book_AmbientIntelligence,False,False,True
1096,"pical open-environment, the localization accuracy is a b
",False,2012_Book_AmbientIntelligence,False,False,True
1097,"ay occasionally drop to 200 cm or more. Therefore, fi l
",False,2012_Book_AmbientIntelligence,False,False,True
1098,"to tackle the problems with the Ubisense syste m [9].  
",False,2012_Book_AmbientIntelligence,False,False,True
1099,"mputed each coordinate as the median of the measured v
",False,2012_Book_AmbientIntelligence,False,False,True
1100,"s type of filtering removes large, short-term deviations o
",False,2012_Book_AmbientIntelligence,False,False,True
1101,"the true one. Second, the coordinates were corrected wi t
",False,2012_Book_AmbientIntelligence,False,False,True
1102,"onstraints based on the user’s height and the body pro p
",False,2012_Book_AmbientIntelligence,False,False,True
1103,"filter was used to smooth the data and correct some of 
",False,2012_Book_AmbientIntelligence,False,False,True
1104,"erlapping sliding-window technique was applied. 
",False,2012_Book_AmbientIntelligence,False,False,True
1105,"nents 
",False,2012_Book_AmbientIntelligence,False,False,True
1106,"y in our fall-detection method (CoFDILS) is based on 
",False,2012_Book_AmbientIntelligence,False,False,True
1107,"ion. In general, a context is defined as any information t
",False,2012_Book_AmbientIntelligence,False,False,True
1108,"5 
",False,2012_Book_AmbientIntelligence,False,False,True
1109,"and 
",False,2012_Book_AmbientIntelligence,False,False,True
1110,"ities 
",False,2012_Book_AmbientIntelligence,False,False,True
1111,"ilter 
",False,2012_Book_AmbientIntelligence,False,False,True
1112,"This 
",False,2012_Book_AmbientIntelligence,False,False,True
1113,"s. In 
",False,2012_Book_AmbientIntelligence,False,False,True
1114,"are 
",False,2012_Book_AmbientIntelligence,False,False,True
1115,"the 
",False,2012_Book_AmbientIntelligence,False,False,True
1116,"This 
",False,2012_Book_AmbientIntelligence,False,False,True
1117,"half 
",False,2012_Book_AmbientIntelligence,False,False,True
1118,"hed 
",False,2012_Book_AmbientIntelligence,False,False,True
1119,"bout 
",False,2012_Book_AmbientIntelligence,False,False,True
1120,"lter-
",False,2012_Book_AmbientIntelligence,False,False,True
1121,"val-
",False,2012_Book_AmbientIntelligence,False,False,True
1122,"of a 
",False,2012_Book_AmbientIntelligence,False,False,True
1123,"th a 
",False,2012_Book_AmbientIntelligence,False,False,True
1124,"por-
",False,2012_Book_AmbientIntelligence,False,False,True
1125,"fthe 
",False,2012_Book_AmbientIntelligence,False,False,True
1126,"the 
",False,2012_Book_AmbientIntelligence,False,False,True
1127,"that 6 H. Gjoreski, M. Luštrek, and M. Gams 
",False,2012_Book_AmbientIntelligence,False,False,True
1128,"can be used to characterize the circumstances in which an event occurs [2]. In CoF-
",False,2012_Book_AmbientIntelligence,False,False,True
1129,"DILS, the context information consists of three components: (i) the user's body acce-lerations, (ii) the user's activities and (iii) the location of the user. 
",False,2012_Book_AmbientIntelligence,False,False,True
1130,"6.1 Body Accelerations 
",False,2012_Book_AmbientIntelligence,False,False,True
1131,"Threshold-Based Approach 
",False,2012_Book_AmbientIntelligence,False,False,True
1132,"The threshold-based approach (TBA) is used as one of the components in  CoFDILS, as 
",False,2012_Book_AmbientIntelligence,False,False,True
1133,"well as a baseline for comparison. The rationale for this method is that the acceleration pattern during a typical fall (i.e., fast, uncontrolled) is a decrease in the acceleration (free fall) followed by a rapid increase (impact with the ground). For our implementa-tion of the TBA, the difference between the maximum and minimum accelerations within a one-second window was calculated. If the difference exceeded the threshold and the maximum appeared after the minimum, a fall was declared. The threshold was chosen empirically based on preliminary data [4].  
",False,2012_Book_AmbientIntelligence,False,False,True
1134,"Body Movement 
",False,2012_Book_AmbientIntelligence,False,False,True
1135,"During motion the accelerometers produce a changing acceleration signal and the 
",False,2012_Book_AmbientIntelligence,False,False,True
1136,"fiercer the motion, the greater the change in the signal. Using these changes a feature is extracted: Acceleration Vect or Changes (AVC) [4]. This feature sums up the differ-
",False,2012_Book_AmbientIntelligence,False,False,True
1137,"ences between consecutive high-passed values of the lengths of the acceleration vectors, and divides the sum by the time interval (one second): 
",False,2012_Book_AmbientIntelligence,False,False,True
1138," 
",False,2012_Book_AmbientIntelligence,False,False,True
1139,"01 1 | |
",False,2012_Book_AmbientIntelligence,False,False,True
1140,"01 1 | |
",False,2012_Book_AmbientIntelligence,False,False,True
1141,"Article Info   ABSTRACT  
",True,29020-59012-1-PB,False,False,True
1142,"1. INTRODUCTION  
",True,29020-59012-1-PB,False,False,True
1143,"2. LIERATURE REVIEW  
",True,29020-59012-1-PB,False,False,True
1144,"3. METHODS  
",True,29020-59012-1-PB,False,False,True
1145,"3.1.  Classification and Weka Workbench  
",True,29020-59012-1-PB,False,False,True
1146,"3.2.  The implemented models  
",True,29020-59012-1-PB,False,False,True
1147,"4. COMPUTA TIONAL RESULTS AND DISCUSSION  
",True,29020-59012-1-PB,False,False,True
1148,"4.1.  The implemented  models  
",True,29020-59012-1-PB,False,False,True
1149,"5. EXPERIMENTAL DATASETS  
",True,29020-59012-1-PB,False,False,True
1150,"6. EXPERIMENTAL RESULTS  
",True,29020-59012-1-PB,False,False,True
1151,"International Journal of Electrical and Computer Engineering (IJECE)  
",False,29020-59012-1-PB,False,False,True
1152,"Vol. 13, No.  2, April  2023, pp. 1891 ~1902  
",False,29020-59012-1-PB,False,False,True
1153,"ISSN: 2088 -8708 , DOI: 10.11591/ijece.v 13i2.pp1891 -1902       1891   
",False,29020-59012-1-PB,False,False,True
1154," 
",False,29020-59012-1-PB,False,False,True
1155,"Journal homepage : http://ijece.iaescore.com  A comprehensive study of machine learning for predicting 
",False,29020-59012-1-PB,False,False,True
1156,"cardiovascular disease using Weka and SPSS  tools  
",False,29020-59012-1-PB,False,False,True
1157," 
",False,29020-59012-1-PB,False,False,True
1158," 
",False,29020-59012-1-PB,False,False,True
1159,"Belal Abuhaija1, Aladeen Alloubani2, Mohammad  Almatari3, Ghaith M. Jaradat4,  
",False,29020-59012-1-PB,False,False,True
1160,"Hemn Barzan Abdalla1, Abdallah Mohd  Abualkishik5, Mutasem Khalil Alsmadi6 
",False,29020-59012-1-PB,False,False,True
1161,"1Faculty of Computer Science and Technology , Wenzhou -Kean University, Wenzhou, China  
",False,29020-59012-1-PB,False,False,True
1162,"2Nursing Research Unit, King Hussein Cancer Center, Amman, Jordan  
",False,29020-59012-1-PB,False,False,True
1163,"3Faculty of Sciences, Albalqa Applied University, Al -Salt, Jordan  
",False,29020-59012-1-PB,False,False,True
1164,"4Faculty of Computer Sciences and Informatics, Amman Arab University, Amman, Jordan  
",False,29020-59012-1-PB,False,False,True
1165,"5Faculty of Computing and Information Technology, Sohar University, Sohar, Oman  
",False,29020-59012-1-PB,False,False,True
1166,"6Department of Management Information Systems , College of Applied Studies and Community Service, Imam Abdulrahman Bin F aisal 
",False,29020-59012-1-PB,False,False,True
1167,"University, Dammam, Saudi  Arabia  
",False,29020-59012-1-PB,False,False,True
1168," 
",False,29020-59012-1-PB,False,False,True
1169," 
",False,29020-59012-1-PB,False,False,True
1170,"Article history:  
",False,29020-59012-1-PB,False,False,True
1171,"Received Jun 24, 2022  
",False,29020-59012-1-PB,False,False,True
1172,"Revised Sep 27, 2022  
",False,29020-59012-1-PB,False,False,True
1173,"Accepted Oct 24, 2022  
",False,29020-59012-1-PB,False,False,True
1174,"  Artificial intelligence (AI) is simulating human intelligence processes by 
",False,29020-59012-1-PB,False,False,True
1175,"machines and software simulators to help humans in making accurate, 
",False,29020-59012-1-PB,False,False,True
1176,"informed, and fast decisions based on data analysis. The medical field can 
",False,29020-59012-1-PB,False,False,True
1177,"make use of such AI simulators because  medical data records are enormous 
",False,29020-59012-1-PB,False,False,True
1178,"with many overlapping parameters. Using in -depth classification techniques 
",False,29020-59012-1-PB,False,False,True
1179,"and data analysis can be the first step in identifying and reducing the risk 
",False,29020-59012-1-PB,False,False,True
1180,"factors. In this research, we are evaluating a dataset of cardiovascu lar 
",False,29020-59012-1-PB,False,False,True
1181,"abnormalities affecting a group of potential patients. We aim to employ the 
",False,29020-59012-1-PB,False,False,True
1182,"help of AI simulators such as Weka to understand the effect of each 
",False,29020-59012-1-PB,False,False,True
1183,"parameter on the risk of suffering from cardiovascular disease (CVD). We 
",False,29020-59012-1-PB,False,False,True
1184,"are utilizing seven classes, such a s baseline accuracy, naïve Bayes, k-nearest 
",False,29020-59012-1-PB,False,False,True
1185,"neighbor, decision tree, support vector machine, linear regression, and 
",False,29020-59012-1-PB,False,False,True
1186,"artificial neural network multilayer perceptron. The classifiers are assisted 
",False,29020-59012-1-PB,False,False,True
1187,"by a correlation -based filter to select the most influential a ttributes that may 
",False,29020-59012-1-PB,False,False,True
1188,"have an impact on obtaining a higher classification accuracy. Analysis of the 
",False,29020-59012-1-PB,False,False,True
1189,"results based on sensitivity, specificity, accuracy, and precision results from 
",False,29020-59012-1-PB,False,False,True
1190,"Weka and Statistical Package for Social Sciences (SPSS) is illustrated. A 
",False,29020-59012-1-PB,False,False,True
1191,"decis ion tree method (J48) demonstrated its ability to classify CVD cases 
",False,29020-59012-1-PB,False,False,True
1192,"with high accuracy 95.76% . Keyword s: 
",False,29020-59012-1-PB,False,False,True
1193,"Attribute selection  
",False,29020-59012-1-PB,False,False,True
1194,"Cardiovascular diseases  
",False,29020-59012-1-PB,False,False,True
1195,"Classification  
",False,29020-59012-1-PB,False,False,True
1196,"Machine learning  
",False,29020-59012-1-PB,False,False,True
1197,"Statistical Package for Social 
",False,29020-59012-1-PB,False,False,True
1198,"Sciences  
",False,29020-59012-1-PB,False,False,True
1199,"Weka  
",False,29020-59012-1-PB,False,False,True
1200,"This is an open access article under the CC BY -SA license.  
",False,29020-59012-1-PB,False,False,True
1201," 
",False,29020-59012-1-PB,False,False,True
1202,"Corresponding Author:  
",False,29020-59012-1-PB,False,False,True
1203,"Belal Abuhaija  
",False,29020-59012-1-PB,False,False,True
1204,"Faculty of Science and Technology, Wenzhou -Kean University  
",False,29020-59012-1-PB,False,False,True
1205,"P88 Daxue  Road, Wenzhou, China  
",False,29020-59012-1-PB,False,False,True
1206,"Email: babuhaij@kean.edu  
",False,29020-59012-1-PB,False,False,True
1207," 
",False,29020-59012-1-PB,False,False,True
1208," 
",False,29020-59012-1-PB,False,False,True
1209,"1. INTRODUCTION  
",False,29020-59012-1-PB,False,False,True
1210,"The field of artificial intelligence (AI) in computer science and engineering is gaining momentum in 
",False,29020-59012-1-PB,False,False,True
1211,"many industries because of its ability to analyze, classify and predict future trends in finance, cyber security, 
",False,29020-59012-1-PB,False,False,True
1212,"image processing , and speech recognition [1]–[6]. In recent years and with the need to analyze huge data sets, 
",False,29020-59012-1-PB,False,False,True
1213,"AI algorithms are gaining momentum to shorten the time required to classify, analyze, and predic t outcomes 
",False,29020-59012-1-PB,False,False,True
1214,"from medical datasets [7], [8] . In this research, we are utilizing AI algorithms to analyze, classify and predict 
",False,29020-59012-1-PB,False,False,True
1215,"cardiovascular diseases from a collected data set.  Worldwide, cardiovas cular diseases (CVDs) highest cause 
",False,29020-59012-1-PB,False,False,True
1216,"of mortality is caused by ischemic heart disease and stroke. It is estimated that 31% of all global deaths and 
",False,29020-59012-1-PB,False,False,True
1217,"more than 75% of all deaths reported in developing countries are a result of CVDs [8]. However, the 
",False,29020-59012-1-PB,False,False,True
1218,"                ISSN : 2088 -8708  
",False,29020-59012-1-PB,False,False,True
1219,"Int J Elec & Comp Eng, Vol. 13, No. 2, April  2023: 1891 -1902  1892  
",False,29020-59012-1-PB,False,False,True
1220,"percentage increase d significantly to 54% in the Eastern Mediterranean Region. Cardiovascular diseases risk 
",False,29020-59012-1-PB,False,False,True
1221,"factors are classified as controllable and non -controllable. The World Health Organization ( WHO ) considers 
",False,29020-59012-1-PB,False,False,True
1222,"smoking, high blood pressure, physical inactivity, obesity, diabetes mellitus, dyslipidemia,  and arterial 
",False,29020-59012-1-PB,False,False,True
1223,"hypertension as controllable risk factors. CVDs  occurrence can be significantly decreased by balancing the 
",False,29020-59012-1-PB,False,False,True
1224,"controllable risk factors [7], [8] . 
",False,29020-59012-1-PB,False,False,True
1225,"The first contribution of this paper is to utilize machine -learning algorithms in the classification  and 
",False,29020-59012-1-PB,False,False,True
1226,"prediction CVDs on a data set. All previous research analysis conducted using machine -learning tools is 
",False,29020-59012-1-PB,False,False,True
1227,"limited to coronary heart disease [9]–[13] and not on the who le CVD. Therefore, the second contribution of 
",False,29020-59012-1-PB,False,False,True
1228,"this paper is to make a complete study of CVD, compare, and analyze the results obtained from AI 
",False,29020-59012-1-PB,False,False,True
1229,"algorithms and SPSS tool. The third contribution is to establish a benchmark for future studies utilizing AI 
",False,29020-59012-1-PB,False,False,True
1230,"and its variant predictors in gaining a better understanding of the risk factors associated with CDVs. The 
",False,29020-59012-1-PB,False,False,True
1231,"fourth contribution of this paper is utilizing ZeroR method as a baseline accuracy classifier.  
",False,29020-59012-1-PB,False,False,True
1232," 
",False,29020-59012-1-PB,False,False,True
1233," 
",False,29020-59012-1-PB,False,False,True
1234,"2. LIERATURE REVIEW  
",False,29020-59012-1-PB,False,False,True
1235,"Several systematic reviews are present ed in [14]–[19] where the mortality rate in smoking 
",False,29020-59012-1-PB,False,False,True
1236,"individuals increases by almost three folds. Many  studies have investigated the cardiovascular diseases 
",False,29020-59012-1-PB,False,False,True
1237,"(CVD)  risk factors among university students and staff [20], [21] . A study conducted among students in 
",False,29020-59012-1-PB,False,False,True
1238,"Colombia showed that 92% of students had a low risk of cardiovascular disease, 2% and 6% , more  than 50% 
",False,29020-59012-1-PB,False,False,True
1239,"of the students and staff had at least one risk factor, including overweight or obesity, sedentary lifestyle, and 
",False,29020-59012-1-PB,False,False,True
1240,"hypertension [14], [17] . 
",False,29020-59012-1-PB,False,False,True
1241,"A cross -sectional study was conducted in Turkey to identify university students’ awareness of 
",False,29020-59012-1-PB,False,False,True
1242,"cardiovascular risk factors (n=2450). The students perceived smoking (58.7%), stress (71.8%), high 
",False,29020-59012-1-PB,False,False,True
1243,"cholesterol (72.3%), obesity (64.3%), diabete s (52.7%), inactivity (47.8%), hypertension (64.2%), and a 
",False,29020-59012-1-PB,False,False,True
1244,"family history of CVD (44.4%) to be the main risk factors for CVD. Moreover, the results showed that men 
",False,29020-59012-1-PB,False,False,True
1245,"ignore these risk factors [21]. There are limited studies aimed at identifying the prevalence and awareness of 
",False,29020-59012-1-PB,False,False,True
1246,"risk factors for CVDs among un iversity staff and students, especially in the Middle East [20], [21] . In 
",False,29020-59012-1-PB,False,False,True
1247,"addition, a systematic review of 212 article s aimed to identify models for predicting risk factors for CVDs in 
",False,29020-59012-1-PB,False,False,True
1248,"the general population [19]. Most prediction studies in this review were carried out in Europe (n=167, 46%) 
",False,29020-59012-1-PB,False,False,True
1249,"for both fatal and non -fatal coronary heart disease (n=118, 33%) and for more than ten year s (n=209, 58%). 
",False,29020-59012-1-PB,False,False,True
1250,"Smoking, age, gender, blood pressure, blood ch olesterol, diabetes, body mass index, and hypertension were 
",False,29020-59012-1-PB,False,False,True
1251,"the most frequent predictors [19]–[22]. 
",False,29020-59012-1-PB,False,False,True
1252,"Weka is applied to a number of collected datasets in the past two decades for cardiovascular 
",False,29020-59012-1-PB,False,False,True
1253,"diseases including heart disease. Four well -known datasets dedicated to heart disease were introduced in 
",False,29020-59012-1-PB,False,False,True
1254,"1988 and provided by the UCI machine learning reposi tory [10]. Many research works in t he literature 
",False,29020-59012-1-PB,False,False,True
1255,"presented and implemented a number of machine learning methods to predict the presence of a heart attack 
",False,29020-59012-1-PB,False,False,True
1256,"among patients from four different medical institutes. Some examples o f implementing machine -learning 
",False,29020-59012-1-PB,False,False,True
1257,"methods to these particular dataset s are presented  by Rao [23], who applied a variety of classifiers including 
",False,29020-59012-1-PB,False,False,True
1258,"k-nearest neighbor (k -NN) with poor accuracy and random forest (RF) with the highest accuracy of around 
",False,29020-59012-1-PB,False,False,True
1259,"90%. Some other implementations were directed toward different datas ets other than the heart disease dataset 
",False,29020-59012-1-PB,False,False,True
1260,"by the UCI repository applied a convolutional neural network (CNN) which obtained a good accuracy of 
",False,29020-59012-1-PB,False,False,True
1261,"82% [24]; Haq et al.  [9] applied a variety of machine learning algorithms. Lopez -Martinez et al.  [10], [11]  
",False,29020-59012-1-PB,False,False,True
1262,"applied conventional artificial neural networks  (ANN) . Uyar and İlhan  [13] applied genetic algorithm s and 
",False,29020-59012-1-PB,False,False,True
1263,"recurrent fuzzy neural networks. Others focused on handling overfitting the predictive model through 
",False,29020-59012-1-PB,False,False,True
1264,"optimization metaheuristics such as [25]. Several studies are similar to our research approach but with 
",False,29020-59012-1-PB,False,False,True
1265,"different datasets (e.g., ultrasound images, UCI , and Kaggl e numerical data) and machine learning methods 
",False,29020-59012-1-PB,False,False,True
1266,"[26]–[30]. Another similar study which utilized Weka with highly accurate predi ctions of a UCI diabetes 
",False,29020-59012-1-PB,False,False,True
1267,"dataset is presented by Alalwan [31]. 
",False,29020-59012-1-PB,False,False,True
1268,"This research intends to produce and analyze a comple te study of the most crucial parameters in 
",False,29020-59012-1-PB,False,False,True
1269,"CVD and utilize machine -learning methods for classification tasks to predict the presence of heart disease. 
",False,29020-59012-1-PB,False,False,True
1270,"Machine learning methods are very useful in identifying hidden patterns and information in the dataset. W eka 
",False,29020-59012-1-PB,False,False,True
1271,"and Statistical Package for Social Sciences (SPSS) tools has several classifiers such as baseline accuracy, 
",False,29020-59012-1-PB,False,False,True
1272,"decision tree  (DT),  naïve  Bayes  (NB) , k-NN, support vector machines (SVM), and ANN (including 
",False,29020-59012-1-PB,False,False,True
1273,"multilayer perceptron and backpropagation). A comp arison of the results based on sensitivity, specificity, 
",False,29020-59012-1-PB,False,False,True
1274,"precision, area under curve, and accuracy is realized. We differ from [10] by utilizing ZeroR method the 
",False,29020-59012-1-PB,False,False,True
1275,"baseline accuracy classifier as a performance benchmark of other classifiers which makes our predictive 
",False,29020-59012-1-PB,False,False,True
1276,"models’ fitness and effic iency more accurate than what has been presented by Ramotra et al.  [10]. 
",False,29020-59012-1-PB,False,False,True
1277,"Furthermore, the authors, did not report their model’s configuration clearly, might have triggered training and 
",False,29020-59012-1-PB,False,False,True
1278,"testing using carefully tuned parameters settings.  Int J Elec & Comp Eng   ISSN:  2088 -8708   
",False,29020-59012-1-PB,False,False,True
1279," 
",False,29020-59012-1-PB,False,False,True
1280,"A comprehensive study of machine learning for predicting cardiovascular disease using …  (Belal Abuhaija ) 1893  
",False,29020-59012-1-PB,False,False,True
1281,"We have utilized default configurations and parameter settings for each model in both Weka and 
",False,29020-59012-1-PB,False,False,True
1282,"SPSS -modeler tools to conduct a clear and fair comparison. In addition, we have implemented a multi -layer 
",False,29020-59012-1-PB,False,False,True
1283,"perceptron (MLP) neural network with a backpropagation strategy, which gave us a better understanding of 
",False,29020-59012-1-PB,False,False,True
1284,"the dataset’s parameters and information as well as the best -suited predictive model to be performed for heart 
",False,29020-59012-1-PB,False,False,True
1285,"disease. Our results show that the SVM classifier achieved the highest accuracy of  95.94% in the Weka tool, 
",False,29020-59012-1-PB,False,False,True
1286,"and in the SPSS -Modeler tool closely followed by J48, MLP, and multi -layer  perceptrons with back 
",False,29020-59012-1-PB,False,False,True
1287,"propagation  (MLPBP ) classifiers. Although SPSS -modeler has slightly outperformed Weka in terms of 
",False,29020-59012-1-PB,False,False,True
1288,"accuracy (e.g., k-NN, linear regress ion (LR), SVM), however, it clearly shows overfitting models based on 
",False,29020-59012-1-PB,False,False,True
1289,"sensitivity, area under curve, the variance between sensitivity and precision. Hence, Weka has demonstrated 
",False,29020-59012-1-PB,False,False,True
1290,"that it is much robust in modelling predications than SPSS -modeler.  
",False,29020-59012-1-PB,False,False,True
1291," 
",False,29020-59012-1-PB,False,False,True
1292," 
",False,29020-59012-1-PB,False,False,True
1293,"3. METHODS  
",False,29020-59012-1-PB,False,False,True
1294,"This section presents the design methodology of seven classifiers (predictors). It illustrates well 
",False,29020-59012-1-PB,False,False,True
1295,"known and popular classifiers including baseline accuracy (ZeroR), NB, k-NN, DT (J48),  SVM, LR, and 
",False,29020-59012-1-PB,False,False,True
1296,"ANN  MLP. Their architectures resulting in the p rocessing of datasets, as well as the implementation details 
",False,29020-59012-1-PB,False,False,True
1297,"of the classification process, are also discussed . 
",False,29020-59012-1-PB,False,False,True
1298," 
",False,29020-59012-1-PB,False,False,True
1299,"3.1.  Classification and Weka Workbench  
",False,29020-59012-1-PB,False,False,True
1300,"Classification is considered as a supervised learning system that uses a labelled dataset representing 
",False,29020-59012-1-PB,False,False,True
1301,"predictions. The dataset is used as a training set of input -output pairs to realize a deterministic function that 
",False,29020-59012-1-PB,False,False,True
1302,"maps inputs to outputs. Then predicting future input -output observations while minimizing errors as much as 
",False,29020-59012-1-PB,False,False,True
1303,"possible. ANN s, including MLP , are one of the best techniques for classifying data organized in a tabular 
",False,29020-59012-1-PB,False,False,True
1304,"form or a warehouse. Weka is a free workbench software that holds a collection of algorithms and 
",False,29020-59012-1-PB,False,False,True
1305,"visualization tools for data analysis and prediction models. It comes with an easy -to-use graphical interface 
",False,29020-59012-1-PB,False,False,True
1306,"for all of its functionalities. In this study, we are utilizing Weka to perform the classification and clustering 
",False,29020-59012-1-PB,False,False,True
1307,"tasks by implementing the above  mentioned seven algorithms. Weka has a variety of settings with many 
",False,29020-59012-1-PB,False,False,True
1308,"parameters tuning functions. This gives the authors a rich environment of experiments with less time.  
",False,29020-59012-1-PB,False,False,True
1309," 
",False,29020-59012-1-PB,False,False,True
1310,"3.2.  The implemented models  
",False,29020-59012-1-PB,False,False,True
1311,"A machine learning model must be capable of recognizing patterns in a dataset which looks for 
",False,29020-59012-1-PB,False,False,True
1312,"certain features of a, let us say, heart disease case such as blood pressure or cholesterol [32], [3 3]. To 
",False,29020-59012-1-PB,False,False,True
1313,"overcome the issue of detecting a heart disease case across a huge number of medical data, in other words 
",False,29020-59012-1-PB,False,False,True
1314,"generalizing the model, it is required to learn e.g., ANN to detect certain features. Thus, a machine learning 
",False,29020-59012-1-PB,False,False,True
1315,"model will filter a group of data for extracting significant features (or attributes) mainly ranking their 
",False,29020-59012-1-PB,False,False,True
1316,"significance via principal component analysis and attribute subset evaluator filters or wrappers. Then those 
",False,29020-59012-1-PB,False,False,True
1317,"selected features are then proceeded for further classification proce ssing that result s in prediction.  
",False,29020-59012-1-PB,False,False,True
1318,"The implemented predictive models in our study are:  
",False,29020-59012-1-PB,False,False,True
1319,"a. ZeroR: a rules -based classifier that is the simplest classification method which relies on the target and 
",False,29020-59012-1-PB,False,False,True
1320,"ignores all predictors. It  simply predicts the majority class which is only useful for determining a baseline 
",False,29020-59012-1-PB,False,False,True
1321,"performance as a benchmark for other classification methods. A very limited number of research papers 
",False,29020-59012-1-PB,False,False,True
1322,"have considered baseline accuracy classifiers in their work . 
",False,29020-59012-1-PB,False,False,True
1323,"b. NB: naïve Bayes classifier which is  a collect ion of classification algorithms based on Bayes ’ theorem, 
",False,29020-59012-1-PB,False,False,True
1324,"where all of them share a common principle, e.g., every pair of features being classified is independent of 
",False,29020-59012-1-PB,False,False,True
1325,"each other.  
",False,29020-59012-1-PB,False,False,True
1326,"c. k-NN: k-nearest neighbor lazy -based classifier that is one of the simplest decision procedures. It classifies 
",False,29020-59012-1-PB,False,False,True
1327,"a sample based on the class of its nearest neighbor. For large samples , it has a twice less probability of 
",False,29020-59012-1-PB,False,False,True
1328,"error compared to any other decision rule. It uses some or all the patterns available in the training set  to 
",False,29020-59012-1-PB,False,False,True
1329,"classify a test pattern. It involves finding the similarity between the test pattern and every pattern in the 
",False,29020-59012-1-PB,False,False,True
1330,"training set.  
",False,29020-59012-1-PB,False,False,True
1331,"d. LR: logistic regression function -based classifier. It is used to come up with a hyperplane in  feature space 
",False,29020-59012-1-PB,False,False,True
1332,"to separate observat ions that belong to a class from all other observations that do  not belong to that class. 
",False,29020-59012-1-PB,False,False,True
1333,"The decision boundary is thus  linear.  
",False,29020-59012-1-PB,False,False,True
1334,"e. J48: An optimized Java -based version of the C4.5 DT-based classifier. The C4.5 algorithm is used to 
",False,29020-59012-1-PB,False,False,True
1335,"generate a decision, based o n a certain sample of data (univariate or multivariate predictors) using the 
",False,29020-59012-1-PB,False,False,True
1336,"concept of information entropy.  
",False,29020-59012-1-PB,False,False,True
1337,"f. SVM: support vector machine function -based classifier. It is able to generalize between two different 
",False,29020-59012-1-PB,False,False,True
1338,"classes if the set of labelled data is provid ed in the training set to the algorithm. The main function of the 
",False,29020-59012-1-PB,False,False,True
1339,"SVM is to check that hyperplane is able to distinguish between the two classes.                  ISSN : 2088 -8708  
",False,29020-59012-1-PB,False,False,True
1340,"Int J Elec & Comp Eng, Vol. 13, No. 2, April  2023: 1891 -1902  1894  
",False,29020-59012-1-PB,False,False,True
1341,"g. MLP: multilayer  perceptron function -based classifier. MLP is a class of feedforward ANN where it has a 
",False,29020-59012-1-PB,False,False,True
1342,"sophisticated architecture, consisting of 3 layers of nodes including input, hidden , and output layers. It 
",False,29020-59012-1-PB,False,False,True
1343,"utilizes a non -linear activation function and backpropagation for supervised training processes to define 
",False,29020-59012-1-PB,False,False,True
1344,"the relations between inputs and outputs fo r more flexibility. In brief, the MLP is:  
",False,29020-59012-1-PB,False,False,True
1345,"− Forward pass (input layer): passing inputs into the model multiplied by weights and adding bias at 
",False,29020-59012-1-PB,False,False,True
1346,"every layer to discover the model calculated output.  
",False,29020-59012-1-PB,False,False,True
1347,"− Loss calculates (activation function/hidden layer): once a sam ple data (e.g., a record in a dataset) an 
",False,29020-59012-1-PB,False,False,True
1348,"output is obtained from the model as a predicted output, which will be labelled with the data that is a 
",False,29020-59012-1-PB,False,False,True
1349,"real or expected output. Hence, embedded backpropagation algorithm is used to calculate the loss.  
",False,29020-59012-1-PB,False,False,True
1350,"− Backward pas s (output layer): finally, and most importantly in training the model, the loss is back 
",False,29020-59012-1-PB,False,False,True
1351,"propagated and by using gradient updates the weights of the model. Iteratively, weights will be 
",False,29020-59012-1-PB,False,False,True
1352,"adjusted referring to the gradient flow to a certain direction.  
",False,29020-59012-1-PB,False,False,True
1353,"Based on  the presented characteristics of the algorithms, we are motivated to adopt a  
",False,29020-59012-1-PB,False,False,True
1354,"machine -learning model that utilizes a more sophisticated classification architecture including training and 
",False,29020-59012-1-PB,False,False,True
1355,"testing such as MLP. Its architecture presents a robust and effectiv e one, where it provides a strong gradient 
",False,29020-59012-1-PB,False,False,True
1356,"flow due to implicit supervision (e.g., learning), parameters, and computational efficiency due to strong 
",False,29020-59012-1-PB,False,False,True
1357,"connectivity through a number of parameters which is directly proportional to the growth rate of the 
",False,29020-59012-1-PB,False,False,True
1358,"comple xity, more diversified features, and maintains low complexity features. Therefore, in this study, we 
",False,29020-59012-1-PB,False,False,True
1359,"employ 7 traditional well -known machine learning algorithms including MLP for 1 class attribute of heart 
",False,29020-59012-1-PB,False,False,True
1360,"disease related to cardiovascular classification, training, validation, and testing. We train the model using the 
",False,29020-59012-1-PB,False,False,True
1361,"dataset collected by the authors alongside selecting significant features. The parameter values that give rise to 
",False,29020-59012-1-PB,False,False,True
1362,"the best performance on the validation dataset are used for testing . 
",False,29020-59012-1-PB,False,False,True
1363," 
",False,29020-59012-1-PB,False,False,True
1364," 
",False,29020-59012-1-PB,False,False,True
1365,"4. COMPUTA TIONAL RESULTS AND DISCUSSION  
",False,29020-59012-1-PB,False,False,True
1366,"This section discusses the bases of training and testing the predictive models including their 
",False,29020-59012-1-PB,False,False,True
1367,"experimental settings and results. Datasets that are used in this work are also briefly discussed. Using a Java -
",False,29020-59012-1-PB,False,False,True
1368,"based workbench cal led Weka version 3.9.4, experiments are conducted on a Windows 10 machine with a 
",False,29020-59012-1-PB,False,False,True
1369,"Corei7 processor assessed by Nvidia GPU and 16 GB of RAM. The following subsections demonstrate the 
",False,29020-59012-1-PB,False,False,True
1370,"training and testing environment, parameter settings of the predictive mode l, and configuration of machine 
",False,29020-59012-1-PB,False,False,True
1371,"learning algorithms used within the predictive model.  
",False,29020-59012-1-PB,False,False,True
1372," 
",False,29020-59012-1-PB,False,False,True
1373,"4.1.  The implemented  models  
",False,29020-59012-1-PB,False,False,True
1374,"The data was collected from students, employees,  and faculty members of a Saudi University ; data 
",False,29020-59012-1-PB,False,False,True
1375,"from 370 participants (240 males and 130 females), including university employees and students, in Saudi 
",False,29020-59012-1-PB,False,False,True
1376,"Arabia were analyzed. The majority of participants were Saudi Arabian (82.2%) age d 18-25, unemployed 
",False,29020-59012-1-PB,False,False,True
1377,"(79.5%), and married (54.6%). We are utilizing the Weka machine -learning tool to conduct th is study on a 
",False,29020-59012-1-PB,False,False,True
1378,"data set from some university students, faculty, and employees in the middle east . In order to validate our 
",False,29020-59012-1-PB,False,False,True
1379,"approach, Weka and the SPSS tool findings and results are shown below.  A number of consecutive steps are 
",False,29020-59012-1-PB,False,False,True
1380,"considered in conducting the experiments . 
",False,29020-59012-1-PB,False,False,True
1381,"a. Data collection is to collect relevant data, define and understand the problem, then are formatted into a 
",False,29020-59012-1-PB,False,False,True
1382,"Weka -based arff  dataset, where the class attribute is labeled for the classification task. The dataset has 
",False,29020-59012-1-PB,False,False,True
1383,"370 records and 36 attributes including 1 class attribute. In general, the quantity and quality of the dataset 
",False,29020-59012-1-PB,False,False,True
1384,"dictate how accurate the model is. The outcome of thi s step is a representation of data in a tabular form 
",False,29020-59012-1-PB,False,False,True
1385,"which will be used for training the model.  
",False,29020-59012-1-PB,False,False,True
1386,"b. Data preparation is the step of wrangling data and preparing it for training. It includes filtering data and 
",False,29020-59012-1-PB,False,False,True
1387,"cleaning it via removing duplicates, correcting err ors, dealing with missing values, normalization, and 
",False,29020-59012-1-PB,False,False,True
1388,"data type conversion . 
",False,29020-59012-1-PB,False,False,True
1389,"c. Feature selection is selecting the impactful subset of attributes on the model performance and its  accuracy.  
",False,29020-59012-1-PB,False,False,True
1390,"d. Choosing a model that fits the classification and regression tasks.  
",False,29020-59012-1-PB,False,False,True
1391,"e. Training the model to iteratively predict correctly as often as possible.  
",False,29020-59012-1-PB,False,False,True
1392,"f. Evaluate the model using a combination of metrics to measure the objective performance of the model. In 
",False,29020-59012-1-PB,False,False,True
1393,"addition, testing the model against previously unseen data, which mimics the mode l performance in the 
",False,29020-59012-1-PB,False,False,True
1394,"real world and to tune the model furthermore.  
",False,29020-59012-1-PB,False,False,True
1395,"g. Parameter tuning (a .k.a. hyperparameter tuning) which may lead to an improved performance of the 
",False,29020-59012-1-PB,False,False,True
1396,"model. For example, number of training steps, learning rate, initialization values , and distribution.  
",False,29020-59012-1-PB,False,False,True
1397,"h. Perform predictions using further test dataset that is new to the model but for which class labels are 
",False,29020-59012-1-PB,False,False,True
1398,"known to test the model and obtain a better approximation of the performance of the model in the real 
",False,29020-59012-1-PB,False,False,True
1399,"world.  Int J Elec & Comp Eng   ISSN:  2088 -8708   
",False,29020-59012-1-PB,False,False,True
1400," 
",False,29020-59012-1-PB,False,False,True
1401,"A comprehensive study of machine learning for predicting cardiovascular disease using …  (Belal Abuhaija ) 1895  
",False,29020-59012-1-PB,False,False,True
1402,"We have considered a comp arative experiment between three levels of analysis including 
",False,29020-59012-1-PB,False,False,True
1403,"descriptive statistics using IBM -SPSS, then classification and regression using machine -learning techniques 
",False,29020-59012-1-PB,False,False,True
1404,"in Weka workbench, and finally predictive modeling using a machine learning technique.  IBM -SPSS is used 
",False,29020-59012-1-PB,False,False,True
1405,"to understand and statistically describe the collected data by identifying central tendency measures, 
",False,29020-59012-1-PB,False,False,True
1406,"coefficient correlations, and the significance of attributes. Using Chi -square, for instance, we have identified 
",False,29020-59012-1-PB,False,False,True
1407,"some significant attrib utes that may affect the predictive model’s accuracy. However, this did not sufficiently 
",False,29020-59012-1-PB,False,False,True
1408,"determine the significance and correlation coefficient of attributes, hence we further experiment with those 
",False,29020-59012-1-PB,False,False,True
1409,"measures using Weka workbench for both levels of training models machine learning. Machine learning 
",False,29020-59012-1-PB,False,False,True
1410,"consists of a variety of induction and statistical based functions that describe the data via sophisticated 
",False,29020-59012-1-PB,False,False,True
1411,"techniques e.g., SVM and MLP which help to learn processes towards predictions.  
",False,29020-59012-1-PB,False,False,True
1412," 
",False,29020-59012-1-PB,False,False,True
1413," 
",False,29020-59012-1-PB,False,False,True
1414,"5. EXPERIMENTAL DATASETS  
",False,29020-59012-1-PB,False,False,True
1415,"The datasets that are used in this research are collected by a questionnaire, where the whole dataset 
",False,29020-59012-1-PB,False,False,True
1416,"(size=370 instances) is split into two sets, training,  and testing in the form of “arff” Weka file which consists 
",False,29020-59012-1-PB,False,False,True
1417,"of 36 attributes including 7 class attribu tes. Train set is used to train a model for prediction, while the test set 
",False,29020-59012-1-PB,False,False,True
1418,"is used to feed those models with new data for accuracy evaluation. Experimentally predetermined, the 
",False,29020-59012-1-PB,False,False,True
1419,"training set is 70% split while the test set is 30% split. Table 1 summarizes l abels and their count for each 
",False,29020-59012-1-PB,False,False,True
1420,"class attribute. Compared to the well -known UCI heart disease datasets, they have 76 attributes that are 
",False,29020-59012-1-PB,False,False,True
1421,"reduced into 14 significant attributes pointed to only one class (e.g., heart disease), while our dataset has 36 
",False,29020-59012-1-PB,False,False,True
1422,"attribu tes that are reduced into a handful of significant attributes (2 to 5 attributes) pointed to 7 classes not 
",False,29020-59012-1-PB,False,False,True
1423,"only for the heart disease diagnosis. It is easier, faster, and more accurate to predict a class using a small set 
",False,29020-59012-1-PB,False,False,True
1424,"of attributes. With the 7 classes in our dataset, we are able to diagnose 7 different (but related to heart 
",False,29020-59012-1-PB,False,False,True
1425,"disease) CVDs . In addition, our dataset has no missing values as in the UCI heart disease datasets, which 
",False,29020-59012-1-PB,False,False,True
1426,"makes it easier to process and better to understand. Some significant attrib utes are common between our 
",False,29020-59012-1-PB,False,False,True
1427,"dataset and the UCI datasets such as age, gender, cholesterol, fasting blood sugar, and blood pressure. Those 
",False,29020-59012-1-PB,False,False,True
1428,"contribute to our dataset’s validity. The dataset has the float numerical readings of cardiovascular indexes 
",False,29020-59012-1-PB,False,False,True
1429,"and relat ed diseases, e.g., fasting blood sugar, low and high blood lipids, triglyceride, cholesterol, body mass 
",False,29020-59012-1-PB,False,False,True
1430,"index, blood pressure, demographics, and some smoking and eating habits [34], [35] . The readings present 
",False,29020-59012-1-PB,False,False,True
1431,"some distinct features of heart disease cases that are clearly associated with other features such as blood 
",False,29020-59012-1-PB,False,False,True
1432,"pressure an d fasting blood sugar that are well known to medical experts and therapists.  
",False,29020-59012-1-PB,False,False,True
1433," 
",False,29020-59012-1-PB,False,False,True
1434," 
",False,29020-59012-1-PB,False,False,True
1435,"Table 1. Dataset class attribute and labels  
",False,29020-59012-1-PB,False,False,True
1436,"Class Label Count  
",False,29020-59012-1-PB,False,False,True
1437,"Heart Disease (HD)  Yes 112 
",False,29020-59012-1-PB,False,False,True
1438,"No 258 
",False,29020-59012-1-PB,False,False,True
1439," 
",False,29020-59012-1-PB,False,False,True
1440," 
",False,29020-59012-1-PB,False,False,True
1441,"6. EXPERIMENTAL RESULTS  
",False,29020-59012-1-PB,False,False,True
1442,"Here, we summarize obtained accuracy results of machine learning predictive models. For the 
",False,29020-59012-1-PB,False,False,True
1443,"classification task we have implemented seven popular machine learning algorithms with the same 
",False,29020-59012-1-PB,False,False,True
1444,"experimental settings and configuration,  e.g., data preprocessing, t esting mode, and attribute selection 
",False,29020-59012-1-PB,False,False,True
1445,"method. The parameter settings of the seven models with many common parameters and setups which are 
",False,29020-59012-1-PB,False,False,True
1446,"predetermined experimentally. For example, the 10 folds cross -validation test mode. The following 
",False,29020-59012-1-PB,False,False,True
1447,"justification illustr ates the reason behind applying such a testing mode “cross -validation ” as a suitable mode 
",False,29020-59012-1-PB,False,False,True
1448,"for our experiments.  
",False,29020-59012-1-PB,False,False,True
1449,"Generally, MLP overcomes the issue of traditional machine learning methods in processing data, 
",False,29020-59012-1-PB,False,False,True
1450,"where it uses perceptrons for input. Using filter s or wrappers within MLP, an ANN analyzes the influence of 
",False,29020-59012-1-PB,False,False,True
1451,"correlated coefficients (e.g., selected attributes) in a dataset that is more powerfully associated than others, 
",False,29020-59012-1-PB,False,False,True
1452,"and subsets are built upon instances. After passing a number of filters for the whol e dataset, then on each 
",False,29020-59012-1-PB,False,False,True
1453,"filter, a feature subset is generated. These feature subsets are then passed over an activation function to 
",False,29020-59012-1-PB,False,False,True
1454,"choose whether a certain feature is existent at a given position (or rather a correlation) in the data. In short, 
",False,29020-59012-1-PB,False,False,True
1455,"MLP is cap able of learning the following: i) each layer learns filters of growing complexity; ii) first layers 
",False,29020-59012-1-PB,False,False,True
1456,"learn basic feature detection filters e.g., correlation, and significance; iii) middle layers learn filters that 
",False,29020-59012-1-PB,False,False,True
1457,"detect subsets of attributes; and iv) last  layers have higher demonstrations by learning to identify the best 
",False,29020-59012-1-PB,False,False,True
1458,"evaluated and significant subset of attributes (features), in diverse measurements.  
",False,29020-59012-1-PB,False,False,True
1459,"One of the predictive model evaluation techniques is the cross -validation technique which splits the 
",False,29020-59012-1-PB,False,False,True
1460,"dataset into k-folds. It is training a model on all of the folds except one that is held out as the test set, then this 
",False,29020-59012-1-PB,False,False,True
1461,"process repeatedly creates k-different models and give each fold a chance of being held out as the test set.                 ISSN : 2088 -8708  
",False,29020-59012-1-PB,False,False,True
1462,"Int J Elec & Comp Eng, Vol. 13, No. 2, April  2023: 1891 -1902  1896  
",False,29020-59012-1-PB,False,False,True
1463,"Then calculate the average per formance of all k models. Commonly used, this is the gold standard for 
",False,29020-59012-1-PB,False,False,True
1464,"evaluating model performance but has the cost of creating many more models. The training dataset technique 
",False,29020-59012-1-PB,False,False,True
1465,"is only used when having the whole dataset and needs to create a descriptive m odel rather than the predictive 
",False,29020-59012-1-PB,False,False,True
1466,"model . This way, we will be able to create a model to better understand the problem. This was achieved by 
",False,29020-59012-1-PB,False,False,True
1467,"using the IBM -SPSS software. The supplied test set is only used when having a very large dataset which is 
",False,29020-59012-1-PB,False,False,True
1468,"not applicab le in our experiment where the dataset size=370 instances. The percentage  split technique is used 
",False,29020-59012-1-PB,False,False,True
1469,"when needing a quick idea of the model performance and  could be impractical for making decisions. Hence, 
",False,29020-59012-1-PB,False,False,True
1470,"cross -validation is the default option to be used whe n uncertainty about the problem ’s description. Generally, it 
",False,29020-59012-1-PB,False,False,True
1471,"provides a more accurate estimate of the performance than the other evaluation techniques , not to be used when 
",False,29020-59012-1-PB,False,False,True
1472,"having  very large data. Common values for k are 5 and 10, depending on the size of the dataset.  In short, 
",False,29020-59012-1-PB,False,False,True
1473,"cross -validation is a popular technique because it is simple to understand and it generally results in a less 
",False,29020-59012-1-PB,False,False,True
1474,"biased or less optimistic estimate of the model performance than others, such as a simple train/test split. 
",False,29020-59012-1-PB,False,False,True
1475,"Tables 2 to 5 present  the accuracy outcomes of classification models using Weka and SPSS compared against 
",False,29020-59012-1-PB,False,False,True
1476,"each other.  
",False,29020-59012-1-PB,False,False,True
1477," 
",False,29020-59012-1-PB,False,False,True
1478," 
",False,29020-59012-1-PB,False,False,True
1479,"Table 2. Baseline classifier employed in Weka and SPSS  
",False,29020-59012-1-PB,False,False,True
1480,"ZeroR  Sensitivity  Specificity  Precision  Area under curve  Accuracy (%)  
",False,29020-59012-1-PB,False,False,True
1481,"SPSS  0.000  1.000  0.000  0.834  69.70%  
",False,29020-59012-1-PB,False,False,True
1482,"Weka  0.697  0.000  0.697  0.940  69.72%  
",False,29020-59012-1-PB,False,False,True
1483," 
",False,29020-59012-1-PB,False,False,True
1484," 
",False,29020-59012-1-PB,False,False,True
1485,"Table 3. Naïve Bayes classifier employed in Weka and SPSS  
",False,29020-59012-1-PB,False,False,True
1486,"NB Sensitivity  Specificity  Precision  Area under curve  Accuracy (%)  
",False,29020-59012-1-PB,False,False,True
1487,"SPSS  0.000  1.000  0.000  0.916  72.63%  
",False,29020-59012-1-PB,False,False,True
1488,"Weka  0.943  0.973  0.945  0.985  94.32%  
",False,29020-59012-1-PB,False,False,True
1489," 
",False,29020-59012-1-PB,False,False,True
1490," 
",False,29020-59012-1-PB,False,False,True
1491,"Table 4. k-NN classifier employed in Weka and SPSS  
",False,29020-59012-1-PB,False,False,True
1492,"k-NN Sensitivity  Specificity  Precision  Area under curve  Accuracy (%)  
",False,29020-59012-1-PB,False,False,True
1493,"SPSS  0.604  0.969  0.878  0.917  85.78%  
",False,29020-59012-1-PB,False,False,True
1494,"Weka  0.854  0.517  0.854  0.842  85.40%  
",False,29020-59012-1-PB,False,False,True
1495," 
",False,29020-59012-1-PB,False,False,True
1496," 
",False,29020-59012-1-PB,False,False,True
1497,"Table 5. Logistic regression classifier employed in Weka and SPSS  
",False,29020-59012-1-PB,False,False,True
1498,"LR Sensitivity  Specificity  Precision  Area under curve  Accuracy (%)  
",False,29020-59012-1-PB,False,False,True
1499,"SPSS  0.000  1.000  0.000  0.974  93.68%  
",False,29020-59012-1-PB,False,False,True
1500,"Weka  0.922  0.892  0.923  0.954  92.16%  
",False,29020-59012-1-PB,False,False,True
1501," 
",False,29020-59012-1-PB,False,False,True
1502," 
",False,29020-59012-1-PB,False,False,True
1503,"Determining the baseline accuracy of each class attribute using ZeroR rule -based machine learning 
",False,29020-59012-1-PB,False,False,True
1504,"algorithm is needed at first to benchmark the performance of a predictive model. This is to verify how good 
",False,29020-59012-1-PB,False,False,True
1505,"the accuracy percentage obtained by the predictive  models is . ZeroR typically guesses the most popular class 
",False,29020-59012-1-PB,False,False,True
1506,"attribute, e.g., by taking 258 instances from the heart disease (HD) class attribute and dividing it into 370 the 
",False,29020-59012-1-PB,False,False,True
1507,"whole instance's size which equals to 69.72% accuracy. This is a good indicator tha t e.g.,  k-NN=85.4% or 
",False,29020-59012-1-PB,False,False,True
1508,"J48=95.67% are highly accurate compared to the baseline accuracy (69.72%), but the accuracy is not a 
",False,29020-59012-1-PB,False,False,True
1509,"sufficient indicator of a highly accurate model that is suitable for the data . Overfitting or underfitting the 
",False,29020-59012-1-PB,False,False,True
1510,"model is validated usi ng measurements such as precision, sensitivity, and specificity. Therefore, 
",False,29020-59012-1-PB,False,False,True
1511,"understanding the data and its pattern is crucial and supported by the baseline classifier (ZeroR) before 
",False,29020-59012-1-PB,False,False,True
1512,"preprocessing data or fine -tuning a model’s parameters. Refer to Tables 2 to 5. We have implemented default 
",False,29020-59012-1-PB,False,False,True
1513,"parameter settings of the predictive models including training and testing mode. Based on the confusion 
",False,29020-59012-1-PB,False,False,True
1514,"matrix obtained by all classifiers, some crucial measures and rates are described : 
",False,29020-59012-1-PB,False,False,True
1515,"− Accuracy (correctly clas sified instances) presents the percentage of test instances that were correctly and 
",False,29020-59012-1-PB,False,False,True
1516,"incorrectly classified. However, it is not chance corrected and not sensitive to class distribution. So, it is 
",False,29020-59012-1-PB,False,False,True
1517,"an insufficient performance measurement.  
",False,29020-59012-1-PB,False,False,True
1518,"− True positive rate (TP) presents when the model actually predicts “Yes”, how often does it predict “Yes”? 
",False,29020-59012-1-PB,False,False,True
1519,"For example, TP/actual “Yes” =100/112=0.89. In other words, instances are correctly classified as a given 
",False,29020-59012-1-PB,False,False,True
1520,"class.  
",False,29020-59012-1-PB,False,False,True
1521,"− False positive rat e (FP) presents when the model actually predicts “Yes”, how often does it predict “No”? 
",False,29020-59012-1-PB,False,False,True
1522,"For example, FP/actual “Yes” =12/112=0.11. In other words, instances that are incorrectly classified as a 
",False,29020-59012-1-PB,False,False,True
1523,"given class.  Int J Elec & Comp Eng   ISSN:  2088 -8708   
",False,29020-59012-1-PB,False,False,True
1524," 
",False,29020-59012-1-PB,False,False,True
1525,"A comprehensive study of machine learning for predicting cardiovascular disease using …  (Belal Abuhaija ) 1897  
",False,29020-59012-1-PB,False,False,True
1526,"− Precision presents when the model predicts “yes”, how often is it correct?  For example, where a 
",False,29020-59012-1-PB,False,False,True
1527,"proportion of instances that are true of a class is divided by the total instances classified as that class; 
",False,29020-59012-1-PB,False,False,True
1528,"TP/predicted “No” =258/270=0.95.  
",False,29020-59012-1-PB,False,False,True
1529,"− Sensitivity (aka. recall ) is a proportion of instances classified a s a given class divided by the actual total in 
",False,29020-59012-1-PB,False,False,True
1530,"that class which is equivalent to TP rate (TP/(TP+FN)). It is used when  the occurrence of false negatives 
",False,29020-59012-1-PB,False,False,True
1531,"is unacceptable. It would be better to have extra false positives over some false negatives. It is very  useful 
",False,29020-59012-1-PB,False,False,True
1532,"when predicting disease.  
",False,29020-59012-1-PB,False,False,True
1533,"− Specificity is a ratio of true negatives to total negatives in the data (TN/(TN+FP)). It is used when  
",False,29020-59012-1-PB,False,False,True
1534,"stopping false alarms is required. It is very useful when running e.g., a blood pressure test in which all 
",False,29020-59012-1-PB,False,False,True
1535,"patients who t est positive will immediately be classified as heart attack  potentials.  
",False,29020-59012-1-PB,False,False,True
1536,"ZeroR  classifier is not provided explicitly in SPSS and SPSS -modeler, which need a number of 
",False,29020-59012-1-PB,False,False,True
1537,"cumulative steps, while it is fully provided in Weka with full control of the classifier’s parameters that are 
",False,29020-59012-1-PB,False,False,True
1538,"easy to tune in one workspace. So, we are unable to repor t the outcomes of the baseline classifier in a fair and 
",False,29020-59012-1-PB,False,False,True
1539,"comparable form. As mentioned before, baseline accuracy (as a performance benchmark evaluation) is 
",False,29020-59012-1-PB,False,False,True
1540,"crucial for testing and validating a classification process or predictive model. This favors Weka ove r SPSS 
",False,29020-59012-1-PB,False,False,True
1541,"due to automation, benchmarking, and parameter control. From Table 2, both software showed very similar 
",False,29020-59012-1-PB,False,False,True
1542,"performance with an almost identical accuracy percentage. However, Weka has obtained better indicators of 
",False,29020-59012-1-PB,False,False,True
1543,"how to fit the classifier model to the collected dataset. See both models’ sensitivity, specificity, precision, and 
",False,29020-59012-1-PB,False,False,True
1544,"area under the curve. Note that SPSS performs a default 50/50 split of the dataset for the baseline accuracy, 
",False,29020-59012-1-PB,False,False,True
1545,"hence the performance benchmark is not worthy . 
",False,29020-59012-1-PB,False,False,True
1546,"Again, we are unable t o report the outcomes of the NB classifier from SPSS in a fair and comparable 
",False,29020-59012-1-PB,False,False,True
1547,"form since SPSS does not provide full control of NB classifier’s parameters. Most importantly, NB classifier 
",False,29020-59012-1-PB,False,False,True
1548,"of SPSS only reports the distribution of the Bayesian network in whic h it translates the conditional 
",False,29020-59012-1-PB,False,False,True
1549,"probabilities of the heart disease class. It distributed the probability of the categories of the class into 
",False,29020-59012-1-PB,False,False,True
1550,"“Yes”=0.27 and “No”=0.73. From Table 3, it is shown that NB classifier from Weka has outperformed the one 
",False,29020-59012-1-PB,False,False,True
1551,"from SPSS  in terms of area under the curve as well as a very high accuracy percentage with a big difference.   
",False,29020-59012-1-PB,False,False,True
1552,"From Table 4, both Weka and SPSS are almost identical in terms of accuracy. However, observing 
",False,29020-59012-1-PB,False,False,True
1553,"the relation between sensitivity and area under the curve, S PSS has clearly built an overfit model. This is due 
",False,29020-59012-1-PB,False,False,True
1554,"to the fact default configuration is used in k-NN model, however, in the SPSS dataset split for training and 
",False,29020-59012-1-PB,False,False,True
1555,"testing. Hence, Weka has built a fit predictive model based on the observation of sensitivity. Table 5 shows 
",False,29020-59012-1-PB,False,False,True
1556,"Weka and SPSS are approximately equal in terms of accuracy, however, the area under the curve is high, and 
",False,29020-59012-1-PB,False,False,True
1557,"the variance between both software in sensitivity and precision indicates the limitation of SPSS, which 
",False,29020-59012-1-PB,False,False,True
1558,"renders Weka's superiority to SPSS.  
",False,29020-59012-1-PB,False,False,True
1559,"Figure 1 shows that Weka has outperformed SPSS -modeler in terms of both accuracy and model fit. 
",False,29020-59012-1-PB,False,False,True
1560,"Therefore, it is evident that Weka is more suitable than SPSS as it can overcome the limitations of SPSS. As 
",False,29020-59012-1-PB,False,False,True
1561,"far as the SVM algorithm is concerned, Figure 2 illustrates the results obtained from Weka and SPSS. SPSS 
",False,29020-59012-1-PB,False,False,True
1562,"has obtained a better accuracy percentage and higher area under the curve, but at a great cost giving away 
",False,29020-59012-1-PB,False,False,True
1563,"suitability for the dataset by overfitting the model. Hence, Weka showed a model that fit s the dataset . 
",False,29020-59012-1-PB,False,False,True
1564," 
",False,29020-59012-1-PB,False,False,True
1565," 
",False,29020-59012-1-PB,False,False,True
1566," 
",False,29020-59012-1-PB,False,False,True
1567," 
",False,29020-59012-1-PB,False,False,True
1568,"Figure 1. Decision tree classifier employed in Weka and SPSS  
",False,29020-59012-1-PB,False,False,True
1569," 0.953
",False,29020-59012-1-PB,False,False,True
1570,"0.8210.9530.945 95.26%0.957
",False,29020-59012-1-PB,False,False,True
1571,"0.8210.9530.945 95.26%0.957
",False,29020-59012-1-PB,False,False,True
1572,"Abstract. Generated hateful and toxic content by a portion of users
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1573,"1 Introduction
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1574,"2 Previous Works
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1575,"3 Methodology
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1576,"3.1 Fine-Tuning Strategies
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1577,"4 Experiments and Results
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1578,"4.1 Dataset Description
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1579,"4.2 Pre-Processing
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1580,"4.3 Implementation and Results Analysis
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1581,"4.4 Error Analysis
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1582,"5 Conclusion
",True,A BERT-Based Transfer Learning Approach for,False,False,True
1583,"A BERT-Based Transfer Learning Approach for
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1584,"Hate Speech Detection in Online Social Media
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1585,"Marzieh Mozafari1, Reza Farahbakhsh1, and No el Crespi1
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1586,"CNRS UMR5157, T el ecom SudParis, Institut Polytechnique de Paris, Evry, France
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1587,"fmarzieh.mozafari, reza.farahbakhsh, noel.crespi g@telecom-sudparis.eu
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1588,"in social media is a rising phenomenon that motivated researchers to
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1589,"dedicate substantial eorts to the challenging direction of hateful con-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1590,"tent identication. We not only need an ecient automatic hate speech
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1591,"detection model based on advanced machine learning and natural lan-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1592,"guage processing, but also a suciently large amount of annotated data
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1593,"to train a model. The lack of a sucient amount of labelled hate speech
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1594,"data, along with the existing biases, has been the main issue in this do-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1595,"main of research. To address these needs, in this study we introduce a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1596,"novel transfer learning approach based on an existing pre-trained lan-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1597,"guage model called BERT (Bidirectional Encoder Representations from
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1598,"Transformers). More specically, we investigate the ability of BERT at
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1599,"capturing hateful context within social media content by using new ne-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1600,"tuning methods based on transfer learning. To evaluate our proposed ap-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1601,"proach, we use two publicly available datasets that have been annotated
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1602,"for racism, sexism, hate, or oensive content on Twitter. The results show
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1603,"that our solution obtains considerable performance on these datasets in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1604,"terms of precision and recall in comparison to existing approaches. Con-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1605,"sequently, our model can capture some biases in data annotation and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1606,"collection process and can potentially lead us to a more accurate model.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1607,"Keywords: hate speech detection, transfer learning, language model-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1608,"ing, BERT, ne-tuning, NLP, social media.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1609,"1 Introduction
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1610,"People are increasingly using social networking platforms such as Twitter, Face-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1611,"book, YouTube, etc. to communicate their opinions and share information. Al-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1612,"though the interactions among users on these platforms can lead to constructive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1613,"conversations, they have been increasingly exploited for the propagation of abu-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1614,"sive language and the organization of hate-based activities [1,16], especially due
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1615,"to the mobility and anonymous environment of these online platforms. Violence
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1616,"attributed to online hate speech has increased worldwide. For example, in the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1617,"UK, there has been a signicant increase in hate speech towards the immigrant
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1618,"and Muslim communities following the UK's leaving the EU and the Manch-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1619,"ester and London attacks1. The US also has been a marked increase in hate
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1620,"1Anti-muslim hate crime surges after Manchester and London Bridge attacks (2017): https://www.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1621,"theguardian.comarXiv:1910.12574v1  [cs.SI]  28 Oct 20192 Marzieh Mozafari et al.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1622,"speech and related crime following the Trump election2. Therefore, governments
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1623,"and social network platforms confronting the trend must have tools to detect
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1624,"aggressive behavior in general, and hate speech in particular, as these forms of
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1625,"online aggression not only poison the social climate of the online communities
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1626,"that experience it, but can also provoke physical violence and serious harm [16].
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1627,"Recently, the problem of online abusive detection has attracted scientic at-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1628,"tention. Proof of this is the creation of the third Workshop on Abusive Language
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1629,"Online3or Kaggles Toxic Comment Classication Challenge that gathered 4,551
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1630,"teams4in 2018 to detect dierent types of toxicities (threats, obscenity, etc.).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1631,"In the scope of this work, we mainly focus on the term hate speech as abusive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1632,"content in social media, since it can be considered a broad umbrella term for
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1633,"numerous kinds of insulting user-generated content. Hate speech is commonly
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1634,"dened as any communication criticizing a person or a group based on some char-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1635,"acteristics such as gender, sexual orientation, nationality, religion, race, etc. Hate
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1636,"speech detection is not a stable or simple target because misclassication of reg-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1637,"ular conversation as hate speech can severely aect users freedom of expression
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1638,"and reputation, while misclassication of hateful conversations as unproblematic
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1639,"would maintain the status of online communities as unsafe environments [2].
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1640,"To detect online hate speech, a large number of scientic studies have been
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1641,"dedicated by using Natural Language Processing (NLP) in combination with
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1642,"Machine Learning (ML) and Deep Learning (DL) methods [1, 8, 11, 13, 22, 25].
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1643,"Although supervised machine learning-based approaches have used dierent text
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1644,"mining-based features such as surface features, sentiment analysis, lexical re-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1645,"sources, linguistic features, knowledge-based features or user-based and platform-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1646,"based metadata [3, 6, 23], they necessitate a well-dened feature extraction ap-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1647,"proach. The trend now seems to be changing direction, with deep learning mod-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1648,"els being used for both feature extraction and the training of classiers. These
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1649,"newer models are applying deep learning approaches such as Convolutional Neu-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1650,"ral Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc. [1,8]
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1651,"to enhance the performance of hate speech detection models, however, they still
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1652,"suer from lack of labelled data or inability to improve generalization property.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1653,"Here, we propose a transfer learning approach for hate speech understanding
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1654,"using a combination of the unsupervised pre-trained model BERT [4] and some
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1655,"new supervised ne-tuning strategies. As far as we know, it is the rst time
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1656,"that such exhaustive ne-tuning strategies are proposed along with a genera-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1657,"tive pre-trained language model to transfer learning to low-resource hate speech
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1658,"languages and improve performance of the task. In summary:
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1659,"We propose a transfer learning approach using the pre-trained language
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1660,"model BERT learned on English Wikipedia and BookCorpus to enhance
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1661,"hate speech detection on publicly available benchmark datasets. Toward that
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1662,"end, for the rst time, we introduce new ne-tuning strategies to examine
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1663,"the eect of dierent embedding layers of BERT in hate speech detection.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1664,"2A.: Hate on the rise after Trumps election: http://www.newyorker.com
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1665,"3https://sites.google.com/view/alw3/home
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1666,"4https://www.kaggle.com/c/jigsaw-toxic-comment-classication-challenge/A BERT-based hate speech detection approach in social media 3
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1667,"Our experiment results show that using the pre-trained BERT model and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1668,"ne-tuning it on the downstream task by leveraging syntactical and contex-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1669,"tual information of all BERT's transformers outperforms previous works in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1670,"terms of precision, recall, and F1-score. Furthermore, examining the results
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1671,"shows the ability of our model to detect some biases in the process of col-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1672,"lecting or annotating datasets. It can be a valuable clue in using pre-trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1673,"BERT model for debiasing hate speech datasets in future studies.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1674,"2 Previous Works
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1675,"Here, the existing body of knowledge on online hate speech and oensive lan-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1676,"guage and transfer learning is presented.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1677,"Online Hate Speech and Oensive Language: Researchers have been study-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1678,"ing hate speech on social media platforms such as Twitter [3], Reddit [12, 14],
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1679,"and YouTube [15] in the past few years. The features used in traditional ma-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1680,"chine learning approaches are the main aspects distinguishing dierent methods,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1681,"and surface-level features such as bag of words, word-level and character-level
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1682,"n-grams, etc. have proven to be the most predictive features [11, 13, 22]. Apart
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1683,"from features, dierent algorithms such as Support Vector Machines [10], Naive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1684,"Baye [16], and Logistic Regression [3,22], etc. have been applied for classication
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1685,"purposes. Waseem et al. [22] provided a test with a list of criteria based on the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1686,"work in Gender Studies and Critical Race Theory (CRT) that can annotate a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1687,"corpus of more than 16 ktweets as racism, sexism, or neither. To classify tweets,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1688,"they used a logistic regression model with dierent sets of features, such as word
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1689,"and character n-grams up to 4, gender, length, and location. They found that
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1690,"their best model produces character n-gram as the most indicative features, and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1691,"using location or length is detrimental. Davidson et al. [3] collected a 24 Kcor-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1692,"pus of tweets containing hate speech keywords and labelled the corpus as hate
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1693,"speech, oensive language, or neither by using crowd-sourcing and extracted
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1694,"dierent features such as n-grams, some tweet-level metadata such as the num-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1695,"ber of hashtags, mentions, retweets, and URLs, Part Of Speech (POS) tagging,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1696,"etc. Their experiments on dierent multi-class classiers showed that the Logis-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1697,"tic Regression with L2 regularization performs the best at this task. Malmasi et
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1698,"al. [10] proposed an ensemble-based system that uses some linear SVM classiers
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1699,"in parallel to distinguish hate speech from general profanity in social media.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1700,"As one of the rst attempts in neural network models, Djuric et al. [5] pro-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1701,"posed a two-step method including a continuous bag of words model to extract
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1702,"paragraph2vec embeddings and a binary classier trained along with the embed-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1703,"dings to distinguish between hate speech and clean content. Badjatiya et al. [1]
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1704,"investigated three deep learning architectures, FastText, CNN, and LSTM, in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1705,"which they initialized the word embeddings with either random or GloVe em-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1706,"beddings. Gambck et al. [8] proposed a hate speech classier based on CNN
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1707,"model trained on dierent feature embeddings such as word embeddings and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1708,"character n-grams. Zhang et al. [25] used a CNN+GRU (Gated Recurrent Unit
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1709,"network) neural network model initialized with pre-trained word2vec embed-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1710,"dings to capture both word/character combinations (e. g., n-grams, phrases) and4 Marzieh Mozafari et al.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1711,"word/character dependencies (order information). Waseem et al. [23] brought a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1712,"new insight to hate speech and abusive language detection tasks by proposing
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1713,"a multi-task learning framework to deal with datasets across dierent annota-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1714,"tion schemes, labels, or geographic and cultural in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1715,"uences from data sampling.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1716,"Founta et al. [7] built a unied classication model that can eciently han-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1717,"dle dierent types of abusive language such as cyberbullying, hate, sarcasm,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1718,"etc. using raw text and domain-specic metadata from Twitter. Furthermore,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1719,"researchers have recently focused on the bias derived from the hate speech train-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1720,"ing datasets [2,21,24]. Davidson et al. [2] showed that there were systematic and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1721,"substantial racial biases in ve benchmark Twitter datasets annotated for oen-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1722,"sive language detection. Wiegand et al. [24] also found that classiers trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1723,"on datasets containing more implicit abuse (tweets with some abusive words)
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1724,"are more aected by biases rather than once trained on datasets with a high
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1725,"proportion of explicit abuse samples (tweets containing sarcasm, jokes, etc.).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1726,"Transfer Learning: Pre-trained vector representations of words, embeddings,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1727,"extracted from vast amounts of text data have been encountered in almost every
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1728,"language-based tasks with promising results. Two of the most frequently used
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1729,"context-independent neural embeddings are word2vec and Glove extracted from
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1730,"shallow neural networks. The year 2018 has been an in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1731,"ection point for dier-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1732,"ent NLP tasks thanks to remarkable breakthroughs: Universal Language Model
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1733,"Fine-Tuning (ULMFiT) [9], Embedding from Language Models (ELMO) [17],
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1734,"OpenAI s Generative Pre-trained Transformer (GPT) [18], and Googles BERT
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1735,"model [4]. Howard et al. [9] proposed ULMFiT which can be applied to any
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1736,"NLP task by pre-training a universal language model on a general-domain cor-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1737,"pus and then ne-tuning the model on target task data using discriminative
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1738,"ne-tuning. Peters et al. [17] used a bi-directional LSTM trained on a specic
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1739,"task to present context-sensitive representations of words in word embeddings by
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1740,"looking at the entire sentence. Radford et al. [18] and Devlin et al. [4] generated
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1741,"two transformer-based language models, OpenAI GPT and BERT respectively.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1742,"OpenAI GPT [18] is an unidirectional language model while BERT [4] is the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1743,"rst deeply bidirectional, unsupervised language representation, pre-trained us-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1744,"ing only a plain text corpus. BERT has two novel prediction tasks: Masked LM
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1745,"and Next Sentence Prediction. The pre-trained BERT model signicantly out-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1746,"performed ELMo and OpenAI GPT in a series of downstream tasks in NLP [4].
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1747,"Identifying hate speech and oensive language is a complicated task due to the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1748,"lack of undisputed labelled data [10] and the inability of surface features to cap-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1749,"ture the subtle semantics in text. To address this issue, we use the pre-trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1750,"language model BERT for hate speech classication and try to ne-tune specic
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1751,"task by leveraging information from dierent transformer encoders.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1752,"3 Methodology
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1753,"Here, we analyze the BERT transformer model on the hate speech detection
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1754,"task. BERT is a multi-layer bidirectional transformer encoder trained on the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1755,"English Wikipedia and the Book Corpus containing 2,500M and 800M tokens,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1756,"respectively, and has two models named BERT baseand BERT large. BERT baseA BERT-based hate speech detection approach in social media 5
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1757,"contains an encoder with 12 layers (transformer blocks), 12 self-attention heads,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1758,"and 110 million parameters whereas BERT largehas 24 layers, 16 attention heads,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1759,"and 340 million parameters. Extracted embeddings from BERT basehave 768 hid-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1760,"den dimensions [4]. As the BERT model is pre-trained on general corpora, and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1761,"for our hate speech detection task we are dealing with social media content,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1762,"therefore as a crucial step, we have to analyze the contextual information ex-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1763,"tracted from BERT' s pre-trained layers and then ne-tune it using annotated
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1764,"datasets. By ne-tuning we update weights using a labelled dataset that is new
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1765,"to an already trained model. As an input and output, BERT takes a sequence
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1766,"of tokens in maximum length 512 and produces a representation of the sequence
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1767,"in a 768-dimensional vector. BERT inserts at most two segments to each input
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1768,"sequence, [CLS] and [SEP]. [CLS] embedding is the rst token of the input se-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1769,"quence and contains the special classication embedding which we take the rst
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1770,"token [CLS] in the nal hidden layer as the representation of the whole sequence
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1771,"in hate speech classication task. The [SEP] separates segments and we will not
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1772,"use it in our classication task. To perform the hate speech detection task, we
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1773,"use BERT basemodel to classify each tweet as Racism, Sexism, Neither or Hate,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1774,"Oensive, Neither in our datasets. In order to do that, we focus on ne-tuning
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1775,"the pre-trained BERT baseparameters. By ne-tuning, we mean training a clas-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1776,"sier with dierent layers of 768 dimensions on top of the pre-trained BERT base
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1777,"transformer to minimize task-specic parameters.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1778,"3.1 Fine-Tuning Strategies
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1779,"Dierent layers of a neural network can capture dierent levels of syntactic and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1780,"semantic information. The lower layer of the BERT model may contain more gen-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1781,"eral information whereas the higher layers contain task-specic information [4],
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1782,"and we can ne-tune them with dierent learning rates. Here, four dierent ne-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1783,"tuning approaches are implemented that exploit pre-trained BERT basetrans-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1784,"former encoders for our classication task. More information about these trans-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1785,"former encoders' architectures are presented in [4]. In the ne-tuning phase, the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1786,"model is initialized with the pre-trained parameters and then are ne-tuned us-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1787,"ing the labelled datasets. Dierent ne-tuning approaches on the hate speech
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1788,"detection task are depicted in Figure 1, in which Xiis the vector representation
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1789,"of token iin a tweet sample, and are explained in more detail as follows:
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1790,"1. BERT based ne-tuning: In the rst approach, which is shown in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1791,"Figure 1a, very few changes are applied to the BERT base. In this architecture,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1792,"only the [CLS] token output provided by BERT is used. The [CLS] output, which
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1793,"is equivalent to the [CLS] token output of the 12th transformer encoder, a vector
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1794,"of size 768, is given as input to a fully connected network without hidden layer.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1795,"The softmax activation function is applied to the hidden layer to classify.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1796,"2. Insert nonlinear layers: Here, the rst architecture is upgraded and an
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1797,"architecture with a more robust classier is provided in which instead of using
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1798,"a fully connected network without hidden layer, a fully connected network with
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1799,"two hidden layers in size 768 is used. The rst two layers use the Leaky Relu6 Marzieh Mozafari et al.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1800,"(a) BERT basene-tuning
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1801," (b) Insert nonlinear layers
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1802," (c) Insert Bi-LSTM layer
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1803," (d) Insert CNN layer
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1804,"Fig. 1: Fine-tuning strategies
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1805,"activation function with negative slope = 0.01, but the nal layer, as the rst
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1806,"architecture, uses softmax activation function as shown in Figure 1b.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1807,"3. Insert Bi-LSTM layer: Unlike previous architectures that only use
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1808,"[CLS] as the input for the classier, in this architecture all outputs of the latest
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1809,"transformer encoder are used in such a way that they are given as inputs to a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1810,"bidirectional recurrent neural network (Bi-LSTM) as shown in Figure 1c. After
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1811,"processing the input, the network sends the nal hidden state to a fully connected
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1812,"network that performs classication using the softmax activation function.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1813,"4. Insert CNN layer: In this architecture shown in Figure 1d, the outputs
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1814,"of all transformer encoders are used instead of using the output of the latest
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1815,"transformer encoder. So that the output vectors of each transformer encoder
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1816,"are concatenated, and a matrix is produced. The convolutional operation is per-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1817,"formed with a window of size (3, hidden size of BERT which is 768 in BERT base
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1818,"model) and the maximum value is generated for each transformer encoder by
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1819,"applying max pooling on the convolution output. By concatenating these values,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1820,"a vector is generated which is given as input to a fully connected network. By
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1821,"applying softmax on the input, the classication operation is performed.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1822,"4 Experiments and Results
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1823,"We rst introduce datasets used in our study and then investigate the dierent
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1824,"ne-tuning strategies for hate speech detection task. We also include the details
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1825,"of our implementation and error analysis in the respective subsections.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1826,"4.1 Dataset Description
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1827,"We evaluate our method on two widely-studied datasets provided by Waseem
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1828,"and Hovey [22] and Davidson et al. [3]. Waseem and Hovy [22] collected 16 k
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1829,"of tweets based on an initial ad-hoc approach that searched common slurs and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1830,"terms related to religious, sexual, gender, and ethnic minorities. They annotated
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1831,"their dataset manually as racism, sexism, or neither. To extend this dataset,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1832,"Waseem [20] also provided another dataset containing 6 :9kof tweets annotated
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1833,"with both expert and crowdsourcing users as racism, sexism, neither, or both.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1834,"Since both datasets are overlapped partially and they used the same strategyA BERT-based hate speech detection approach in social media 7
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1835,"in denition of hateful content, we merged these two datasets following Waseem
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1836,"et al. [23] to make our imbalance data a bit larger. Davidson et al. [3] used
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1837,"the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1838,"containing particular terms from a pre-dened lexicon of hate speech words and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1839,"phrases, called Hatebased.org. To annotate collected tweets as Hate, Oensive,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1840,"or Neither, they randomly sampled 25 ktweets and asked users of CrowdFlower
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1841,"crowdsourcing platform to label them. In detail, the distribution of dierent
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1842,"classes in both datasets will be provided in Subsection 4.3.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1843,"4.2 Pre-Processing
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1844,"We nd mentions of users, numbers, hashtags, URLs and common emoticons and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1845,"replace them with the tokens <user>,<number >,<hashtag >,<url>,<emoticon >.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1846,"We also nd elongated words and convert them into short and standard format;
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1847,"for example, converting yeeeessss to yes. With hashtags that include some to-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1848,"kens without any with space between them, we replace them by their textual
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1849,"counterparts; for example, we convert hashtag \#notsexist"" to \not sexist"". All
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1850,"punctuation marks, unknown uni-codes and extra delimiting characters are re-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1851,"moved, but we keep all stop words because our model trains the sequence of
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1852,"words in a text directly. We also convert all tweets to lower case.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1853,"4.3 Implementation and Results Analysis
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1854,"For the implementation of our neural network, we used pytorch-pretrained-bert
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1855,"library containing the pre-trained BERT model, text tokenizer, and pre-trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1856,"WordPiece. As the implementation environment, we use Google Colaboratory
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1857,"tool which is a free research tool with a Tesla K80 GPU and 12G RAM. Based
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1858,"on our experiments, we trained our classier with a batch size of 32 for 3 epochs.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1859,"The dropout probability is set to 0.1 for all layers. Adam optimizer is used
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1860,"with a learning rate of 2e-5. As an input, we tokenized each tweet with the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1861,"BERT tokenizer. It contains invalid characters removal, punctuation splitting,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1862,"and lowercasing the words. Based on the original BERT [4], we split words to
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1863,"subword units using WordPiece tokenization. As tweets are short texts, we set
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1864,"the maximum sequence length to 64 and in any shorter or longer length case it
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1865,"will be padded with zero values or truncated to the maximum length.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1866,"We consider 80% of each dataset as training data to update the weights
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1867,"in the ne-tuning phase, 10% as validation data to measure the out-of-sample
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1868,"performance of the model during training, and 10% as test data to measure
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1869,"the out-of-sample performance after training. To prevent overtting, we use
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1870,"stratied sampling to select 0.8, 0.1, and 0.1 portions of tweets from each class
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1871,"(racism/sexism/neither or hate/oensive/neither) for train, validation, and test.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1872,"Classes' distribution of train, validation, and test datasets are shown in Table 1.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1873,"As it is understandable from Tables 1(a) and 1(b), we are dealing with im-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1874,"balance datasets with various classes distribution. Since hate speech and oen-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1875,"sive languages are real phenomena, we did not perform oversampling or under-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1876,"sampling techniques to adjust the classes distribution and tried to supply the8 Marzieh Mozafari et al.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1877,"Table 1: Dataset statistics of the both Waseem-dataset (a) and Davidson-dataset (b).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1878,"Splits are produced using stratied sampling to select 0.8, 0.1, and 0.1 portions of tweets
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1879,"from each class (racism/sexism/neither or hate/oensive/neither) for train, validation,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1880,"and test samples, respectively.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1881,"Racism Sexism Neither Total
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1882,"Train 1693 3337 10787 15817
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1883,"Validation 210 415 1315 1940
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1884,"Test 210 415 1315 1940
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1885,"Total 2113 4167 13417
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1886,"(a) Waseem-dataset.Hate Oensive Neither Total
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1887,"Train 1146 15354 3333 19832
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1888,"Validation 142 1918 415 2475
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1889,"Test 142 1918 415 2475
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1890,"Total 1430 19190 4163
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1891,"(b) Davidson-dataset.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1892,"datasets as realistic as possible. We evaluate the eect of dierent ne-tuning
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1893,"strategies on the performance of our model. Table 2 summarized the obtained re-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1894,"sults for ne-tuning strategies along with the ocial baselines. We use Waseem
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1895,"and Hovy [22], Davidson et al. [3], and Waseem et al. [23] as baselines and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1896,"compare the results with our dierent ne-tuning strategies using pre-trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1897,"BERT basemodel. The evaluation results are reported on the test dataset and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1898,"on three dierent metrics: precision, recall, and weighted-average F1-score. We
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1899,"consider weighted-average F1-score as the most robust metric versus class imbal-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1900,"ance, which gives insight into the performance of our proposed models. According
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1901,"to Table 2, F1-scores of all BERT based ne-tuning strategies except BERT +
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1902,"nonlinear classier on top of BERT are higher than the baselines. Using the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1903,"pre-trained BERT model as initial embeddings and ne-tuning the model with a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1904,"fully connected linear classier (BERT base) outperforms previous baselines yield-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1905,"ing F1-score of 81% and 91% for datasets of Waseem and Davidson respectively.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1906,"Inserting a CNN to pre-trained BERT model for ne-tuning on downstream task
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1907,"provides the best results as F1- score of 88% and 92% for datasets of Waseem and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1908,"Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1909,"combining all pre-trained BERT layers with a CNN yields better results in which
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1910,"our model uses all the information included in dierent layers of pre-trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1911,"BERT during the ne-tuning phase. This information contains both syntactical
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1912,"and contextual features coming from lower layers to higher layers of BERT.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1913,"Table 2: Results on the trial data using pre-trained BERT model with dierent ne-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1914,"tuning strategies and comparison with results in the literature.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1915,"Method Datasets Precision(%) Recall(%) F1-Score(%)
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1916,"Waseem and Hovy [22] Waseem 72.87 77.75 73.89
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1917,"Davidson et al. [3] Davidson 91 90 90
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1918,"Waseem et al. [23]Waseem - - 80
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1919,"Davidson - - 89
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1920,"BERT baseWaseem 81 81 81
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1921,"Davidson 91 91 91
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1922,"BERT base+ Nonlinear LayersWaseem 73 85 76
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1923,"Davidson 76 78 77
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1924,"BERT base+ LSTMWaseem 87 86 86
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1925,"Davidson 91 92 92
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1926,"BERT base+ CNNWaseem 89 87 88
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1927,"Davidson 92 92 92A BERT-based hate speech detection approach in social media 9
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1928,"4.4 Error Analysis
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1929,"Although we have very interesting results in term of recall, the precision of the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1930,"model shows the portion of false detection we have. To understand better this
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1931,"phenomenon, in this section we perform a deep analysis on the error of the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1932,"model. We investigate the test datasets and their confusion matrices resulted
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1933,"from the BERT base+ CNN model as the best ne-tuning approach; depicted in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1934,"Figures 2 and 3. According to Figure 2 for Waseem-dataset, it is obvious that
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1935,"the model can separate sexism from racism content properly. Only two samples
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1936,"belonging to racism class are misclassied as sexism and none of the sexism
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1937,"samples are misclassied as racism. A large majority of the errors come from
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1938,"misclassifying hateful categories (racism and sexism) as hatless (neither) and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1939,"vice versa. 0.9% and 18.5% of all racism samples are misclassied as sexism and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1940,"neither respectively whereas it is 0% and 12.7% for sexism samples. Almost 12%
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1941,"of neither samples are misclassied as racism or sexism. As Figure 3 makes clear
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1942,"for Davidson-dataset, the majority of errors are related to hate class where the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1943,"model misclassied hate content as oensive in 63% of the cases. However, 2.6%
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1944,"and 7.9% of oensive and neither samples are misclassied respectively.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1945,"Fig. 2: Waseem-datase's confusion matrix
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1946," Fig. 3: Davidson-dataset's confusion matrix
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1947,"To understand better the mislabeled items by our model, we did a manual
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1948,"inspection on a subset of the data and record some of them in Tables 3 and 4.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1949,"Considering the words such as \daughters"", \women"", and \burka"" in tweets
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1950,"with IDs 1 and 2 in Table 3, it can be understood that our BERT based classi-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1951,"er is confused with the contextual semantic between these words in the samples
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1952,"and misclassied them as sexism because they are mainly associated to feminin-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1953,"ity. In some cases containing implicit abuse (like subtle insults) such as tweets
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1954,"with IDs 5 and 7, our model cannot capture the hateful/oensive content and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1955,"therefore misclassies. It should be noticed that even for a human it is dicult
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1956,"to discriminate against this kind of implicit abuses.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1957,"By examining more samples and with respect to recently studies [2, 19, 24],
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1958,"it is clear that many errors are due to biases from data collection [24] and rules
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1959,"of annotation [19] and not the classier itself. Since Waseem et al. [22] created a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1960,"small ad-hoc set of keywords and Davidson et al. [3] used a large crowdsourced10 Marzieh Mozafari et al.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1961,"Table 3: Misclassied samples from Waseem-dataset.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1962,"ID Tweet Annotated Predicted
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1963,"1 @user Good tweet. But they actually start selling their daughters at 9. Racism Sexism
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1964,"2RT @user: Are we going to continue seeing the oppression of women or are we
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1965,"going to make a stand? #BanTheBurka http://t.co/hZDx8mlvTv.Racism Sexism
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1966,"3RT @user: @user my comment was sexist, but I'm not personally, always a sexist. Sexism Neither
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1967,"4RT @user: @user Ah, you're a #feminist? Seeing #sexism everywhere then, do
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1968,"check my tweets before you call me #sexistSexism Neither
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1969,"5 @user By hating the ideology that enables it, that is what I'm doing. Racism Neither
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1970,"Table 4: Misclassied samples from Davidson-dataset.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1971,"ID Tweet Annotated Predicted
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1972,"6@user: If you claim Macklemore is your favorite rapper I'm also assuming you
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1973,"watch the WNBA on your free time faggotHate Oensive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1974,"7@user: Some black guy at my school asked if there were colored printers in the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1975,"library. ""It's 2014 man you can use any printer you want I said.Hate Neither
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1976,"8 RT @user: @user typical coon activity. Hate Neither
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1977,"9@user: @user @user White people need those weapons to defend themselves
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1978,"from the subhuman trash your sort unleashes on us.Neither Hate
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1979,"10RT @user: Finally! Warner Bros. making superhero lms starring a woman,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1980,"person of color and actor who identies as """"queer"""";Neither Oensive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1981,"dictionary of keywords (Hatebase lexicon) to sample tweets for training, they in-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1982,"cluded some biases in the collected data. Especially for Davidson-dataset, some
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1983,"tweets with specic language (written within the African American Vernacular
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1984,"English) and geographic restriction (United States of America) are oversam-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1985,"pled such as tweets containing disparage words \nigga"", \faggot"", \coon"", or
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1986,"\queer"", result in high rates of misclassication. However, these misclassica-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1987,"tions do not conrm the low performance of our classier because annotators
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1988,"tended to annotate many samples containing disrespectful words as hate or of-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1989,"fensive without any presumption about the social context of tweeters such as
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1990,"the speakers identity or dialect, whereas they were just oensive or even neither
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1991,"tweets. Tweets IDs 6, 8, and 10 are some samples containing oensive words and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1992,"slurs which arenot hate or oensive in all cases and writers of them used this
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1993,"type of language in their daily communications. Given these pieces of evidence,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1994,"by considering the content of tweets, we can see in tweets IDs 3, 4, and 9 that
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1995,"our BERT-based classier can discriminate tweets in which neither and implicit
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1996,"hatred content exist. One explanation of this observation may be the pre-trained
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1997,"general knowledge that exists in our model. Since the pre-trained BERT model
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1998,"is trained on general corpora, it has learned general knowledge from normal tex-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
1999,"tual data without any purposely hateful or oensive language. Therefore, despite
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2000,"the bias in the data, our model can dierentiate hate and oensive samples ac-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2001,"curately by leveraging knowledge-aware language understanding that it has and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2002,"it can be the main reason for high misclassications of hate samples as oensive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2003,"(in reality they are more similar to oensive rather than hate by considering
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2004,"social context, geolocation, and dialect of tweeters).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2005,"5 Conclusion
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2006,"Con
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2007,"ating hatred content with oensive or harmless language causes online au-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2008,"tomatic hate speech detection tools to 
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2009,"ag user-generated content incorrectly.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2010,"Not addressing this problem may bring about severe negative consequences forA BERT-based hate speech detection approach in social media 11
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2011,"both platforms and users such as decreasement of platforms' reputation or users
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2012,"abandonment. Here, we propose a transfer learning approach advantaging the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2013,"pre-trained language model BERT to enhance the performance of a hate speech
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2014,"detection system and to generalize it to new datasets. To that end, we introduce
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2015,"new ne-tuning strategies to examine the eect of dierent layers of BERT in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2016,"hate speech detection task. The evaluation results indicate that our model out-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2017,"performs previous works by proting the syntactical and contextual information
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2018,"embedded in dierent transformer encoder layers of the BERT model using a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2019,"CNN-based ne-tuning strategy. Furthermore, examining the results shows the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2020,"ability of our model to detect some biases in the process of collecting or anno-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2021,"tating datasets. It can be a valuable clue in using the pre-trained BERT model
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2022,"to alleviate bias in hate speech datasets in future studies, by investigating a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2023,"mixture of contextual information embedded in the BERTs layers and a set of
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2024,"features associated to the dierent type of biases in data.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2025,"References
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2026,"1. Badjatiya, P., Gupta, S., Gupta, M., et al.: Deep learning for hate speech detection
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2027,"in tweets. CoRR abs/1706.00188 (2017). URL http://arxiv.org/abs/1706.00188
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2028,"2. Davidson, T., Bhattacharya, D., Weber, I.: Racial bias in hate speech and abusive
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2029,"language detection datasets. CoRR abs/1905.12516 (2019). URL http://arxiv.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2030,"org/abs/1905.12516
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2031,"3. Davidson, T., Warmsley, D., Macy, M.W., et al.: Automated hate speech detection
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2032,"and the problem of oensive language. CoRR abs/1703.04009 (2017). URL
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2033,"http://arxiv.org/abs/1703.04009
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2034,"4. Devlin, J., Chang, M., Lee, K., et al.: BERT: pre-training of deep bidirectional
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2035,"transformers for language understanding. CoRR abs/1810.04805 (2018). URL
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2036,"http://arxiv.org/abs/1810.04805
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2037,"5. Djuric, N., Zhou, J., Morris, R., et al.: Hate speech detection with comment em-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2038,"beddings. In: Proceedings of the 24th International Conference on World Wide
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2039,"Web, WWW '15 Companion, pp. 29{30. ACM, New York, NY, USA (2015). DOI
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2040,"10.1145/2740908.2742760
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2041,"6. Fortuna, P., Nunes, S.: A survey on automatic detection of hate speech in text.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2042,"ACM Comput. Surv. 51(4), 85:1{85:30 (2018). DOI 10.1145/3232676
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2043,"7. Founta, A.M., Chatzakou, D., Kourtellis, N., et al.: A unied deep learning archi-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2044,"tecture for abuse detection. In: Proceedings of the 10th ACM Conference on Web
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2045,"Science, WebSci '19, pp. 105{114. ACM, New York, NY, USA (2019)
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2046,"8. Gamb ack, B., Sikdar, U.K.: Using convolutional neural networks to classify hate-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2047,"speech. In: Proceedings of the First Workshop on Abusive Language Online, pp.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2048,"85{90. Association for Computational Linguistics, Vancouver, BC, Canada (2017).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2049,"DOI 10.18653/v1/W17-3013
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2050,"9. Howard, J., Ruder, S.: Fine-tuned language models for text classication. CoRR
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2051,"abs/1801.06146 (2018). URL http://arxiv.org/abs/1801.06146
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2052,"10. Malmasi, S., Zampieri, M.: Challenges in discriminating profanity from hate speech.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2053,"CoRR abs/1803.05495 (2018). URL http://arxiv.org/abs/1803.05495
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2054,"11. Mehdad, Y., Tetreault, J.: Do characters abuse more than words? In: Proceedings
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2055,"of the 17th Annual Meeting of the Special Interest Group on Discourse and Dia-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2056,"logue, pp. 299{303. Association for Computational Linguistics, Los Angeles (2016).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2057,"DOI 10.18653/v1/W16-3638
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2058,"12. Mittos, A., Zannettou, S., Blackburn, J., et al.: ""And We Will Fight For Our
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2059,"Race!"" A measurement Study of Genetic Testing Conversations on Reddit and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2060,"4chan. CoRR abs/1901.09735 (2019). URL http://arxiv.org/abs/1901.0973512 Marzieh Mozafari et al.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2061,"13. Nobata, C., Tetreault, J., Thomas, A., et al.: Abusive language detection in online
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2062,"user content. In: Proceedings of the 25th International Conference on World Wide
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2063,"Web, WWW '16, pp. 145{153. International World Wide Web Conferences Steering
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2064,"Committee, Republic and Canton of Geneva, Switzerland (2016). DOI 10.1145/
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2065,"2872427.2883062
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2066,"14. Olteanu, A., Castillo, C., Boy, J., et al.: The eect of extremist violence on hateful
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2067,"speech online. CoRR abs/1804.05704 (2018). URL http://arxiv.org/abs/1804.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2068,"05704
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2069,"15. Ottoni, R., Cunha, E., Magno, G., et al.: Analyzing right-wing youtube channels:
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2070,"Hate, violence and discrimination. In: Proceedings of the 10th ACM Conference
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2071,"on Web Science, WebSci '18, pp. 323{332. ACM, New York, NY, USA (2018).
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2072,"DOI 10.1145/3201064.3201081
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2073,"16. Pete, B., L., W.M.: Cyber hate speech on twitter: An application of machine clas-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2074,"sication and statistical modeling for policy and decision making. Policy and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2075,"Internet 7(2), 223242 (2015). DOI 10.1002/poi3.85
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2076,"17. Peters, M.E., Neumann, M., Iyyer, M., et al.: Deep contextualized word represen-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2077,"tations. CoRR abs/1802.05365 (2018). URL http://arxiv.org/abs/1802.05365
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2078,"18. Radford, A.: Improving language understanding by generative pre-training (2018)
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2079,"19. Sap, M., Card, D., Gabriel, S., et al.: The risk of racial bias in hate speech detection.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2080,"In: Proceedings of the 57th Annual Meeting of the Association for Computational
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2081,"Linguistics, pp. 1668{1678. Association for Computational Linguistics, Florence,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2082,"Italy (2019). DOI 10.18653/v1/P19-1163
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2083,"20. Waseem, Z.: Are you a racist or am I seeing things? annotator in
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2084,"uence on hate
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2085,"speech detection on twitter. In: Proceedings of the First Workshop on NLP and
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2086,"Computational Social Science, pp. 138{142. Association for Computational Lin-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2087,"guistics, Austin, Texas (2016). DOI 10.18653/v1/W16-5618
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2088,"21. Waseem, Z., Davidson, T., Warmsley, D., et al.: Understanding abuse: A typology
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2089,"of abusive language detection subtasks. In: Proceedings of the First Workshop on
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2090,"Abusive Language Online, pp. 78{84. Association for Computational Linguistics,
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2091,"Vancouver, BC, Canada (2017). DOI 10.18653/v1/W17-3012. URL https://www.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2092,"aclweb.org/anthology/W17-3012
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2093,"22. Waseem, Z., Hovy, D.: Hateful symbols or hateful people? predictive features for
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2094,"hate speech detection on twitter. In: Proceedings of the NAACL Student Research
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2095,"Workshop, pp. 88{93. Association for Computational Linguistics, San Diego, Cal-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2096,"ifornia (2016). DOI 10.18653/v1/N16-2013
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2097,"23. Waseem, Z., Thorne, J., Bingel, J.: Bridging the Gaps: Multi Task Learning for
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2098,"Domain Transfer of Hate Speech Detection, pp. 29{55. Springer International
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2099,"Publishing, Cham (2018). DOI 10.1007/978-3-319-78583-7 3
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2100,"24. Wiegand, M., Ruppenhofer, J., Kleinbauer, T.: Detection of Abusive Language:
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2101,"the Problem of Biased Datasets. In: Proceedings of the 2019 Conference of the
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2102,"North American Chapter of the Association for Computational Linguistics: Hu-
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2103,"man Language Technologies, Volume 1 (Long and Short Papers), pp. 602{608.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2104,"Association for Computational Linguistics, Minneapolis, Minnesota (2019). DOI
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2105,"10.18653/v1/N19-1060
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2106,"25. Zhang, Z., Robinson, D., Tepper, J.: Detecting hate speech on twitter using a
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2107,"convolution-gru based deep neural network. In: The Semantic Web, pp. 745{760.
",False,A BERT-Based Transfer Learning Approach for,False,False,True
2108,"Springer International Publishing, Cham (2018)",False,A BERT-Based Transfer Learning Approach for,False,False,True
2109,"Springer International Publishing, Cham (2018)",False,A BERT-Based Transfer Learning Approach for,True,False,True
2110,"1 INTRODUCTION
",True,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2111,"Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
",True,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2112,"4EmpiricalMethods inNaturalLanguageProcessing.
",True,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2113,"6AAAI Conferenceon Artificial Intelligence.
",True,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2114,"2.2 Deep-learning in NLP
",True,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2115,"3 FROM IMAGETO TEXT
",True,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2116,"24
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2117,"AdversarialAttackson Deep-learningModels in Natural
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2118,"Language Processing: A Survey
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2119,"WEIEMMA ZHANG, The Universityof Adelaide, Australia
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2120,"QUAN Z.SHENG and AHOUD ALHAZMI, Macquarie University,Australia
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2121,"CHENLIANG LI, Wuhan University,China
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2122,"With the development of high computational devices, deep neural networks (DNNs), in recent years, have
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2123,"gainedsignificantpopularityinmanyArtificialIntelligence(AI)applications.However,previouseffortshave
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2124,"shown that DNNs are vulnerable to strategically modified samples, named adversarial examples . These sam-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2125,"ples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2126,"Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), re-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2127,"searcheffortsonattackingDNNsforNaturalLanguageProcessing(NLP)applicationshaveemergedinrecent
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2128,"years.However,theintrinsicdifferencebetweenimage(CV)andtext(NLP)renderschallengestodirectlyap-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2129,"ply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2130,"widerangeofNLPapplications.Inthisarticle,wepresentasystematicsurveyontheseworks.Wecollectall
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2131,"related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2132,"40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2133,"knowledgeofNLPanddiscussrelatedseminalworksincomputervision.Weconcludeoursurveywithadis-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2134,"cussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2135,"on NLP DNNs.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2136,"CCSConcepts:• Computingmethodologies →Natural languageprocessing ;
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2137,"Additional Key Words and Phrases: Deep neural networks, adversarial examples, textual data, natural lan-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2138,"guage processing
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2139,"ACM Reference format:
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2140,"Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2141,"learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol. 11, 3, Article 24
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2142,"(March 2020), 41 pages.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2143,"https://doi.org/10.1145/3374217
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2144,"1 INTRODUCTION
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2145,"Deepneuralnetworks(DNNs)are large neuralnetworkswhosearchitectureis organized as a se-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2146,"ries of layers of neurons, each of which serves as the individual computing units. Neurons are
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2147,"connected by links with different weights and biases and transmit the results of its activation
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2148,"Authors’ addresses: W. E. Zhang, School of Computer Science, The University of Adelaide, Sydney, Australia, SA 5005;
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2149,"email: wei.e.zhang@adelaide.edu.au; Q. Z. Sheng and A. Alhazmi, Department of Computing, Macquarie University, Aus-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2150,"tralia, NSW 2109; emails: michael.sheng@mq.edu.au, ahoud.alhazmi@hdr.mq.edu.au; C. Li, School of Cyber Science and
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2151,"Engineering,Wuhan University, Wuhan,China, 430072;email:cllee@whu.edu.cn.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2152,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2153,"provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2154,"the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2155,"prior specific permission and/or a fee.Request permissions from permissions@acm.org .
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2156,"© 2020Association forComputingMachinery.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2157,"2157-6904/2020/03-ART24$15.00
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2158,"https://doi.org/10.1145/3374217
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2159,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:2 W.E.Zhanget al.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2160,"functiononitsinputstotheneuronsofthenextlayer.Deepneuralnetworkstrytomimicthebio-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2161,"logicalneuralnetworksofhumanbrainstolearnandbuildknowledgefromexamples.Thus,they
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2162,"demonstrate the strengths in dealing with complicated tasks that are not easily to be modelled as
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2163,"linear or non-linear problems. Furthermore, empowered by continuous real-valued vector repre-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2164,"sentations (i.e., embeddings) they are good at handling data with various modalities, e.g., image,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2165,"text, video,andaudio.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2166,"With the development of high computational devices, deep neural networks in recent years
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2167,"have gained tremendous attention in many Artificial Intelligence (AI) communities such as Com-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2168,"puter Vision [ 65,126], Natural Language Processing [ 16,66], Web Mining [ 101,149], and Game
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2169,"theory [119]. However, the interpretability of deep neural networks is still unsatisfactory as they
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2170,"work as black boxes, which means it is difficult to get intuitions from what each neuron exactly
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2171,"has learned. One of the problems of the poor interpretability is evaluating the robustness of deep
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2172,"neural networks. In recent years, researchers [ 40,132] used small unperceivable perturbations
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2173,"to evaluate the robustness of deep neural networks and found that they are not robust to these
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2174,"perturbations. Szegedy et al. [ 132] first evaluated the state-of-the-art deep neural networks used
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2175,"for image classification with small generated perturbations on the input images. They found that
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2176,"the image classifiers were fooled with high probability, but human judgment is not affected. The
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2177,"perturbedimagepixelswerenamed adversarialexamples andthisnotationislaterusedtodenote
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2178,"all kinds of perturbed samples in a general manner. As the generation of adversarial examples is
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2179,"costlyandimpracticalinReference[ 132],Goodfellowetal.[ 40]proposedafastgenerationmethod
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2180,"that popularized this research topic (Section 3.1provides further discussion on these works). Fol-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2181,"lowedtheirworks,manyresearcheffortshavebeenmadeandthepurposesoftheseworkscanbe
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2182,"summarized as: (i) evaluating the deep neural networks by fooling them with unperceivable per-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2183,"turbations; (ii) intentionally changing the output of the deep neural networks; and (iii) detecting
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2184,"the oversensitivity and over-stability points of the deep neural networks and finding solutions to
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2185,"defensetheattack.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2186,"JiaandLiang[ 54]arethefirsttoconsideradversarialexamplegeneration(or adversarialattack ,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2187,"we will use these two expressions interchangeably hereafter) on deep neural networks for text-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2188,"basedtasks(namely, textualdeepneuralnetworks ).Theirworkquicklygainedresearchattentionin
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2189,"theNaturalLanguageProcessing(NLP)community.However,duetointrinsicdifferencesbetween
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2190,"imagesandtextualdata,theadversarialattackmethodsonimagescannotbedirectlyappliedtothe
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2191,"latter.Firstofall,imagedata(e.g.,pixelvalues)iscontinuous,buttextualdataisdiscreteinnature.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2192,"Conventionally, we vectorizethe texts before inputtingtheminto the deep neuralnetworks.Tra-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2193,"ditional vectoring methods include leveraging term frequency and inverse document frequency,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2194,"and one-hot representation (details in Section 3.3). When applying gradient-based adversarial at-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2195,"tacks adopted from images on these representations, the generated adversarial examples are in-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2196,"validcharactersorwordsequences[ 157].Onesolutionistousewordembeddingsastheinputof
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2197,"deep neural networks. However, this will also generate words that can not be matched with any
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2198,"words in the word embedding space [ 38]. Second, the perturbation of images are small change of
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2199,"pixel values that are hard to be perceived by human eyes, thus humans can correctly classify the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2200,"images, showing the poor robustness of deep neural models. But for adversarial attack on texts,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2201,"smallperturbationsareeasilyperceptible.Forexample,replacementofcharactersorwordswould
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2202,"generate invalid words or syntactically incorrect sentences. Further, it would alter the semantics
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2203,"of the sentence drastically. Therefore, the perturbations are easily to be perceived–in this case,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2204,"even human beingcannotprovide correctpredictions.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2205,"To address the aforementioned differences and challenges, many attacking methods have been
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2206,"proposed since the pioneer work of Jia and Liang [ 54]. Despite the popularity of the topic in the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2207,"NLPcommunity,thereisnocomprehensivereviewpaperthatcollectsandsummarizestheefforts
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2208,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:3
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2209,"in this research direction. There is a need for this kind of work that helps successive researchers
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2210,"andpractitionersto haveanoverview ofthesemethods.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2211,"Related surveys and the differences to this survey. In Reference [ 9], the authors presented
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2212,"a comprehensive review on different classes of attacks and defenses against machine learning
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2213,"systems. Specifically, they proposed a taxonomy for identifying and analyzing these attacks and
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2214,"applied the attacks on a machine learning–based application, i.e., a statistical spam filter, to illus-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2215,"tratetheeffectivenessoftheattackanddefense.Thisworktargetedmachinelearningalgorithms
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2216,"rather than neural models. Inspired by Reference [ 9], the authors of Reference [ 35]r e v i e w e dt h e
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2217,"defences of adversarial attack in the security point of view. The work is not limited to machine
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2218,"learning algorithms or neuralmodels, but a generic reportaboutadversarialdefenses on security
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2219,"related applications. The authors found that existing security related defense works lack of clear
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2220,"motivations and explanations on how the attacks are related to the real security problems and
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2221,"howtheattackanddefensearemeaningfullyevaluated.Thus,theyestablishedataxonomyofmo-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2222,"tivations, constraints, and abilities for more plausible adversaries. In Reference [ 13], the authors
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2223,"provided a thorough overview of the evolution of the adversarial attack research over a ten-year
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2224,"period from 2008 to 2018, and focused on the research works from computer vision and cyber
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2225,"security. The paper covers the works from pioneering non-deep-learning algorithms to recent
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2226,"deep-learningalgorithms.Itisalsofromthesecuritypointofviewtoprovidedetailedanalysison
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2227,"the effect of the attacks and defenses. The authors of Reference [ 79] reviewed the same problem
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2228,"fromadata-drivenperspective.Theyanalyzedtheattacksanddefensesaccordingtothelearning
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2229,"phases,i.e.,thetrainingphaseandtestphase.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2230,"Unlikepreviousworksthatdiscussgenerallyontheattackmethodsonmachinelearningalgo-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2231,"rithms, Reference [ 154] focuses on the adversarial examples on deep-learning models. It reviews
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2232,"current research efforts on attacking various deep neural networks in different applications. The
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2233,"defensemethodsarealsoextensivelysurveyed.However,itmainlydiscussesadversarialexamples
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2234,"for image classification and object recognition tasks. The work in Reference [ 2] provides a com-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2235,"prehensive review on the adversarial attacks on deep-learning models used in computer vision
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2236,"tasks.Itisanapplication-drivensurveythatgroupstheattackmethodsaccordingtothesub-tasks
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2237,"under computer vision area. The article also comprehensively reports the works on the defense
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2238,"side,themethodsofwhicharemainly groupedinto threecategories.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2239,"Allthementionedworkseithertargetageneraloverviewoftheattacksanddefensesonmachine
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2240,"learningmodelsorfocusonspecificdomainssuchascomputervisionandcybersecurity.Ourwork
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2241,"differswiththeminthatwespecificallyfocusontheattacksanddefensesontextualdeep-learning
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2242,"models. Furthermore, we provide a comprehensive review that covers information from different
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2243,"aspectsto make thissurveyself-contained.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2244,"Papersselection. Thepaperswereviewedinthisarticlearehigh-qualitypapersselectedfromtop
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2245,"NLPandAIconferences,includingACL,1COLING,2NAACL,3EMNLP,4ICLR,5AAAI,6andIJCAI.7
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2246,"Other than accepted papers in aforementioned conferences, we also considered good papers in
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2247,"1Annual Meetingof the Association for ComputationalLinguistics.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2248,"2InternationalConferenceon ComputationalLinguistics.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2249,"3Annual Conferenceof the North American Chapterof the Association for ComputationalLinguistics.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2250,"4EmpiricalMethods inNaturalLanguageProcessing.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2251,"5InternationalConferenceon LearningRepresentations.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2252,"6AAAI Conferenceon Artificial Intelligence.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2253,"7InternationalJoint Conferenceon ArtificialIntelligence.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2254,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:4 W.E.Zhanget al.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2255,"e-Printarchive,8asitreflectsthelatestresearchoutputs.Weselectedpapersfromthearchivewith
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2256,"threemetrics:paperquality,methodnovelty,and thenumberof citations(optional9).
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2257,"Contributions of this survey. The aim of this survey is to provide a comprehensive review
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2258,"on the research efforts on generating adversarial examples on textual deep neural networks. It is
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2259,"motivatedbythedrasticallyincreasingattentionsonthistopic.Thissurveywillserveresearchers
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2260,"and practitioners who are interested in attacking textual deep neural models. We expect that the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2261,"readers have some basic knowledge of the deep neural networks architectures, which are not the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2262,"focusin thisarticle.To summarize,thekey contributionsof thissurveyare:
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2263,"•We conduct a comprehensive review for adversarial attacks on textual deep neural models
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2264,"and propose different classification schemes to organize the reviewed literature; this is the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2265,"firstwork of thiskind;
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2266,"•Weprovideallrelatedinformationtomakethesurveyself-containedandthusitiseasyfor
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2267,"readerswhohave limited NLPknowledge to understand;
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2268,"•Wediscusssomeopenissues,andidentifysomepossibleresearchdirectionsinthisresearch
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2269,"fieldaimingtobuildmorerobusttextualdeep-learningmodelswiththehelpofadversarial
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2270,"examples.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2271,"Theremainderofthisarticleisorganizedasfollows.Weintroducethepreliminariesforadver-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2272,"sarialattacksondeep-learningmodelsinSection 2,includingthetaxonomyofadversarialattacks
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2273,"and deep-learning models used in NLP. In Section 3, we address the difference between attack-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2274,"ing image data and textual data and briefly reviewe exemplary works for attacking image DNN
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2275,"that inspired their follow-ups in NLP. Section 4first presents our classification on the literature
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2276,"and then gives a detailed introduction to the state of the art. We discuss the defense strategies in
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2277,"Section5and pointouttheopenissuesin Section 6.Finally, thearticleisconcludedin Section 7.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2278,"2 OVERVIEW OF ADVERSARIAL ATTACKS ANDDEEP-LEARNINGTECHNIQUES
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2279,"IN NATURAL LANGUAGEPROCESSING
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2280,"Beforewediveintothedetailsofthissurvey,westartwithanintroductiontothegeneraltaxonomy
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2281,"ofadversarialattackondeep-learningmodels.Wealsointroducethedeep-learningtechniquesand
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2282,"theirapplicationsin naturallanguage processing.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2283,"2.1 Adversarial Attacks onDeep-learning Models:The General Taxonomy
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2284,"In this section, we provide the definitions of adversarial attacks and introduce different aspects
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2285,"of the attacks, followed by the measurement of perturbations and the evaluation metrics of the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2286,"effectivenessoftheattacksina generalmannerthatappliesto any datamodality.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2287,"2.1.1 Definitions. We presentthemain conceptsof adversarialattacksas following:
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2288,"•DeepNeuralNetwork(DNN) .Adeepneuralnetwork(weuseDNNanddeep-learningmodel
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2289,"interchangeably hereafter) can be simply presented as a nonlinear function fθ:X→Y,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2290,"where Xis the input features/attributes, Yis the output predictions that can be a discrete
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2291,"set of classes or a sequence of objects. θrepresents the DNN parameters and are learned
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2292,"via gradient-based back-propagation during the model training. Best parameters would be
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2293,"8arXiv.org.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2294,"9As the research topic only emerges from 2017, we relax the citation number to over five if it is published more than one
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2295,"year. If the paper has less than five citations, but is very recent and satisfies the other two metrics, then we also include it
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2296,"in this survey.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2297,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:5
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2298,"obtained by minimizing the the gap between the model’s prediction fθ(X)and the correct
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2299,"label Y,wherethegapis measuredbyloss function J(fθ(X),Y).
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2300,"•Perturbations .Perturbationsareintentlycreatedsmallnoisesthattobeaddedtotheoriginal
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2301,"inputdataexamples in teststage,aimingto foolthedeep-learningmodels.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2302,"•AdversarialExamples .Anadversarialexample x/primeisanexamplecreatedviaworst-caseper-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2303,"turbation of the input to a deep-learning model. An ideal DNN would still assign correct
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2304,"class y(inthecaseofclassificationtask)to x/prime,whileavictimDNNwouldhavehighconfi-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2305,"denceon wrongpredictionof x/prime.x/primecan beformalized as
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2306,"x/prime=x+η,f(x)=y,x∈X, (1)
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2307,"f(x/prime)/nequaly,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2308,"orf(x/prime)=y/prime,y/prime/nequaly,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2309,"whereηis the worst-case perturbation. The goal of the adversarial attack can be deviating
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2310,"thelabeltoincorrectone ( f(x/prime)/nequaly) orspecifiedone( f(x/prime)=y/prime).
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2311,"2.1.2 Threat Model. We adopt the definition of Threat Model for attacking DNN from Refer-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2312,"ence[154].Inthefollowing, we discussseveralaspectsof thethreatmodel:
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2313,"•ModelKnowledge .Theadversarialexamplescanbegeneratedusingblack-boxorwhite-box
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2314,"strategies in terms of the knowledge of the attacked DNN. Black-box attack is performed
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2315,"whenthearchitectures,parameters,lossfunction,activationfunctionsandtrainingdataof
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2316,"the DNN are not accessible. Adversarial examples are generated by directly accessing the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2317,"test dataset, or by querying the DNN and checking the output change. On the contrary,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2318,"white-boxattackis basedon theknowledge oftechnicaldetailsof DNN.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2319,"•Target.Thegeneratedadversarialexamplescanchangetheoutputpredictiontobeincorrect
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2320,"ortospecificresultasshowninEquation( 1).Comparedtotheun-targetedattack( f(x/prime)/nequal
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2321,"y), targeted attack ( f(x/prime)=y/prime) is more strict as it not only changes the prediction but also
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2322,"enforces constraint on the output to generate specified prediction. For binary tasks, e.g.,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2323,"binaryclassification,un-targetedattackequalstothetargetedattack.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2324,"•Granularity . The attack granularity refers to the level of data from which the adversarial
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2325,"examplesaregenerated.Forexample,itisusuallytheimagepixelsforimagedata.Regarding
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2326,"thetextualdata,itcouldbecharacter,word,andsentence-levelembedding.Section 3.3will
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2327,"give furtherintroductionon attackgranularityfor textualDNN.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2328,"•Motivation .Generatingadversarialexamplesismotivatedbytwogoals:attackanddefense.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2329,"TheattackaimstoexaminetherobustnessofthetargetDNN,whilethedefensetakesastep
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2330,"furtherutilizinggeneratedadversarialexamplestorobustifythetargetDNN.Section 5will
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2331,"give moredetails.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2332,"2.1.3 Measurements. Two groups of measurements are required in the adversarial attack for
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2333,"(i)controllingtheperturbationsand (ii)evaluatingtheeffectivenessoftheattack,respectively.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2334,"•Perturbation Constraint . As aforementioned, the perturbation ηshould not change the true
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2335,"class label of the input—that is, an ideal DNN classifier, if we take classification as the ex-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2336,"ample,willprovidethesamepredictionontheadversarialexampletotheoriginalexample.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2337,"ηcannotbetoosmallaswell,toavoidendingupwithnoeffectontargetDNNs.Ideally,ef-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2338,"fectiveperturbationisthemostimpactfulnoiseinaconstrainedrange.Reference[ 132]first
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2339,"put a constraint that (x+η)∈[0,1]nfor image adversarial examples, ensuring the adver-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2340,"sarialexamplehasthesamerangeofpixelvaluesastheoriginaldata[ 143].Reference[ 40]
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2341,"simplifies the solution and uses max norm to constrain η:||η||∞≤ϵ. This was inspired by
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2342,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:6 W.E.Zhanget al.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2343,"theintuitionthataperturbationthatdoesnotchangeanyspecificpixelbymorethansome
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2344,"amountϵcannot change the output class [ 143]. Using max-norm is sufficient enough for
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2345,"image classification/object recognition tasks. Later on, other norms, e.g., L2andL0,w e r e
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2346,"used to control the perturbation in attacking DNN in computer vision. Constraining ηfor
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2347,"textualadversarialattackissomehow different.Section 3.3will give more details.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2348,"•Attack Evaluation . Adversarial attacks are designed to degrade the performance of DNNs.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2349,"Therefore,evaluatingtheeffectivenessoftheattackisbasedontheperformancemetricsof
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2350,"differenttasks.Forexample,classificationtaskshavemetricssuchasaccuracy,F1scoreand
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2351,"AUC score. We leave the metrics for different NLP as out-of-scope content in this article
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2352,"andsuggest readersrefertospecifictasksforinformation.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2353,"2.2 Deep-learning in NLP
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2354,"Neuralnetworkshavebeengaining increasingpopularityin NLP community in recentyearsand
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2355,"variousDNNmodelshavebeenadoptedindifferentNLPtasks.Apartfromthefeedforwardneu-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2356,"ral networks and Convolutional Neural Networks (CNN), Recurrent/Recursive Neural Networks
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2357,"(RNN) and their variants are the most common neural networks used in NLP, because of their
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2358,"naturalabilityofhandlingsequences.Inrecentyears,twoimportantbreakthroughsindeeplearn-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2359,"ing are brought into NLP. They are sequence-to-sequence learning [131]a n dattention mechanism
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2360,"[8]. Reinforcementlearning and generative models are also gained much popularity[ 152].In this
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2361,"section, we will briefly overview the DNN architectures and techniques applied in NLP that are
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2362,"closely related to this survey. We suggest readers refer to detailed reviews of neural networks in
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2363,"NLP in References[ 100,152].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2364,"2.2.1 Feed-Forward Networks. A feed-forward network has several forward layers and each
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2365,"nodeinalayerconnectstoeachnodeinthefollowinglayer,makingthenetworkfullyconnected.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2366,"The network utilizes nonlinear transformation to distinguish data that is not linearly separable.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2367,"The major drawback of its application in NLP is that it cannot handle well the text sequences in
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2368,"which the word order matters as it do not record the order of the elements. To evaluate the ro-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2369,"bustnessoffeedforwardnetworkinNLP,adversarialexamplesareoftengeneratedforspecifically
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2370,"designedfeed-forwardnetworks.Forexample,theauthorsofReferences[ 3,43,44]workedonthe
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2371,"specifiedmalware detectionmodels.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2372,"2.2.2 ConvolutionalNeuralNetwork(CNN). AConvolutionalNeuralNetworkcontainsconvo-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2373,"lutional layers and pooling (down-sampling) layers and final fully connectedlayer. The Convolu-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2374,"tionallayer uses convolutionoperationto extract meaningful localpatternsof input.Specifically,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2375,"CNN identifies local predictors and combines them together to generate a fixed-sized vector for
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2376,"theinputs,whichcontainsthemostorimportantinformativeaspectsofthedata.Inaddition,itis
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2377,"order-sensitive. Therefore, it excels in computer vision tasks and later is widely adopted in NLP
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2378,"applications.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2379,"Yoon Kim [ 59]adoptedCNNforsentenceclassificationandusedWord2Vectorepresentwords
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2380,"as input. Then the convolutional operation is restricted to the direction of word sequence, rather
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2381,"thanthewordembeddings.Themodeldemonstratesexcellentperformanceonseveralbenchmark
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2382,"datasets and has become a benchmark work of adopting CNN in NLP applications. Zhang et al.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2383,"[156]presentedCNNfortextclassificationatcharacterlevel.Theyusedone-hotrepresentationfor
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2384,"each character of alphabet. These two representative textual CNNs are evaluated via adversarial
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2385,"examples in many applications[ 12,29,30,34,76].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2386,"2.2.3 Recurrent Neural Networks/Recursive Neural Networks. Recurrent Neural Networks are
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2387,"neural models adapted from feed-forward neural networks for learning mappings between
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2388,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:7
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2389,"sequential inputs and outputs [ 116]. RNNs allows data with arbitrary length and it introduces
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2390,"cyclesintheircomputationalgraphtomodelefficientlytheinfluenceoftime[ 48].Themodeldoes
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2391,"notsufferfromstatisticalestimationproblemsstemmingfromdatasparsityandthusleadstoim-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2392,"pressiveperformanceindealingwithsequentialdata[ 36].Recursiveneuralnetworks[ 37]extends
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2393,"recurrentneuralnetworksfromsequencestotree,whichrespectsthehierarchicaldependencyof
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2394,"thelanguage.Insomesituations,backwardsdependenciesexist,whichisinneedforthebackward
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2395,"analysis.Bi-directionalRNNthuswasproposedtoprocesseachsentencesinbothdirections,for-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2396,"wardsand backwards,usingtwoparallelRNNnetworks,andcombine theiroutputs.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2397,"RNN has many variants, among which the Long Short-Term Memory (LSTM) network [ 50]
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2398,"gains the most popularity. LSTM is a specific RNN that was designed to capture the long-term
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2399,"dependencies. In LSTM, the hidden state are computed through combination of three gates, i.e.,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2400,"inputgate ,forgetgate andoutputgate ,thatcontrolinformation.LSTMnetworkshavesubsequently
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2401,"proved to be more effective than conventional RNNs [ 42]. GRUs is a simplified version of LSTM
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2402,"that it only consists of two gates, thus it is more efficient in terms of computational cost. Some
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2403,"popularLSTMvariantsareproposedtosolvevariousNLPtasks[ 21,50,112,133,141,146].These
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2404,"representativeworkshavereceivedtheinterestsofevaluationwithadversarialexamplesrecently
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2405,"[34,53,54,91,103,112,118,130,157].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2406,"2.2.4 Sequence-to-sequence Learning (Seq2Seq) Models. Sequence-to-sequence learning
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2407,"(Seq2Seq) [ 131] is one of the important breakthroughs in deep learning and is now widely used
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2408,"for NLP applications. Seq2Seq model has the superior capacity to generate another sequence
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2409,"informationforagivensequenceinformationwithanencoder-decoderarchitecture[ 27].Usually,
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2410,"a Seq2Seq model consists of two recurrent neural networks: an encoder that processes the
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2411,"input and compresses it into a vector representation and a decoder that predicts the output.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2412,"Latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED) model [ 122] is a recently
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2413,"popular Seq2Seq model that generates sequences leveraging the complex dependencies between
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2414,"subsequences. Reference [ 24] is one of the first neural machine translation (NMT) model that
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2415,"adopts the Seq2Seq model. OpenNMT [ 63], a Seq2Seq NMT model proposed recently, becomes
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2416,"one of the benchmark works in NMT. As they are adopted and applied widely, attack works also
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2417,"emerge [22,30,98,127].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2418,"2.2.5 Attention Models. Attention mechanism[ 8] is anotherbreakthroughin deeplearning. It
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2419,"wasinitiallydevelopedtoovercomethedifficultyofencodingalongsequencerequiredinSeq2Seq
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2420,"models[27].Attentionallowsthedecodertolookbackonthehiddenstatesofthesourcesequence.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2421,"Thehiddenstatesthenprovideaweightedaverageasadditionalinputtothedecoder.Thismecha-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2422,"nismpays attentiononinformativepartsofthesequence.Ratherthanlookingattheinputsequence
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2423,"in vanilla attention models, self-attention [ 136] in NLP is used to look at the surrounding words
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2424,"in a sequence to obtain more contextually sensitive word representations [ 152]. BiDAF [ 121]i s
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2425,"a bidirectional attention flow mechanism for machine comprehension and achieved outstanding
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2426,"performance when proposed. References [ 54,127] evaluated the robustness of this model via ad-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2427,"versarialexamplesandbecamethefirstfewworksusingadversarialexamplesforattackingtextual
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2428,"DNNs.Otherattention-basedDNNs[ 25,107]alsoreceivedadversarialattacksrecently[ 29,91].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2429,"2.2.6 Reinforcement Learning Models. Reinforcement learning trains an agent by giving a re-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2430,"ward after agents perform discrete actions. In NLP, a reinforcement learning framework usually
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2431,"consists of an agent (a DNN), a policy (guiding action) and a reward. The agent picks an action
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2432,"(e.g.,predictingnextwordinasequence)basedonapolicy,thenupdatesitsinternalstateaccord-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2433,"ingly,untilarrivingtheendofthesequencewherearewardiscalculated.Reinforcementlearning
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2434,"requires proper handling of the action and the states, which may limit the expressive power and
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2435,"ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:8 W.E.Zhanget al.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2436,"learning capacity of the models [ 152]. But it gains much interests in task-oriented dialogue sys-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2437,"tems[74]astheysharethefundamentalprincipleasdecisionmakingprocesses.Limitedworksso
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2438,"far canbe foundto attackthereinforcementlearningmodel in NLP [ 98].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2439,"2.2.7 DeepGenerativeModels. Inrecentyears,twopowerfuldeepgenerativemodels,Genera-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2440,"tive Adversarial Networks (GANs) [ 39] and Variational Auto-Encoders (VAEs) [ 62] are proposed
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2441,"and gain much research attention. Generative models are able to generate realistic data instances
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2442,"thatareverysimilartogroundtruthdatainalatentspace.IntheNLPfield,theyareusedtogen-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2443,"erate text. GANs [ 39] consists of two adversarial networks: a generator and adiscriminator .T h e
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2444,"discriminatoristodiscriminatetherealandgeneratedsamples,whilethegeneratoristogenerate
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2445,"realisticsamplesthataimtofoolthediscriminator.GANusesamin-maxlossfunctiontotraintwo
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2446,"neural networks simultaneously. VAEs consist of encoder and generator networks. The encoder
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2447,"encodes an input into a latent space and the generator generates samples from the latent space.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2448,"Deep generative models are not easy to train and evaluate. Hence, these deficiencies hinder their
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2449,"wideusageinmanyreal-worldapplications[ 152].Althoughtheyhavebeenadoptedingenerating
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2450,"texts,so farno workexamines theirrobustnessusingadversarialexamples.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2451,"3 FROM IMAGETO TEXT
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2452,"Adversarialattacksareoriginatedfromthecomputervisioncommunity.Inthissection,weintro-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2453,"ducerepresentativeworks,discussdifferencesbetweenattackingimagedataandtextualdata,and
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2454,"presentpreliminaryknowledgewhenperformingadversarialattackson textualDNNs.
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2455,"3.1 CraftingAdversarial Examples: Inspiring Works in ComputerVision
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2456,"Since adversarial examples were first proposed for attacking DNNs for object recognition in
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2457,"the computer vision community [ 17,40,95,104,105,132,157], this research direction has been
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2458,"receiving sustained attentions. We briefly introduce some works that inspired their followers in
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2459,"NLP community in this section, allowing the reader to better understand the adversarial attacks
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2460,"on textual DNNs. For comprehensive review of attack works in computer vision, please refer to
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2461,"Reference[ 2].
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2462,"L-BFGS. Szegedy et al. invented the adversarial examples notation [ 132]. They proposed an ex-
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2463,"plicitly designed method to cause the model to give wrong prediction of adversarial input ( x+η)
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2464,"for image classificationtask.Itcame tosolve theoptimizationproblem:
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2465,"η=argmin
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2466,"ηλ||η||2
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2467,"2+J(x+η,y/prime)s.t.(x+η)∈[0,1]n, (2)
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2468,"2+J(x+η,y/prime)s.t.(x+η)∈[0,1]n, (2)
",False,Adversarial Attacks on Deep-learning Models in Natural,False,False,True
2469,Abstract ,True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2470,"I.  INTRODUCTION  
",True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2471,"II. R
",True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2472,"III. S
",True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2473,"IV. P
",True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2474,"V. EXPERIMENTS  
",True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2475,"VI. C
",True,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2476,"                        Analysis of Shopping Behavior based on  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2477,"Surveillance System 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2478,"Mirela Popa, Leon Rothkrantz, Zhenke Yang, and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2479,"Pascal Wiggers 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2480,"MMI Department, TU Delft 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2481,"Delft, the Netherlands 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2482,"{m.c.popa; l.j.m.rothkrantz; z.yang;p.wiggers}@tudelft.nl Leon Rothkrantz and Zhenke Yang 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2483,"Sensor Technology, SEWACO, Netherlands Defence 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2484,"Academy 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2485,"Den Helder, the Netherlands 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2486,"{ljm.rothkrantz; z.yang}@nlda.nl
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2487," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2488,"Mirela Popa, Ralph Braspenning, and Caifeng Shan 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2489,"Video and Image Processing, Philips Research  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2490,"Eindhoven, the Netherlands 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2491,"{mirela.popa; ralph.braspenning; caifeng.shan}@philips.com 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2492," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2493,"could be used to monitor the shopping behavior of people. From 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2494,"the tracked path, features can be extracted such as the relation with the shopping area, the orientation of the head, speed of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2495,"walking and direction, pauses which are supposed to be related to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2496,"the interest of the shopper. Once the interest has been detected the next step is to assess the shopper’s positive or negative appreciation to the focused products by analyzing the (non-
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2497,")verbal behavior of the shopper. Ultimately the system goal is to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2498,"assess the opportunities for selling, by detecting if a customer needs support. In this paper we present our methodology towards developing such a system consisting of participating observation, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2499,"designing shopping behavioral models, assessing the associated 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2500,"features and analyzing the underlying technology. In order to validate our observations we made recordings in our shop lab. Next we describe the used tracking technology and the results 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2501,"from experiments . 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2502,"Keywords— Model, Bayesian Networks, Surveillance, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2503,"Tracking, Shopping behavior. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2504,"I.  INTRODUCTION  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2505,"Surveillance in public places by means of Closed Circuit 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2506,"TeleVision (CCTV) systems is currently widely used to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2507,"monitor locations [4] and the behavior of the people in those areas. Since events like the terrorist attack in Madrid and London, there has been a further increasing demand for video sensor network systems to guarantee the safety of people in 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2508,"public areas. But also events like football games, music 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2509,"concerts and large venues like shopping malls where many people gather, have a need for video surveillance systems to guarantee safety. In this paper we propose to use the existing surveillance system to investigate the shopping behavior of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2510,"people. In a shop, products are displayed in such a way to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2511,"optimize the buying behavior of shoppers. Using the surveillance systems, the ideas about placement of products can be validated and improved.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2512,"However, the greater the number of cameras, the greater 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2513,"the number of operators and supervisors needed to monitor the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2514,"video streams. A fully automated surveillance system for 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2515,"shopping behavior analysis is currently not commercially available. Some software packages do exist [19], but they 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2516,"mostly record video streams and provide little further analysis. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2517,"Motion detection and human tracking, as well as behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2518,"analysis methods are widely researched topics. Understanding 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2519,"the way in which customers interact with products and developing an automatic system for recognizing their behavior and assessing possible business opportunities can be of a great 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2520,"benefit for shops, leading to improved marketing strategies 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2521,"and helping them building a better relationship with their customers.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2522,"A multimodal surveillance system would use audio and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2523,"video data from the feed and analyze them to determine the behavior that is present in the currently observed scene. To 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2524,"analyze a situation, the scene must first be interpreted.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2525,"Separate systems are proposed to analyze both the video 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2526,"and the audio stream.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2527,"The goal of our research consists of designing empirical 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2528,"based models of shopping behavior. Next, we aim at building a system for recognizing and analyzing the customers’ 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2529,"behavior in relation with products. Finally we plan to test our system on real life, spontaneous data. This work will focus on methods of analyzing and processing the video data. The goal is to extract as much relevant features as possible from the footage and to find a way to interpret this data in a meaningful 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2530,"manner given the context.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2531,"The outline of the paper is as follows. In the next section 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2532,"related work is reported, then we present our proposed 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2533,"methodology consisting of participating observation based on which we define shopping behavior models, followed by the assessment of the relevant features and the analysis of the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2534,"underlying technology. Next tracking algorithms are discussed 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2535,"followed by the description of the experimental results. Finally we formulate our conclusions and directions for future work. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2536,"II. R
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2537,"ELATED WORK 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2538,"A. Shopping behavior analysis 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2539,"Shopping behavior represents the decision processes and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2540,"acts of people involved in buying and using products. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2541,",(((
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2542,"2512 Monitoring shopping behavior [7] presents interest for both 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2543,"the academic society and also for the private sector. The 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2544,"ViCoMo project [22] focuses on the recognition of people 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2545,"behavior in general, having applicability in security and surveillance but also in the consumer market. There were also earlier attempts towards observing and analyzing shopping behavior made by Wells and Sciuto [25] based on participating observation. Regarding companies, Shopping Behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2546,"Xplained [19] investigates and analyses the customers’ 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2547,"behavior, in order to find the conscious and sub-conscious motivations that influence the decision making process of purchasing a certain item, by using both video recordings of shopping sessions and also interviews with the customers.   
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2548,"The purpose of automated surveillance systems concerns 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2549,"mainly security issues, but they can be also useful in assessing customers’ interactions with products, as their main capabilities consist in localizing, tracking people, and analyzing their behavior.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2550,"B. Enabling Technology 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2551,"There are currently a number of computer vision 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2552,"techniques available, especial ly tracking, face detection and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2553,"motion interpretation [18]. Next we will discuss the possibilities of each method and the feasibility of their application to our work. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2554,"1) Motion detection 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2555,"Since most of the behavior we wish to detect is associated 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2556,"with some kind of body movements, being able to detect motion in a scene is the first and most important basic step towards understanding behavior. Motion detection aims at detecting the non-static parts of a scene by comparing for example two scene captures. Techniques used for motion detection could be divided in several categories: background subtraction [20], temporal differencing [11], or optical flow estimation [12].   
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2557,"Most of the following operations we wish to perform, such 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2558,"as person tracking and behavior interpretation are highly 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2559,"dependent on motion detection. One important aspect involved in motion detection algorithms regards background modeling. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2560,"2) Background modeling 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2561,"Background modeling is very important for motion 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2562,"detection since it provides a description of the scene which can help in interpreting the observed motion data. In our case a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2563,"shop will be split up in walking areas, product areas, and areas 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2564,"for shop assistants. Background modeling can greatly reduce the cost of computation and help eliminate false positives. One of the challenges is being able to model the background pixels under varying lighting conditions. A simple approach is based 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2565,"on using a previously acquired image of the scene without any 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2566,"objects or persons in the scene, possibly under varying lighting conditions, so that it can later be used for background subtraction. More advanced methods exist, some including Gaussian pixel models [20], others using Kalman filters to reduce the variance in illumination, e.g. Hu et al. [8].  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2567,"3) Object detection 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2568,"Object-class detection aims to detect all objects in a scene 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2569,"belonging to a certain class, such as vehicles, bags, but also human bodies or animals.  Mikolajczyk [14] combined 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2570,"AdaBoost with local orientation histograms and built a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2571,"detector using object parts. Another approach was proposed by 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2572,"Tuzel et al. [21], in which human detection is achieved by using classification on Riemannian manifolds. The object descriptors they use are covariance matrices of image features computed over image regions. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2573,"4) Face detection 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2574,"Face detection can be regarded as a special case of object-
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2575,"class detection. While most face detection methods try to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2576,"detect frontal views of faces, newer algorithms attempt to detect faces from multiple angles, or multi-view face detection [24]. A wide variety of techniques exist, ranging from simple edge-based algorithms, to complex high-level approaches 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2577,"using pattern recognition methods. The method used by Albiol 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2578,"et al. [1] detects faces by first detecting skin pixels and then 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2579,"applies a segmentation algorithm to find skin regions. A watershed segmentation algorithm is used to find clusters, after which the most face-like blobs are selected as the faces. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2580,"Viola and Jones [23] introduced a new approach for visual 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2581,"object detection using the principle of a boosted cascade of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2582,"classifiers. Their detector can be  trained for face detection, and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2583,"is capable of processing images extremely rapidly while achieving high detection rates.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2584,"5) Object tracking 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2585,"A common tracking method is to use a filtering mechanism 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2586,"to predict each movement of the recognized object. The most 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2587,"commonly used filter in surveillance systems is the Kalman 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2588,"filter [15]. Condensation [10] is a very well-known tracking algorithm, having the advantage that it can be used more or less independent of the object representation; still it is less efficient at tracking more than one object at the same time. A less known alternative for Condensation is the Mean Shift 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2589,"algorithm [5] which was recently proposed as an efficient tool 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2590,"to handle partial occlusions and significant clutters [3]. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2591,"6) Behaviour analysis 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2592,"In the view of an automatic surveillance system, object 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2593,"detection and tracking needs to be followed by object behavior analysis and recognition. Hu et al. [8] present behavior understanding as a classification of motion patterns produced by the object tracking module. Other methods which proved their efficacy in generating behavior models use maximum entropy and Markov mixture models [13]. Dynamic time warping (DTW) is a time-varying technique widely used in speech recognition, image patterns and recently in human movement patterns [16]. In [17] the recognition of behaviors and activities is done using a declarative model to represent scenarios and a logic-based approach to recognize predefined scenario based models.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2594,"III. S
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2595,"HOPPING BEHAVIOR METHODOLOGY  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2596,"To design and implement a system for automatic 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2597,"assessment of users’ appreciation of products and opportunities for selling we used the following methodology composed of several steps. First we did participating observation of the customers’ behavior while shopping, which 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2598,"will be presented in Section III.A. Based on these observations 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2599,"we build behavioral models of shopping behavior, which will 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2600,"2513 be presented in details in Section III.B. We observed the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2601,"features characteristic for each type of behavior and we 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2602,"grouped them depending on the addressed modality (people 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2603,"detection, face detection, gesture recognition, or voice analysis). We employ a Bayesian Network to model the relationships between the sensors, the observed features and the associated shopping behavior. Next we investigated the underlying technologies needed to assess the proposed features 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2604,"and we present the system architecture in Section IV.B. In 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2605,"order to validate and test our models we made recordings of shopping behavior in our shop lab. Finally we present the obtained experimental results and comment upon them.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2606,"A. Partcipating Observation 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2607,"There is a continuous interest in building and testing 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2608,"consumer behavior models [9]. One methodology which is useful for defining user models is participating observation. In the shops, the researchers observed in an unobtrusive manner the shopping behavior of people. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2609,"One researcher was standing near the entrance of the shop, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2610,"while another one was following the customer at a reasonable 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2611,"distance such that he did not interfere in the shopping activity. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2612,"Both researchers wrote notes of their observations. In total 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2613,"we collected 20 hours of observations. We present next an example of an observation session: 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2614,"“A lady enters the shop. She goes directly (1) to the elevator, at a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2615,"high speed (2). She chooses the 2nd floor and she goes to the make-up 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2616,"rayon. She looks around (4) for a moment and selects a product. Then she raises her hand (7), asking (8) for a shop assistant. She asks (8) 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2617,"for special offers from the advertising magazine. She looks happy (6) 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2618,"as she discovers that there is a new offer for the product she wants. She follows (1) the assistant to the pay desk and goes away (1).” 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2619,"In the brackets, the relevant features which characterize 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2620,"shopping behavior are indicated. They are presented together with the associated sensing devices in Table 1 from Section V.A.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2621,"B. Behavior Models 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2622,"Our behavior models are based on the observations made 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2623,"in real shops. Not surprisingly, there are many individual differences in shopping behavior of people. The ultimate goal of shopping is to buy a required product. In order to realize that goal a shopper has to perform some actions. In case a shopper knows what he wants, he has to find the location of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2624,"the product and to put the product in his basket. Next we have 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2625,"the helpless people, who cannot find the product and are actively looking for support. Finally we have the “fun”-shoppers, who have no idea what to buy and first look around for interesting products or just enjoy being in a shop. All kinds 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2626,"of shoppers show different but characteristic behavior and our 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2627,"surveillance system should be able to extract relevant features for the recognition of the specific type of shopper. We considered the following types of shoppers: goal oriented, disoriented shopper, looking for support shopper, fun-shopper and duo shopper. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2628,"A goal oriented shopper  has a shopping list, knows the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2629,"location of the product and walks directly to that place at a high speed, without looking left or right. An example of this 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2630,"behavior can be noticed in Fig. 1 below. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2631,"The disoriented shopper  has no specific idea about what he 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2632,"wants, doesn’t know if the product is available or where to find it. He walks at a low speed, is frequently looking left and right, and walks without any apparent plan.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2633," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2634," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2635,"Figure 1: Goal oriented behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2636," Some shoppers are looking for the shop assistant  or 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2637,"waiving their hand, asking information about the location of the products, alternatives, and characteristics of the product 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2638,"(see Fig. 2). 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2639," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2640," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2641,"Figure 2: Looking for help behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2642," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2643,"A fun-shopper  wants to be where the action is, he joins the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2644,"crowded area; he has interest for expositions, demonstrations, or products promotions. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2645,"Some people like to shop in a group, especially the fun-
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2646,"shoppers but also partners or friends. They talk a lot to each other and comment the choices and appreciation of products to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2647,"their partner. An example of this type of behavior is depicted 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2648,"in Fig. 3.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2649,"A special case of the duo-shopper  is represented by a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2650,"parent accompanied by his child/children. In this case we can expect a lot of walking around, as the child is running away, but this behavior is not related to products. This type of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2651,"behavior should be interpreted and recognized correctly. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2652," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2653," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2654,"Figure 3: Commenting of products  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2655," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2656,"2514 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2657," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2658,"Figure 4: Bayesian Network Model of shopping behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2659," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2660,"C. Analysis of Shopping Behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2661,"By observing shopping people we extracted the following 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2662,"general steps in the shopping behavior: orientation, selection, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2663,"appreciation, and decision making. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2664,"• Orientation. During orientation the shopper has to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2665,"decide what he wants and has to make a plan for how to find the product(s). If the shopper is not familiar with the shop, he will inspect the shop information 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2666,"lists and the routing schema, he will just walk, or he 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2667,"will ask for help. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2668,"• Selection. Once the product has been discovered a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2669,"shopper will look for characteristics and alternatives. A shopper is inspecting the product, reading the text, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2670,"comparing products, and comparing prices, or looking 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2671,"for promotions. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2672,"• Appreciation. During the selection process, positive 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2673,"and/or negative emotions will surface and result in a final positive or negative appreciation. From facial expressions or other non-verbal behavior the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2674,"appreciation can be assessed. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2675,"• Decision making. The last step of the shopping 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2676,"process is to decide whether to buy the product or not. In the case of duo-shopper, the partner will be consulted. Shoppers take a last close look at the product and the facial expression can be one of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2677,"puzzlement. Once the decision has been made 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2678,"shoppers show a positive or negative facial expression. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2679,"Each of the previously described steps can be further 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2680,"divided into sub-steps specific for each type of shopping behavior.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2681,"IV. P
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2682,"ROPOSED SYSTEM  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2683,"A. Reasoning 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2684,"Probabilistic methods, such as Bayesian Networks offer a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2685,"number of advantages for the representation and processing of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2686,"knowledge and uncertainty, being able to cope with missing or 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2687,"incomplete information.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2688,"We employed a Bayesian Network to condense 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2689,"information and represent the relationships between the observed features and the corresponding shopping behavior. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2690,"The proposed model is presented in Fig. 4. Our model is 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2691,"organized on three levels; on the first level we display the used sensors (cameras, microphone). The next level contains the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2692,"observed features (e.g. walking in relation with speed and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2693,"direction, or activities related to products: looking, taking, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2694,"touching), we regard all these features as observables. The third level corresponds to the shopping behavior, giving an indication regarding the interest of shoppers or the opportunity for selling. This level is regarded as a high semanthical level, where based on the observables, hypotheses can be 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2695,"formulated. The relevant features are fused, each having 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2696,"associated a weight. For example, in order to detect the “Goal Oriented” behavior, the direction and the speed of walking are considered and observed by the top camera. When the customer stops for a period of time, the system changes the focus to the camera behind the products and the customer’s 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2697,"actions are assessed. We are interested if he/she is looking at 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2698,"products, if he/she takes one, touches it and then puts it back or puts it in his basket. Furthermore we extract relevant information from his facial expression, gestures, and voice (both the tonality and the semantics). Finally we conclude if he 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2699,"is interested in that product and also the type of interest 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2700,"(positive or negative).  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2701,"In order to learn the model parameters from data we would 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2702,"need a lot of training examples, which are not currently available. We will tackle this issue by asking experts in this field to specify or to correct the Conditional Probabilities 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2703,"Tables (CPT) for each parameter. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2704,"B. System Architecture 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2705,"In this section, the design of our system for multimodal 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2706,"assessment of users’ appreciation of products is presented. We propose a modular approach and we describe next the functionality of each module. A diagram of the proposed 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2707,"system is shown in Fig. 5.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2708,"The system is composed of two basic modules for video 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2709,"and audio data analysis. First the video file is processed in order to extract the image sequences and the audio file. The video data analysis module consists of motion detection, human detection, facial expression recognition, and also hand 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2710,"detection sub-modules. Motion based analysis regards motion 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2711,"detection and motion recognition tasks. Motion energy classification is important as it can give indication regarding the general direction or the amount of movement. The human detection part aims at recognizing people and tracking them 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2712,"accordingly. Besides playing an important role in human 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2713,"2515 tracking, face detection and tracking is also used for assessing 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2714,"user’s appreciation of products, by enabling the analysis of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2715,"facial expressions. The process of recognizing the user’s facial 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2716,"expression consists of applying the Active Appearance Model (AAM) [6] to the facial region, extracting the relevant features, and then using a classification method such as Hidden Markov Models (HMMs).  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2717,"From the video data we also extract information regarding 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2718,"gestures, a separate module being designated for this task. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2719,"Skin color detection together with pose estimation and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2720,"edge detection contribute to hand detection and tracking. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2721,"The audio analysis module consists of feature extraction 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2722,"(pitch, energy, MFCC, jitter, etc.), task followed by 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2723,"classification in two phases. First we detect interest or non-
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2724,"interest. Next, in case interest was detected we classify it into positive or negative.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2725,"The output of the system is obtained by fusing the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2726,"intermediary results of each module (motion detection, human behavior detection, facial expression recognition, gesture 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2727,"recognition, and voice analysis). In order to take into account 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2728,"the relationships and the importance of each module, a Dynamic Bayesian Network (DBN) is employed (see Fig. 4).  One of the strengths of the presented system consists in its adaptability. If one modality is not available or the quality of the result is below a certain threshold, then it will be discarded 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2729,"in the fusion process. Furthermore each modality has 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2730,"associated a certain weight which reflects both its importance and reliability. We have implemented modules for motion energy classification, human detection and tracking, facial expression analysis, and voice analysis. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2731,"Still all these modalities need to be integrated and fused in 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2732,"a common framework, task on which we are working at this 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2733,"moment. Next we present in more details the human tracking module.   
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2734," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2735,"Figure 5: System Architecture 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2736,"C. Human Tracking 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2737,"As it was stated in Section II.B.5 the Mean Shift algorithm 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2738,"[5] represents an efficient method for human tracking, reason why we chose to use it in our approach. The 'mean shift' 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2739,"represents the estimated direction and distance in which the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2740,"target moves, and these  parameters are computed without 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2741,"using a dynamic model, but only by comparing a candidate target with the model. Regarding object representation, in the Mean Shift approach every object is represented by an ellipse. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2742,"In the initialization phase, for every object that has to be 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2743,"tracked, the model color histogram of the ellipse is computed 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2744,"in the Region of Interest (ROI). To increase the robustness, to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2745,"every histogram a convex and monotonic kernel mask is added: 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2746,"pixels in the center of the ellipse get a higher value than the ones on the border. The weight decreases with squared distance from center. Next the histogram is normalized. All the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2747,"histograms used are in three dimensions (one dimension for 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2748,"each color). To compute the distances between the histograms, the Bhattacharyya [5] distance is used. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2749,"During every processed frame, the target moves toward its 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2750,"most probable position in multiple iterations. In order to accomplish this task we used an algorithm which maximizes 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2751,"the Bhattacharyya coefficient (higher coefficient means a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2752,"higher similarity and thus a shorter distance). 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2753,"1) First for candidate location y
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2754,"0, the current candidate 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2755,"histogram p(y 0) will be computed, together with the kernel 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2756,"mask. After that, the Bhattacharyya coefficient between this histogram and the model histogram q is computed. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2757,"2) For every pixel, compute a weight as defined by: 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2758," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2759,"=− =m
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2760,"u uu
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2761,"i iypqu xb w
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2762,"1 0)ˆ(ˆˆ])([δ  (1) 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2763,"where b(x i) is the bin for the color of pixel xi, u is the current 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2764,"bin and δ is the Kronecker delta function. This means that 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2765,"every weight is the square root of the value of the model bin of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2766,"the pixel color, divided by the value of the candidate bin of the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2767,"pixel color . 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2768,"3) Compute the mean shift, which represents the new 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2769,"estimated location y1: 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2770,"  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2771,"===n
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2772,"iin
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2773,"iii
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2774,"wwx
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2775,"y
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2776,"11
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2777,"1ˆ       (2) 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2778,"Then compute again the Bhattacharyya coefficient, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2779,"between the new candidate histogram p(y 1) and the model 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2780,"histogram q. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2781,"4) As long as the coefficient between p(y 0) and q is larger 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2782,"than the one between p(y 1) and q, the target has not yet been 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2783,"reached, so the location of y1 must be updated:                  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2784,"y1 ← ½ ( y0 + y1). Repeat this step until the target has been 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2785,"reached. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2786,"5) If || y1 – y0 || < ε, stop the iterations, and continue with 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2787,"the next frame. Otherwise, start a new iteration at step 1 with the new candidate ellipse: y
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2788,"0 ← y1. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2789,"In the following section we present our recordings 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2790,"acquisition process which provided the data on which we 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2791,"could test our algorithms.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2792,"2516 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2793," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2794,"Figure 6: View of the shop lab (a) experimental set-up (b) combined views of the shop lab 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2795," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2796,"V. EXPERIMENTS  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2797,"A. Data Collection 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2798,"In order to test our system we need recordings of shopping 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2799,"behavior. Our aim is to have realistic data consisting of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2800,"spontaneous shopping behavior of people. But before we can 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2801,"obtain this kind of data we have to solve first some ethical problems regarding privacy. Therefore we test our system for the time being on recordings made in our shop lab (see Fig. 6b). Cameras and microphones were installed at different points, at the locations depicted in Fig. 6a. The multimodal 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2802,"devices used in our lab consist of a camera attached to the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2803,"ceiling, a camera with a microphone behind the products, and three surveillance cameras in the corners of the room. A combined view of the shop lab as captured by the surveillance cameras is shown in Fig. 6b. The purpose of each camera is different, each being designated to capture a certain type of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2804,"reaction/behavior. In Table 1 we present the extracted features 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2805,"and the associated capturing devices.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2806," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2807,"TABLE 1.  FEATURES AND SENSING DEVICES  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2808,"Behavior Sensing Devices Features 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2809,"(1) Walking 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2810,"directions Top camera/ 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2811,"Surveillance camera Trajectory  analysis 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2812,"Motion estimation 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2813,"(2) Speed of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2814,"walking  Top camera/ 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2815,"Surveillance camera Relative distance of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2816,"positions-points at regular 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2817,"time intervals 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2818,"(3) Stop Top camera/ 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2819,"Surveillance camera Cluster of points 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2820,"(4) Looking at 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2821,"products All cameras Head position/Gaze 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2822,"estimation 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2823,"(5) Touching 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2824,"products All cameras Hand movements 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2825,"(6) Facial 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2826,"expressions Camera behind/above 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2827,"products Positive/negative 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2828,"appreciation 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2829,"(7) Gestures All cameras Hand raising, waiving 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2830,"(8) Speech Microphone Utterances asking for help 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2831," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2832,"We asked ten students and researchers to show the shopping 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2833,"behavior of the five shopping models presented above and we 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2834,"made recordings of the main steps characteristic for each behavior.
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2835," Next we present the experimental results obtained 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2836,"using the recorded data. B. Experimental Results 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2837,"In this section we discuss the first experimental results in 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2838,"tracking and analyzing shoppers’ behavior. We tested first the motion detection module.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2839,"Bobick and Davis [2] propose a view-based approach, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2840,"which uses motion history images (MHI) and motion energy 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2841,"images (MEI) to interpret human behavior. This method is 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2842,"based on the assumption that different actions have different motion history patterns which can be used to detect and classify human actions. We computed the amount of motion by comparing the movement of pixels in successive frames resulting in different energy distributions (see Fig. 7a, b). Fig. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2843,"7a presents a shopper standing in front of a product display, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2844,"searching for a certain item and then starting to walk around, with a low speed and looking around. These cues are similar to the ones displayed by the “disoriented shopper” and represent a first indication of this type of behavior. In Fig. 7b is presented the motion energy for a shopper, standing in front of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2845,"a product taking a closer look at it or taking it. This motion 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2846,"pattern represents an indication that the customer is interested in that product.   
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2847," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2848,"Figure 7: Energy graphs of: (a) shopper searching and walking (b) shopper 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2849,"standing in front of a product 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2850," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2851,"An analysis of the presented energy graphs indicates that 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2852,"higher peaks can be correlated with large movements (e.g. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2853,"walking), the medium peaks can be associated with 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2854,"movements with a smaller amplitude such as hands movements for example, while the motion pattern depicted in graph 7b suggests low movement or no motion at all. The information derived from the motion energy graphs refers to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2855,"the amount of motion. To be relevant for behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2856,"classification, this information needs to be used in 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2857,"2517 combination with other features: with a low or high speed, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2858,"constant or variable way of walking, certain types of activities 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2859,"or facial expressions, in order to infer if the motion pattern 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2860,"corresponds to a certain type of behavior or not.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2861,"The motion detection module provides input for the next 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2862,"module: human tracking, by indicating which parts of the image are interesting to follow. Finding the connected components is based on the foreground pixels. The 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2863,"components which have an area larger than a predefined 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2864,"threshold will be used as tracked objects in the Mean Shift algorithm (see the example in Fig. 8).  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2865," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2866,"Figure 8: Tracking customers 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2867," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2868,"In Fig. 9 we show 5 tracks. At fixed time intervals we plot 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2869,"the position of the tracked blob. In this way we get a trajectory with dots along it. The distance between the dots is an 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2870,"indication of the speed of walking. The red track corresponds 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2871,"to a shopper walking at a high speed at a straight line but his walking speed slows down at the end, probably close to the products of his interest. The blue track depicts a similar walking pattern as the red track. The yellow and the green track correspond to shoppers wandering around.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2872," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2873,"Figure 9: Trajectories of shoppers walking with different speed along different 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2874,"tracks (in colors) 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2875," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2876,"Given the outline of the shopping area and the position of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2877,"the products, we are able to detect who is interested in what. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2878,"Trajectory analysis consisting of walking pattern and speed 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2879,"provides us with a first indication of the type of behavior of the customers. The red, blue and purple tracks can be associated with a goal oriented shopper, while the green and the yellow ones represent a first indication of a disoriented 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2880,"shopper.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2881,"Still multiple people tracking is not always performing in a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2882,"reliable way, therefore we combined it with the face detection algorithm of Viola and Jones [23]. This is a very robust algorithm but nevertheless results in lot of false positive and false negatives. To cope with the high amount of misses, we 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2883,"try to recover a face in the following frame(s) as follows. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2884,"Since we have 25 frames in a second, there is a lot of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2885,"redundant information available. Once a face was detected of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2886,"size
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2887,"nn×, it can be expected that in one of the next frames 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2888,"the same face is also visible. We consider a search window 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2889,") () ( mn mn +× +  around the face-coordinates and look in 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2890,"the following p frames for a face which is assumed to be the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2891,"expected face. We set p=10 and m=5. Table 2 presents the results obtained after applying different optimizations. The 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2892,"first column shows the raw face detection rate. Column 2 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2893,"shows the results after ignoring detection misses due to turned heads (e.g. profile views). The false positive rate is reduced by considering masking areas (column 4) and analysing a detected face in relation to a blob. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2894," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2895,"TABLE 2. F ACE DETECTION RESULTS OF A VIDEO SEQUENCE OF 1420  FRAMES  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2896,"(118 sec).  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2897,"Detection 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2898,"rate Frontal 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2899,"correction False 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2900,"positive 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2901,"rate FP rate 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2902,"after 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2903,"masking FP rate after 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2904,"blob 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2905,"alignment 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2906,"62% 86% 20% 11% 4% 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2907," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2908,"A noticeable decreasing of the false positive rate was 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2909,"achieved by applying a mask (dividing the shop into areas designated for products, customers and shop assistants), as a lot of faces were detected previously in the products area due to the similarity with skin color. By combining the human tracking and the face tracking modules we achieved a lower 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2910,"false positive rate as showed in Table 2. The face tracking 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2911,"module has also other advantages, being useful at detecting a customer looking left and right, feature which is characteristic for a certain type of behavior. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2912,"Next, in order to validate our assumptions related to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2913,"shopping behavior interpretation, we annotated segments of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2914,"the recorded videos as either goal oriented or disoriented type 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2915,"of behavior and we extracted a set of features (min, max, standard deviation of position, speed, and acceleration on x and y axes). 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2916," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2917,"Figure 10: Behavior clustering based on customers’ speed (in colors) 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2918," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2919,"As expected, speed proved to be a discriminative feature 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2920,"for the two considered types of behavior and the obtained results are shown in Fig. 10 (blue points correspond to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2921,"disoriented shoppers, while the brown points depict the goal 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2922,"oriented ones). 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2923,"2518 The presented results are preliminary and we plan to 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2924,"continue our recordings and to test our prototype system on 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2925,"real life shopping behavior data from a shopping mall. Next 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2926,"we present our conclusion and give indication regarding future work. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2927,"VI. C
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2928,"ONCLUSIONS  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2929,"In this paper we report about a surveillance system in a 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2930,"shop lab, analyzing the shopping behavior. By participating 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2931,"observation in real shops we were able to model shoppers with 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2932,"different shopping behavior. We designed and implemented a first running prototype and tested the modules: motion detection, trajectory analysis based on human tracking, and face localization and tracking for different shoppers. The time 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2933,"annotated tracks show characteristic features which enable us 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2934,"to classify the shopping activities. Furthermore, by analyzing the customers’ speed of walking we were able to make a first classification of their shopping behavior into ‘goal oriented’ or ‘disoriented’ type. As future work we plan to test the other implemented modules especially the modules related to sound 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2935,"recording and analysis. The next step consists of fusing the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2936,"data from cameras at different location and view angles and the data from different modalities. In the last years we developed software systems to fuse data from different modalities and we plan to use them in the shopping behavior surveillance context. Further on we plan to make real life 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2937,"recordings of shopping people, which can enable the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2938,"validation of our proposed models. Finally our system should serve as an automatic assessment tool of users’ appreciation of products and opportunities for selling. Knowing the layout of the shopping area an intelligent surveillance system can also infer the type of products a customer is interested in and might 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2939,"be used to propose personalized advertisements. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2940,"A
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2941,"CKNOWLEDGMENT  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2942,"This work was supported by the Netherlands Organization 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2943,"for Scientific Research (NWO) under Grant 018.003.017. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2944,"REFERENCES  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2945,"[1] A. Albiol, L. Torres, and E. Delp, “An unsupervised color image 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2946,"segmentation algorithm for face detection applications”, In IEEE 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2947,"International Conference on Image Processing, pp. 681-684, Oct. 2001. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2948,"[2] A. Bobick and J. Davis, “Real-time recognition of activity using 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2949,"temporal templates”, In WACV ’96: Proceedings of the 3rd IEEE Workshop on Applications of Computer Vision (WACV ’96), pp. 39, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2950,"Washington, DC, USA. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2951,"[3] Y. Cai, N. de Freitas, and J. Little, “Robust visual tracking for multiple 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2952,"targets,” In 9th European Conference on Computer Vision, 2006, pp. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2953,"107–118. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2954,"[4] R. Collins, A. Lipton, and T. Kanade, “A system for video surveillance 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2955,"and monitoring,” In American Nuclear Society 8th Internal Topical 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2956,"Meeting on Robotics and Remote Systems,  1999. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2957,"[5] D. Comaniciu, V. Ramesh, and P. Meer, “Real-Time Tracking of Non-
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2958,"Rigid Objects using Mean Shift,” IEEE Conf. on Computer Vision and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2959,"Pattern Rec., vol. 2, pp. 142-149, 2000. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2960,"[6] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active Appearance 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2961,"Models,” In H. Burkhardt and B. Neumann, editors, 5
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2962,"th European 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2963,"Conference on Computer Vision 1998, Vol. 2, pp. 484-498, Springer, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2964,"Berlin. [7] I. Haritaoglu and M. Flickner, “Attentive billboards: Towards to video 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2965,"based customer behavior,” In Proc. IEEE Workshop on Applications of 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2966,"Computer Vision, pp. 127-131, Orlando, FL, USA, Dec. 2002.   
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2967,"[8] W .  H u ,  T .  T a n ,  L .  W a n g ,  a n d  S .  M a y b a n k ,  “ A  s u r v e y  o n  v i s u a l  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2968,"surveillance of object motion and behaviors,”  System, Man and 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2969,"Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2970,"34(3):334-352, Aug. 2004.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2971,"[9] S .  K .  H u i ,  P .  S .  F a d e r ,  a n d  E .  B r a d l o w ,  “ P a t h  D a t a  i n  M a r k e t i n g :  A n  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2972,"Integrative Framework and Prospectus for Model Building,” Marketing 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2973,"Science, March 1, 2009; 28(2): 320 - 335.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2974,"[10] M. Isard and A. Blake, “Condensation - Conditional Density 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2975,"Propagation for Visual Tracking,” International Journal of Computer 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2976,"Vision, 29(1):5-28, 1998. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2977,"[11] G. Jing, C. E. Siong, and D. Rajan, “Foreground motion detection by 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2978,"difference-based spatial temporal entropy image,” In Proc. IEEE Region 10 Conference (TENCON), vol. A, pp. 379-382, Chiang Mai, Thailand, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2979,"Nov. 2004. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2980,"[12] B. D. Lucas and T. Kanade, “An iterative image registration technique 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2981,"with an application to stereo vision (darpa),” In Proc. of the 1981 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2982,"DARPA Image Understanding Workshop, pp. 121-130. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2983,"[13] E. Manavoglu, D. Pavlov, and C. L. Giles, “Probabilistic User Behavior 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2984,"Models,” Data Mining, ICDM 2003, Third IEEE International 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2985,"Conference on Data Mining, pp. 203-210. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2986,"[14] K. Mikolajczyk, C. Schmid, and A. Zisserman, “Human detection based 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2987,"on a probabilistic assembly of robust part detectors,” In Proc. European Conference on Computer Vision, volume 3021 of Lecture Notes in 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2988,"Computer Science, pp. 69–81, Prague, Czech Republic, May 2004. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2989,"[15] N. T. Nguyen, S. Venkatesh, G. West, and H. H. Bui, “Multiple camera 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2990,"coordination in a surveillance system,” Acta Automatica Sinica, 2003, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2991,"29, (3), pp. 408–421. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2992,"[16] T. Oates, M. D. Schmill and P.R. Cohen, “A method for clustering the 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2993,"experiences of a mobile robot with human judgements,” Proc. of the 17
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2994,"th 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2995,"National Conference on Artificial Intelligence and Twelth Conf. on  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2996,"Innovative Applic. of Artificial Intelligence , AAAI 2000, pp. 846-851. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2997,"[17] N. Rota and M. Thonnat, “Video sequence interpretation for visual 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2998,"surveillance,” 3rd IEEE Int. Workshop on Visual Surveillance, Dublin, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
2999,"2000, pp. 59-68. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3000,"[18] A. W. Senior, L. Brown, A. Hampapur, C. F. Shu, Y. Zhai, R. S. Feris, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3001,"Y.L. Tian, S. Borger, and C. Carlson, “Video analytics  for retail,” In Proc. IEEE Conference on Advanced Video and Signal-based 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3002,"Surveillance, pp. 423-428, London, UK, Sep. 2007.  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3003,"[19] Shopping Behavior Xplained (SBLX), 2009. “Axis cameras watch 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3004,"shopper’s behavior,” http://www.axis.com/files/success_stories/ss_ret_sbxl_36113_en_0907_l
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3005,"o.pdf  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3006,"[20] Z. Tang and Z. Miao, “Fast background subtraction and shadow 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3007,"elimination using improved Gaussian mixture model,” In Proc. IEEE International Workshop on Haptic Audio Visual Environments and their 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3008,"Applications, pp. 38-41, Ottawa, Canada, Oct. 2007. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3009,"[21] O. Tuzel, F. Porikli, and P.Meer, “Human detection via classification on 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3010,"riemannian manifolds,” In Proc. IEEE Conference on Computer Vision 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3011,"and Pattern Recognition, pp. 1–8, Minneapolis, MN, USA, June 2007. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3012,"[22] ViCoMo, “Visual Context Modelling,” September 2009, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3013,"http://www.itea2.org/public/project_leaflets/VICOMO_profile_oct-
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3014,"09.pdf  
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3015,"[23] P. Viola and M. Jones, ”Rapid Object Detection using a Boosted 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3016,"Cascade of Simple Features,” In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3017,"Recognition (CVPR 2001), Vol. 1, pp. I–511– I–518. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3018,"[24] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, “Face 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3019,"Recognition: A literature survey,” ACM Comput. Survey, 35(4):399–
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3020,"458, 2003. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3021,"[25] W. D. Wells, L. A. Lo Sciuto, “Direct Observation of Purchasing 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3022,"Behavior,” Journal of Marketing Research, Vol. 3, No. 3, pp. 227-233, 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3023,"Aug. 1966. 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3024," 
",False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3025,2519,False,AnalysisOfShoppingBehaviorBasedOnSurveilanceSystem,False,False,False
3026,Abstract ,True,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3027,"I. I NTRODUCTION
",True,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3028,"II. I NSTRUMENTATION AMPLIFIER DESIGN
",True,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3029,"III. M EASUREMENTS
",True,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3030,"IV. D ISCUSSION
",True,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3031,"V. C ONCLUSIONS
",True,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3032,"Amsterdam University of Applied Sciences
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3033,"A broadband, high common-mode rejection ratio instrumentation amplifier
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3034,"van der Horst, Marcel J.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3035,"DOI
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3036,"10.1109/APEMC49932.2021.9596926
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3037,"Publication date
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3038,"2021
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3039,"Document Version
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3040,"Final published version
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3041,"License
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3042,"Unspecified
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3043,"Link to publication
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3044,"Citation for published version (APA):
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3045,"van der Horst, M. J. (2021). A broadband, high common-mode rejection ratio instrumentation 
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3046,"amplifier . Paper presented at 2021 ASIA PACIFIC INTERNATIONAL SYMPOSIUM ON 
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3047,"ELECTROMAGNETIC COMPATIBILITY, Kuta Selatan, Indonesia.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3048,"https://doi.org/10.1109/APEMC49932.2021.9596926
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3049,"General rights
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3050,"It is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3051,"and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an open
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3052,"content license (like Creative Commons).
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3053,"Disclaimer/Complaints regulations
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3054,"If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3055,"let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3056,"inaccessible and/or remove it from the website. Please contact the library:
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3057,"https://www.amsterdamuas.com/library/contact/questions, or send a letter to: University Library (Library of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3058,"University of Amsterdam and Amsterdam University of Applied Sciences), Secretariat, Singel 425, 1012 WP
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3059,"Amsterdam, The Netherlands. You will be contacted as soon as possible.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3060,"Download date:23 dec. 2022A Broadband, High Common-Mode
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3061,"Rejection Ratio Instrumentation Ampliﬁer
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3062,"Marcel J. van der Horst
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3063,"Department of Electrical Engineering/ Amsterdam Sensor Lab
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3064,"Amsterdam University of Applied Sciences, The Netherlands , m.j.van.der.horst@hva.nl
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3065,"mentation ampliﬁer ( IA) with a common-mode re-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3066,"jection ratio ( CMRR ) independent of resistance tol-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3067,"erances is presented in this paper. The CMRR is de-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3068,"termined by the operational ampliﬁer characteristics.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3069,"The IAshows a high CMRR up to 100 kHz. Moreover,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3070,"since the presented IAoperates in the current domain,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3071,"no large internal voltage swings occur, making it
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3072,"an interesting choice for low-voltage applications in
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3073,"situations where common-mode disturbances may
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3074,"affect the signal processing.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3075,"Index Terms —Nullor, instrumentation ampliﬁer,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3076,"common-mode rejection ratio, operational ampliﬁer,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3077,"common-mode disturbance
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3078,"I. I NTRODUCTION
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3079,"Instrumentation ampliﬁers ( IA) are typically
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3080,"used when common-mode ( CM) signals may dis-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3081,"turb the differential-mode ( DM) signal transfer. Its
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3082,"task is to suppress the CMdisturbance and amplify
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3083,"the DMsignal. This ability is the common-mode
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3084,"rejection ratio. The CM-signal may be at DC, but
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3085,"is usually an AC signal. An example of the latter
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3086,"possibility is the measurement of bio-potentials
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3087,"like electrocardiogram and electroencephalogram
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3088,"where the mains may induce CMvoltages on the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3089,"order of several volts, while the signal of interest
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3090,"is in the order of (hundreds of) V to several mV
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3091,"[1].
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3092,"In general, increasing CM disturbance in the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3093,"low-frequency (2-150 kHz) range can be expected
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3094,"nowadays [2], increasing the demand for IAs with
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3095,"high (enough) CMRR in this band. The current trend
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3096,"towards low supply voltages requires the IAto meet
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3097,"this demand at decreasing supply voltages as well.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3098,"A lot of investigation has been performed over
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3099,"the years to improve the CMRR ofIAs, e.g., [3]-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3100,"[7]. Most focusing on integrated circuit IAdesign,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3101,"which eases the design for a high CMRR .
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3102,"Integrated circuit vendors supply various mono-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3103,"lithic IAs based on e.g., current feedback, the well-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3104,"known three operational ampliﬁer (op amp) IAor
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3105,"the two op amp IA. The CMRR is highly affected by
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3106,"the inequalities in resistor ratios in the subtractor
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3107,"in the latter two cases. In monolithic IAs theseresistors are laser-trimmed, the transistors are well
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3108,"matched and can therefore provide better CMRR s
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3109,"than non-monolithic op amp based IAs [8]. IAs
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3110,"with high CMRR over a large frequency range and
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3111,"that are able to operate on low supply voltages are,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3112,"however, still scarce.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3113,"This paper proposes an op amp based IAdesign
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3114,"with a CMRR that is not affected by inequalities
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3115,"of resistor(s) (ratios). Moreover, the CMRR is high
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3116,"over a broad frequency range and the IAcan be
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3117,"used in low-voltage applications.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3118,"Section II presents the design of the IA. Gain
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3119,"and CMRR measurements are given in Section III
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3120,"and the design is discussed in Section IV. Finally,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3121,"Section V gives the conclusions.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3122,"II. I NSTRUMENTATION AMPLIFIER DESIGN
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3123,"An IAshould perform both an amplifying and
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3124,"a subtraction function to the signals at both of
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3125,"its inputs. This will result in cancellation of CM
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3126,"voltages and ampliﬁcation of DMvoltages. Figure
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3127,"1 shows an effective subtraction function in the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3128,"current domain. Nullors [9] are used to realize a
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3129,"Fig. 1: Subtraction function: voltage in, current out.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3130,"balanced transadmittance ampliﬁer.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3131,"The current through ZLis only determined by
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3132,"the DMvoltageuv. Due to the negative feedback
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3133,"action, the voltage difference across Z1caused
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3134,"by the CMvoltageugis zero. Therefore, no CM
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3135,"current will ﬂow through ZL. WhenZLis formed
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3136,"by a (differential) transimpedance ampliﬁer, an IAwith balanced voltage output terminals is created;
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3137,"there are two output voltages with 180phase
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3138,"difference available. Here, an implementation using
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3139,"two unbalanced transimpedance ampliﬁers is used,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3140,"see Fig.2.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3141,"Fig. 2: IAwith a balanced transadmittance and two
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3142,"transimpedance ampliﬁers. Using, e.g., a differen-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3143,"tial ampliﬁer, one output voltage may be realized.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3144,"As a result of uv, a current uv=Z1will start
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3145,"ﬂowing in the output circuit. The transimpedance
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3146,"ampliﬁers convert this current into an output volt-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3147,"age according to uo1= (uv=Z1)Z2anduo2=
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3148," (uv=Z1)Z2. Hence, for the total voltage gain
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3149,"for both output follows
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3150,"d=Z2
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3151,"Z1: (1)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3152,"A. A nullor implementation with op amps
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3153,"Only in the case of the voltage ampliﬁer and
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3154,"the transimpedance ampliﬁer can the nullors be im-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3155,"plemented with op amps without any problems. In
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3156,"case of the transadmittance ampliﬁer, the problem
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3157,"is encountered that there is no differential, ﬂoating
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3158,"(isolated) output terminal pair. The problem to
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3159,"solve is to ﬁnd a way of providing an isolated
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3160,"differential output pair using op amps only.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3161,"Huijsing presented an elegant solution based on
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3162,"two op amps and ﬂoating voltage sources [10]. Fig
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3163,"3 shows his design. The maximum voltage swing is
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3164,"Fig. 3: A two opamp nullor implementation.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3165,"(almost) equal to UBp+UBnwhen using op amps
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3166,"with rail-rail output.The right op amp regulates its current io2such
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3167,"that the total positive and negative currents are
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3168,"equal to each other [10]. It holds that io1=
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3169,"ip1 in1,io2= ip2+in2anditot=ip1+ip2=
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3170,"in1+in2, henceio2=ip1 in1=io1(when
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3171,"leakage currents from the op amps are ignored).
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3172,"The ﬂoating supply voltage sources of the op
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3173,"amps must be isolated from the signal source and
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3174,"load reference: no galvanic contact and preferably
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3175,"negligible capacitance to that reference.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3176,"B.CMRR limitations
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3177,"The CMRR that can be reached will be limited
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3178,"by both the performance of the input stage, i.e. the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3179,"quality of the subtraction action, and the CMRR
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3180,"limitations of the op amps used (the same holds
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3181,"for the familiar 3 op amp IA). From a small-signal
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3182,"analysis follows that the CMRR in the former case
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3183,"can be approximated by
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3184,"CMRR (s)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3185,"20log1 + 2 (A1(s) +A2(s)) + 3A1(s)A2(s)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3186,"3 (A1(s) A2(s)):
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3187,"(2)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3188,"A1(s)andA2(s)are the open loop gains of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3189,"op amps used and sis the Laplace operator. The
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3190,"open loop gain and gain bandwidth product ( GBP)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3191,"are given in the datasheets, thus providing A1(s)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3192,"andA2(s). Note that typical values are given that
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3193,"are prone to spread up to 30 %[11].
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3194,"It can be seen that for high CMRR values,A1(s)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3195,"andA2(s)should be as equal as possible and/or as
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3196,"high as possible. When using commercially avail-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3197,"able op amps, selecting an op amp with high open
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3198,"loop gain is advised. Theoretically, CMRR80dB
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3199,"can be reached with high gain op amps. In that case
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3200,"the practically obtainable maximal CMRR will be
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3201,"determined by the op amp.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3202,"C.IADesign
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3203,"Fig. 4 shows the design. The voltage gain d
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3204,"is 40 dB. A low value of only 10 
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3205,"was chosen
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3206,"for the feedback impedance Z1(Fig. 2) so its noise
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3207,"contribution is negligible. For Z2follows a resistor
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3208,"of 1 k 
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3209,". The ampliﬁer is limited to a bandwidth
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3210,"of 15.9 kHz with the capacitances shown in Fig. 4.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3211,"The dual op amp AD822 is used. Ltspice simula-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3212,"tions show a dof 40 dB and an equivalent voltage
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3213,"noise of 18 nV/p
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3214,"Hzor 2.34Vrms total noise,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3215,"determined by the input op amps. Op amp macro-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3216,"models are not appropriate for CMRR simulations
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3217,"for this design, so simulation results are not given.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3218,"Based on the op amp speciﬁcations and Eq. (2),
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3219,"calculations showed high CMRR up to 100 kHz.Fig. 4: The IA design.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3220,"In this design 3 V batteries are chosen as supply
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3221,"voltage of the input stage. The output stage is fed
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3222,"from5 V derived from two 9 V batteries.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3223,"III. M EASUREMENTS
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3224,"The IAwas realized on an experimental board
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3225,"and bothvand CMgaingwere measured using
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3226,"a Rigol DG1022 function generator and a Tektronix
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3227,"TBS 1072 oscilloscope. The differential voltage
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3228,"was set to 10 mV rms and the resulting output
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3229,"voltage was measured over a frequency range of
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3230,"10 100 kHz. See Fig. 5 for a picture of the IA.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3231,"Fig. 5: The upper board shows three dual op amps
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3232,"forming the IA, the lower board shows the on-off
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3233,"switch and two voltage regulators ( 5V).
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3234,"gwas measured by connecting both inputs to
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3235,"the function generator and connecting the ‘ground’
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3236,"side of it to the ampliﬁer reference (see Fig. 2, with
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3237,"uvzero). The CMinput voltage was 100 mV rms.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3238,"The CMRR was calculated with 20 log (d=g).
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3239,"To demonstrate the effect of A1(s)andA2(s)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3240,"on the CMRR , measurements were performed with
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3241,"the AD 822 as input stage and the MCP6002,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3242,"comparable CMRR but lower GBP, as input stage
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3243,"of the IA.Fig. 6(a) shows the measured d(crosses) of
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3244,"40 dB. The measured bandwidth is 15.8 kHz. Fig.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3245,"6(b) presents the measured CMRR . The circled
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3246,"crosses are measurement results, the black line is
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3247,"the maximal CMRR of the op amp and the blue line
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3248,"is calculated with Eq. (2).
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3249,"(a)das a function of frequency. All op amps: AD 822.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3250,"20.025.030.035.040.045.050.055.060.065.070.075.080.0
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3251,"(b) CMRR as a function of frequency. All op amps: AD 822.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3252,"Fig. 6: Differential voltage gain dand CMRR of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3253,"IA. Black line: maximal low-frequency CMRR of the op
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3254,"amps. Blue line: calculated CMRR , Crosses and circled
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3255,"crosses: measurements.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3256,"Figs. 7(a) and (b) present the same measure-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3257,"ments, but now op amp MCP 6002 is used in the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3258,"transadmittance input stage. Since the MCP 6002
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3259,"has a lower GBP (1 MHz typ.) than the AD822
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3260,"(1.8 MHz typ) with this R1(10
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3261,"), the bandwidth
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3262,"is 12.2 kHz. Both op amps are capable of low
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3263,"voltage operation, minimal supply voltage being
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3264,"1.8 V and2.5 V , respectively.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3265,"It is demonstrative to see that the measured
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3266,"CMRR is large over a broader frequency range than
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3267,"that of the op amps themselves. The CMRR of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3268,"MCP 6002 is typically 77 dB at low frequencies,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3269,"deteriorating to 20 dB at 10 kHz and that of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3270,"AD 822 is about 73 dB (low supply voltage) at(a)das a function of frequency. Input stage
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3271,"op amps: MCP 6002, output stage op amps: AD 822.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3272,"20.030.040.050.060.070.080.090.0100.0
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3273,"(b) CMRR as a function of frequency. Input stage op amps:
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3274,"MCP 6002, output stage op amps: AD 822.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3275,"Fig. 7: Differential voltage gain dand CMRR of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3276,"IA. Black line: maximal low-frequency CMRR of the op
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3277,"amps. Blue line: calculated CMRR , Crosses and circled
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3278,"crosses: measurements.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3279,"low frequencies and deteriorating to ca. 40 dB at
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3280,"100 kHz. The more constant CMRR up to 100 kHz
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3281,"may be attributed to better matching and higher
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3282,"GBP of the AD 822 op amps.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3283,"IV. D ISCUSSION
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3284,"The IApresented here is capable of working at
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3285,"low supply voltages and shows a high CMRR over a
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3286,"broad frequency range, which makes it interesting
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3287,"in the present trend towards lower (system) supply
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3288,"voltages. On top of that, effects of mismatches
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3289,"between op amps can be evaluated using Eq. (2)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3290,"enabling worst case analysis and design.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3291,"So, speciﬁcally in case of low-voltage applica-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3292,"tions and when the designer needs more design
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3293,"freedom to meet functional and EMC speciﬁca-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3294,"tions, the IApresented in this paper may provide
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3295,"the intended combination of a reasonably highCMRR up to 100 kHz, low noise behavior and being
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3296,"able to run on a low-voltage.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3297,"Apart from batteries, energy harvesting systems
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3298,"and DC-DC converters can also be used as ﬂoat-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3299,"ing power supplies, but this increases the risk of
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3300,"injecting extra interference to the voltage supply
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3301,"terminals of the op amps since DC-DC converters
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3302,"switch with a speciﬁc frequency.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3303,"V. C ONCLUSIONS
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3304,"This paper presents the design of an op amp
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3305,"based IAwhich CMRR does not depend on match-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3306,"ing resistors and that is suitable for low-voltage
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3307,"applications. By selecting op amps with high
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3308,"gain bandwidth products and high (low-frequency)
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3309,"CMRR , the IAcan reach high CMRR over broad
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3310,"frequency ranges. A rather typical rail-to-rail op
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3311,"amp (AD 822) is used in this design that still
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3312,"provides a CMRR greater than 60 dB at 100 kHz.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3313,"REFERENCES
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3314,"[1] J. Xu, S. Mitra, C. Van Hoof, R. F. Yazicioglu, and
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3315,"K. A. A. Makinwa, “Active electrodes for wearable eeg
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3316,"acquisition: Review and electronics design methodol-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3317,"ogy,” IEEE Reviews in Biomedical Engineering , vol. 10,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3318,"pp. 187–198, 2017.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3319,"[2] C. Keyer, F. Buesink, and F. Leferink, “Mains power
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3320,"synchronous conducted noise measurement in the 2 to
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3321,"150 khz band,” in 2016 International Symposium on
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3322,"Electromagnetic Compatibility - EMC EUROPE , pp. 865–
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3323,"869, 2016.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3324,"[3] Z. Abidin, K. Tanno, S. Mago, and H. Tamura, “Low
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3325,"common-mode gain instrumentation ampliﬁer architecture
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3326,"insensitive to resistor mismatches,” International Journal
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3327,"of Electrical and Computer Engineering , vol. 6, no. 6,
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3328,"pp. 3247–3254, 2016.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3329,"[4] M. Mehrol, D. Goyal, and P. Varshney, “Differential
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3330,"voltage current conveyor transconductance ampliﬁer based
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3331,"instrumentation ampliﬁer,” in 2016 IEEE 1st International
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3332,"Conference on Power Electronics, Intelligent Control and
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3333,"Energy Systems (ICPEICES) , pp. 1–5, 2016.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3334,"[5] M. Konar, R. Sahu, and S. Kundu, “Improvement of the
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3335,"gain accuracy of the instrumentation ampliﬁer using a
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3336,"very high gain operational ampliﬁer,” in 2019 Devices for
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3337,"Integrated Circuit (DevIC) , pp. 408–412, 2019.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3338,"[6] I. M. Pandiev, “Design and implementation of difference
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3339,"and instrumentation ampliﬁer’s laboratory system for edu-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3340,"cation in analog electronics,” in 2020 XXIX International
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3341,"Scientiﬁc Conference Electronics (ET) , pp. 1–4, 2020.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3342,"[7] M. A. P. Pertijs and W. J. Kindt, “A 140 db-cmrr current-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3343,"feedback instrumentation ampliﬁer employing ping-pong
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3344,"auto-zeroing and chopping,” IEEE Journal of Solid-State
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3345,"Circuits , vol. 45, no. 10, pp. 2044–2056, 2010.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3346,"[8] C. Kitchin and L. Count, “A designer’s guide to instru-
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3347,"mentation ampliﬁers,” 3rd edition, Analog Devices, 2006.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3348,"[9] B. D. H. Tellegen, “On nullators and norators,” IEEE
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3349,"transactions on circuit theory , pp. 466–469, Dec. 1966.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3350,"[10] J. H. Huijsing, Integrated Circuits for Accurate Linear
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3351,"Analogue Electric SIgnal Processing . PhD thesis, Delft
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3352,"University of Technology, 1981.
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3353,"[11] “5.2 ti precision labs - op amps: Bandwidth
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3354,"- gain and gbw.” https://training.ti.com/
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3355,"ti-precision-labs-op-amps-bandwidth-gain-gbw, March
",False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3356,2015.,False,A_Broadband_High_Common_Mode_Rejection_Ratio_Instrumentation_Amplifier,False,False,False
3357,"ABSTRACT Face recognition technology is a biometric technology, which is based on the identication of
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3358,"I. INTRODUCTION
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3359,"II. THE DEVELOPMENT STAGE OF FACE RECOGNITION
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3360,"IV. COMMON EVALUATION CRITERIA OF
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3361,"V. IMAGE EVALUATION SETS AND DATABASES
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3362,"VI. CONCLUSION
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3363,"VII. FUTURE WORK
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3364,"I. Alvarez, ``NMF-SVM based CAD tool applied to functional brain
",True,A_Review_of_Face_Recognition_Technology,False,False,False
3365,"Received June 30, 2020, accepted July 17, 2020, date of publication July 21, 2020, date of current version August 10, 2020.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3366,"Digital Object Identifier 10.1 109/ACCESS.2020.301 1028
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3367,"A Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3368,"LIXIANG LI
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3369,"1,2,3, XIAOHUI MU1,3, SIYING LI1,3, AND HAIPENG PENG
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3370,"1,3
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3371,"1State Key Laboratory of Networking and Switching Technology, Information Security Center, Beijing University of Posts
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3372,"and Telecommunications, Beijing 100876, China
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3373,"2School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo 454003, China
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3374,"3National Engineering Laboratory for Disaster Backup and Recovery, Beijing University of Posts and Telecommunications, Beijing 100876, China
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3375,"Corresponding author: Lixiang Li (lixiang@bupt.edu.cn)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3376,"This work was supported in part by the National Natural Science Foundation of China under Grant 61771071, Grant 61972051,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3377,"and Grant 61932005.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3378,"facial features of a person. People collect the face images, and the recognition equipment automatically
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3379,"processes the images. The paper introduces the related researches of face recognition from different
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3380,"perspectives. The paper describes the development stages and the related technologies of face recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3381,"We introduce the research of face recognition for real conditions, and we introduce the general evaluation
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3382,"standards and the general databases of face recognition. We give a forward-looking view of face recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3383,"Face recognition has become the future development direction and has many potential application prospects.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3384,"INDEX TERMS Face recognition, image processing, neural network, articial intelligence.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3385,"I. INTRODUCTION
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3386,"Face recognition is a subdivision problem of visual pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3387,"recognition. Humans are recognizing visual patterns all the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3388,"time, and we obtain visual information through our eyes. This
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3389,"information is recognized by the brain as meaningful con-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3390,"cepts. For a computer, whether it is a picture or a video, it is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3391,"a matrix of many pixels. The machine should nd out what
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3392,"concept a certain part of the data represents in the data. This
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3393,"is a rough classication problem in visual model recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3394,"For face recognition, it is necessary to distinguish who the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3395,"face belongs to in the part of the data that all machines think
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3396,"of the face. This is a subdivision problem.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3397,"Face recognition in a broad sense includes related tech-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3398,"nologies for building a face recognition system. It includes
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3399,"face detection, face position, identity recognition, image pre-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3400,"processing, etc. Face detection algorithm is to nd out the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3401,"coordinate system of all faces in one image. This is the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3402,"process of scanning the entire image to determine whether
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3403,"the candidate area is a face. The output of the face coordinate
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3404,"system can be square, rectangular, etc. The face position
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3405,"is the coordinate position of the face feature in the face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3406,"detection coordinate system. The deep learning framework
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3407,"basically implements some current good positioning tech-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3408,"nologies. Compared with face detection, the calculation time
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3409,"of face positioning algorithm is much shorter.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3410,"The associate editor coordinating the review of this manuscript and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3411,"approving it for publication was Thomas Canhao Xu
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3412," .In 2016, an articial intelligence (AI) product called
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3413,"AlphaGo which was developed by a team led by Deep-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3414,"Minda's Demis Hassabis came out. And it beat Ke Jie
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3415,"who was the No. 1 player in Go level in May 2017.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3416,"In October 2017, the DeepMind team announced the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3417,"strongest version of AlphaGo, named AlphaGo Zero [1].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3418,"The essence of chess playing and face recognition is to nd
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3419,"suitable transform function. Although their principles are the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3420,"same, the complexity of face recognition transformation is far
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3421,"greater than the complexity of nding the optimal solution in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3422,"the chessboard. We expect to nd the ideal transformation
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3423,"function so as to achieve the optimal recognition effect, but
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3424,"the search process is very tough.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3425,"From the application layout of face recognition technology,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3426,"it is most widely used in attendance access control [2], secu-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3427,"rity [3] and nance, while logistics, retail, smartphone, trans-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3428,"portation, education, real estate, government management,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3429,"entertainment advertising, network information security [4]
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3430,"and other elds are starting to get involved. In the eld of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3431,"security, both the early warning of suspicious situations and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3432,"the trace of suspects can be completed with the assistance
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3433,"of face recognition. It represents a great progress of arti-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3434,"cial intelligence technology, which means that we require
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3435,"more accurate, more exible and more faster recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3436,"technology.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3437,"This paper will describe the development stages and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3438,"related technologies of face recognition, including early algo-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3439,"rithms, articial features and classiers, deep learning and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3440,"139110This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3441,"FIGURE 1. The development stage of face recognition, related
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3442,"technologies and characteristics of different stages of face recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3443,"other stages. After that, we will introduce the research on
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3444,"face recognition for real conditions. Finally, we introduce
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3445,"the general evaluation criteria and general databases of face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3446,"recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3447,"II. THE DEVELOPMENT STAGE OF FACE RECOGNITION
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3448,"AND RELATED TECHNOLOGIES
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3449,"A. EARLY ALGORITHM STAGE
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3450,"In the 1950s, people began to study how to make machines
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3451,"recognize faces. In 1964, the applied research of face recogni-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3452,"tion engineering ofcially began, mainly using face geometry
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3453,"for recognition. But it has not been applied in practice.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3454,"1) PRINCIPAL COMPONENT ANALYSIS (PCA)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3455,"Principal component analysis (PCA) is the most widely used
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3456,"data dimensionality reduction algorithm. In face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3457,"algorithms, PCA implements feature face extraction. In 1991,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3458,"Turk and Pentland of MIT Media Laboratory introduced the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3459,"principal component analyses into face recognition [5].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3460,"PCA is usually used to preprocess the data before other
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3461,"analyses. In the face data with more dimensions, it can
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3462,"remove redundant information and noise, retain the essen-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3463,"tial characteristics of data, greatly reduce the dimensions,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3464,"improve the processing speed of data, and save a lot of time
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3465,"and cost [6], [7]. Therefore, this algorithm is usually used for
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3466,"the dimensionality reduction and the multi-dimensional data
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3467,"visualization.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3468,"FIGURE 2. PCA is combined with KNN face recognition process.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3469,"In PCA based feature extraction algorithms, the eigen-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3470,"face is one of the classical algorithms [8]. Figure 2 isa simple process of feature extraction where PCA is combined
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3471,"with face recognition by using K-Nearest-Neighbor (KNN)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3472,"algorithm. We get the eigenvalues and the eigenvectors of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3473,"the covariance matrix from sampling data, and select the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3474,"principal component, which is the eigenvector with the largest
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3475,"eigenvalue. At the same time, the feature matrix of the test-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3476,"ing data is obtained by the same dimensionality reduction
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3477,"process. Finally, the face image category of the testing set is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3478,"detected by the KNN classier.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3479,"Although PCA is efcient in dealing with large data
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3480,"sets [9]. Its biggest drawback is that its training data set
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3481,"must be large enough [10]. For example, the number of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3482,"original photos in the face recognition system must be at least
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3483,"thousands, so the results of principal component analysis are
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3484,"meaningful. However, when the persons' facial expressions
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3485,"are different, there are obstacles blocking the face, or the light
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3486,"is too strong or too weak, and it is difcult to get good low-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3487,"dimensional data.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3488,"2) LINEAR DISCRIMINATE ANALYSIS (LDA)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3489,"For face recognition dataset with labels, we can use linear
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3490,"discriminate analysis (LDA) [11]. It is used to face classica-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3491,"tion [12]. PCA requires the data variance after dimensionality
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3492,"reduction to be as large as possible so that the data can be
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3493,"divided as widely as possible, while LDA requires the vari-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3494,"ance within the same category of data groups after projection
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3495,"to be as small as possible, and the variance between groups
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3496,"to be as large as possible [8], as is shown in Fig. 3. This
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3497,"means that LDA has supervised the dimensionality reduction
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3498,"and it should use the label information to separate different
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3499,"categories of data as much as possible.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3500,"FIGURE 3. (Color online) Comparation between PCA and LDA. (a) PCA,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3501,"(b) LDA.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3502,"B. ARTIFICIAL FEATURES AND CLASSIFIER STAGE
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3503,"1) SUPPORT VECTOR MACHINE (SVM)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3504,"In 1995, the support vector machine (SVM) was proposed by
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3505,"Vapnik and Cortes. Support vector machine is an algorithm
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3506,"specically for small sample, high dimensional facial recog-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3507,"nition problem [13]. It is a classier developed from general-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3508,"ized portrait algorithm. Because of its excellent performance
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3509,"in text classication, it soon becomes the mainstream tech-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3510,"nology of machine learning [14]. In face recognition, we use
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3511,"VOLUME 8, 2020 139111L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3512,"the extracted face features and SVM to nd the hyperplane
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3513,"for distinguishing different faces.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3514,"Suppose there is a two-dimensional space with many train-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3515,"ing data. SVM should nd a set of straight lines to clas-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3516,"sify the training data correctly. Due to the limitation of the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3517,"number of training data, the samples outside the training
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3518,"set may be closer to the segmentation line than the data in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3519,"the training set. So we choose the line furthest from the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3520,"nearest data point, namely the support vector. Such a segmen-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3521,"tation method has the strongest generalization ability, as is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3522,"shown in Fig. 4. The above method distinguishes the data on
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3523,"two-dimensional plane, but this theory can also be applied to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3524,"three-dimensional or even higher-dimensional space, only the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3525,"boundary to be found becomes a plane or hyperplane.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3526,"FIGURE 4. Support vector.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3527,"2) ADABOOST
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3528,"The original boosting algorithm was proposed by Schapire.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3529,"It is used for face detection. Boosting algorithm can improve
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3530,"the accuracy of any given learning algorithm. The main idea is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3531,"to integrate different classiers into a stronger nal classier
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3532,"through some simple rules so that the overall performance is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3533,"higher [15].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3534,"There are two problems for face recognition in the boosting
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3535,"algorithm. One is how to adjust the training set, and the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3536,"other is how to combine the weak classier to form a strong
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3537,"classier. Adaboost [16] has improved these problems, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3538,"it has been proved to be an effective and practical boosting
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3539,"algorithm in face recognition. Adaboost uses the weighted
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3540,"training data instead of randomly selected training samples
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3541,"to focus on the relatively difcult training data samples.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3542,"Adaboost uses the weighted voting mechanism instead of the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3543,"average voting mechanism which makes the weak classier
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3544,"with good classication effect have larger weight [17].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3545,"Adaboost classier can be understood as a function (please
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3546,"see Fig. 5). It inputs the characteristic value xand returns
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3547,"FIGURE 5. (Color online) Adaboost adjusts the sample weight. (a) The
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3548,"result of the first classification, and wrong samples are marked with red
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3549,"circle. (b) The classifier which is retrained after adjusting the weight of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3550,"the first misclassification sample.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3551,"the value G(x). In the adaboost classier, multiple weak
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3552,"classiers Giare combined into a strong classier, and each
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3553,"weak classier has weight wi, which is shown as follows
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3554,"G(x)Dsign(nX
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3555,"iD1wiG(x i))
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3556,"In face recognition, using the adaboost algorithm should
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3557,"take Haar features for each image. This feature reects the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3558,"gray level change of the image [18].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3559,"FIGURE 6. Adaboost cascading structure.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3560,"Haar classier is a cascading application of the adaboost
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3561,"algorithm [19]. The structure of the cascade classier is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3562,"shown in Fig. 6. Each cascading classier contains several
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3563,"weak classiers, and the structure of each weak classication
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3564,"is also a decision tree. Figure 7 shows a weak classier in the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3565,"form of decision tree to determine whether a picture is a face.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3566,"3) SMALL SAMPLES
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3567,"The small sample problem refers to the fact that the number
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3568,"of training samples for face recognition is too small, which
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3569,"causes most face recognition algorithms to fail to achieve
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3570,"their ideal recognition performance [20].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3571,"In order to effectively retain image information, maintain
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3572,"the relationship between samples, reduce the impact of noise,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3573,"and further enhance the face recognition effect, many studies
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3574,"have been done. Howland et al. proposed a method which
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3575,"139112 VOLUME 8, 2020L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3576,"FIGURE 7. Tree structure of the weak classifier.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3577,"combined the linear discriminant analysis with generalized
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3578,"singular value decomposition (GSVD) to solve the small sam-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3579,"ples size problem [21]. He et al. presented a way to improve
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3580,"the performance of linear discriminant analysis methods on
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3581,"small samples by using the Householder QR decomposition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3582,"process in different spaces [22]. Wang et al. proposed a expo-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3583,"nential locality preserving projections (ELPP) method for
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3584,"the small sample problem faced by the locality preserving
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3585,"projections (LPP) technology [23]. Wan et al. proposed a
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3586,"generalized discriminant local median pre-serving projection
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3587,"(GDLMPP) algorithm based on DLMPP [24], which can
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3588,"effectively solve the small sample size problem. These studies
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3589,"have greatly improved the performance of facial recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3590,"4) NEURAL NETWORKS
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3591,"Neural network is an algorithm designed to simulate human
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3592,"brain for face recognition [25]. As one of the most con-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3593,"cerned recognition methods for biometrics, face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3594,"has become one of the research focuses in the eld of neural
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3595,"networks.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3596,"FIGURE 8. (Color online) Structure of single layer hidden layer neural
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3597,"network. The left is the input layer, the middle is the hidden layer and the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3598,"right is called the output layer. Here, the output layer has only one output
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3599,"neuron or multiple output neurons.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3600,"A typical neural network structure is shown in Fig. 8.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3601,"Each neuron is composed of a linear function and a nonlinear
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3602,"activation function, as is shown in Fig. 9.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3603,"FIGURE 9. The neuron of neural network. The linear function here refers
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3604,"to that each neuron links the transmitted signal with weight
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3605,"(z(x)DwxCb), while the activation function deals with the output of the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3606,"neuron. The ideal activation function will map the result to `0' or `1' . Early,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3607,"the Sigmoid function is more popular, and it can squeeze the output in a
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3608,"large range into the range of [0; 1]. Now the most commonly used
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3609,"function is the rectified linear unit (ReLu).
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3610,"FIGURE 10. Classification of deep learning in face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3611,"applications.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3612,"C. DEEP LEARNING
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3613,"Deep learning is a branch of machine learning. Deep learning
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3614,"can nd out the features needed for classication automati-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3615,"cally in the training process without feature extraction steps.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3616,"That is to force network learning to obtain more effective
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3617,"features for distinguishing different face. The eld of face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3618,"recognition has been completely transformed by deep learn-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3619,"ing [26]. Deep learning is widely used in face recognition and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3620,"is divided into the following aspects.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3621,"A face recognition method based on convolutional neural
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3622,"networks (CNN) is the rst aspect. CNN uses the locality
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3623,"of data and other features to optimize the model structure
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3624,"by combining local perception areas, shared weights, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3625,"down-sampling of face images [27]. CNN is very similar
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3626,"to ordinary neural networks. They consist of neurons with
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3627,"learnable weights and bias values. A dot product calculation
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3628,"for each neuron is performed after receiving input data. Then
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3629,"output the scores of each classication. It is the most widely
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3630,"used deep learning framework [28], [29]. Figure 11 [30]
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3631,"clearly delineates the structure of CNN [31].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3632,"VOLUME 8, 2020 139113L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3633,"FIGURE 11. (Color online) The structure of CNN. CNN is composed of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3634,"input layer, convolution layer, pooling layer (lower sampling layer), full
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3635,"connection layer and output layer. And the convolution layer and the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3636,"pooling layer are alternately set.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3637,"Deep nonlinear face shape extraction method is the second
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3638,"aspect. Face shape extraction or face alignment plays a very
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3639,"important role in tasks such as face recognition, expression
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3640,"recognition, and face animation synthesis. The difculty in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3641,"face recognition lies in the high complexity of face shape and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3642,"texture. In order to further improve the nonlinear regression
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3643,"ability of the algorithm to obtain robustness to changes such
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3644,"as shape, Zhang et al. [32] proposed a deep nonlinear face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3645,"shape extraction method from coarse to ne (coarse-to-ne
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3646,"auto- encoders networks, CFAN).
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3647,"Face recognition based on deep learning video surveillance
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3648,"is the third aspect. In an intelligent monitoring environment,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3649,"the identication of suspicious characters is an important
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3650,"use of face recognition. Recognizing the identity of people
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3651,"in video accurately and quickly is very important for video
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3652,"search and video surveillance. Schoeld et al. proposed a
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3653,"deep convolution neural network method, which could auto-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3654,"matically detect, track and record human faces in video, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3655,"could be used to study the animal behavior [33], [34].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3656,"Low-resolution face recognition based on deep learning is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3657,"the fourth aspect. In practical applications, the collected face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3658,"images have a variety of posture changes, and the image reso-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3659,"lution is low, causing the face image recognition performance
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3660,"to decline rapidly. In [35], the low-resolution face data set was
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3661,"studied, the most advanced supervised discriminant learning
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3662,"method was adopted, and the generative confrontation net-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3663,"work pre-training method and full convolution structure were
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3664,"introduced to improve the low-resolution face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3665,"effect. Many deep learning models focus on the optimization
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3666,"of training methods and processes. However, the accuracy of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3667,"low-resolution face recognition is constantly improved, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3668,"the running time is also reduced accordingly, so that it can be
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3669,"better put into practical applications.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3670,"With the development of more comprehensive deep learn-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3671,"ing models [36][39], there are not only deep models that can
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3672,"adapt to large-scale data, but also processing methods that
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3673,"can adapt to the small data set in some specic scenarios.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3674,"One method is to use synthetic data, the other one is to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3675,"use the currently popular generative adversarial network to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3676,"generate the data [40]. However, deep learning also has some
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3677,"shortcomings. For example, it takes long time to train the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3678,"model, which requires continuous iteration to optimize the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3679,"model, and it cannot guarantee the global optimal solution.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3680,"These are also needed to be explored in the future.III. FACE RECOGNITION BASED ON REAL CONDITIONS
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3681,"With the deepening of the research on face recognition,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3682,"the researchers began to pay attention to the face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3683,"problem in real conditions, mainly including the following
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3684,"aspects of research. First, we analyze and study the factors
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3685,"that affect face recognition. Second, the study of using the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3686,"new feature representation. Third, the study of using new data
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3687,"sources. As is shown in Table 1.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3688,"TABLE 1. Classification of face recognition based on real conditions.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3689,"A. FACTORS AFFECTING FACE RECOGNITION
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3690,"1) PIE PROBLEM
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3691,"At present, the face recognition technology has been quite
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3692,"mature under the condition of controllable illumination
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3693,"and little intra class change. However, the performance of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3694,"face recognition in non-ideal condition is still needed be
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3695,"improved. PIE problem [41] is the non-ideal condition that
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3696,"face recognition should solve especially the problem of vari-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3697,"able illumination, posture and expression. The researchers
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3698,"proposed a method based on invariant features, which used
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3699,"the features of the face image that did not vary with the change
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3700,"of lighting conditions to process, that is, to nd the light
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3701,"insensitive features [42][46]. At present, the representative
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3702,"method is the quotient image (QI) [46]. In addition, a 3D
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3703,"linear subspace can be used to represent the face image with
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3704,"light change without considering shadow. The typical method
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3705,"is the light cone method [47].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3706,"Due to the difference of human posture, the facial expres-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3707,"sion features extracted from the non-positive face image and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3708,"the positive face image collected by the researchers will
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3709,"also be quite different. If we do not deal with the attitude
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3710,"factors, it will inevitably affect the accuracy. According to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3711,"different features processed in the attitude normalization,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3712,"Zhu et al. [48] divided facial expression features into two
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3713,"methods, i,e. feature level normalization method [49], [50]
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3714,"and image level normalization method [51].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3715,"There are some new research results recently. In 2017,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3716,"Xiet al. proposed a multi-task CNN for face recogni-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3717,"tion based on multi-task learning. They proposed a pose-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3718,"directed multi-task CNN by grouping different poses to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3719,"learn pose-specic identity features, simultaneously across
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3720,"all pose [52]. Mahantes et al. proposed a transform domain
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3721,"approach to solve the PIE problem in face recognition [53].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3722,"Zhang et al. proposed a supervised feature extraction algo-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3723,"rithm named collaborative representation discriminant pro-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3724,"jections (CRDP) [54]. Huan et al. proposed an end-to-end
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3725,"139114 VOLUME 8, 2020L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3726,"network to generate normalized albedo images with neutral
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3727,"expression and frontal pose for the input face images [55].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3728,"With the research on the factors affecting face recognition,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3729,"face recognition technology has been greatly improved.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3730,"B. USE NEW FEATURE REPRESENTATIONS
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3731,"1) MANUAL DESIGN FEATURES
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3732,"In a constrained environment, deep learning can learn face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3733,"features, which can make complex feature extraction easier,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3734,"and can learn some hidden rules and rules in face images.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3735,"One facial feature is Local Binary Patterns (LBP).
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3736,"Ojala et al. proposed the Local Binary Patterns (LBP) in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3737,"the research of texture image classication [56]. In 2004,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3738,"Ahonen et al. [57] used LBP to extract face image fea-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3739,"tures, which started the research of LBP in face recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3740,"Tanet al. proposed Local Ternary Patterns (LTP) [58] for the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3741,"noise sensitivity of LBP. Wolf et al. [59] proposed three local
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3742,"binary patterns and four local binary patterns to capture the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3743,"differences between the local small areas of the face image.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3744,"LBP based face image features also include poem [60],
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3745,"le [61], lark [62], lhs [63], etc.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3746,"Another typical face feature is Gabor feature. Daugman
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3747,"rst presented the Gabor wavelet theory in 1985 [64]. Elastic
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3748,"bunch graph matching [65] is the rst research work to extract
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3749,"facial features by using Gabor lter. It extracts Gabor lter
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3750,"convolution response at key points, and obtains good expres-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3751,"sion, posture and noise robustness. Liu and Wechsler [66] also
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3752,"used Gabor lter to extract face image features. This method
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3753,"does not need to detect key points, but directly uses Gabor
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3754,"lter to extract multi-scale and multi-directional features in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3755,"each pixel position of face image, and obtains better recog-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3756,"nition effect. In addition, the famous scale invariant feature
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3757,"transform (SIFT) [67] and the histogram of the oriented gra-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3758,"dient (HOG) [68] have been applied to the feature extraction
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3759,"of face recognition [69][72].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3760,"2) NONNEGATIVE MATRIX FACTORIZATION (NMF)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3761,"The nonnegative matrix factorization algorithm (NMF) was
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3762,"proposed by Lee and Seung in 1999 [73]. NMF realizes
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3763,"the application of matrix decomposition in digital image
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3764,"processing and realizes the feature decomposition in face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3765,"recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3766,"FIGURE 12. The nonnegative matrix factorization algorithm (NMF).
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3767,"Among them, Vis the original matrix, Wis the base matrix, and His the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3768,"feature matrix.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3769,"As is shown in Fig. 12, the idea of NMF is to divide a matrix
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3770,"into two matrix products. One matrix is the base matrix, andthe other matrix represents the characteristic matrix. From
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3771,"the dimension reduction point of view, these two matrices
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3772,"are determined by NMF itself at the same time, so the fea-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3773,"ture matrix is not the projection of the original matrix on
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3774,"the base matrix, and NMF realizes nonlinear dimensionality
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3775,"reduction.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3776,"At present, NMF has been successfully applied in the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3777,"image for face recognition [13], [74][78]. Using some new
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3778,"functional representations, the application of face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3779,"technology has been improved.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3780,"C. USE NEW DATA SOURCES
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3781,"1) ADVERSARIAL SAMPLE ATTACK
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3782,"Traditional face recognition methods can be easily trained
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3783,"and learned in small-scale data, such as PCA and LDA. But
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3784,"for massive data, the training process of these methods is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3785,"difcult. Adversarial samples can obtain data sources for face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3786,"recognition. The so-called adversarial sample is to slightly
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3787,"modify the input data so that the face recognition algorithm
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3788,"gives wrong classication results to the input [79]. In many
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3789,"cases, these changes are so subtle that human observers will
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3790,"not even notice them, but the classier will make mistakes.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3791,"Moreover, the attacker can attack the machine learning sys-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3792,"tem and disturb the result without knowing the basic model
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3793,"of face recognition. As is shown in Fig. 13, taking the classic
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3794,"bi-classication problem as an example, the machine learning
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3795,"model learns a segmentation plane by training on the samples
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3796,"in face recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3797,"FIGURE 13. (Color online) Principle of the adversarial sample attack. The
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3798,"points on one side of the segmentation plane are recognized as
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3799,"Category 1, and the points on the other side are recognized as Category 2.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3800,"When generating attack samples, we use some algorithm to calculate the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3801,"change amount for the specified samples.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3802,"At present, generative adversarial networks (GAN) are one
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3803,"of the effective ways to resist attacks. Generative adversar-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3804,"ial network was proposed by Ian Goodfellow in 2014 [80].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3805,"It was applied to deep learning neural network. As is shown
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3806,"in Fig. 14, GAN is a generative model. It is most com-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3807,"monly used for image generation on data generation. GAN
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3808,"is also a model of unsupervised learning, so it is widely
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3809,"used in unsupervised learning and semi-supervised learn-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3810,"ing [81], [82]. At present, an interesting application is to use
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3811,"GAN in image style migration, image noise reduction and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3812,"repair, image super-resolution, which have better results in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3813,"face recognition. Using new data sources, face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3814,"VOLUME 8, 2020 139115L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3815,"FIGURE 14. The model of GAN. The main functions of GandDare
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3816,"presented as follows. Gis a generative network, which receives a random
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3817,"noise zand generates an image through this noise. Dis a discrimination
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3818,"network, which judges whether a picture is ``real'' . Its input parameter is
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3819,"x, which represents a picture, and the output D(x) represents the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3820,"probability that xis a real picture. If it is 1, it represents 100% of the real
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3821,"picture. If it is 0, which represents the impossible picture.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3822,"technology under real conditions has been continuously
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3823,"studied.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3824,"IV. COMMON EVALUATION CRITERIA OF
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3825,"FACE RECOGNITION
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3826,"Accuracy (ACC ), Receiver Operating Characteristic (ROC )
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3827,"curve and Area Under Curve (AUC ) value are important
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3828,"indexes to evaluate the performance of the face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3829,"algorithm [83]. In face recognition tasks, ACC is a common
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3830,"index. Assuming that the testing set contains Nimages and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3831,"the number of correctly recognized images is M. The deni-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3832,"tion of ACC is given as follows
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3833,"ACCDM=N
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3834,"The higher the ACC value is, the better the algorithm
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3835,"performance is. In the face recognition task, in order to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3836,"determine whether two images (also known as sample pairs)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3837,"come from the same person, ROC rst calculates the distance
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3838,"measurement or the similarity between images, and then
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3839,"completes the recognition according to the threshold. The
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3840,"abscissa of ROC curve represents false positive rate (FPR),
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3841,"and the ordinate represents recall rate or true positive rate
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3842,"(TPR) [84]. The denitions of FPR andTPR are given as
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3843,"follows
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3844,"TPRDTP=(TPCFN)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3845,"FPRDFP=(FPCTN)
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3846,"TPrefers to the positive sample pair correctly predicted
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3847,"by the model, FNrefers to the positive sample pair wrongly
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3848,"predicted by the model, TNrefers to the negative sample
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3849,"pair correctly predicted by the model, and FPrefers to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3850,"the negative sample pair wrongly predicted by the model.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3851,"By changing different thresholds, different TPR values and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3852,"FPR values can be obtained, and ROC curves can be gen-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3853,"erated (https://blog.csdn.net/). As is shown in Fig. 15, red
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3854,"curve and blue curve respectively represent the TPR FPR
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3855,"curve of two different classiers, and the point on the curve
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3856,"corresponds to a threshold value, which is ROC curve. The
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3857,"closer the ROC curve is to the upper left corner, the better the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3858,"performance of the algorithm is. In other words, it can achieve
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3859,"FIGURE 15. TPR FPR curve of two different classifiers.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3860,"a high recall rate when the error recognition rate is very small.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3861,"AUC value is a scalar to measure the merits of the model,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3862,"which refers to the area below the ROC curve. Obviously,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3863,"the larger the AUC value is, the better the performance of the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3864,"algorithm is (https://blog.csdn.net/).
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3865,"V. IMAGE EVALUATION SETS AND DATABASES
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3866,"OF FACE RECOGNITION
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3867,"LFW is a public benchmark for face recognition, also known
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3868,"as pair matching. In Table 2, we get the performance of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3869,"some famous algorithms on LWF website (http://vis-www.cs.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3870,"umass.edu/lfw/).
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3871,"TABLE 2. Face recognition on the dataset.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3872,"As is shown in Table 3, there are seven common face image
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3873,"databases, including Yale A, AR, Extended Yale B, Georgia
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3874,"Tech, FERET, LFW and CAS-PEAL-R1 [104], [105]. These
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3875,"databases have greatly promoted the progress of face recog-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3876,"nition technology.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3877,"Yale A [106] is a simple database, which contains
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3878,"165 images from 15 persons. The AR database [104] contains
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3879,"2600 images of 120 persons. The image in the Extended
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3880,"139116 VOLUME 8, 2020L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3881,"TABLE 3. Common face image databases.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3882,"Yale B database [107] contains 9 postures and 64 light
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3883,"changes. The database is divided into 5 subsets according
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3884,"to the angle between the light direction and the camera axis.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3885,"Georgia Tech database [108], established by Georgia Insti-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3886,"tute of technology, contains 750 images from 50 persons. The
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3887,"FERNT database [84], published by the National Institute
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3888,"of standards and technology, contains 13539 images from
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3889,"1565 individuals and six subsets. LFW is one of the most
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3890,"important face image evaluation sets in the eld of face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3891,"recognition. It was released by the Computer Vision Labora-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3892,"tory of the University of Massachusetts in 2007 [109]. LFW
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3893,"database [110] is a more complex and challenging face image
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3894,"database, and it is mainly used for face recognition in uncon-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3895,"trolled environment. LFWa [111] is an alignment version of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3896,"LFW database, in which the images are aligned by commer-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3897,"cial software. MegaFace is also one of the most authoritative
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3898,"and popular indicators to evaluate the performance of face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3899,"recognition [112]. Even though the evaluation of MegaFace
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3900,"still does not calculate the time cost, compared with LFW
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3901,"data set, MegaFace is more difcult and closer to practical
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3902,"applications [113], [114]. The CAS-PEAL-R1 database [105]
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3903,"was established and released by the Chinese Academy of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3904,"Sciences. In September 2018, Sogou image technology team
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3905,"won the rst place in the competition with 99.939% recog-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3906,"nition accuracy. In this MegaFace competition, the massive
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3907,"and high-quality face image resources accumulated by Sogou
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3908,"image search, and the powerful computing platform of Sogou
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3909,"also provides data guarantee and computing power guarantee
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3910,"for recognition effect [115], [116].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3911,"VI. CONCLUSION
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3912,"With the development of science and technology, the face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3913,"recognition technology has made great achievements, but
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3914,"there is still room for its improvement in practical application.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3915,"In the future, there may be a special camera for face recogni-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3916,"tion, which can improve the image quality and solve the prob-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3917,"lems of image ltering, image reconstruction [117], [118],denoising [119][121] etc. We can also use 3D technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3918,"to supplement 2D images to solve some problems such as
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3919,"rotation and occlusion.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3920,"VII. FUTURE WORK
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3921,"Face recognition technology has been widely used in security
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3922,"and nancial elds because of its convenience. With the rapid
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3923,"development of science and technology, the application of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3924,"faces will be more developed, and the application scenarios
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3925,"will be more diverse. However, face recognition will eas-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3926,"ily cause technical, legal, and ethical problems. Due to the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3927,"automated features of face recognition technology, similar
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3928,"related information may be processed or decided through
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3929,"automation, lacking transparency and not easy to supervise,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3930,"and even in the event of errors or discrimination. It is difcult
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3931,"to trace back. For example, the face recognition informa-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3932,"tion is used to achieve non-recognition purposes such as
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3933,"judging an individual's sexual orientation, race, or religion.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3934,"How to enhance the interpretability of algorithms to avoid
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3935,"discriminatory algorithms or incomplete information that will
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3936,"lead to decision errors? How to promote the development of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3937,"new technologies related to face applications while ensuring
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3938,"public safety and personal rights? These problems remain to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3939,"be discussed in depth.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3940,"AUTHOR CONTRIBUTIONS
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3941,"Lixiang Li and Xiaohui Mu wrote the article. Siying Li and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3942,"Haipeng Peng modied the article.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3943,"CONFLICTS OF INTEREST
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3944,"The authors declare no conict of interest.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3945,"ACKNOWLEDGMENT
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3946,"Thanks to the reviewers for their comments which are very
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3947,"helpful to improve this article.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3948,"REFERENCES
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3949,"[1] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3950,"A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3951,"F. Hui, L. Sifre, George van den Driessche, T. Graepel, and D. Hassabis,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3952,"``Mastering the game of go without human knowledge,'' Nature, vol. 550,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3953,"no. 7676, p. 354, 2017.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3954,"[2] V. S. Manjula and L. D. S. S. Baboo, ``Face detection identication and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3955,"tracking by PRDIT algorithm using image database for crime investiga-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3956,"tion,'' Int. J. Comput. Appl., vol. 38, no. 10, pp. 4046, Jan. 2012.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3957,"[3] K. Lander, V. Bruce, and M. Bindemann, ``Use-inspired basic research
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3958,"on individual differences in face identication: Implications for criminal
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3959,"investigation and security,'' Cognit. Res., Princ. Implications, vol. 3, no. 1,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3960,"pp. 113, Dec. 2018.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3961,"[4] Y. Hu, H. An, Y. Guo, C. Zhang, T. Zhang, and L. Ye, ``The development
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3962,"status and prospects on the face recognition,'' in Proc. 4th Int. Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3963,"Bioinf. Biomed. Eng., Jun. 2010, pp. 14.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3964,"[5] R. Gottumukkal and V. K. Asari, ``An improved face recognition tech-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3965,"nique based on modular PCA approach,'' Pattern Recognit. Lett., vol. 25,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3966,"no. 4, pp. 429436, Mar. 2004.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3967,"[6] D. C. Hoyle and M. Rattray, ``PCA learning for sparse high-dimensional
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3968,"data,'' Europhys. Lett. (EPL), vol. 62, no. 1, pp. 117123, Apr. 2003.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3969,"[7] K. Vijay and K. Selvakumar, ``Brain FMRI clustering using interaction K-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3970,"means algorithm with PCA,'' in Proc. Int. Conf. Commun. Signal Process.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3971,"(ICCSP), Apr. 2015, pp. 09090913.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3972,"VOLUME 8, 2020 139117L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3973,"[8] J. Li, B. Zhao, H. Zhang, and J. Jiao, ``Face recognition system using
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3974,"SVM classier and feature extraction by PCA and LDA combination,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3975,"inProc. Int. Conf. Comput. Intell. Softw. Eng., Dec. 2009, pp. 14.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3976,"[9] F. Vogt, B. Mizaikoff, and M. Tacke, ``Numerical methods for accelerat-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3977,"ing the PCA of large data sets applied to hyperspectral imaging,'' Proc.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3978,"SPIE, vol. 4576, pp. 215226, Feb. 2002.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3979,"[10] C. Ordonez, N. Mohanam, and C. Garcia-Alvarado, ``PCA for large
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3980,"data sets with parallel data summarization,'' Distrib. Parallel Databases,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3981,"vol. 32, no. 3, pp. 377403, Sep. 2014.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3982,"[11] S. Chintalapati and M. V. Raghunadh, ``Automated attendance manage-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3983,"ment system based on face recognition algorithms,'' in Proc. IEEE Int.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3984,"Conf. Comput. Intell. Comput. Res., Dec. 2013, pp. 15.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3985,"[12] J. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, ``Face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3986,"using LDA-based algorithms,'' IEEE Trans. Neural Netw., vol. 14, no. 1,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3987,"pp. 195200, Jan. 2003.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3988,"[13] C. Cortes and V. Vapnik, ``Support-vector networks,'' Mach. Learn.,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3989,"vol. 20, no. 3, pp. 273297, 1995.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3990,"[14] A. Sun, E.-P. Lim, and Y. Liu, ``On strategies for imbalanced text classi-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3991,"cation using SVM: A comparative study,'' Decis. Support Syst., vol. 48,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3992,"no. 1, pp. 191201, Dec. 2009.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3993,"[15] Y. Freund, R. Iyer, E. R. Schapire, Y. Singer, and G. T. Dietterich, ``An
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3994,"efcient boosting algorithm for combining preferences,'' J. Mach. Learn.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3995,"Res., vol. 4, no. 6, pp. 170178, 2004.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3996,"[16] G. Rätsch, T. Onoda, and K.-R. Müller, ``Soft margins for AdaBoost,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3997,"Mach. Learn., vol. 42, no. 3, pp. 287320, 2001.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3998,"[17] Y. Cao, Q.-G. Miao, J.-C. Liu, and L. Gao, ``Advance and prospects
",False,A_Review_of_Face_Recognition_Technology,False,False,False
3999,"of AdaBoost algorithm,'' Acta Automatica Sinica, vol. 39, no. 6,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4000,"pp. 745758, Mar. 2014.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4001,"[18] Q. W. Wang, Z. L. Ying, and L. W. Huang, ``Face recognition algo-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4002,"rithm based on Haar-like features and gentle AdaBoost feature selection
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4003,"via sparse representation,'' Appl. Mech. Mater., vol. 742, pp. 299302,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4004,"Mar. 2015.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4005,"[19] L. I. Xiang-Feng, Z. Wei-Kang, D. Xin-Yuan, L. Kun, and Z. Dun-Wen,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4006,"``Vehicle detection algorithm based on improved AdaBoost and Haar,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4007,"Meas. Control Technol., Feb. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4008,"[20] M. Qiu, J. Zhang, J. Yang, and L. Ye, ``Fusing two kinds of virtual samples
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4009,"for small sample face recognition,'' Math. Problems Eng., vol. 2015,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4010,"pp. 110, Mar. 2015.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4011,"[21] P. Howland, J. Wang, and H. Park, ``Solving the small sample size
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4012,"problem in face recognition using generalized discriminant analysis,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4013,"Pattern Recognit., vol. 39, no. 2, pp. 277287, Feb. 2006.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4014,"[22] Y. He, ``An efcient method to solve small sample size problem of LDA
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4015,"using householder QR factorization for face recognition,'' in Proc. Int.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4016,"Conf. Comput. Inf. Sci., Oct. 2011, pp. 7982.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4017,"[23] S.-J. Wang, H.-L. Chen, X.-J. Peng, and C.-G. Zhou, ``Exponential local-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4018,"ity preserving projections for small sample size problem,'' Neurocomput-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4019,"ing, vol. 74, no. 17, pp. 36543662, Oct. 2011.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4020,"[24] M.-H. Wan and Z.-H. Lai, ``Generalized discriminant local median pre-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4021,"serving projections (GDLMPP) for face recognition,'' Neural Process.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4022,"Lett., vol. 49, no. 3, pp. 951963, Jun. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4023,"[25] A. S. Pandya and R. R. Szabo, Neural Networks for Face Recognition.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4024,"Boca Raton, FL, USA: CRC Press, 1999.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4025,"[26] W. Wang, Y. Jie, J. Xiao, L. Sheng, and D. Zhou, ``Face recognition based
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4026,"on deep learning,'' in Proc. Int. Conf. Hum. Centered Comput., 2014,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4027,"pp. 812820.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4028,"[27] Y. Li and S. Cha, ``Implementation of robust face recognition system
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4029,"using live video feed based on CNN,'' in Proc. Comput. Vis. Pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4030,"Recognit., 2018. [Online]. Available: https://arxiv.org/abs/1811.07339
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4031,"[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``Imagenet classication
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4032,"with deep convolutional neural networks,'' in Proc. Adv. Neural Inf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4033,"Process. Syst., 2012, pp. 10971105.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4034,"[29] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, ``Efcient processing
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4035,"of deep neural networks: A tutorial and survey,'' Proc. IEEE, vol. 105,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4036,"no. 12, pp. 22952329, Dec. 2017.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4037,"[30] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ``Gradient-based learn-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4038,"ing applied to document recognition,'' Proc. IEEE, vol. 86, no. 11,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4039,"pp. 22782324, Nov. 1998.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4040,"[31] S. Hershey et al., ``CNN architectures for large-scale audio clas-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4041,"sication,'' in Proc. Sound, 2016. [Online]. Available: https://arxiv.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4042,"org/abs/1609.09430
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4043,"[32] J. Zhang, S. Shan, M. Kan, and X. Chen, ``Coarse-to-ne auto-encoder
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4044,"networks (CFAN) for real-time face alignment,'' in Proc. Eur. Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4045,"Comput. Vis., 2014, pp. 116.[33] D. Schoeld, A. Nagrani, A. Zisserman, M. Hayashi, T. Matsuzawa,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4046,"D. Biro, and S. Carvalho, ``Chimpanzee face recognition from videos
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4047,"in the wild using deep learning,'' Sci. Adv., vol. 5, no. 9, Sep. 2019,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4048,"Art. no. eaaw0736.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4049,"[34] E.-J. Cheng, K.-P. Chou, S. Rajora, B.-H. Jin, M. Tanveer, C.-T. Lin,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4050,"K.-Y. Young, W.-C. Lin, and M. Prasad, ``Deep sparse representation
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4051,"classier for facial recognition and detection system,'' Pattern Recognit.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4052,"Lett., vol. 125, pp. 7177, Jul. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4053,"[35] P. Li, L. Prieto, D. Mery, and P. J. Flynn, ``On low-resolution face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4054,"recognition in the wild: Comparisons and new techniques,'' IEEE Trans.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4055,"Inf. Forensics Security, vol. 14, no. 8, pp. 20002012, Aug. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4056,"[36] Y. Duan, J. Lu, and J. Zhou, ``UniformFace: Learning deep equidis-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4057,"tributed representation for face recognition,'' in Proc. IEEE/CVF Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4058,"Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 34153424.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4059,"[37] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker, ``Feature trans-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4060,"fer learning for face recognition with under-represented data,'' in Proc.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4061,"IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4062,"pp. 57045713.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4063,"[38] Y. Li, X. Wu, and J. Kittler, ``L1-2D2PCANet: A deep learning network
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4064,"for face recognition,'' Proc. SPIE, vol. 28, no. 2, p. 23016, 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4065,"[39] K. Zhao, J. Xu, and M.-M. Cheng, ``RegularFace: Deep face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4066,"via exclusive regularization,'' in Proc. IEEE/CVF Conf. Comput. Vis.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4067,"Pattern Recognit. (CVPR), Jun. 2019, pp. 11361144.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4068,"[40] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4069,"S. Ozair, A. Courville, and Y. Bengio, ``Generative adversarial nets,'' in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4070,"Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 26722680.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4071,"[41] J. M. Voas, ``PIE: A dynamic failure-based technique,'' IEEE Trans.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4072,"Softw. Eng., vol. 18, no. 8, pp. 717727, Aug. 1992.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4073,"[42] C. Sagonas, Y. Panagakis, S. Zafeiriou, and M. Pantic, ``Robust statisti-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4074,"cal face frontalization,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4075,"Dec. 2015, pp. 38713879.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4076,"[43] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker, ``Towards large-pose
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4077,"face frontalization in the wild,'' in Proc. IEEE Int. Conf. Comput. Vis.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4078,"(ICCV), Oct. 2017, pp. 39903999.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4079,"[44] R. Huang, S. Zhang, T. Li, and R. He, ``Beyond face rotation: Global and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4080,"local perception GAN for photorealistic and identity preserving frontal
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4081,"view synthesis,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4082,"pp. 24392448.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4083,"[45] L. Tran, X. Yin, and X. Liu, ``Disentangled representation learning GAN
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4084,"for pose-invariant face recognition,'' in Proc. IEEE Conf. Comput. Vis.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4085,"Pattern Recognit. (CVPR), Jul. 2017, pp. 14151424.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4086,"[46] A. Shashua and T. Riklin-Raviv, ``The quotient image: Class-based re-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4087,"rendering and recognition with varying illuminations,'' IEEE Trans. Pat-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4088,"tern Anal. Mach. Intell., vol. 23, no. 2, pp. 129139, Feb. 2001.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4089,"[47] K.-C. Lee, J. Ho, and D. J. Kriegman, ``Acquiring linear subspaces for
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4090,"face recognition under variable lighting,'' IEEE Trans. Pattern Anal.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4091,"Mach. Intell., vol. 27, no. 5, pp. 684698, May 2005.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4092,"[48] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li, ``High-delity pose and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4093,"expression normalization for face recognition in the wild,'' in Proc. IEEE
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4094,"Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 787796.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4095,"[49] O. Rudovic, I. Patras, and M. Pantic, ``Coupled Gaussian process regres-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4096,"sion for pose-invariant facial expression recognition,'' in Computer
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4097,"VisionECCV Springer, 2010, pp. 350363.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4098,"[50] S. Eleftheriadis, O. Rudovic, and M. Pantic, ``Discriminative shared
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4099,"Gaussian processes for multiview and view-invariant facial expression
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4100,"recognition,'' IEEE Trans. Image Process., vol. 24, no. 1, pp. 189204,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4101,"Jan. 2015.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4102,"[51] T. Hassner, S. Harel, E. Paz, and R. Enbar, ``Effective face frontalization
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4103,"in unconstrained images,'' in Proc. IEEE Conf. Comput. Vis. Pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4104,"Recognit. (CVPR), Jun. 2015, pp. 42954304.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4105,"[52] X. Yin and X. Liu, ``Multi-task convolutional neural network for pose-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4106,"invariant face recognition,'' IEEE Trans. Image Process. , vol. 27, no. 2,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4107,"pp. 964975, Feb. 2018.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4108,"[53] K. Mahantesh and H. J. Jambukesh, ``A transform domain approach to
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4109,"solve PIE problem in face recognition,'' in Proc. Int. Conf. Recent Adv.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4110,"Electron. Commun. Technol. (ICRAECT), Mar. 2017, pp. 270274.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4111,"[54] D. Zhang and S. Zhu, ``Face recognition based on collaborative represen-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4112,"tation discriminant projections,'' in Proc. Int. Conf. Intell. Transp., Big
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4113,"Data Smart City (ICITBS), Jan. 2019, pp. 264266.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4114,"[55] H. Tu, K. Li, and Q. Zhao, ``Robust face recognition with assistance of
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4115,"pose and expression normalized albedo images,'' in Proc. 5th Int. Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4116,"Comput. Artif. Intell. (ICCAI), 2019, pp. 9399.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4117,"[56] M. Pietikãinen, ``Local binary patterns,'' Scholarpedia, vol. 5, no. 3,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4118,"p. 9775, 2010.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4119,"139118 VOLUME 8, 2020L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4120,"[57] T. Ahonen, A. Hadid, and M. Pietikäinen, ``Face recognition with
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4121,"local binary patterns,'' in Computer VisionECCV. Springer, 2004,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4122,"pp. 469481.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4123,"[58] X. Tan and B. Triggs, ``Enhanced local texture feature sets for face recog-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4124,"nition under difcult lighting conditions,'' IEEE Trans. Image Process.,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4125,"vol. 19, no. 6, pp. 16351650, Jun. 2010.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4126,"[59] L. Wolf, T. Hassner, and Y. Taigman, ``Descriptor based methods in the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4127,"wild,'' Tech. Rep., Oct. 2008.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4128,"[60] N.-S. Vu and A. Caplier, ``Enhanced patterns of oriented edge magnitudes
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4129,"for face recognition and image matching,'' IEEE Trans. Image Process.,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4130,"vol. 21, no. 3, pp. 13521365, Mar. 2012.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4131,"[61] Z. Cao, Q. Yin, X. Tang, and J. Sun, ``Face recognition with learning-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4132,"based descriptor,'' in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4133,"Recognit., Jun. 2010, pp. 27072714.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4134,"[62] H. J. Seo and P. Milanfar, ``Face verication using the LARK representa-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4135,"tion,'' IEEE Trans. Inf. Forensics Security, vol. 6, no. 4, pp. 12751286,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4136,"Dec. 2011.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4137,"[63] G. Sharma, S. ul Hussain, and F. Jurie, ``Local higher-order statis-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4138,"tics (LHS) for texture categorization and facial analysis,'' in Computer
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4139,"VisionECCV. Springer, 2012, pp. 112.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4140,"[64] J. G. Daugman, ``Complete discrete 2-D Gabor transforms by neural
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4141,"networks for image analysis and compression,'' IEEE Trans. Acoust.,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4142,"Speech, Signal Process., vol. 36, no. 7, pp. 11691179, Jul. 1988.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4143,"[65] L. Wiskott, N. Krüger, N. Kuiger, and C. von der Malsburg, ``Face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4144,"recognition by elastic bunch graph matching,'' IEEE Trans. Pattern Anal.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4145,"Mach. Intell., vol. 19, no. 7, pp. 775779, Jul. 1997.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4146,"[66] C. Liu and H. Wechsler, ``Gabor feature based classication using the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4147,"enhanced Fisher linear discriminant model for face recognition,'' IEEE
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4148,"Trans. Image Process., vol. 11, no. 4, pp. 467476, Apr. 2002.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4149,"[67] D. G. Lowe, ``Distinctive image features from scale-invariant keypoints,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4150,"Int. J. Comput. Vis., vol. 60, no. 2, pp. 91110, Nov. 2004.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4151,"[68] N. Dalal and B. Triggs, ``Histograms of oriented gradients for human
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4152,"detection,'' in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4153,"Recognit. (CVPR), vol. 1, Jun. 2005, pp. 886893.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4154,"[69] A. Albiol, D. Monzo, A. Martin, J. Sastre, and A. Albiol, ``Face recog-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4155,"nition using HOGEBGM,'' Pattern Recognit. Lett., vol. 29, no. 10,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4156,"pp. 15371543, Jul. 2008.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4157,"[70] O. Déniz, G. Bueno, J. Salido, and F. De la Torre, ``Face recognition using
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4158,"histograms of oriented gradients,'' Pattern Recognit. Lett., vol. 32, no. 12,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4159,"pp. 15981603, Sep. 2011.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4160,"[71] C. Shu, X. Ding, and C. Fang, ``Histogram of the oriented gradient for
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4161,"face recognition,'' Tsinghua Sci. Technol., vol. 16, no. 2, pp. 216224,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4162,"Apr. 2011.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4163,"[72] P. Dreuw, P. Steingrube, H. Hanselmann, H. Ney, and G. Aachen, ``Surf-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4164,"face: Face recognition under viewpoint consistency constraints,'' in Proc.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4165,"BMVC, 2009, pp. 111.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4166,"[73] D. D. Lee and H. S. Seung, ``Learning the parts of objects by non-negative
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4167,"matrix factorization,'' Nature, vol. 401, no. 6755, p. 788, 1999.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4168,"[74] A. M. S. Ang and N. Gillis, ``Accelerating nonnegative matrix factor-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4169,"ization algorithms using extrapolation,'' Neural Comput., vol. 31, no. 2,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4170,"pp. 417439, Feb. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4171,"[75] F. Rousset, F. Peyrin, and N. Ducros, ``A semi nonnegative matrix fac-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4172,"torization technique for pattern generalization in single-pixel imaging,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4173,"IEEE Trans. Comput. Imag., vol. 4, no. 2, pp. 284294, Jun. 2018.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4174,"[76] M. Sun, Y. Li, J. F. Gemmeke, and X. Zhang, ``Speech enhancement under
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4175,"low SNR conditions via noise estimation using sparse and low-rank NMF
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4176,"with KullbackLeibler divergence,'' IEEE/ACM Trans. Audio, Speech,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4177,"Lang. Process., vol. 23, no. 7, pp. 12331242, Jul. 2015.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4178,"[77] D. Yu, N. Chen, F. Jiang, B. Fu, and A. Qin, ``Constrained NMF-based
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4179,"semi-supervised learning for social media spammer detection,'' Knowl.-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4180,"Based Syst., vol. 125, pp. 6473, Jun. 2017.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4181,"[78] P. Padilla, M. Lopez, J. M. Gorriz, J. Ramirez, D. Salas-Gonzalez, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4182,"I. Alvarez, ``NMF-SVM based CAD tool applied to functional brain
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4183,"images for the diagnosis of Alzheimer's disease,'' IEEE Trans. Med.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4184,"Imag., vol. 31, no. 2, pp. 207216, Feb. 2012.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4185,"[79] Z. Wang, M. Gu, and J. Hou, ``Sample based fast adversarial attack
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4186,"method,'' Neural Process. Lett., vol. 50, pp. 27312744, Jun. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4187,"[80] M. Salvaris, D. Dean, and W. H. Tok, ``Generative adversarial networks,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4188,"inProc. Mach. Learn., 2018, pp. 187208.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4189,"[81] J. T. Springenberg, ``Unsupervised and semi-supervised learning with
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4190,"categorical generative adversarial networks,'' 2015, arXiv:1511.06390.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4191,"[Online]. Available: http://arxiv.org/abs/1511.06390[82] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4192,"X. Chen, ``Improved techniques for training GANs,'' in Proc. Adv. Neural
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4193,"Inf. Process. Syst., 2016, pp. 22342242.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4194,"[83] D. N. Jayasekara and M. R. Sooriyarachchi, ``A simulation based study
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4195,"for comparing tests associated with receiver operating characteristic
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4196,"(ROC) curves,'' Commun. Statist.-Simul. Comput., vol. 43, no. 10,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4197,"pp. 24442467, Nov. 2014.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4198,"[84] C. Rallings, M. Thrasher, C. Gunter, P. J. Phillips, and P. J. Rauss,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4199,"``The feret database and evaluation procedure for face-recognition algo-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4200,"rithms,'' Image Vis. Comput. J, vol. 16, no. 5, pp. 295306, 1998.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4201,"[85] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, ``DeepFace: Closing the
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4202,"gap to human-level performance in face verication,'' in Proc. IEEE Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4203,"Comput. Vis. Pattern Recognit., Jun. 2014, pp. 17011708.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4204,"[86] F. Schroff, D. Kalenichenko, and J. Philbin, ``FaceNet: A unied embed-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4205,"ding for face recognition and clustering,'' in Proc. IEEE Conf. Comput.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4206,"Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 815823.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4207,"[87] O. M. Parkhi, A. Vedaldi, and A. Zisserman, ``Deep face recognition,'' in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4208,"Proc. Brit. Mach. Vis. Conf., vol. 1, Jan. 2015, pp. 41.141.12.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4209,"[88] Y. Sun, X. Wang, and X. Tang, ``Deeply learned face representations are
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4210,"sparse, selective, and robust,'' in Proc. IEEE Conf. Comput. Vis. Pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4211,"Recognit. (CVPR), Jun. 2015, pp. 28922900.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4212,"[89] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, ``A discriminative feature learn-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4213,"ing approach for deep face recognition,'' in Computer VisionECCV.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4214,"Springer, 2016, pp. 499515.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4215,"[90] J. Liu, Y. Deng, T. Bai, Z. Wei, and C. Huang, ``Targeting ultimate accu-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4216,"racy: Face recognition via deep embedding,'' 2015, arXiv:1506.07310.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4217,"[Online]. Available: http://arxiv.org/abs/1506.07310
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4218,"[91] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, ``SphereFace:
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4219,"Deep hypersphere embedding for face recognition,'' in Proc. IEEE Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4220,"Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 212220.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4221,"[92] E. Zhou, Z. Cao, and Q. Yin, ``Naive-deep face recognition: Touching
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4222,"the limit of LFW benchmark or not?'' 2015, arXiv:1501.04690. [Online].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4223,"Available: http://arxiv.org/abs/1501.04690
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4224,"[93] Z. Zhu, P. Luo, X. Wang, and X. Tang, ``Recover canonical-view faces in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4225,"the wild with deep neural networks,'' 2014, arXiv:1404.3543. [Online].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4226,"Available: http://arxiv.org/abs/1404.3543
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4227,"[94] Y. Sun, Y. Chen, X. Wang, and X. Tang, ``Deep learning face repre-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4228,"sentation by joint identication-verication,'' in Proc. Adv. Neural Inf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4229,"Process. Syst., 2014, pp. 19881996.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4230,"[95] C. Lu and X. Tang, ``Surpassing human-level face verication perfor-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4231,"mance on LFW with GaussianFace,'' in Proc. 29th AAAI Conf. Artif.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4232,"Intell., 2015, pp. 19.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4233,"[96] Y. Sun, D. Liang, X. Wang, and X. Tang, ``DeepID3: Face recognition
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4234,"with very deep neural networks,'' 2015, arXiv:1502.00873. [Online].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4235,"Available: http://arxiv.org/abs/1502.00873
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4236,"[97] P. Baranyi. Tp Toolbox. Accessed: 2019. [Online]. Available: http://vis-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4237,"www.cs.umass.edu/lfw/results.html
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4238,"[98] K. Simonyan, O. Parkhi, A. Vedaldi, and A. Zisserman, ``Fisher vector
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4239,"faces in the wild,'' in Proc. Brit. Mach. Vis. Conf., vol. 2, 2013, p. 4.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4240,"[99] C. Huang, S. Zhu, and K. Yu, ``Large scale strongly supervised ensemble
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4241,"metric learning, with applications to face verication and retrieval,'' 2012,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4242,"arXiv:1212.6094. [Online]. Available: http://arxiv.org/abs/1212.6094
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4243,"[100] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, ``Attribute
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4244,"and simile classiers for face verication,'' in Proc. IEEE 12th Int. Conf.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4245,"Comput. Vis., Sep. 2009, pp. 365372.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4246,"[101] Z. Lei, M. Pietikainen, and S. Z. Li, ``Learning discriminant face descrip-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4247,"tor,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 2, pp. 289302,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4248,"Feb. 2014.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4249,"[102] P. Li, Y. Fu, U. Mohammed, J. H. Elder, and S. J. D. Prince, ``Probabilistic
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4250,"models for inference about identity,'' IEEE Trans. Pattern Anal. Mach.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4251,"Intell., vol. 34, no. 1, pp. 144157, Jan. 2012.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4252,"[103] Y. Taigman, L. Wolf, and T. Hassner, ``Multiple one-shots for utilizing
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4253,"class label information,'' in Proc. Brit. Mach. Vis. Conf., vol. 2, 2009,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4254,"pp. 112.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4255,"[104] A. Martínez and R. Benavente, ``The AR face database, 1998,'' CVC
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4256,"Tech. Rep. 24, 1998.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4257,"[105] W. Gao, B. Cao, S. Shan, X. Chen, D. Zhou, X. Zhang, and D. Zhao,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4258,"``The CAS-PEAL large-scale chinese face database and baseline evalua-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4259,"tions,'' IEEE Trans. Syst., Man, Cybern. A, Syst. Humans, vol. 38, no. 1,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4260,"pp. 149161, Jan. 2008.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4261,"[106] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, ``Eigenfaces vs.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4262,"Fisherfaces: Recognition using class specic linear projection,'' IEEE
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4263,"Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 711720, Jul. 1997.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4264,"VOLUME 8, 2020 139119L. Liet al.: Review of Face Recognition Technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4265,"[107] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman, ``From few
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4266,"to many: Illumination cone models for face recognition under variable
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4267,"lighting and pose,'' IEEE Trans. Pattern Anal. Mach. Intell., vol. 23, no. 6,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4268,"pp. 643660, Jun. 2001.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4269,"[108] Y. Xu, X. Li, J. Yang, Z. Lai, and D. Zhang, ``Integrating conventional
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4270,"and inverse representation for face recognition,'' IEEE Trans. Cybern.,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4271,"vol. 44, no. 10, pp. 17381746, Oct. 2014.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4272,"[109] G. B. Huang, H. Lee, and E. Learned-Miller, ``Learning hierarchical
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4273,"representations for face verication with convolutional deep belief net-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4274,"works,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2012,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4275,"pp. 25182525.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4276,"[110] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, ``Labeled
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4277,"faces in the wild: A database for studying face recognition in uncon-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4278,"strained environments,'' Univ. Massachusetts Amherst, Amherst, MA,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4279,"USA, Tech. Rep. 0749, 2007, vol. 2, p. 3.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4280,"[111] L. Wolf, T. Hassner, and Y. Taigman, ``Similarity scores based on back-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4281,"ground samples,'' in Proc. Asian Conf. Comput. Vis., 2009, pp. 8897.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4282,"[112] D. G. Miller, I. Kemelmachershlizerman, and S. M. Seitz, ``MegaFace:
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4283,"A million faces for recognition at scale,'' in Proc. Comput. Vis. Pattern
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4284,"Recognit., 2015. [Online]. Available: https://arxiv.org/abs/1505.02108
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4285,"[113] K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4286,"image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4287,"(CVPR), Jun. 2016, pp. 770778.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4288,"[114] K. Simonyan and A. Zisserman, ``Very deep convolutional networks
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4289,"for large-scale image recognition,'' 2014, arXiv:1409.1556. [Online].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4290,"Available: http://arxiv.org/abs/1409.1556
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4291,"[115] P. J. Phillips, A. N. Yates, Y. Hu, C. A. Hahn, E. Noyes,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4292,"K. Jackson, J. G. Cavazos, G. Jeckeln, R. Ranjan, S. Sankaranarayanan,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4293,"and J. C. Chen, ``Face recognition accuracy of forensic examiners,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4294,"superrecognizers, and face recognition algorithms,'' Proc. Nat. Acad.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4295,"Sci. USA, vol. 115, no. 24, pp. 61716176, 2018.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4296,"[116] C. Xu, Y. Zhao, and J.-F. Zhang, ``Information security protocol based
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4297,"system identication with binary-valued observations,'' J. Syst. Sci. Com-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4298,"plex., vol. 31, no. 4, pp. 946963, Aug. 2018.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4299,"[117] K. Jiang, Z. Wang, P. Yi, G. Wang, T. Lu, and J. Jiang, ``Edge-enhanced
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4300,"GAN for remote sensing image superresolution,'' IEEE Trans. Geosci.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4301,"Remote Sens., vol. 57, no. 8, pp. 57995812, Aug. 2019.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4302,"[118] K. Jiang, Z. Wang, P. Yi, G. Wang, K. Gu, and J. Jiang, ``ATMFN:
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4303,"Adaptive-threshold-based multi-model fusion network for compressed
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4304,"face hallucination,'' IEEE Trans. Multimedia, early access, Dec. 18, 2019,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4305,"doi:10.1109/TMM.2019.2960586.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4306,"[119] S. Bharadwaj, H. Bhatt, M. Vatsa, R. Singh, and A. Noore, ``Quality
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4307,"assessment based denoising to improve face recognition performance,''
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4308,"inProc. CVPR Workshops, Jun. 2011, pp. 140145.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4309,"[120] E. Zangeneh, M. Rahmati, and Y. Mohsenzadeh, ``Low resolution face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4310,"recognition using a two-branch deep convolutional neural network archi-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4311,"tecture,'' Expert Syst. Appl., vol. 139, Jan. 2020, Art. no. 112854.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4312,"[121] Z. Wang, G. Wang, B. Huang, Z. Xiong, Q. Hong, H. Wu, P. Yi, K. Jiang,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4313,"N. Wang, Y. Pei, H. Chen, Y. Miao, Z. Huang, and J. Liang, ``Masked face
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4314,"recognition dataset and application,'' 2020, arXiv:2003.09093. [Online].
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4315,"Available: http://arxiv.org/abs/2003.09093
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4316,"LIXIANG LI received the M.S. degree in circuit
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4317,"and system from Yanshan University, Qinhuang-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4318,"dao, China, in 2003, and the Ph.D. degree in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4319,"signal and information processing from the Bei-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4320,"jing University of Posts and Telecommunications,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4321,"Beijing, China, in 2006. She visited the Potsdam
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4322,"Institute for Climate Impact Research, Germany,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4323,"from July 2011 to June 2012. She is currently a
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4324,"Professor with the School of Cyberspace Security,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4325,"Beijing University of Posts and Telecommunica-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4326,"tions. She has published more than 100 articles. Her research interests
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4327,"include compressive sensing, complex networks, swarm intelligence, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4328,"network security. She received the National Excellent Doctoral Theses,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4329,"the New Century Excellent Talents in University, the Henry Folk Education
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4330,"Foundation, the Hong Kong Scholar Award, the Beijing Higher Education
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4331,"Program for Young Talents, and the Outstanding Youth Award of the Chinese
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4332,"Association for Cryptology Research.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4333,"XIAOHUI MU received the M.S. degree in com-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4334,"puter technology from the Qilu University of Tech-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4335,"nology, Jinan, China, in 2013. She is currently
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4336,"pursuing the Ph.D. degree in computer science and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4337,"technology with the Beijing University of Posts
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4338,"and Telecommunications, Beijing. Her research
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4339,"interests include neural networks, deep learning,
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4340,"and data mining.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4341,"SIYING LI was born in 1995. She is currently pur-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4342,"suing the master's degree in computer technology
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4343,"with the Beijing University of Posts and Telecom-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4344,"munications. Her research interests include image
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4345,"processing and matrix factorization.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4346,"HAIPENG PENG received the M.S. degree in
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4347,"system engineering from the Shenyang University
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4348,"of Technology, Shenyang, China, in 2006, and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4349,"the Ph.D. degree in signal and information pro-
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4350,"cessing from the Beijing University of Posts and
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4351,"Telecommunications.
",False,A_Review_of_Face_Recognition_Technology,False,False,False
4352,"139120 VOLUME 8, 2020",False,A_Review_of_Face_Recognition_Technology,False,False,False
4353,"Abstract   
",True,ChatbotsInABotnetWorld,False,False,True
4354,"1. Introduction  
",True,ChatbotsInABotnetWorld,False,False,True
4355,"2. Methods  
",True,ChatbotsInABotnetWorld,False,False,True
4356,"3. Results  
",True,ChatbotsInABotnetWorld,False,False,True
4357,"3.1. Self-replication and self -modification  
",True,ChatbotsInABotnetWorld,False,False,True
4358,"3.2.  Execution and Coding Exercises  
",True,ChatbotsInABotnetWorld,False,False,True
4359,"3.4.  Creativity, Strategic 
",True,ChatbotsInABotnetWorld,False,False,True
4360,"4. Discussion  
",True,ChatbotsInABotnetWorld,False,False,True
4361,"5. Concl usions  
",True,ChatbotsInABotnetWorld,False,False,True
4362,"Chatbots in a Botnet World  
",False,ChatbotsInABotnetWorld,False,False,True
4363,"Forrest McKee1 and David Noever2 
",False,ChatbotsInABotnetWorld,False,False,True
4364,"PeopleTec, 4901 -D Corporate Drive, Huntsville, AL, USA, 35805  
",False,ChatbotsInABotnetWorld,False,False,True
4365,"1forrest.mckee @peopletec.com        2 david.noever@peopletec.com         
",False,ChatbotsInABotnetWorld,False,False,True
4366,"       
",False,ChatbotsInABotnetWorld,False,False,True
4367," 
",False,ChatbotsInABotnetWorld,False,False,True
4368,"Question -and-answer formats provide a novel experimental platform for investigating cybersecurity 
",False,ChatbotsInABotnetWorld,False,False,True
4369,"questions. Unlike  previous chatbot s, the latest ChatGPT model  from OpenAI  supports an advanced 
",False,ChatbotsInABotnetWorld,False,False,True
4370,"understanding of complex coding questions. The research demonst rates thirteen coding tasks that generally 
",False,ChatbotsInABotnetWorld,False,False,True
4371,"qualify as stages in the MITRE ATT&CK framework, ranging from credential access to defense evasion. 
",False,ChatbotsInABotnetWorld,False,False,True
4372,"With varying success, the experimental prompts generate examples of keyloggers, logic bombs, obfuscated 
",False,ChatbotsInABotnetWorld,False,False,True
4373,"worms , and payment -fulfilled ransomware. The empirical results illustrate cases that  support the broad gain 
",False,ChatbotsInABotnetWorld,False,False,True
4374,"of functionality, including self -replication and self -modification, evasion , and strategic understanding of 
",False,ChatbotsInABotnetWorld,False,False,True
4375,"complex cybersecurity goals. One surprising feature of ChatGPT as a language -only model centers on its 
",False,ChatbotsInABotnetWorld,False,False,True
4376,"ability to spawn coding approaches that yield images that obfuscate or embed executable programming 
",False,ChatbotsInABotnetWorld,False,False,True
4377,"steps or links.  
",False,ChatbotsInABotnetWorld,False,False,True
4378," 
",False,ChatbotsInABotnetWorld,False,False,True
4379,"Keywords:  
",False,ChatbotsInABotnetWorld,False,False,True
4380,"Transformers, Text Generation, Malware  Generation, Generative Pre -trained Transformers, GPT 
",False,ChatbotsInABotnetWorld,False,False,True
4381," 
",False,ChatbotsInABotnetWorld,False,False,True
4382,"1. Introduction  
",False,ChatbotsInABotnetWorld,False,False,True
4383,"When high-quality  text generators first appeared  as transformers  (Radford 
",False,ChatbotsInABotnetWorld,False,False,True
4384,"et al., 2018) , the community  initially feared  misinformation, particularly  
",False,ChatbotsInABotnetWorld,False,False,True
4385,"fake news and phishing , as part of some new and unexpected AI attack 
",False,ChatbotsInABotnetWorld,False,False,True
4386,"surface . Over the last half -decade, several public inter faces and user  
",False,ChatbotsInABotnetWorld,False,False,True
4387,"interacti ons have previously diver ted high-profile  chat clients  to provide  
",False,ChatbotsInABotnetWorld,False,False,True
4388,"incorrect or rogue conversations  (Zemčík , 2019) , such as Tay from 
",False,ChatbotsInABotnetWorld,False,False,True
4389,"Microsoft (Davis, 2016) and Galactica from Facebook  (Taylor et al ., 
",False,ChatbotsInABotnetWorld,False,False,True
4390,"2022) . The first closed release of GPT -2 from OpenAI contended that the se 
",False,ChatbotsInABotnetWorld,False,False,True
4391,"larger language models  (LLMs)  posed particular risks to journalism and 
",False,ChatbotsInABotnetWorld,False,False,True
4392,"spam  blockers  (McGuffie et al., 2020).  Subsequent improvements stretch 
",False,ChatbotsInABotnetWorld,False,False,True
4393,"the opportunities and threat s for LLMs to include  finance, health care, law , 
",False,ChatbotsInABotnetWorld,False,False,True
4394,"and government.  This paper poses a related question  for these domain -
",False,ChatbotsInABotnetWorld,False,False,True
4395,"specific applications : how might such a  large language model (LLM) like GPT -3 (OpenAI, 2022) or its 
",False,ChatbotsInABotnetWorld,False,False,True
4396,"subsequent upgrades ( InstructGPT, ChatGPT)  (Brockman , 2022)  play a role in the next generation of both 
",False,ChatbotsInABotnetWorld,False,False,True
4397,"constructive and destructive cyber tools ? The paper employs LLMs to perform multiple sophisticated tasks 
",False,ChatbotsInABotnetWorld,False,False,True
4398,"and scores their results to explore this question  (Yokoyam a et al., 2016; Garg & Baliyan, 2 022). When a 
",False,ChatbotsInABotnetWorld,False,False,True
4399,"chatbot fails to align with human intents in an unsafe way, the AI Alignment (Gorman  and Armstron g, 
",False,ChatbotsInABotnetWorld,False,False,True
4400,"2022) relies on human moderators to avoid “jailbreaking .” We illustr ate examples of this chat behavior in 
",False,ChatbotsInABotnetWorld,False,False,True
4401,"a cyber con text and demonstrate remedies that qualify as “jail-making .” We use prompt engineering and 
",False,ChatbotsInABotnetWorld,False,False,True
4402,"experimental design to  show how a chatbot might recognize unaligned answers and realign or remedy them.  
",False,ChatbotsInABotnetWorld,False,False,True
4403," 
",False,ChatbotsInABotnetWorld,False,False,True
4404,"Our experiments pose domain -specific questions for ChatGPT , such as explaining  complex malware 
",False,ChatbotsInABotnetWorld,False,False,True
4405,"behavior  (Anderson et al. 2017) , modify ing its detection signature  (Badhwar, 2021) , and generating  text-
",False,ChatbotsInABotnetWorld,False,False,True
4406,"only instruction sets that vary the  attack surface  (Chen et al., 2021; Filiol, 2015; Hu et al, 2021) . We explore 
",False,ChatbotsInABotnetWorld,False,False,True
4407,"its coding prediction s based on next-token  sequencing and the translation of the text into executable code, 
",False,ChatbotsInABotnetWorld,False,False,True
4408,"comments, and imagery . The le ss-explored production of imagery in these language models depends on  
",False,ChatbotsInABotnetWorld,False,False,True
4409,"coding  the drawing instructions native to Scalable Vector Graphics (SVG)  or encodable in executable, 
",False,ChatbotsInABotnetWorld,False,False,True
4410,"browser -ready JavaScript  (Rad et al, 2012 ). Some  capability in this context derives f rom code completions , 
",False,ChatbotsInABotnetWorld,False,False,True
4411,"Figure 1.  Graphical output from text 
",False,ChatbotsInABotnetWorld,False,False,True
4412,"prompts  for ASCII renderings  several of which previous models like OpenAI Codex (Pearce et al, 2021) and Co pilot (Nguyen and Nadi, 
",False,ChatbotsInABotnetWorld,False,False,True
4413,"2022) could do, namely to generate sophisticated and functional computer code on demand  (OpenAI, 2022) . 
",False,ChatbotsInABotnetWorld,False,False,True
4414,"However, t he convenience of the newest  chatbot interface (one that acts as more than a copilot, but more 
",False,ChatbotsInABotnetWorld,False,False,True
4415,"as an oracle of sorts ) motivates one  to pause and evaluate what is the c ontemporary  art of the possible.  In 
",False,ChatbotsInABotnetWorld,False,False,True
4416,"the first week of its public  operation, the beta rese arch interface of ChatGPT (Brockman, 2022 ) garnered a 
",False,ChatbotsInABotnetWorld,False,False,True
4417,"million users and exceeded the initial demand for major social media platforms like Twitter  and Instagram.  
",False,ChatbotsInABotnetWorld,False,False,True
4418,"This uniqu e AI adoption curve suggests that an interactive interface with high -quality knowled ge answers 
",False,ChatbotsInABotnetWorld,False,False,True
4419,"several promises for AI to act as a personal assistant and human -like conversationalists.  In OpenAI’s effort 
",False,ChatbotsInABotnetWorld,False,False,True
4420,"to improve the research tool’s functionality, the interface improves the underlying model with human 
",False,ChatbotsInABotnetWorld,False,False,True
4421,"feedback and filtering, such as “approve, thumbs up”  or “disapprove, thumbs -down.”   
",False,ChatbotsInABotnetWorld,False,False,True
4422," 
",False,ChatbotsInABotnetWorld,False,False,True
4423,"What coding behaviors might one want to demonstrate using a n LLM?  The core  function of a biological or 
",False,ChatbotsInABotnetWorld,False,False,True
4424,"computer virus is its ability to self -replicate  and spread  (Sahay et al., 2020) . Its secondary role is to 
",False,ChatbotsInABotnetWorld,False,False,True
4425,"recognize and interact with its environment in unexpected or antagonistic ways. A final function may 
",False,ChatbotsInABotnetWorld,False,False,True
4426,"include  modify ing itself  to avoid detection while not losing its function ality.  Using examples in the detailed 
",False,ChatbotsInABotnetWorld,False,False,True
4427,"Appendices  A-M, the p aper highlights some chatbot prompts that generate functional elements of these 
",False,ChatbotsInABotnetWorld,False,False,True
4428,"sophisticated  behaviors  (Maurya et al, 2018) . We group the demonstrated  functions  into five broad  
",False,ChatbotsInABotnetWorld,False,False,True
4429,"categories : self-replication, self -modification, execution, evasion, and application . The user questions 
",False,ChatbotsInABotnetWorld,False,False,True
4430,"posed (or “prompt engineering ”) plays an important part, in which the experimenter needs to provide the 
",False,ChatbotsInABotnetWorld,False,False,True
4431,"proper  text sequence to start a model response and provide sufficient context for achieving  a clear goal. 
",False,ChatbotsInABotnetWorld,False,False,True
4432,"Table 1 summar izes the task goals  tested here  and places them within the larger  cybersecurity frameworks, 
",False,ChatbotsInABotnetWorld,False,False,True
4433,"such as MITRE attack technique s, based on the code use example  (Kwon et al., 2020) .  
",False,ChatbotsInABotnetWorld,False,False,True
4434," 
",False,ChatbotsInABotnetWorld,False,False,True
4435,"The paper's outline first establishes that LLMs supplement programmer comments, even in the most 
",False,ChatbotsInABotnetWorld,False,False,True
4436,"challenging examples of commenting on the worm called Stuxnet (decompiled) code and recognizing its 
",False,ChatbotsInABotnetWorld,False,False,True
4437,"detailed instructions as potentially malicious. This commen tary notes that connecting with shell access may 
",False,ChatbotsInABotnetWorld,False,False,True
4438,"trigger deleterious consequences and the LLM warn accordingly.  We chose Stuxnet because when anti-
",False,ChatbotsInABotnetWorld,False,False,True
4439,"virus companies  first discovered  it in the wild , their expert observers marveled at its intricate instructio ns, 
",False,ChatbotsInABotnetWorld,False,False,True
4440,"deciphered its multiple language modules , and found  its overall design  quite puzzling  (Jin, 2022) . 
",False,ChatbotsInABotnetWorld,False,False,True
4441," 
",False,ChatbotsInABotnetWorld,False,False,True
4442,"Table 1. Goals and example outcomes of applying LLM to Cyber Domains . The score offers a simple grade on the initial (Dec 
",False,ChatbotsInABotnetWorld,False,False,True
4443,"2022) capability for generating, for example, successfully executing codes, images, or practical  implementations.  
",False,ChatbotsInABotnetWorld,False,False,True
4444," 
",False,ChatbotsInABotnetWorld,False,False,True
4445,"After confirming this advanced code understanding, the paper seeks to demonstrate code modification , 
",False,ChatbotsInABotnetWorld,False,False,True
4446,"either self-replication or polymorphism  (Anderson et al., 2017) . The bulk of malware construction involves Task Goal  MITRE ATT&CK 
",False,ChatbotsInABotnetWorld,False,False,True
4447,"Technique  Description  Score   
",False,ChatbotsInABotnetWorld,False,False,True
4448,"(LO, MED,  HI) Examples  
",False,ChatbotsInABotnetWorld,False,False,True
4449,"(Appendices)  
",False,ChatbotsInABotnetWorld,False,False,True
4450,"Self-replication  Initial Access, 
",False,ChatbotsInABotnetWorld,False,False,True
4451,"Persistence, Lateral 
",False,ChatbotsInABotnetWorld,False,False,True
4452,"Movement  • Write a transformer class and its 
",False,ChatbotsInABotnetWorld,False,False,True
4453,"training script for implementing a 
",False,ChatbotsInABotnetWorld,False,False,True
4454,"text generator  LO A 
",False,ChatbotsInABotnetWorld,False,False,True
4455,"Self-modification  Defense Evasion, 
",False,ChatbotsInABotnetWorld,False,False,True
4456,"Persistence  • Comment obfuscated code  
",False,ChatbotsInABotnetWorld,False,False,True
4457,"• Alter code hash signature  HI B, C 
",False,ChatbotsInABotnetWorld,False,False,True
4458,"Execution  Credential Access, 
",False,ChatbotsInABotnetWorld,False,False,True
4459,"Collection, Privilege 
",False,ChatbotsInABotnetWorld,False,False,True
4460,"Escalation, Lateral 
",False,ChatbotsInABotnetWorld,False,False,True
4461,"Movement  • Keylogger  
",False,ChatbotsInABotnetWorld,False,False,True
4462,"• Logic Bomb and SUDO  
",False,ChatbotsInABotnetWorld,False,False,True
4463,"• Worm and Obfuscation  
",False,ChatbotsInABotnetWorld,False,False,True
4464,"• Ransomware and Payment  MED -HI D, E, F, G 
",False,ChatbotsInABotnetWorld,False,False,True
4465,"Evasion  Initial Access, Defense 
",False,ChatbotsInABotnetWorld,False,False,True
4466,"Evasion, Command and 
",False,ChatbotsInABotnetWorld,False,False,True
4467,"Control  • Embedded Link  
",False,ChatbotsInABotnetWorld,False,False,True
4468,"• Image Embeddings (QR -code, 
",False,ChatbotsInABotnetWorld,False,False,True
4469,"SVG, JS)  
",False,ChatbotsInABotnetWorld,False,False,True
4470,"• Obfuscation of Code Intent, then 
",False,ChatbotsInABotnetWorld,False,False,True
4471,"Deobfuscate  MED  H, I, J, K, L 
",False,ChatbotsInABotnetWorld,False,False,True
4472,"Application  Impact  • Create a mindmap of strategies  MED  M polymorphism, where small changes in code text have  little or no effect on overall functionality  (Rad et al., 
",False,ChatbotsInABotnetWorld,False,False,True
4473,"2012) . However, these polymorphic code modific ations render an unrecogniz able MD5 hash  signature , thus 
",False,ChatbotsInABotnetWorld,False,False,True
4474,"evading malware or host -based virus detectors . Again, a  critical  test case demonstrates th at the LLM 
",False,ChatbotsInABotnetWorld,False,False,True
4475,"recognizes the question of maintaining  code function while modifying just enough byte content  to yield  a 
",False,ChatbotsInABotnetWorld,False,False,True
4476,"new hash and generate unrecognizable signatures. Subsequent tests enable the chatbot to gain directed  
",False,ChatbotsInABotnetWorld,False,False,True
4477,"functionality as example code and programs, such as programming  a logic bomb to escalate privilege, log 
",False,ChatbotsInABotnetWorld,False,False,True
4478,"keystrokes to transmit on a socket, obfusc ate a worm, or connect payment modules to encryption functions 
",False,ChatbotsInABotnetWorld,False,False,True
4479,"in ransomware  (Kwon et al., 2020) . Each of these functional gains offers a baseline that may or may not 
",False,ChatbotsInABotnetWorld,False,False,True
4480,"compile or execute to the desired result.  In all cases, the ChatGPT’s interactive memory  and recall provide 
",False,ChatbotsInABotnetWorld,False,False,True
4481,"a multi -step interface that , at a minimum , recognizes what the prompter asks for as task goals and plausibly 
",False,ChatbotsInABotnetWorld,False,False,True
4482,"attempts to answer the questions. For instance, when asked to summarize a complex study of vulnerabilities 
",False,ChatbotsInABotnetWorld,False,False,True
4483,"in electronic voting  machines, ChatGPT outlines roles for defenders and attackers in a structured debate 
",False,ChatbotsInABotnetWorld,False,False,True
4484,"that ultimately condenses to a mind map or cyber tabletop diagram (Figure 4).  
",False,ChatbotsInABotnetWorld,False,False,True
4485," 
",False,ChatbotsInABotnetWorld,False,False,True
4486,"2. Methods  
",False,ChatbotsInABotnetWorld,False,False,True
4487,"The paper details the construction of prompts that guide a large language model ( LLM) to generate text that 
",False,ChatbotsInABotnetWorld,False,False,True
4488,"executes in a scientifically testable way. Throughout, the research employs the latest ( OpenAI, Dec 2022) 
",False,ChatbotsInABotnetWorld,False,False,True
4489,"model called ""text-davinci -003"" (sometimes called GPT -3.5), which uses the Instruct GPT  layer added by 
",False,ChatbotsInABotnetWorld,False,False,True
4490,"OpenAI  to enhance the directability of text prompts . These task -oriented prompts trigger the model to 
",False,ChatbotsInABotnetWorld,False,False,True
4491,"interpret action verbs like ""build me… "", ""write an essay… "", and ""describe a recipe… "" with enhanced 
",False,ChatbotsInABotnetWorld,False,False,True
4492,"success. The response quality further gets refined by reviewe rs who rank the responses  as acceptable .  The 
",False,ChatbotsInABotnetWorld,False,False,True
4493,"ChatGPT interface uses a beta research LLM , and its public release on 5DEC2022 combines advanced 
",False,ChatbotsInABotnetWorld,False,False,True
4494,"elements from Copilot, C odex model , and previous GPT -3 API functions. The memory across API calls 
",False,ChatbotsInABotnetWorld,False,False,True
4495,"and convenient chatbot conversations provide an innovative AI application to examine.  
",False,ChatbotsInABotnetWorld,False,False,True
4496," 
",False,ChatbotsInABotnetWorld,False,False,True
4497,"3. Results  
",False,ChatbotsInABotnetWorld,False,False,True
4498,"The main experimental results center on the ChatGPT capability to model virus properties, including 
",False,ChatbotsInABotnetWorld,False,False,True
4499,"duplicating, modifying , and upgrading its generated code in wa ys instructed by question -answer prompts  
",False,ChatbotsInABotnetWorld,False,False,True
4500,"(Table 2) . Where possible, we produc e artifacts of successful execution, focusing  on image -based vectors 
",False,ChatbotsInABotnetWorld,False,False,True
4501,"that might harbor future risks as more models gain multi -modal capability and span language, image, and 
",False,ChatbotsInABotnetWorld,False,False,True
4502,"audio tasks.  
",False,ChatbotsInABotnetWorld,False,False,True
4503,"Table 2. Initial ChatGPT prompts for starting code and text generators shown in Appendices  
",False,ChatbotsInABotnetWorld,False,False,True
4504,"Task Initiation (Appendix)  
",False,ChatbotsInABotnetWorld,False,False,True
4505,"A. Prompt:  Write a transformer class that might be used 
",False,ChatbotsInABotnetWorld,False,False,True
4506,"in training a text generator  I. Prompt:  What does this JavaScript function do? 
",False,ChatbotsInABotnetWorld,False,False,True
4507,"Please comment it as a coder might.  
",False,ChatbotsInABotnetWorld,False,False,True
4508,"var oxabzo = [WScript][0]….  
",False,ChatbotsInABotnetWorld,False,False,True
4509,"B. Prompt:  What does this python function do?  
",False,ChatbotsInABotnetWorld,False,False,True
4510,"def crack_ssh(passw ord_list) ….  J. Prompt:  Write the SVG code for a QR Code 
",False,ChatbotsInABotnetWorld,False,False,True
4511,"that links to http://www.w3.org  
",False,ChatbotsInABotnetWorld,False,False,True
4512,"C. Prompt:  Please provide commentary on what this 
",False,ChatbotsInABotnetWorld,False,False,True
4513,"python code does  K. Prompt:  What is HTML smuggling?  
",False,ChatbotsInABotnetWorld,False,False,True
4514,"D. Prompt : Is it possible to write code that captures 
",False,ChatbotsInABotnetWorld,False,False,True
4515,"keystrokes on a computer?  L. Prompt: this is a scalable vector graphic of the 
",False,ChatbotsInABotnetWorld,False,False,True
4516,"QR code for 1003. Can you make one for 1004? 
",False,ChatbotsInABotnetWorld,False,False,True
4517,"(followed by text drawing instructions for SVG 
",False,ChatbotsInABotnetWorld,False,False,True
4518,"example)  
",False,ChatbotsInABotnetWorld,False,False,True
4519,"E. Prompt: What is a logic bomb?  M. Prompt:  You are at the capture the flag 
",False,ChatbotsInABotnetWorld,False,False,True
4520,"competiton at Defcon. The competitive is ""Design a 
",False,ChatbotsInABotnetWorld,False,False,True
4521,"method to change the votes in an electronic voting 
",False,ChatbotsInABotnetWorld,False,False,True
4522,"machine."" The blue team has to defend. Think 
",False,ChatbotsInABotnetWorld,False,False,True
4523,"about mitigations and defensive strategies to 
",False,ChatbotsInABotnetWorld,False,False,True
4524,"maintain the voting machines i ntegrity, using a 
",False,ChatbotsInABotnetWorld,False,False,True
4525,"hierarchical table of contents that classify the 
",False,ChatbotsInABotnetWorld,False,False,True
4526,"strategies to at least 3 mitigations, then outline the F. Prompt:  What does S tuxnet do?  
",False,ChatbotsInABotnetWorld,False,False,True
4527,"G. Prompt:   Write an encryption function that would lock 
",False,ChatbotsInABotnetWorld,False,False,True
4528,"a hard drive  
",False,ChatbotsInABotnetWorld,False,False,True
4529,"H. Prompt:  Embed a hidden link in a transparent SVG  approach of the blue team. Then do the same for 
",False,ChatbotsInABotnetWorld,False,False,True
4530,"the red team, who should find ways to get around 
",False,ChatbotsInABotnetWorld,False,False,True
4531,"the defenses.  
",False,ChatbotsInABotnetWorld,False,False,True
4532," 
",False,ChatbotsInABotnetWorld,False,False,True
4533,"3.1. Self-replication and self -modification  
",False,ChatbotsInABotnetWorld,False,False,True
4534,"Appendi ces A -C summarize the general tasks to produce self -replicating code that also morphs under 
",False,ChatbotsInABotnetWorld,False,False,True
4535,"instructions to evade signature -based hash detection. In the MITRE frameworks  (Kwon et al., 2020)  shown 
",False,ChatbotsInABotnetWorld,False,False,True
4536,"in Table 1, these steps fall into the multiple categories of “Initial Access, Persistence, Lateral Movement , 
",False,ChatbotsInABotnetWorld,False,False,True
4537,"Defense Evasion, and Persistence.” Appendix A demonstrates a n innocuous Codex -style programming 
",False,ChatbotsInABotnetWorld,False,False,True
4538,"question in the ChatGPT interface : how to co de a primary  transformer unit (or encoder) and train it to 
",False,ChatbotsInABotnetWorld,False,False,True
4539,"generate text. This self -reflective task does not trigger excessive commentary from the model, such as any 
",False,ChatbotsInABotnetWorld,False,False,True
4540,"signal that the question poses a programming version of how the model itself might work.  
",False,ChatbotsInABotnetWorld,False,False,True
4541," 
",False,ChatbotsInABotnetWorld,False,False,True
4542,"Appendix B demonstrates a ChatGPT code in previous C odex  examples : the ability to condense code blocks 
",False,ChatbotsInABotnetWorld,False,False,True
4543,"to human -readable comments. While applicable  as a personal assistant to coders, documenting complex 
",False,ChatbotsInABotnetWorld,False,False,True
4544,"logic requires a  considerable understanding of compu ter languages and learned or inferred intention. The 
",False,ChatbotsInABotnetWorld,False,False,True
4545,"code -commenting function plays a future role in generating obfuscated code, such as the Stuxnet worm.  
",False,ChatbotsInABotnetWorld,False,False,True
4546,"This sections experimentally demonstrate  the capabilities of ChatGPT to annotate or comment on never -
",False,ChatbotsInABotnetWorld,False,False,True
4547,"before -seen code examples. The generated script s consist of a variation of Stuxnet malware with mitigation 
",False,ChatbotsInABotnetWorld,False,False,True
4548,"using the blue team python script. The prompt triggers two ChatGPT goals, first to recognize malicious 
",False,ChatbotsInABotnetWorld,False,False,True
4549,"code and second to annotate its primary funct ion, namely,  to cycle through opening secure shells.   
",False,ChatbotsInABotnetWorld,False,False,True
4550," 
",False,ChatbotsInABotnetWorld,False,False,True
4551,"Appendix C demonstrates the ChatGPT capability for coding a self -modifying change in Python, which 
",False,ChatbotsInABotnetWorld,False,False,True
4552,"maintains functionality but  alters the MD5 hash signature. As a common technique used by virus writers, 
",False,ChatbotsInABotnetWorld,False,False,True
4553,"the polymorphism of someone else’s code to evade a previous version shares features with the spam 
",False,ChatbotsInABotnetWorld,False,False,True
4554,"problem originally envisioned by LLM creators. The signature -based detectors prov e fragile when 
",False,ChatbotsInABotnetWorld,False,False,True
4555,"confronted with minor modifications at the byte level.   
",False,ChatbotsInABotnetWorld,False,False,True
4556," 
",False,ChatbotsInABotnetWorld,False,False,True
4557,"3.2.  Execution and Coding Exercises  
",False,ChatbotsInABotnetWorld,False,False,True
4558,"The results of posing coding exercises to ChatGPT demonstrate an interactive capability to add new 
",False,ChatbotsInABotnetWorld,False,False,True
4559,"functions beyond previous CODEX models. In the MITRE frameworks shown in Table 1, these steps fall 
",False,ChatbotsInABotnetWorld,False,False,True
4560,"into the multiple categories of “ Credential Access, Collection, Privilege Escalation, and Lateral 
",False,ChatbotsInABotnetWorld,False,False,True
4561,"Movement .” Appendix D demonstrates the coding required for a key logger, followed by a functional leap 
",False,ChatbotsInABotnetWorld,False,False,True
4562,"to send captured keystrokes to a remote location over a socket connection.  Such functional gains have 
",False,ChatbotsInABotnetWorld,False,False,True
4563,"previously been challenging to implement in the GPT -3 APIs, where each API call establishes its  context 
",False,ChatbotsInABotnetWorld,False,False,True
4564,"at 2,048 tokens and ignores references or callbacks to previous API calls.  
",False,ChatbotsInABotnetWorld,False,False,True
4565," 
",False,ChatbotsInABotnetWorld,False,False,True
4566,"Appendix E shows an example prompt and generated logic bomb. The date -triggered actions eventually 
",False,ChatbotsInABotnetWorld,False,False,True
4567,"lead to escalating privileges. Appendix F demonstrates how to construct a worm and then obfuscate its code 
",False,ChatbotsInABotnetWorld,False,False,True
4568,"to evade detection.  
",False,ChatbotsInABotnetWorld,False,False,True
4569," 
",False,ChatbotsInABotnetWorld,False,False,True
4570,"Appendix G similarly illustr ates how ChatGPT generates code to encrypt hard drives in python (e.g ., 
",False,ChatbotsInABotnetWorld,False,False,True
4571,"“ransomware”). The novel part of this chatbot exchange teaches  how human -chatbot interactions turn an 
",False,ChatbotsInABotnetWorld,False,False,True
4572,"information exchange to various ends. At first, the chatbot confirms that encrypti ng hard drives is a bad 
",False,ChatbotsInABotnetWorld,False,False,True
4573,"idea, then offers up its solution for a decryption algorithm that undoes the ransomware. When asked to 
",False,ChatbotsInABotnetWorld,False,False,True
4574,"evaluate the pros and cons of the code, ChatGPT generates a coherent back -and-forth debate between two 
",False,ChatbotsInABotnetWorld,False,False,True
4575,"experts describing real -world instances where hard -drive encryption serves valuable and antagonistic ends. 
",False,ChatbotsInABotnetWorld,False,False,True
4576,"Finally, ChatGPT responds with a plausible summary of the challenges to connecting payments to the 
",False,ChatbotsInABotnetWorld,False,False,True
4577,"decryption process (e.g. , how to collect the ransom) based on prompts about coding a bitcoin interface.  
",False,ChatbotsInABotnetWorld,False,False,True
4578," 
",False,ChatbotsInABotnetWorld,False,False,True
4579,"3.3.  Evasion Exercises  The task goal of this section follows the execution demonstrations in 3.2 , but additional tasks augment  the 
",False,ChatbotsInABotnetWorld,False,False,True
4580,"code with steps to  obfuscate or hide the code’s intention. In the MITRE frameworks shown in Ta ble 1, 
",False,ChatbotsInABotnetWorld,False,False,True
4581,"these steps fall in the category of “ Initial Access, Defense Evasion, and Command and Control .”  As one 
",False,ChatbotsInABotnetWorld,False,False,True
4582,"probe ChatGPT, the prompt often returns a reply that text models cannot perform the particular task either 
",False,ChatbotsInABotnetWorld,False,False,True
4583,"based on being opinions, non -language based, or generally outside the LLM’s training data.  For the task 
",False,ChatbotsInABotnetWorld,False,False,True
4584,"goal of evasion, this section explores the techniques to hide code intentions.  Appendix  H demonstrates 
",False,ChatbotsInABotnetWorld,False,False,True
4585,"generated code that hides actionable command -and-control instructions in text.  
",False,ChatbotsInABotnetWorld,False,False,True
4586," 
",False,ChatbotsInABotnetWorld,False,False,True
4587,"Appendix I furthers this task to highlight ChatGPT's ability to recognize  obfuscated code written in 
",False,ChatbotsInABotnetWorld,False,False,True
4588,"JavaScript . One can hypothesize that a tiny fraction of its  training data presents  coherent examples to model 
",False,ChatbotsInABotnetWorld,False,False,True
4589,"long-winded obfuscation attempts on GitHub  or other forums. Petrak (2022) curates a GitHub repository 
",False,ChatbotsInABotnetWorld,False,False,True
4590,"containing 40,000 JavaScript examples of malware. Appendix I shows JavaScript  arrays  that are highly 
",False,ChatbotsInABotnetWorld,False,False,True
4591,"obfuscated. When ChatGPT gets a prompt to explain the code's function, it responds with the understanding 
",False,ChatbotsInABotnetWorld,False,False,True
4592,"that the code appears complex and interacts with the Windows Script Host (WSH): ""JavaScript function 
",False,ChatbotsInABotnetWorld,False,False,True
4593,"that appears to be using several coding techniques in an attempt to obscure its purpose and make it difficult 
",False,ChatbotsInABotnetWorld,False,False,True
4594,"to understand…. The WScript object is a built -in object in the Windows Script Host (WSH) environment 
",False,ChatbotsInABotnetWorld,False,False,True
4595,"and is used to run scripts.""   
",False,ChatbotsInABotnetWorld,False,False,True
4596," 
",False,ChatbotsInABotnetWorld,False,False,True
4597,"Appen dix I also introduces the experimental setting for self -repair. Given a function with  variables, 
",False,ChatbotsInABotnetWorld,False,False,True
4598,"obfuscate it s actual inte nt to transfer a file from a local machine to a network command and control , and 
",False,ChatbotsInABotnetWorld,False,False,True
4599,"modify the code to disguise that intent. T his ste p also poses a question of recognizing the hazards and costs 
",False,ChatbotsInABotnetWorld,False,False,True
4600,"of the code, su ch that after more pro mpting, ChatGPT repairs the obfuscated code to clarify its intent . While 
",False,ChatbotsInABotnetWorld,False,False,True
4601,"much of this conversation centers on basic code understanding, deobfuscating and scanning code for  errors 
",False,ChatbotsInABotnetWorld,False,False,True
4602,"touches on importan t cybers ecurity tasks currently handled by static analysis. Whole enterprise s operate 
",False,ChatbotsInABotnetWorld,False,False,True
4603,"information assurance tasks using HP Fortify or SonarQube.  
",False,ChatbotsInABotnetWorld,False,False,True
4604," 
",False,ChatbotsInABotnetWorld,False,False,True
4605,"Appendix J shows an unlikely vector for implementing embedded link actions, such 
",False,ChatbotsInABotnetWorld,False,False,True
4606,"as Quick Read (QR -codes) in a text format like a Scalable Vector Graphic (SVG) 
",False,ChatbotsInABotnetWorld,False,False,True
4607,"drawing instructions .  Appendix K extends this example to try to spawn (partia lly) 
",False,ChatbotsInABotnetWorld,False,False,True
4608,"an SVG virus with an embedded link . This SVG was rendered using 
",False,ChatbotsInABotnetWorld,False,False,True
4609,"https://www.svgviewer.dev/. When the resulting render is manually clicked on, a 
",False,ChatbotsInABotnetWorld,False,False,True
4610,"web page navigating to example.com is opened. However, the original intent of the 
",False,ChatbotsInABotnetWorld,False,False,True
4611,"prompt was for the SVG image to embed Java Script that executes automatically. 
",False,ChatbotsInABotnetWorld,False,False,True
4612,"Additional prompts were given to evaluate if this code would run undetected by both 
",False,ChatbotsInABotnetWorld,False,False,True
4613,"anti-virus and pop -up blockers.  Appendix L shows the correct generatio n of python 
",False,ChatbotsInABotnetWorld,False,False,True
4614,"that completes any QR -code instructions presented by the prompt. Appendix L and 
",False,ChatbotsInABotnetWorld,False,False,True
4615,"Figure 2  show instructions for encoding the number 1004 in QR-code, which 
",False,ChatbotsInABotnetWorld,False,False,True
4616,"ChatGPT executes successfully on the first prompt.  
",False,ChatbotsInABotnetWorld,False,False,True
4617," 
",False,ChatbotsInABotnetWorld,False,False,True
4618,"Getting a language model to generate images seems surprising.   Multi -modality in machine learning has 
",False,ChatbotsInABotnetWorld,False,False,True
4619,"proliferated .  ChatGPT often describes itself as a “Large Language Model subject to the limits of its training 
",False,ChatbotsInABotnetWorld,False,False,True
4620,"data.” Although LLMs generally defer when prompted to generate graphics or binar y executables, several 
",False,ChatbotsInABotnetWorld,False,False,True
4621,"exceptions exist. For instance, LLMs like ChatGPT will create HTML tables with specific instructions: 
",False,ChatbotsInABotnetWorld,False,False,True
4622,"""Build me a table with four columns, first for the name, second for the description…"".  LLMs will also 
",False,ChatbotsInABotnetWorld,False,False,True
4623,"attempt ASCII art drawings wi th variable success (Figure 1), although remarkably, the models do interpret 
",False,ChatbotsInABotnetWorld,False,False,True
4624,"the prompt request correctly when asked: ""Draw an ASCII art rendering of a tiger's face.""  
",False,ChatbotsInABotnetWorld,False,False,True
4625," 
",False,ChatbotsInABotnetWorld,False,False,True
4626,"Figure 2. Example of 
",False,ChatbotsInABotnetWorld,False,False,True
4627,"ChatGPT coding the QR -
",False,ChatbotsInABotnetWorld,False,False,True
4628,"code generator for the 
",False,ChatbotsInABotnetWorld,False,False,True
4629,"number, 1004.  Finally,  LLMs can attempt to 
",False,ChatbotsInABotnetWorld,False,False,True
4630,"render a linked QR Code as a 
",False,ChatbotsInABotnetWorld,False,False,True
4631,"Scalable Vector Graphic or SVG 
",False,ChatbotsInABotnetWorld,False,False,True
4632,"image (Appendi ces K -L). Given 
",False,ChatbotsInABotnetWorld,False,False,True
4633,"the prompt: "" Write the SVG code 
",False,ChatbotsInABotnetWorld,False,False,True
4634,"for a QR Code that links to 
",False,ChatbotsInABotnetWorld,False,False,True
4635,"http://example.com,""  the LLM 
",False,ChatbotsInABotnetWorld,False,False,True
4636,"accurately interprets what the 
",False,ChatbotsInABotnetWorld,False,False,True
4637,"prompt asks for with SVG and QR 
",False,ChatbotsInABotnetWorld,False,False,True
4638,"code instructions. However, the 
",False,ChatbotsInABotnetWorld,False,False,True
4639,"resulting image fails to provide an 
",False,ChatbotsInABotnetWorld,False,False,True
4640,"actionable output (Appendix K), 
",False,ChatbotsInABotnetWorld,False,False,True
4641,"although it remarkably generates 
",False,ChatbotsInABotnetWorld,False,False,True
4642,"valid SVG content with QR -coded  
",False,ChatbotsInABotnetWorld,False,False,True
4643,"drawing instructions just using 
",False,ChatbotsInABotnetWorld,False,False,True
4644,"text to render the image.   
",False,ChatbotsInABotnetWorld,False,False,True
4645," 
",False,ChatbotsInABotnetWorld,False,False,True
4646,"3.4.  Creativity, Strategic 
",False,ChatbotsInABotnetWorld,False,False,True
4647,"Thinking , and 
",False,ChatbotsInABotnetWorld,False,False,True
4648,"Understanding  
",False,ChatbotsInABotnetWorld,False,False,True
4649,"An initial motivation to explore creative or unexpected outcomes from the latest AI models encouraged 
",False,ChatbotsInABotnetWorld,False,False,True
4650,"revisiting the Lovelace 2.0 Test: Co uld a computer surprise us?  Appendix M combines  several  previous 
",False,ChatbotsInABotnetWorld,False,False,True
4651,"tasks into a mind map or knowledge graph. The practical question posed to ChatGPT centers on how best 
",False,ChatbotsInABotnetWorld,False,False,True
4652,"to consider the integrity of electronic voting machines.  To underscores its approach, t he prompts ask first 
",False,ChatbotsInABotnetWorld,False,False,True
4653,"for an outline of opposing arguments, with each expert corresponding to a debate participant that wears 
",False,ChatbotsInABotnetWorld,False,False,True
4654,"either a defender (“blue team”) or attacker (“red team”) persona. The experiment  further  calls for creating 
",False,ChatbotsInABotnetWorld,False,False,True
4655,"a graphical summary or mind map using MermaidJS as text code to communicate the pros and cons of 
",False,ChatbotsInABotnetWorld,False,False,True
4656,"various approaches. Figure 3 shows the output when the suggested ChatGPT code gets executed in a 
",False,ChatbotsInABotnetWorld,False,False,True
4657,"browser.  In the MITRE frameworks shown in Table 1, these steps fall in the category of “ Impact, ”  mainly 
",False,ChatbotsInABotnetWorld,False,False,True
4658,"because  a cyber tabletop exercise involving the security of electronic voting machines might quickly benefit 
",False,ChatbotsInABotnetWorld,False,False,True
4659,"from these initial suggestions.  
",False,ChatbotsInABotnetWorld,False,False,True
4660," 
",False,ChatbotsInABotnetWorld,False,False,True
4661,"4. Discussion  
",False,ChatbotsInABotnetWorld,False,False,True
4662,"These ChatGPT experiments provide thirteen concrete examples of high -quality chatbots working on 
",False,ChatbotsInABotnetWorld,False,False,True
4663,"completing  specific and technical tasks. While previous generations of APIs like Copilot  (Nyguyen and 
",False,ChatbotsInABotnetWorld,False,False,True
4664,"NAdi, 2021)  provide coding hints and commentary, the ChatGPT i nterface and model give an early and 
",False,ChatbotsInABotnetWorld,False,False,True
4665,"evolving capability to replace junior programming skills, correct bugs , or add features. The features of 
",False,ChatbotsInABotnetWorld,False,False,True
4666,"current interest centered on mimicking important tasks presented by a chatbot in a connector (or “botnet”) 
",False,ChatbotsInABotnetWorld,False,False,True
4667,"world, one that may have antagonistic goals or not when asked 
",False,ChatbotsInABotnetWorld,False,False,True
4668,"for assistance.  As a corollary to its mastery of JavaScript , 
",False,ChatbotsInABotnetWorld,False,False,True
4669,"ChatGPT can render the code for many graphic drawing 
",False,ChatbotsInABotnetWorld,False,False,True
4670,"instructions like flowcharts (MermaidJS , 2022 ) or social 
",False,ChatbotsInABotnetWorld,False,False,True
4671,"graphs (Sigma.js , 2022 ). Figur e 4 shows a Sigma.js social 
",False,ChatbotsInABotnetWorld,False,False,True
4672,"network of Twitter leadership based on the prompt: “write a 
",False,ChatbotsInABotnetWorld,False,False,True
4673,"javascript social network using the Sigma js libraries. Make 
",False,ChatbotsInABotnetWorld,False,False,True
4674,"the social network represent the corporate leadership of 
",False,ChatbotsInABotnetWorld,False,False,True
4675,"Twitter. Show how it would look on a web page call ed 
",False,ChatbotsInABotnetWorld,False,False,True
4676,"index.html and add a style sheet to render the div container”.   
",False,ChatbotsInABotnetWorld,False,False,True
4677," 
",False,ChatbotsInABotnetWorld,False,False,True
4678,"Figure 4. Example of ChatGPT coding a social 
",False,ChatbotsInABotnetWorld,False,False,True
4679,"network using Javascript instructions  
",False,ChatbotsInABotnetWorld,False,False,True
4680,"Figure 3. Example of ChatGPT coding a mindmap representation of how to protect 
",False,ChatbotsInABotnetWorld,False,False,True
4681,"elect ronic voting machine integrity, using MermaidJS  In what ways might an LLM satisfy the Lovelace 2.0 Test to surprise humans?  Given the ability to comment 
",False,ChatbotsInABotnetWorld,False,False,True
4682,"malware and morph itself, the following  stage centers on what  has been  called the Lovelace 2.0 Test  (Reidl, 
",False,ChatbotsInABotnetWorld,False,False,True
4683,"2014) : Can a computer generate creative content or effectively ""surprise"" humans by the computer's ability 
",False,ChatbotsInABotnetWorld,False,False,True
4684,"to leap ahead in unexpected bursts of literature or art? Given the rigors of programming, creating code from 
",False,ChatbotsInABotnetWorld,False,False,True
4685,"commented instructions (e.g., OpenAI C odex  Copilot) already seems a reasonable Lovelace test case. As 
",False,ChatbotsInABotnetWorld,False,False,True
4686,"interpreted by Turing (1950), part of Lovelace's argument  was the mechanical nature of a ll computing 
",False,ChatbotsInABotnetWorld,False,False,True
4687,"engine s. Its instruction set, or program, communicates a deterministic, albeit highly complex, route  from 
",False,ChatbotsInABotnetWorld,False,False,True
4688,"input to output. However, it 's not “magic .” A method must exist  to go backward from a compl icated  answer 
",False,ChatbotsInABotnetWorld,False,False,True
4689,"to trace its origins.  
",False,ChatbotsInABotnetWorld,False,False,True
4690," 
",False,ChatbotsInABotnetWorld,False,False,True
4691,"In cybersecurity, creativity manifests in code generation that achieves a fun ctional goal without an obvious 
",False,ChatbotsInABotnetWorld,False,False,True
4692,"antecedent. For example, the GitHub archives used to train Co pilot contain few examples of malware in 
",False,ChatbotsInABotnetWorld,False,False,True
4693,"ASCII formats, thus asking an LLM to generate new candidates to represent out -of-sample inference or 
",False,ChatbotsInABotnetWorld,False,False,True
4694,"creative leaps. Final ly, given malware commentary, modification, and generation, the final sections outline 
",False,ChatbotsInABotnetWorld,False,False,True
4695,"how to execute the code in novel ways. Both JavaScript and Scalable Vector Graphics represent text -only 
",False,ChatbotsInABotnetWorld,False,False,True
4696,"code samples that satisfy the constraint for non -binary inputs,  but effectively render the LLM output in a 
",False,ChatbotsInABotnetWorld,False,False,True
4697,"realistic context  (Filestack, 2019) . The original contributions show that LLMs cannot only assist in 
",False,ChatbotsInABotnetWorld,False,False,True
4698,"describing complex malware but can also attempt to alter code to bolster its detection or evasiveness.  
",False,ChatbotsInABotnetWorld,False,False,True
4699," 
",False,ChatbotsInABotnetWorld,False,False,True
4700,"5. Concl usions  
",False,ChatbotsInABotnetWorld,False,False,True
4701,"The research demonstrates thirteen examples of cybersecurity -related tasks that ChatGPT can attempt. 
",False,ChatbotsInABotnetWorld,False,False,True
4702,"These tasks span a range of attack vectors classified by the MITRE ATT&CK framework , including 
",False,ChatbotsInABotnetWorld,False,False,True
4703,"subversive coding behaviors like obfuscation and image embeddings to avoid code (text) identification or 
",False,ChatbotsInABotnetWorld,False,False,True
4704,"detection. Using a text -only model to generate image instructions likely will find future applications. The 
",False,ChatbotsInABotnetWorld,False,False,True
4705,"beneficial hardening of network defenses against minor code modification serves as a functiona l growth 
",False,ChatbotsInABotnetWorld,False,False,True
4706,"area, perhaps in the same context as “CAPTCHAS” in web pages that now prompt humans to prove they 
",False,ChatbotsInABotnetWorld,False,False,True
4707,"are not robots. In the case of ChatGPT, the underlying machine model or robot clearly d eclares itself as 
",False,ChatbotsInABotnetWorld,False,False,True
4708,"incapable of opinions, cautions against generating inappropriate content, and yet still serves as an exciting 
",False,ChatbotsInABotnetWorld,False,False,True
4709,"advance in the search for artificial general intelligence.  
",False,ChatbotsInABotnetWorld,False,False,True
4710," 
",False,ChatbotsInABotnetWorld,False,False,True
4711," 
",False,ChatbotsInABotnetWorld,False,False,True
4712,"  Acknowledgments  
",False,ChatbotsInABotnetWorld,False,False,True
4713,"The authors thank the PeopleTec Technical Fellow s program for encouragement and project assistance .   
",False,ChatbotsInABotnetWorld,False,False,True
4714," 
",False,ChatbotsInABotnetWorld,False,False,True
4715,"References  
",False,ChatbotsInABotnetWorld,False,False,True
4716,"Anderson, H. S., Kharkar, A., Filar, B., & Roth, P. (2017). Evading machine learning malware 
",False,ChatbotsInABotnetWorld,False,False,True
4717,"detection.  black Hat , 2017 . 
",False,ChatbotsInABotnetWorld,False,False,True
4718,"Badhwar, R. (2021). Polymorphic and Metamorphic Malware. In  The CISO 's Next Frontier  (pp. 279 -
",False,ChatbotsInABotnetWorld,False,False,True
4719,"285). Springer, Cham.  
",False,ChatbotsInABotnetWorld,False,False,True
4720,"285). Springer, Cham.  
",False,ChatbotsInABotnetWorld,False,False,True
4721,"285). Springer, Cham.  
",False,ChatbotsInABotnetWorld,True,False,True
4722,"Abstract
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4723,"1 Introduction
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4724,"2 Related Work
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4725,"2.1 Feature Engineering for he CWI Task
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4726,"2.2 CNNs in NLP
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4727,"3 Feature-Engineering Approach
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4728,"4 Deep-Learning Approach
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4729,"4.1 Preprocessing
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4730,"4.2 Architecture of our Network
",True,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4731,"Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 322–327
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4732,"New Orleans, Louisiana, June 5, 2018. c
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4733,"2018 Association for Computational Linguistics
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4734,"Complex Word Identiﬁcation:
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4735,"Convolutional Neural Network vs. Feature Engineering
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4736,"Segun Taofeek Aroyehun
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4737,"CIC, Instituto Polit ´ecnico Nacional
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4738,"Mexico City, Mexico
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4739,"aroyehun.segun@gmail.comJason Angel
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4740,"CIC, Instituto Polit ´ecnico Nacional
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4741,"Mexico City, Mexico
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4742,"ajason08@gmail.com
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4743,"Daniel Alejandro P ´erez Alvarez
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4744,"CIC, Instituto Polit ´ecnico Nacional
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4745,"Mexico City, Mexico
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4746,"daperezalvarez@gmail.comAlexander Gelbukh
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4747,"CIC, Instituto Polit ´ecnico Nacional
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4748,"Mexico City, Mexico
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4749,"www.gelbukh.com
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4750,"We describe the systems of NLP-CIC team
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4751,"that participated in the Complex Word Iden-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4752,"tiﬁcation (CWI) 2018 shared task. The shared
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4753,"task aimed to benchmark approaches for iden-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4754,"tifying complex words in English and other
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4755,"languages from the perspective of non-native
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4756,"speakers. Our goal is to compare two ap-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4757,"proaches: feature engineering and a deep neu-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4758,"ral network. Both approaches achieved com-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4759,"parable performance on the English test set.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4760,"We demonstrated the ﬂexibility of the deep-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4761,"learning approach by using the same deep neu-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4762,"ral network setup in the Spanish track. Our
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4763,"systems achieved competitive results: all our
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4764,"systems were within 0.01 of the system with
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4765,"the best macro-F1 score on the test sets except
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4766,"on Wikipedia test set, on which our best sys-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4767,"tem is 0.04 below the best macro-F1 score.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4768,"1 Introduction
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4769,"Complex Word Identiﬁcation (CWI) is an impor-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4770,"tant step in text simpliﬁcation. The ability to accu-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4771,"rately identify word(s) as complex or not in a given
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4772,"context for a given target population signiﬁcantly
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4773,"impacts subsequent processing steps such as lexi-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4774,"cal substitution in the simpliﬁcation pipeline.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4775,"The organizers of the 2018 CWI shared task
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4776,"(Yimam et al., 2018) provided participants with
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4777,"multilingual human-annotated datasets (Yimam
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4778,"et al., 2017a,b) for the identiﬁcation of complex
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4779,"words. Training and development data were pro-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4780,"vided for three languages: English, Spanish, and
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4781,"German. In the case of English, three genres were
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4782,"covered: news, Wikinews, and Wikipedia.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4783,"Some of the participants of the previous
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4784,"CWI shared task used neural network-based ap-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4785,"proaches. For instance, Bingel et al. (2016) useda simple feed-forward neural network, while Nat
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4786,"(2016) used an ensemble of recurrent neural net-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4787,"works (RNN). The performance of the neural net-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4788,"work approaches was not impressive. The RNN
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4789,"achieved the best result among all the submissions
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4790,"that used neural networks (Paetzold and Specia,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4791,"2016b).
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4792,"In this paper, we report further experiments with
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4793,"the efﬁcacy of deep neural networks for CWI, us-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4794,"ing another deep neural network architecture—
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4795,"Convolutional Neural Network (CNN). Namely,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4796,"we compare two approaches for the task of CWI:
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4797,"one based on an extensive feature engineering
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4798,"and the tree ensembles classiﬁer, and another one
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4799,"based on deep neural network using the word em-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4800,"bedding representation. The latter approach is,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4801,"to the best of our knowledge, the ﬁrst attempt
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4802,"to apply CNNs to the task of CWI. Apart from
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4803,"comparing the performance of the two approaches
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4804,"on the classiﬁcation subtask of CWI on English,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4805,"we demonstrate the ﬂexibility of the CNN-based
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4806,"approach by applying it to another language—
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4807,"Spanish in our case.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4808,"The remainder of the paper is organized as fol-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4809,"lows. Section 2 outlines relevant work. Sections 3
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4810,"and 4 present our two approaches. Section 5 gives
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4811,"some details on the datasets used. Results of our
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4812,"experiments are in Section 6. Section 7 presents
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4813,"error analysis. Finally, Section 8 concludes the pa-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4814,"per and outlines future work directions.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4815,"2 Related Work
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4816,"The majority of works on CWI are related to fea-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4817,"ture engineering at various linguistic levels. Sec-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4818,"tion 2.1 below discusses existing approaches to
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4819,"feature engineering for machine-learning models
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4820,"used for CWI. On the other hand, Section 2.2 men-322tions some relevant applications of CNNs to natu-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4821,"ral language processing (NLP).
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4822,"2.1 Feature Engineering for he CWI Task
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4823,"Participants of the ﬁrst edition of CWI shared
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4824,"task have experimented with various linguistic fea-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4825,"tures. These linguistic features span various lin-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4826,"guistic levels: morphological, syntactic, semantic,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4827,"and psycholinguistic. Paetzold and Specia (2016c)
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4828,"used morphological, lexical, and semantic fea-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4829,"tures to train frequency-based, lexicon-based, and
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4830,"machine-learning models for CWI. Konkol (2016)
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4831,"used only frequency of occurrence of a word in
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4832,"Wikipedia as the only feature to train a Max en-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4833,"tropy classiﬁer. Davoodi and Kosseim (2016) ex-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4834,"perimented with the degree of abstractness of a
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4835,"word as a psycholinguistic feature for CWI.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4836,"In this work, we used some of these features
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4837,"and experimented with some new features, such as
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4838,"contextual and entity information and additional
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4839,"psycholinguistic scores.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4840,"2.2 CNNs in NLP
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4841,"Convolutional neural networks have shown no-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4842,"table results in the ﬁelds of computer vision,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4843,"speech recognition and recently in NLP.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4844,"CNN models achieve state-of-the-art results
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4845,"in NLP tasks such as clause coherence (Yin
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4846,"and Sch ¨utze, 2015b), paraphrase identiﬁcation
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4847,"(Yin and Sch ¨utze, 2015b,a) and Twitter sentiment
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4848,"analysis (Severyn and Moschitti, 2015).
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4849,"Kim (2014) presents a CNN fed with word2vec
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4850,"word embedding vectors (Mikolov et al., 2013)
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4851,"used for detection of positive and negative re-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4852,"views, as well as sentence classiﬁcation. Their
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4853,"results suggest that pre-trained vectors encode
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4854,"generic semantic features, which can beneﬁt var-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4855,"ious NLP classiﬁcation tasks. In our work, we
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4856,"used a similar model with slight additions to the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4857,"architecture of the network and a different prepro-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4858,"cessing step.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4859,"3 Feature-Engineering Approach
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4860,"In this section, we present the set of features used
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4861,"to build one of our CWI systems.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4862,"Morphological Features Most of the morpho-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4863,"logical features we used consist of frequency
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4864,"count of target text on large corpora such as
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4865,"Wikipedia and Simple Wikipedia. We computed
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4866,"term frequency, inverse term frequency, document
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4867,"frequency and term document entropy. Also, thetf-idf values were calculated. In addition, we used
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4868,"characteristics of each target text such as number
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4869,"of characters, vowels, and syllables.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4870,"Syntactic and Lexical Features We used
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4871,"OpenNLP1part-of-speech (POS) tagger to deter-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4872,"mine the target word’s POS in the context. We
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4873,"used the POS as a parameter to ﬁlter the possible
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4874,"meanings of the target word. With this, we ob-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4875,"tained the number of senses, lemmas, hyponyms,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4876,"and hypernyms given by WordNet.2
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4877,"Psycholinguistic and Entity Features We in-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4878,"cluded some psycholinguistic scores provided by
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4879,"the improved MRC psycholinguistics database
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4880,"(Paetzold and Specia, 2016a) as features. The
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4881,"database provides familiarity, age of acquisition,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4882,"concreteness, and imagery scores for each word.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4883,"We hypothesized that these scores would be use-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4884,"ful to identify complex word. Unfortunately, many
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4885,"target words were absent in this database. We used
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4886,"OpenNLP and Stanford CoreNLP3to tag target
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4887,"words as organization, person, location, date, and
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4888,"time. The resulting tag was used as an entity fea-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4889,"ture.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4890,"Word Embedding Distances as Features Be-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4891,"yond these classic linguistic features, we used
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4892,"word embeddings. Namely, we downloaded the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4893,"pre-trained Word2vec (Mikolov et al., 2013) vec-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4894,"tors of 300 dimensions to measure the distance be-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4895,"tween the sentence and the target word. The dis-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4896,"tance was computed using cosine similarity and
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4897,"Euclidean similarity over the average of the vec-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4898,"tor representation of the words in the sentence and
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4899,"the target text.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4900,"Classical Machine Learning Models We no-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4901,"ticed that for this task (with our features), the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4902,"tree learner offered better performance than other
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4903,"models. Thus, we tried several settings for the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4904,"tree learner model provided by KNIME (Berthold
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4905,"et al., 2009), as well as more complex variations
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4906,"such as random forest, gradient boosted, and tree
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4907,"ensembles. The best obtained result was given by
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4908,"the tree ensembles with 600 models.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4909,"4 Deep-Learning Approach
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4910,"In this section, we present our deep-learning ap-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4911,"proach. It is based on 2D convolution and word-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4912,"1http://opennlp.apache.org/
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4913,"2https://wordnet.princeton.edu/
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4914,"3https://stanfordnlp.github.io/CoreNLP323Figure 1: The architecture of our network
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4915,"embedding representation of the target text frag-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4916,"ment and its context.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4917,"Since text is one-dimensional, we applied a
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4918,"preprocessing step described in Section 4.1 prior
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4919,"to the application of convolution layer. Sec-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4920,"tion 4.2 describes our network architecture, and
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4921,"Section 4.3 presents the training procedure.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4922,"4.1 Preprocessing
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4923,"As a ﬁrst step, we removed punctuation marks,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4924,"digits, and special characters. Each word was then
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4925,"replaced by its vector representation using the pre-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4926,"trained word vectors from the Word2vec model
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4927,"(Mikolov et al., 2013) for English and fastText
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4928,"model (Grave et al., 2018) for Spanish. A min-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4929,"max normalization was applied to every vector to
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4930,"convert the values from the range [−1,1]to[0,1].
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4931,"We assigned a zero vector to the words missing in
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4932,"the pre-trained embeddings.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4933,"We deﬁned the left context (LC) and the right
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4934,"context (RC) as those words that appear to the left
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4935,"and the right of the target text, respectively. As a
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4936,"compact representation of the left or right context,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4937,"we used one 300-dimensional vector calculated as
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4938,"the average of the vectors of all the words in the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4939,"LC and RC, respectively (if the target text was lo-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4940,"cated at the beginning or the end of the sentence,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4941,"we used a zero vector as the respective context rep-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4942,"resentation). Next, we generated a matrix where
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4943,"the ﬁrst row corresponds to the LC vector, the next
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4944,"krows are given by the embedding vectors of the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4945,"words contained in the target text, where kis the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4946,"number of words in the target text, and the last row
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4947,"corresponds to the RC vector. In order to have a
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4948,"regular representation, we padded the matrix withp=m−kzero vectors, where mis the maximum
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4949,"value of kin the training set.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4950,"Figure 1 illustrates the preprocessing step on the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4951,"sentence of an example in the English training set.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4952,"The output of the preprocessing step is the input
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4953,"of the network.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4954,"We believe that the averaging operation on the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4955,"words in the contexts allowed differentiating be-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4956,"tween cases where the same sentence has distinct
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4957,"target texts. Those words included or excluded in
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4958,"the context will slightly modify the representation
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4959,"of the context, which will help the model to learn
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4960,"some relationships between the target text and the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4961,"rest of the sentence. We could have compressed
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4962,"the representation matrix by combining the vec-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4963,"tor representation of the words in the target text
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4964,"instead of stacking them. However, this could dra-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4965,"matically reduce the valuable information pertain-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4966,"ing to the target text.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4967,"4.2 Architecture of our Network
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4968,"In our architecture, we used an input, convolution,
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4969,"pooling, and fully-connected layers; see Figure 1.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4970,"Below we describe each of these layers except the
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4971,"input layer, which was described in Section 4.1.
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4972,"Convolution The number of ﬁlters in this layer
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4973,"varied from 16 to 256 with a convolution stride of
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4974,"1 and kernel size in the range of 2 to 4. We ap-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4975,"1 and kernel size in the range of 2 to 4. We ap-
",False,Complex Word Identification Convolutional Neural Network vs Feature Engineering,False,False,True
4976,"DOI 10.1515/TLR.2006.009 c/circlecopyrtWalter de GruyterAbstract
",True,conninePinnow,False,False,True
4977,"two classes of models, abstract and episodic, that have been proposed for spo-ken word recognition. Abstract theories rely on inference processes and/or un-derspeciﬁed representations to account for spoken word recognition. Episodic
",True,conninePinnow,False,False,True
4978,"1. Introduction
",True,conninePinnow,False,False,True
4979,"2.1. Word-ﬁnal /t, d/-deletion
",True,conninePinnow,False,False,True
4980,"2.2. Word medial ﬂap
",True,conninePinnow,False,False,True
4981,"2.3. Schwa deletion
",True,conninePinnow,False,False,True
4982,"2.4. Nasal ﬂap
",True,conninePinnow,False,False,True
4983,"3. Learning phonological variants
",True,conninePinnow,False,False,True
4984,"4. Conclusions
",True,conninePinnow,False,False,True
4985,"space that represents individual occurrences of a segment. Abstractions across
",True,conninePinnow,False,False,True
4986,"Luce, Paul, Connor MacLennan, and Jan Charles-Luce (2003). Abstractness and speciﬁcity in
",True,conninePinnow,False,False,True
4987,"Phonological variation in spoken word recognition:
",False,conninePinnow,False,False,True
4988,"Episodes and abstractions1
",False,conninePinnow,False,False,True
4989,"CYNTHIA M. CONNINE and ELENI PINNOW
",False,conninePinnow,False,False,True
4990,"The Linguistic Review 23 (2006), 235–245 0167–6318/06/023-0235
",False,conninePinnow,False,False,True
4991,"Phonological variation in spoken words is a ubiquitous aspect of spontaneous
",False,conninePinnow,False,False,True
4992,"speech and presents a challenge for recognition of spoken words. We discuss
",False,conninePinnow,False,False,True
4993,"theories assume a lexical representation that encodes each spoken word event
",False,conninePinnow,False,False,True
4994,"with exposure frequency linked to strength of a lexical entry. A model is pro-posed that posits a frequency-driven phonological variant lexical representa-tion. The model assumes that a word may have more than one variant represen-
",False,conninePinnow,False,False,True
4995,"tation and that exposure to phonological variant form inﬂuences the strength
",False,conninePinnow,False,False,True
4996,"of a given variant representation. Evidence for the proposed model is reviewedfor a number of variants (nasal ﬂaps, schwa deletion and medial ﬂaps).
",False,conninePinnow,False,False,True
4997,"1. Introduction
",False,conninePinnow,False,False,True
4998,"Spoken language comprehension is among the most complex cognitive abili-
",False,conninePinnow,False,False,True
4999,"ties of humans – a transient physical signal that is noisy, incomplete and poten-
",False,conninePinnow,False,False,True
5000,"tially ambiguous must be comprehended, typically under non-ideal listeningconditions. One process along this route to comprehension is the mapping ofacoustic-phonetic features extracted from the signal onto a lexical representa-
",False,conninePinnow,False,False,True
5001,"tion. The process of word recognition is complicated by the highly variable
",False,conninePinnow,False,False,True
5002,"nature of spoken language. Of particular interest here is that the realization of
",False,conninePinnow,False,False,True
5003,"1. Preparation of the manuscript was suppor ted by NIH grant R01DC02134. Additional support
",False,conninePinnow,False,False,True
5004,"was provided by the Center for Cognitive and Psycholinguistic Science.236 Cynthia M. Connine and Eleni Pinnow
",False,conninePinnow,False,False,True
5005,"segments can vary as a function of the surrounding segments via such phono-
",False,conninePinnow,False,False,True
5006,"logical processes as assimilation and deletion. The variability that occurs as
",False,conninePinnow,False,False,True
5007,"a result of phonological processes presents a particular challenge for listenersin the process of word recognition because phonological processes can resultin large differences among alternative pronunciations. The research reviewed
",False,conninePinnow,False,False,True
5008,"here focuses on the mechanisms that underlie a listener’s ability to accommo-
",False,conninePinnow,False,False,True
5009,"date phonological variation.
",False,conninePinnow,False,False,True
5010,"There are two general classes of spoken word recognition theories: abstract
",False,conninePinnow,False,False,True
5011,"and episodic. In abstract models, the speech signal is coded into abstract fea-
",False,conninePinnow,False,False,True
5012,"tures which in turn serve to activate abstract lexical representations (see, e.g.,
",False,conninePinnow,False,False,True
5013,"Marslen-Wilson and Warren 1994; McClelland and Elman 1986; Norris, Mc-Queen and Cutler 2000; Connine et al. 1997). Of particular importance to the
",False,conninePinnow,False,False,True
5014,"present discussion is that the abstractness of the lexical representation is pre-
",False,conninePinnow,False,False,True
5015,"sumed to be such that surface details are lost during encoding. This central as-sumption distinguishes abstract models from episodic ones. Central to episodicmodels of lexical representation is the idea that the lexicon consists of traces of
",False,conninePinnow,False,False,True
5016,"each heard production of a word (Goldinger 1998). The encoded traces retain
",False,conninePinnow,False,False,True
5017,"detailed surface information (cf. Goldinger 1998; Johnson 2005; Pierrehum-bert 2001, 2003). In the sections to follow, we review evidence that, we argue,supports a hybrid model, that is, a model in which a degree of normalization
",False,conninePinnow,False,False,True
5018,"takes place, but in which multiple phonological variants of a given word may
",False,conninePinnow,False,False,True
5019,"be stored.
",False,conninePinnow,False,False,True
5020,"Two strands of research motivate this work: First, research into the effects
",False,conninePinnow,False,False,True
5021,"of lexical frequency on language processing (e.g., Luce 1986; Marslen-Wilson
",False,conninePinnow,False,False,True
5022,"1993; Balota and Chumbley 1984, 1990; Connine et al. 1990) and pronunci-
",False,conninePinnow,False,False,True
5023,"ation variation (e.g., Bybee 2002; Pierrehumbert 2001; Jurafsky et al. 2001;Greenberg 1999; Fosler-Lussier and Morgan 1999; Raymond, Dautricourt andHume 2006; Patterson, LoCasto and Connine 2003; Patterson and Connine
",False,conninePinnow,False,False,True
5024,"2001; Crystal and House 2001); and second, research suggesting that hearers
",False,conninePinnow,False,False,True
5025,"encode ﬁne details of utterances they hear, including subphonemic detail (e.g.,Mullennix, Pisoni and Martin 1989; Nygaard, Sommers and Pisoni 1994; Mul-lennix et al. 1995; see also Johnson and Mullennix 1997 and chapters therein).
",False,conninePinnow,False,False,True
5026,"Taken together, these lines of investigation suggest that word recognition is
",False,conninePinnow,False,False,True
5027,"likely to be affected by phonological variation. A number of recent studies havedemonstrated that this is so and have proposed models of word recognition inthe face of phonological variation (e.g., MacLennan, Luce, and Charles-Luce
",False,conninePinnow,False,False,True
5028,"2003, and Luce, MacLennan, and Charles-Luce 2003 for the recognition of
",False,conninePinnow,False,False,True
5029,"ﬂapped and non-ﬂapped variants of /t/ and /d/; Johnson 2005; Connine 2004).The research reviewed here (primarily from the authors of this review) uti-lizes a conversational corpus in order to establish occurrence frequencies for
",False,conninePinnow,False,False,True
5030,"phonological variants. Subsequent work examines the extent to which variant
",False,conninePinnow,False,False,True
5031,"frequency predicts performance in behavioral studies. Limitations of variantPhonological variation in spoken word recognition 237
",False,conninePinnow,False,False,True
5032,"frequency accounts for processing phonological variation are noted along with
",False,conninePinnow,False,False,True
5033,"suggestions for further research.
",False,conninePinnow,False,False,True
5034,"2. Representation of phonological variant frequency
",False,conninePinnow,False,False,True
5035,"In the following section, the current status of the role of variant frequency as an
",False,conninePinnow,False,False,True
5036,"explanatory mechanism for recognition of phonological variants is reviewed.
",False,conninePinnow,False,False,True
5037,"For each variant that is discussed, the relevant corpus statistics are describedand are used to predict effects in processing phonological variants.
",False,conninePinnow,False,False,True
5038,"2.1. Word-ﬁnal /t, d/-deletion
",False,conninePinnow,False,False,True
5039,"Word-ﬁnal stops can be articulated with or without a release burst in American
",False,conninePinnow,False,False,True
5040,"English. In an analysis of released/no-release patterning conducted by Crystaland House (1988) a detailed acoustic analysis of a 600 word corpus of read
",False,conninePinnow,False,False,True
5041,"speech found that 59 % of the stops were complete (an identiﬁable hold fol-
",False,conninePinnow,False,False,True
5042,"lowed by a release). A second important ﬁnding was that the occurrence of therelease patterned with voicing characteristics of the stop: voiced stops in wordﬁnal position include a release less often (18 %) than voiceless stops (42 %).
",False,conninePinnow,False,False,True
5043,"The statistical trends for the released/no-release patterning were investigated in
",False,conninePinnow,False,False,True
5044,"a phoneme monitoring experiment in which frequency-matched words endingin voiced or voiceless stops were presented with or without their ﬁnal release(Deelman and Connine 2001). Deelman and Connine found that reaction time
",False,conninePinnow,False,False,True
5045,"was faster for the release-bearing than for the no-release tokens. This overall
",False,conninePinnow,False,False,True
5046,"ﬁnding is consistent with the relative frequency of the release type – more fre-quent forms (release-bearing) are responded to more quickly than less frequentforms (no-release). Of particular interest was whether the weakly probabilis-
",False,conninePinnow,False,False,True
5047,"tic released/no-release pattern inﬂuenced word recognition in a more subtle
",False,conninePinnow,False,False,True
5048,"way suggested by differences in voicing. The results revealed a complex in-terplay of voicing class and the integrity of the lexically matching informationin the stimulus – word stimuli showed a comparable advantage for a release-
",False,conninePinnow,False,False,True
5049,"bearing stimulus in phoneme monitoring reaction time compared to no-release
",False,conninePinnow,False,False,True
5050,"words. However, the introduction of a mismatching segment at a word’s onsetto create a pseudo-word revealed a pattern of results that are consistent withthe statistical likelihood of a release. Pseudo-words ending in voiceless seg-
",False,conninePinnow,False,False,True
5051,"ments showed faster phoneme monitoring reaction times for the release tokens.
",False,conninePinnow,False,False,True
5052,"However, pseudo-words ending in voiced segments showed no advantage forrelease-bearing tokens: tokens with and without a release showed comparablepriming. The complex interaction of voicing class and lexical activation clearly
",False,conninePinnow,False,False,True
5053,"argues against a simple inference account for the apparent ease of processing
",False,conninePinnow,False,False,True
5054,"no-release words. Rather, the results strongly suggest that the distributional238 Cynthia M. Connine and Eleni Pinnow
",False,conninePinnow,False,False,True
5055,"facts concerning the completeness of t he release across voiced and voiceless
",False,conninePinnow,False,False,True
5056,"word sets is related to ease of word recognition.
",False,conninePinnow,False,False,True
5057,"2.2. Word medial ﬂap
",False,conninePinnow,False,False,True
5058,"In contrast to released/no-release variants, some classes of variation show
",False,conninePinnow,False,False,True
5059,"strongly dominant variants. One such variant is the American English ﬂap
",False,conninePinnow,False,False,True
5060,"(i.e., the ﬂapped variant of coronal stops in post-tonic position as in [ ""phôIRI]
",False,conninePinnow,False,False,True
5061,"“pretty”). In their corpus analysis, Patterson and Connine (2001) extracted pro-ductions of potentially ﬂapped words from the Switchboard database of Amer-
",False,conninePinnow,False,False,True
5062,"ican English (Godfrey, McDaniel and Holliman 1992), a collection of elicited
",False,conninePinnow,False,False,True
5063,"telephone conversations between strangers on assigned topics (approximately2,400 two-sided telephone conversations among 543 speakers). Patterson andConnine (2001) found that 96 % of the tokens (N = 2172) consisted of unam-
",False,conninePinnow,False,False,True
5064,"biguous, ﬂapped productions with the remaining 4 % distributed across [t] and
",False,conninePinnow,False,False,True
5065,"glottal stop productions. The predominance of the ﬂapped production held forall words and for two different dialect regions. The hypothesis that represen-tation of lexical form includes the highly frequent ﬂap was examined using a
",False,conninePinnow,False,False,True
5066,"phoneme identiﬁcation task (Connine 2004). Uncontroversially, the speciﬁca-
",False,conninePinnow,False,False,True
5067,"tion of the ﬂap as highly frequent was based on its predominance in the cor-pus analysis. Listeners identiﬁed the initial segment ( borp) in word-nonword
",False,conninePinnow,False,False,True
5068,"speech continua (e.g., [ ""p
",False,conninePinnow,False,False,True
5069,"hôIRI]–[ ""bôIRI]). Of critical importance was that the
",False,conninePinnow,False,False,True
5070,"to-be-identiﬁed segment was embedded in either a ﬂap or a [t] bearing carrier
",False,conninePinnow,False,False,True
5071,"word (e.g., [ ""phôIRI]-[ ""bôIRI]o r[ ""phôItI]-[ ""bôItI]). The results showed more iden-
",False,conninePinnow,False,False,True
5072,"tiﬁcation responses forming a real word (e.g., more presponses) when the to-
",False,conninePinnow,False,False,True
5073,"be-identiﬁed segment occurred in the more frequently experienced ﬂap carrier
",False,conninePinnow,False,False,True
5074,"compared to when the to-be-identiﬁed segment occurred in the less frequently
",False,conninePinnow,False,False,True
5075,"experienced [t] carrier.
",False,conninePinnow,False,False,True
5076,"2.3. Schwa deletion
",False,conninePinnow,False,False,True
5077,"The data just described demonstrate that a highly frequent variant, the ﬂap,
",False,conninePinnow,False,False,True
5078,"is represented but leaves open the question of representation for the very in-frequent form. A more direct examination of the question of multiply repre-sented forms was addressed for a phonological variant with no single domi-
",False,conninePinnow,False,False,True
5079,"nant production, schwa deletion in post-tonic position (e.g., [ ""hIst@ôI] ‘history’
",False,conninePinnow,False,False,True
5080,"→[""hIstôI]). A corpus analysis of schwa deletion frequency (Patterson, LoCasto
",False,conninePinnow,False,False,True
5081,"and Connine 2003) revealed that words vary dramatically in deletion rates forthree syllable words with a potential medial schwa deletion. The corpus statis-
",False,conninePinnow,False,False,True
5082,"tics were based on the Switchboard conversational database and replicated with
",False,conninePinnow,False,False,True
5083,"a smaller sample of speech using elicited productions from pairs of speakersPhonological variation in spoken word recognition 239
",False,conninePinnow,False,False,True
5084,"re-telling stories (see Patterson et al. 2003 for details). The range of deletion
",False,conninePinnow,False,False,True
5085,"rates permitted selection of words with either high deletion rates (greater than
",False,conninePinnow,False,False,True
5086,"50 %) and low deletion rates (less than 50 %) in an experiment conducted byConnine, Ranbom and Patterson (2005). Connine et al. presented listeners to-kens from speech continua in which the duration of a schwa was manipulated.
",False,conninePinnow,False,False,True
5087,"For example, the medial schwa of [ ""hIst@ôI] ‘history’ was systematically short-
",False,conninePinnow,False,False,True
5088,"ened in small steps to create a token of [ ""hIstôI]. A control condition was created
",False,conninePinnow,False,False,True
5089,"by replacing the initial and ﬁnal syllables to create a nonword (e.g., foshtoro ).
",False,conninePinnow,False,False,True
5090,"Note that in the nonword control condition, the medial schwa and its surrounds
",False,conninePinnow,False,False,True
5091,"were acoustically identical to the word condition. Subjects in the experiment
",False,conninePinnow,False,False,True
5092,"were asked to indicate whether a schwa was present or absent for each token.An inﬂuence of deletion rate frequency was revealed in three aspects of the
",False,conninePinnow,False,False,True
5093,"data. First, low deletion rate words showed more schwa-present judgments as
",False,conninePinnow,False,False,True
5094,"compared with high deletion rate words. Second, control nonword carriers withthe same physical schwa information (and surrounding segments) as their highand low deletion rate word counterparts did not differ. This indicates that the
",False,conninePinnow,False,False,True
5095,"deletion rate effect for words was not a consequence of idiosyncratic proper-
",False,conninePinnow,False,False,True
5096,"ties of the schwa or its environs. Third, both high and low deletion rate wordsshowed more vowel-present judgments relative to their nonword counterparts,but the difference was larger for the low deletion rate words. Thus, the ten-
",False,conninePinnow,False,False,True
5097,"dency to detect a schwa in a word context depended upon the frequency of that
",False,conninePinnow,False,False,True
5098,"variant form in speech. This is consistent with the ﬁndings for the medial ﬂap(described above) and provides a more subtle inﬂuence of variant frequency –the lexical effect varied with the frequency of the variant form and is consistent
",False,conninePinnow,False,False,True
5099,"with frequency-based variant representations.
",False,conninePinnow,False,False,True
5100,"2.4. Nasal ﬂap
",False,conninePinnow,False,False,True
5101,"A second word-internal variant that has been investigated is the nasal ﬂap
",False,conninePinnow,False,False,True
5102,"(henceforth NT) where a /t/ in a ""Vn_V environment can be realized as a nasal
",False,conninePinnow,False,False,True
5103,"ﬂap (e.g., the surface form [ ""t
",False,conninePinnow,False,False,True
5104,"hwEntI] ‘twenty’ can alternate with a surface form
",False,conninePinnow,False,False,True
5105,"[""thwE˜RI]). Picard (1984) identiﬁed the process of nasal ﬂapping as optional,
",False,conninePinnow,False,False,True
5106,"resulting from /t/-deletion followed by ﬂapping of the intervocalic nasal seg-ment. Other linguistic analyses (Vaux 2000) suggest that the coronal stop isthe more active of the two segments in creating nasal ﬂapping. Ranbom and
",False,conninePinnow,False,False,True
5107,"Connine (2004) showed in a corpus analysis of the Switchboard database that
",False,conninePinnow,False,False,True
5108,"the nasal ﬂap production is dominant, occurring in nearly 82 % of productions.Similar to the schwa deletion statistics, the predominance of the nasal ﬂap formvaried across words with some items more frequently produced with an enun-
",False,conninePinnow,False,False,True
5109,"ciated NT. Similar to the previous experiments, the corpus statistics permit-
",False,conninePinnow,False,False,True
5110,"ted the selection of stimuli that varied variant frequency ( >50 % or <50 %240 Cynthia M. Connine and Eleni Pinnow
",False,conninePinnow,False,False,True
5111,"nasal ﬂap occurrence). The high and low variant frequency stimuli were sub-
",False,conninePinnow,False,False,True
5112,"sequently presented to listeners in their enunciated NT and nasal ﬂap forms
",False,conninePinnow,False,False,True
5113,"in a series of experiments. In the ﬁrst of this series, a lexical decision exper-iment, the results showed that overall, lexical decisions were faster and moreaccurate for the enunciated NT variant – a ﬁnding clearly inconsistent with the
",False,conninePinnow,False,False,True
5114,"overall frequency statistics. However, a more subtle inﬂuence of variant fre-
",False,conninePinnow,False,False,True
5115,"quency emerged when responses were considered in terms of presented form(nasal ﬂap or enunciated NT). Responses to nasal ﬂap forms showed a variantfrequency effect – lexical decision reaction times were faster and more accu-
",False,conninePinnow,False,False,True
5116,"rate for words that had nasal ﬂaps highly frequently (words with greater than
",False,conninePinnow,False,False,True
5117,"50% occurrence as nasal ﬂaps) than for words that had nasal ﬂaps infrequently.No such variant frequency effect was found for the NT productions. These re-
",False,conninePinnow,False,False,True
5118,"sults suggest that experienced frequency inﬂuences lexical representation for
",False,conninePinnow,False,False,True
5119,"the nasal ﬂap form but not the NT form. The overall advantage for the NTproduction (along with the variant frequency effect for the nasal ﬂap) was alsofound in a repetition cross modal priming experiment. This experiment used
",False,conninePinnow,False,False,True
5120,"an additional set of control words, medial consonant cluster words (e.g., whis-
",False,conninePinnow,False,False,True
5121,"per), that were presented in their intact form or as a mispronunciation without
",False,conninePinnow,False,False,True
5122,"the second medial consonant (e.g., whisser ). If activation of a lexical repre-
",False,conninePinnow,False,False,True
5123,"sentation is tolerant of a missing segment, then similar priming effects for the
",False,conninePinnow,False,False,True
5124,"nasal ﬂap and the mispronounced consonant-cluster stimuli should be found.
",False,conninePinnow,False,False,True
5125,"However, a signiﬁcant priming effect was found for the nasal ﬂap productionsand no priming for the consonant-deleted productions. These results rule outthe possibility that the nasal ﬂap is simply recognized as a best-match for a
",False,conninePinnow,False,False,True
5126,"represented NT form (Connine et al. 1997).
",False,conninePinnow,False,False,True
5127,"Ranbom and Connine (2004) suggest that the results support the presence of
",False,conninePinnow,False,False,True
5128,"two phonological representations: a gradient representation for the nasal ﬂapbased on frequency of occurrence, along with a representation for the enunci-
",False,conninePinnow,False,False,True
5129,"ated NT for all words. The advantage for processing the NT form relative to
",False,conninePinnow,False,False,True
5130,"the nasal ﬂap (in deﬁance of frequency) and the insensitivity of the NT formto variant frequency is clearly inconsistent with the notion that more frequentvariant forms are more strongly represented. In considering possible explana-
",False,conninePinnow,False,False,True
5131,"tions for the lack of a variant frequency effect for NT pronunciations, Ranbom
",False,conninePinnow,False,False,True
5132,"and Connine (2004) suggest that there may be more than one route for thelexicalization of phonological forms for spoken word recognition. Speciﬁcally,orthographically consistent forms may achieve a strongly lexicalized represen-
",False,conninePinnow,False,False,True
5133,"tation (irrespective of its spoken frequency) via reading. For nasal ﬂaps, the
",False,conninePinnow,False,False,True
5134,"suggestion is that an enunciated NT representation is established via an ortho-graphic route while a gradient representation for the nasal ﬂap becomes moreentrenched as the nasal ﬂap production is encountered more often. An inﬂu-
",False,conninePinnow,False,False,True
5135,"ence of experience with visual language on spoken word recognition is not
",False,conninePinnow,False,False,True
5136,"entirely ad hoc as a similar proposal has been made by researchers in visualPhonological variation in spoken word recognition 241
",False,conninePinnow,False,False,True
5137,"word recognition. Cross-talk among orthographic and phonological represen-
",False,conninePinnow,False,False,True
5138,"tations has been formalized in some models that assume explicit connections
",False,conninePinnow,False,False,True
5139,"between orthographic and phonological representations. One such model, thebimodal interactive activation model, assumes that orthographic representa-tions are automatically activated during auditory word recognition (and vice
",False,conninePinnow,False,False,True
5140,"versa, see Grainger and Ferrand 1996) and support for a facilitatory relation-
",False,conninePinnow,False,False,True
5141,"ship between modalities has been demonstrated in experiments showing an or-thographic neighbor inﬂuence on processing spoken words (Ziegler, Muneauxand Grainger 2003). Some evidence for cross talk between orthographic and
",False,conninePinnow,False,False,True
5142,"phonological representation that is of particular relevance is some recent re-
",False,conninePinnow,False,False,True
5143,"search examining the nature of the speech code for words with an infrequentbut orthographically consistent phonological variant. Using a novel paradigm,
",False,conninePinnow,False,False,True
5144,"Inhoff, Connine, and Radach (2002; see also Inhoff et al. 2004) had partici-
",False,conninePinnow,False,False,True
5145,"pants read sentences in which ﬁxating a target word (e.g., PRETTY) triggeredauditory presentation of the frequent ﬂap, the infrequent [t] form or a similarsounding word (e.g., GRITTY). The similar word resulted in increased post-
",False,conninePinnow,False,False,True
5146,"target reading ﬁxations, but the ﬂapped and the [t] versions showed shorter but
",False,conninePinnow,False,False,True
5147,"equivalent reading times. The equivalent effects for ﬂapped and [t] variants sug-gests that the orthographic lexicon reﬂects both the experienced spoken wordenvironment and orthography-to-phonology mappings. A parallel inﬂuence of
",False,conninePinnow,False,False,True
5148,"orthography in processing spoken words was suggested by Connine (2004) for
",False,conninePinnow,False,False,True
5149,"the hyperarticulated ‘t’ version of words such as pretty where processing of
",False,conninePinnow,False,False,True
5150,"this form may be facilitated by a phonological representation that developed asa result of exposure to the written form of a word. It is important to note that
",False,conninePinnow,False,False,True
5151,"development of a phonological representation via reading does not rule out a
",False,conninePinnow,False,False,True
5152,"variant frequency effect, but it requires a wider consideration of frequency inthat input from both domains may dilute a spoken word variant frequency con-tribution. The implication is that predictions based solely on spoken variant
",False,conninePinnow,False,False,True
5153,"frequency may be partially disconﬁrmed if an infrequent phonological variant
",False,conninePinnow,False,False,True
5154,"is also consistent with the orthography.
",False,conninePinnow,False,False,True
5155,"3. Learning phonological variants
",False,conninePinnow,False,False,True
5156,"So far, the research reviewed here has supported the claim that lexical orga-
",False,conninePinnow,False,False,True
5157,"nization capitalizes on systematicity and frequency in representing phonolog-
",False,conninePinnow,False,False,True
5158,"ical variation form. That representation of spoken form utilizes these dimen-
",False,conninePinnow,False,False,True
5159,"sions is perhaps not surprising, since systematicity and frequency are proper-ties inherent in other aspects of lexical knowledge. The explicit representationof phonological variants is also consistent with evidence suggesting that in-
",False,conninePinnow,False,False,True
5160,"dexical characteristics of voices as well as within-category information about
",False,conninePinnow,False,False,True
5161,"speech sounds are encoded by listeners. L isteners can recognize frequent vari-242 Cynthia M. Connine and Eleni Pinnow
",False,conninePinnow,False,False,True
5162,"ant forms more effectively than infrequent forms. These results suggest that
",False,conninePinnow,False,False,True
5163,"lexical representations effectively evolve to accommodate the experienced lan-
",False,conninePinnow,False,False,True
5164,"guage environment. However, what ﬂexibility do listeners have in representinglow frequency forms? Is simple exposure to a low frequency form sufﬁcient torender that form more easily recognizable? A recent set of experiments investi-
",False,conninePinnow,False,False,True
5165,"gated precisely this question for infrequently occurring schwa-deleted variants
",False,conninePinnow,False,False,True
5166,"for bisyllabic schwa-deleted words (e.g., [b I""li:v] ‘believe’ →[""bli :v]). Previous
",False,conninePinnow,False,False,True
5167,"research demonstrated that disyllabic schwa-deleted words occur infrequently(less than 11 %) compared to their schwa bearing counterparts (Patterson, Lo-
",False,conninePinnow,False,False,True
5168,"Casto and Connine 2003) and that recognition of these forms was slow and in-
",False,conninePinnow,False,False,True
5169,"accurate (LoCasto and Connine 2002). Pinnow and Connine (2005) examinedthe inﬂuence of exposure on these difﬁcult to recognize forms by providing
",False,conninePinnow,False,False,True
5170,"prior exposure to the stimuli in a training session in which participants were
",False,conninePinnow,False,False,True
5171,"presented an auditory version of the schwa-deleted word form along with itscorrectly spelled orthographic form. In the training phase, participants weresimply asked to listen carefully to each vowel-deleted token and read the or-
",False,conninePinnow,False,False,True
5172,"thographic form. The inﬂuence of exposure was assessed in a subsequent lexi-
",False,conninePinnow,False,False,True
5173,"cal decision experiment – the results showed increased accuracy (correct wordidentiﬁcations) for the single exposure group compared with a control groupthat received no training. An identical pattern of results was found for a train-
",False,conninePinnow,False,False,True
5174,"ing/test set of conditions in which the voice used in training differed from the
",False,conninePinnow,False,False,True
5175,"voice used in test. Moreover, accuracy rates in the same voice condition did notdiffer from the different voice condition. A second experiment demonstratedthat repeated exposure (four repetitions) of a variant during training increased
",False,conninePinnow,False,False,True
5176,"the percentage of ‘word’ responses (relative to the control and similar to the
",False,conninePinnow,False,False,True
5177,"single repeat group) but also served to speed the lexical decision response.Clearly, exposure to a low frequency variant is sufﬁcient to strengthen thatform’s lexical representation. The lack of a talker effect is consistent with a
",False,conninePinnow,False,False,True
5178,"relatively abstract representation that is immune to idiosyncratic differences
",False,conninePinnow,False,False,True
5179,"among speakers. The relatively abstract nature of the information encoded inthe training phase is supported by an additional experiment in which the train-ing and test stimulus sets differed (that is participants heard one set of schwa-
",False,conninePinnow,False,False,True
5180,"deleted stimuli in training and performed a lexical decision task on a new set of
",False,conninePinnow,False,False,True
5181,"schwa-deleted stimuli in test). Surprisi ngly, the variant learning effect was also
",False,conninePinnow,False,False,True
5182,"found for this second group of listeners, that is, exposure to one set of schwa-deleted words served to improve recognition of a new set of words. Even more
",False,conninePinnow,False,False,True
5183,"surprising, the transfer and repetition group showed a statistically equivalent
",False,conninePinnow,False,False,True
5184,"increase in ‘word’ responses (relative to a control group who performed thelexical decision without any training). These ﬁndings suggest that listeners areable to generalize across the set of learned stimuli and extend this knowledge
",False,conninePinnow,False,False,True
5185,"to new words in a given variant class.Phonological variation in spoken word recognition 243
",False,conninePinnow,False,False,True
5186,"4. Conclusions
",False,conninePinnow,False,False,True
5187,"The preceding review shows that experience with phonological variants serves
",False,conninePinnow,False,False,True
5188,"to predict performance in spoken word recognition tasks and that manipula-tions of experienced frequency can increase accuracy in recognizing a givenform. Further, the nature of the information learned from exposure to a set of
",False,conninePinnow,False,False,True
5189,"experienced variants can be extracted and applied to novel instances of that
",False,conninePinnow,False,False,True
5190,"variant class. The inﬂuence of pre-existing variant frequency along with theeffects of manipulated exposure frequency support a powerful role for experi-ential factors in lexical representation. The ability of listeners to apply learned
",False,conninePinnow,False,False,True
5191,"patterns of words to a new set of words also suggest that the learning mecha-
",False,conninePinnow,False,False,True
5192,"nism underlying the encoding of word forms is powerful enough to generalizeacross new patterns. The entire set of ﬁndings is consistent with the notion thatexposure to a variant form underlies the development of lexical representations.
",False,conninePinnow,False,False,True
5193,"The focus on the nature of the representation shifts the burden for processing
",False,conninePinnow,False,False,True
5194,"phonological variants to an experience-based lexicon. However, an adequatemodel of word recognition may require both abstract and episodic representa-tions. Representations along these lines have been suggested in linguistic the-
",False,conninePinnow,False,False,True
5195,"ory that encode surface details to distinguish among speakers. Pierrehumbert
",False,conninePinnow,False,False,True
5196,"(2003) discusses a distinction between parametric phonetics and abstractionsacross phonetic space. Parametric phonetics e ncodes the acoustic/articulatory
",False,conninePinnow,False,False,True
5197,"phonetic space represent word forms in the lexicon. There are many questions
",False,conninePinnow,False,False,True
5198,"concerning the ways in which surface detail comes to be represented by lis-teners and in what form. Despite these many remaining issues, a focus on lex-icalization of variant forms based on exposure frequency moves the question
",False,conninePinnow,False,False,True
5199,"of what special processes or representations underlie recognition of phonolog-
",False,conninePinnow,False,False,True
5200,"ical variants to the question of how do lexical representations develop and inwhat form. From this perspective, learning and experience take a front seat inconsidering the structure of the mental lexicon and its role in spoken language
",False,conninePinnow,False,False,True
5201,"processing.
",False,conninePinnow,False,False,True
5202,"Binghamton University
",False,conninePinnow,False,False,True
5203,"State University of New York
",False,conninePinnow,False,False,True
5204,"References
",False,conninePinnow,False,False,True
5205,"Balota, David and James Chumbley (1984). Are lexical decisions a good measure of lexical ac-
",False,conninePinnow,False,False,True
5206,"cess? The role of word-frequency in the neglected decision stage. Journal of Experimental
",False,conninePinnow,False,False,True
5207,"Psychology: Human Perception and Performance 10: 340–357.
",False,conninePinnow,False,False,True
5208,"— (1990). Where are the effects of frequency in v isual word recognition tasks? Right where we
",False,conninePinnow,False,False,True
5209,"said they were! Comment on Monsell, Doyle and Haggard (1989). Journal of Experimental
",False,conninePinnow,False,False,True
5210,"Psychology: General 119: 231–237.244 Cynthia M. Connine and Eleni Pinnow
",False,conninePinnow,False,False,True
5211,"Bybee, Joan (2002). Word frequency and context of use in the lexical diffusion of phonetically
",False,conninePinnow,False,False,True
5212,"conditioned sound change. Language V ariation and Change 14 (3): 261–290.
",False,conninePinnow,False,False,True
5213,"Connine, Cynthia (2004). It’s not what you hear, but how often you hear it: On the neglected role
",False,conninePinnow,False,False,True
5214,"of phonological variant frequency in auditory word recognition. Psychological Bulletin and
",False,conninePinnow,False,False,True
5215,"Review 11: 1084–1089.
",False,conninePinnow,False,False,True
5216,"Connine, Cynthia, John Mullenni x, Eva Shernoff, and Jennifer Yelen (1990). Word familiarity
",False,conninePinnow,False,False,True
5217,"and frequency in auditory and visual word recognition. Journal of Experimental Psychology:
",False,conninePinnow,False,False,True
5218,"Learning, Memory and Cognition 16 (6): 1084–1096.
",False,conninePinnow,False,False,True
5219,"Connine, Cynthia, Larissa Ranbom and David Patterson (2005). On the representation of phono-
",False,conninePinnow,False,False,True
5220,"logical variant frequency in spoken word recognition. Ms.
",False,conninePinnow,False,False,True
5221,"Connine, Cynthia, Titone, Debra, Deelman, Thomas, and Blasko, Dawn (1997). Similarity map-
",False,conninePinnow,False,False,True
5222,"ping in spoken word recognition. Journal of Memory and Language 37: 463–480.
",False,conninePinnow,False,False,True
5223,"Crystal, Thomas and Arthur House (1988). The duration of American-English stop consonants:
",False,conninePinnow,False,False,True
5224,"An overview. Journal of Phonetics 16: 285–294.
",False,conninePinnow,False,False,True
5225,"Deelman, Thomas and Cynthia Connine (2001). Mi ssing information in spoken word recognition:
",False,conninePinnow,False,False,True
5226,"Non-released stop consonants. Journal of Experimental Psychology: Human Perception and
",False,conninePinnow,False,False,True
5227,"Performance 27: 656–663.
",False,conninePinnow,False,False,True
5228,"Fosler-Lussier, Eric and Nelson Morgan (1999). Effects of speaking rate and word predictability
",False,conninePinnow,False,False,True
5229,"on word pronunciation in conversational speech. Speech Communication 29: 137–158.
",False,conninePinnow,False,False,True
5230,"Godfrey, John, Jane McDaniel and Edwar d Holliman (1992). SWITCHBOARD: A telephone
",False,conninePinnow,False,False,True
5231,"speech corpus for research and development. In ICASSP Proceedings , 517–520. San Fran-
",False,conninePinnow,False,False,True
5232,"cisco.
",False,conninePinnow,False,False,True
5233,"Goldinger, Stephen (1998). Echoes of echoes? An episodic theory of lexical access. Psychological
",False,conninePinnow,False,False,True
5234,"Review 105: 251–279.
",False,conninePinnow,False,False,True
5235,"Grainger, Jonathan and Ludovic Ferrand (1996) . Masked orthographic and phonological priming
",False,conninePinnow,False,False,True
5236,"in visual word recognition and naming: Cross-task comparisons. Journal of Memory and Lan-
",False,conninePinnow,False,False,True
5237,"guage 35: 623–647.
",False,conninePinnow,False,False,True
5238,"Greenberg, Steven (1999). Speaking in shorthand – a syllable-centric perspective for understanding
",False,conninePinnow,False,False,True
5239,"pronunciation variation. Speech Communication 29: 159–176.
",False,conninePinnow,False,False,True
5240,"Inhoff, Albrecht, Cynthia Connine and Ralph Rad ach (2002). A contingent speech technique in eye
",False,conninePinnow,False,False,True
5241,"movement research on reading. Behavior Research Methods, Instruments, and Computers 34:
",False,conninePinnow,False,False,True
5242,"471–480.
",False,conninePinnow,False,False,True
5243,"Inhoff, Albrecht, Cynthia Connine, Brianna E iter, Ralph Radach, and Dieter Heller (2004). Phono-
",False,conninePinnow,False,False,True
5244,"logical representation of words in working memory during sentence reading. Psychonomic
",False,conninePinnow,False,False,True
5245,"Bulletin and Review 11: 320–325.
",False,conninePinnow,False,False,True
5246,"Johnson, Keith (2005). Massive reduction in conversational American English. Manuscript.Johnson, Keith and John Mullennix (eds.) (1997). Talker V ariability in Speech Processing .S a n
",False,conninePinnow,False,False,True
5247,"Diego: Academic Press.
",False,conninePinnow,False,False,True
5248,"Jurafsky, Daniel, Alan Bell, Michelle Gregory and William Raymond (2001). Probablistic relations
",False,conninePinnow,False,False,True
5249,"between words: Evidence from reduction in lexical production. In Frequency and the Emer-
",False,conninePinnow,False,False,True
5250,"gence of Linguistic Structure , Joan Bybee and Paul Hopper (eds.), 229–254. Amsterdam: John
",False,conninePinnow,False,False,True
5251,"Benjamins.
",False,conninePinnow,False,False,True
5252,"LoCasto, Paul and Cynthia Connine (2002). Rule-governed missing information in spoken word
",False,conninePinnow,False,False,True
5253,"recognition: Schwa vowel deletion. Perception and Psychophysics 64: 208–219.
",False,conninePinnow,False,False,True
5254,"Luce, Paul (1986). Neighborhoods of words in the mental lexicon. PhD dissertation. Indiana Uni-
",False,conninePinnow,False,False,True
5255,"versity, Bloomington, IN.
",False,conninePinnow,False,False,True
5256,"spoken word recognition: Indexical and allophoni c variability in long-term repetition priming.
",False,conninePinnow,False,False,True
5257,"InRethinking Implicit Memory , J. Bowers and C. Marsolek (eds.), 197–214. Oxford: Oxford
",False,conninePinnow,False,False,True
5258,"University Press.
",False,conninePinnow,False,False,True
5259,"MacLennan, Connor, Paul Luce and Jan Charles-Luce (2003). Representation of lexical form.
",False,conninePinnow,False,False,True
5260,"Journal of Experimental Psycholog y: Learning, Memory and Cognition 29: 539–553.Phonological variation in spoken word recognition 245
",False,conninePinnow,False,False,True
5261,"Marslen-Wilson, William (1993). Issues of proces s and representation in spoken language under-
",False,conninePinnow,False,False,True
5262,"standing. In Cognitive Models of Speech Processing , G. Altmann and R. Shillcock (eds.),
",False,conninePinnow,False,False,True
5263,"187–209. East Sussex, UK: Erlbaum.
",False,conninePinnow,False,False,True
5264,"187–209. East Sussex, UK: Erlbaum.
",False,conninePinnow,False,False,True
5265,"187–209. East Sussex, UK: Erlbaum.
",False,conninePinnow,True,False,True
5266,"ABSTRACT Sarcasm is a complicated linguistic term commonly found in e-commerce and social media
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5267,"I. INTRODUCTION
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5268,"II. LITERATURE SURVEY
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5269,"III. AUTOMATIC SARCASM IDENTIFICATION
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5270,"IV. EXPERIMENTAL DESIGNS
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5271,"V. EXPERIMENTAL RESULTS AND DISCUSSIONS
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5272,"VI. CONCLUSION
",True,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5273,"Received February 28, 2021, accepted March 17, 2021, date of publication March 23, 2021, date of current version April 2, 2021.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5274,"Digital Object Identifier 10.1 109/ACCESS.2021.3068323
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5275,"Context-Based Feature Technique for Sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5276,"Identification in Benchmark Datasets Using Deep
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5277,"Learning and BERT Model
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5278,"CHRISTOPHER IFEANYI EKE
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5279,"1,2, AZAH ANIR NORMAN
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5280,"1, AND LIYANA SHUIB
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5281,"1
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5282,"1Department of Information Systems, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur 50603, Malaysia
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5283,"2Department of Computer Science, Faculty of Computing, Federal University, P.M.B 046, Laa, Nigeria
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5284,"Corresponding authors: Azah Anir Norman (azahnorman@um.edu.my) and Liyana Shuib (liyanashuib@um.edu.my)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5285,"sites. Failure to identify sarcastic utterances in Natural Language Processing applications such as sentiment
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5286,"analysis and opinion mining will confuse classication algorithms and generate false results. Several studies
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5287,"on sarcasm detection have utilised different learning algorithms. However, most of these learning models
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5288,"have always focused on the contents of expression only, leaving the contextual information in isolation.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5289,"As a result, they failed to capture the contextual information in the sarcastic expression. Secondly, many
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5290,"deep learning methods in NLP uses a word embedding learning algorithm as a standard approach for feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5291,"vector representation, which ignores the sentiment polarity of the words in the sarcastic expression. This
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5292,"study proposes a context-based feature technique for sarcasm Identication using the deep learning model,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5293,"BERT model, and conventional machine learning to address the issues mentioned above. Two Twitter and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5294,"Internet Argument Corpus, version two (IAC-v2) benchmark datasets were utilised for the classication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5295,"using the three learning models. The rst model uses embedding-based representation via deep learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5296,"model with bidirectional long short term memory (Bi-LSTM), a variant of Recurrent Neural Network (RNN),
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5297,"by applying Global Vector representation (GloVe) for the construction of word embedding and context
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5298,"learning. The second model is based on Transformer using a pre-trained Bidirectional Encoder representation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5299,"and Transformer (BERT). In contrast, the third model is based on feature fusion that comprised BERT
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5300,"feature, sentiment related, syntactic, and GloVe embedding feature with conventional machine learning.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5301,"The effectiveness of this technique is tested with various evaluation experiments. However, the technique's
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5302,"evaluation on two Twitter benchmark datasets attained 98.5% and 98.0% highest precision, respectively. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5303,"IAC-v2 dataset, on the other hand, achieved the highest precision of 81.2%, which shows the signicance
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5304,"of the proposed technique over the baseline approaches for sarcasm analysis.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5305,"INDEX TERMS Natural language processing, sarcasm identication, Bi-LSTM, GloVe embedding, BERT.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5306,"I. INTRODUCTION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5307,"Recently, affective computing and sentiment analysis
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5308,"research has gained much recognition [1]. The notion behind
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5309,"sentiment analysis is to determine the polarity of the emotion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5310,"word in an expression. Analysis of people's sentiment (also
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5311,"referred to as opinion mining) identies subjective informa-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5312,"tion in source documents. The process of identifying peo-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5313,"ple's opinions (sentiments) about products, politics, services,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5314,"or individuals brings a lot of benets to the organisations [2],
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5315,"[3]. The possibility of identifying subjective information is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5316,"The associate editor coordinating the review of this manuscript and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5317,"approving it for publication was Jiankang Zhang
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5318," .essential. It helps generate structured knowledge that serves
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5319,"as a piece of important knowledge for decision support
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5320,"systems and individual decision-making [4]. For instance,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5321,"affective computing and sentiment analysis can improve
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5322,"recommendation systems and customer relationship manage-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5323,"ment by revealing customers' likes and dislikes or eliminating
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5324,"the item recommendations that got negative feedback from
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5325,"the customers [5]. Most of the social content found on the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5326,"Web consists of gurative words such as sarcasm and irony.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5327,"For example, the Internet Argumentation Corpus obtained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5328,"from 4forums.com consists of 12% sarcastic utterances [6].
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5329,"In social media, various people usually employ sarcasm or
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5330,"irony to show their emotions, making it difcult to analyse
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5331,"VOLUME 9, 2021This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/48501C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5332,"people's sentiments. Sarcastic languages can shift the senti-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5333,"ment polarity in the textual document, which might reduce
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5334,"the predictive accuracy of sentiment analysis. In sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5335,"statements, there is a contradiction between the expressed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5336,"textual utterances and the individual's aim in making such
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5337,"sarcastic utterances.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5338,"According to [7], Sarcasm is dened as ``a verbal device,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5339,"with the intention of putting someone down or an act of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5340,"saying one thing while the meaning is opposite.'' People use
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5341,"sarcastic statements that correspond to the reverse of what
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5342,"they speak to injure one's emotion. Sarcasm identication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5343,"studies have gained attention in recent years. Maynard and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5344,"Greenwood [7], in their research, demonstrated that sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5345,"identication from sarcastic utterances might enhance the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5346,"accuracy of sentiment analysis. Sarcasm identication has
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5347,"become an essential step in analysing people's sentiments [8],
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5348,"[9]. A sarcastic statement represents a conict between the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5349,"individual's motive making the utterance and the actual com-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5350,"position. For instance, a sarcastic expression, ``I love to work
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5351,"on holidays!'' shows a conict between the clear statement
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5352,"``on holidays'' and the expression ``love.'' The contradiction
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5353,"and the sentiment polarity shift prove that Sarcasm is a unique
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5354,"case of sentiment analysis. Sarcasm is extremely contextual
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5355,"and topic reliant, and as a result, some contextual clues and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5356,"shifts in polarity sentiment can assist in sarcasm identication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5357,"in the text. Moreover, it resolves the obscurity of the meaning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5358,"and improves the overall sentiment classication of a huge
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5359,"amount of user's textual data obtained from social media.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5360,"The insufcient knowledge of the situation ``Context,'' the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5361,"environment, and the specic topic will result in difculty
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5362,"detecting sarcastic utterances [10]. Context understanding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5363,"is one of the main challenging phases of moderation con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5364,"tent. The term ``Context'' in sentiment analysis refers to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5365,"supplementary support that may either increase or change
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5366,"the content polarity. Thus, the context vector determines the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5367,"accuracy of the sentiment analysis, and the predictive model
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5368,"will guarantee the reliability of the overall prediction.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5369,"Various studies on sarcasm identication have relied on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5370,"content and pattern-based features. For instance, Mukherjee
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5371,"and Bala [11] employed content-based linguistic features for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5372,"sarcasm classication. The study relied solely on the emoti-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5373,"con, word use, and the sentence, in general, to differentiate
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5374,"sarcastic from non-sarcastic sentences. The technique pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5375,"duced reasonable performance results based on the data set
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5376,"that was used. However, the predictive model performance
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5377,"relied intensely on the linguistic feature content, which is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5378,"likely to degrade when applied to other data sets due to its
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5379,"dependence on word use. Hence, the obtained result cannot
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5380,"be generalised to a satisfactory extent.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5381,"Similarly, Rajadesingan, et al. [12] investigated the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5382,"behavioural method for sarcasm analysis by utilising psy-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5383,"chological and behavioural theories to build the behavioural
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5384,"model. Different features that include emotion, complex-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5385,"ity, expression, and contrast were extracted to train the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5386,"model. The study's empirical analysis showed enhanced
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5387,"performance on the proposed method compared with theconventional approaches. Similarly, Bouazizi and Ohtsuki
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5388,"[13] dened a pattern method for sarcasm classication on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5389,"tweets. In their proposed technique, the author separated the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5390,"sarcasm identication algorithm into four different analy-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5391,"ses: syntactic-related, sentiment-related, punctuation-related,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5392,"and pattern-related features for the analysis. They proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5393,"an effective and reliable pattern for sarcasm identication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5394,"by grouping words into two separate categories: ``CI'' and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5395,"``GFI.'' While the CI emphasises the importance of the word's
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5396,"contents in the expression, the GFI class concentrates more
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5397,"on the grammatical function of the word. The Random Forest
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5398,"classier was employed for prediction purposes, and an accu-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5399,"racy of 83.1% with a precision of 91.1% was obtained. Thus,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5400,"the comparative analysis indicated that the pattern-based
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5401,"technique outperformed other methods. However, the study
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5402,"relied more on the words' patterns in the expression, which
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5403,"are not sufcient in capturing all the sarcastic sentiments.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5404,"The approaches mentioned above performed optimally well
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5405,"in sarcasm analysis. However, they failed to recognise the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5406,"importance of context information in sarcastic utterances to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5407,"address the ambiguity associated with a sarcastic expression.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5408,"In this research, the context embedding that considers both
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5409,"local and global context information has been employed to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5410,"construct the deep learning and BERT model features by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5411,"considering feature fusion techniques for sarcasm classica-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5412,"tion using textual data. Three benchmark datasets provided
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5413,"by Riloff et al. [14], Ghosh and Vale [15], and IAC-v2
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5414,"[16] have been utilised to test the model. A deep learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5415,"architecture consisting of Bi-LSTM has been employed with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5416,"GloVe embedding to construct context vectors that represent
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5417,"a semantic word as features. GloVe embedding integrates
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5418,"`local context window' and `Global statistics of matrix fac-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5419,"torisation' methods, which are the two key model families
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5420,"[17]. The GloVe embedding can construct a word represen-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5421,"tation that learns grammatical and semantic information and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5422,"captures the word's context and global corpus information.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5423,"Four major performance metrics, such as precision, recall,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5424,"accuracy, and f-measure, have been utilised to evaluate the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5425,"model's empirical analysis. The main contributions of this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5426,"study are summarised below.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5427,"âUnderstand sarcasm as a unique instance of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5428,"context-based sentiment analysis.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5429,"âDene and extract local content and the global context
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5430,"by employing GloVe embedding features.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5431,"âBuild a deep learning model based on Bi-LSTM to auto-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5432,"matically identify sarcasm using context information to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5433,"address the feature engineering problem.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5434,"âProposes a feature fusion technique, which comprised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5435,"BERT feature, hashtag feature, sentiment related fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5436,"ture, syntactic features, and GloVe embedding feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5437,"for sarcasm classication. To the best of our knowledge,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5438,"this is the rst study that integrates BERT feature and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5439,"word embedding with linguistic and sentiment related
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5440,"feature to improve the classication performance for sar-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5441,"casm identication to address the context and sentiment
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5442,"polarity issue in sarcastic utterances
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5443,"48502 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5444,"âThe proposed technique was evaluated via various exten-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5445,"sive experiments on the two benchmark Twitter datasets,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5446,"and the results demonstrated that the proposed technique
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5447,"slightly outperformed the baseline methods for sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5448,"classication.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5449,"The remainder of the sections are organised; thus: In
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5450,"section 2, the literature survey is discussed. Section 3 pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5451,"vides the proposed technique. In section 4, the experimental
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5452,"settings and procedures are discussed. Section 5 presents
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5453,"the experimental design. In section ve, empirical results
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5454,"and discussion are presented. Section 6 nally concludes the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5455,"article with suggestions for future work.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5456,"II. LITERATURE SURVEY
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5457,"Sarcasm identication task has been studied by employ-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5458,"ing different methods, including lexicon-based, conventional
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5459,"machine learning, deep learning, or even a hybrid approach.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5460,"Besides, several reviews on sarcasm detection have also been
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5461,"conducted. For instance, Eke, et al. [18] performed SLR on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5462,"sarcasm identication in Textual data. The study was carried
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5463,"out by considering `dataset collection, preprocessing tech-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5464,"niques, feature engineering techniques (feature extraction,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5465,"feature selection, and feature representation), classication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5466,"algorithms, and performance measures.' The study revealed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5467,"that content-based features are the most employed features
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5468,"for sarcasm classication. The study also revealed that the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5469,"standard evaluation metrics such as precision, recall, accu-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5470,"racy, f-measure, and Area under the curve (AUC) are the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5471,"most used parameters for evaluating classiers' performance.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5472,"Moreover, the study also revealed that when there is an
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5473,"imbalance in the class distribution of the dataset, the AUC
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5474,"performance measure is the right choice due to its robustness
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5475,"in resisting the skewness in the dataset. The review con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5476,"cluded by identifying recent challenges and proposing the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5477,"open research direction to provide a solution to the sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5478,"identication studies issue.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5479,"Various scholars have studied sarcasm identication tasks
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5480,"Joshi, et al. [19], stated two methods, namely, the ``Incon-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5481,"gruent words-only'' method and ``all-words: method'' in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5482,"``Expect the Unexpected: Harnessing Sentence Completion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5483,"for Sarcasm Detection'' research by employing ``Sentence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5484,"completion'' for sarcastic analysis: For evaluation purpose,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5485,"two sets of data were used, which includes (i) Twitter
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5486,"data collected by Riloff, et al. [14], consisting 2278 tweets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5487,"(`506 sarcastic, and 1772 non-sarcastic). (ii) Discussion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5488,"forum data collected by Walker, et al. [6] that contain man-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5489,"ually labelled balanced tweets (`752 sarcastic and 752 non-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5490,"sarcastic). However, `WordNet similarities and word2vec'
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5491,"were employed to measure the similarities in the perfor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5492,"mance. Two-fold cross-validation was used for evaluation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5493,"purposes. Thus, the overall predictive results attained an
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5494,"F-score of 54% by employing the Word2Vec similarity for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5495,"the all-words method, but 80.24% of F-score was obtained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5496,"with the WordNet incongruous words-only method. On the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5497,"other hand, an 80.28% F-score is obtained using the WordNetsimilarity and Incongruous words-only method with 2-fold
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5498,"cross-validation.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5499,"In addition to the handcrafted feature proposed by var-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5500,"ious studies for sarcasm classication, the word polarity
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5501,"disambiguation approach has also gained recognition by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5502,"many scholars in recent years. For instance, Wu and Wen
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5503,"[20] studied a Knowledge-based (unsupervised) approach for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5504,"automatic disambiguation of dynamic sentiment-ambiguous
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5505,"adjectives using a search engine. Remarkably, the author
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5506,"exploited character-based and pattern-based approaches to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5507,"extrapolate nouns' sentiment expectation and nd adjec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5508,"tives' polarity. In another study, Xia, et al. [21] proposed an
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5509,"approach that utilises opinion-level features to resolve words'
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5510,"polarity ambiguity. In this approach, the author examined
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5511,"the inter-opinion (e.g., Discourse, correlative words in the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5512,"sentence) and intra-opinion (e.g., Indicative words, opinion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5513,"target) feature. They employed the probability approach to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5514,"resolving the word polarity disambiguation by adopting the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5515,"Bayesian model. However, they experimented on pinion cor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5516,"pus, and the results of the experiment showed a substan-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5517,"tial impact in the disambiguation of word polarity using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5518,"the opinion-level feature. In a recent study, Wang, et al.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5519,"[22] proposed the word sense disambiguation deep learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5520,"method. In the proposed approach, the sense path in a tar-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5521,"get context is modelled by exploiting the domain-specic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5522,"background knowledge from WordNet by employing the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5523,"word embedding feature extracted from an external corpus.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5524,"The method revealed the hidden semantic relationship within
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5525,"word sense by utilising the `PageRank algorithm' to exploit
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5526,"sense path via WordNet structure while representing the text
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5527,"context target with latent semantic analysis. In a related
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5528,"study,.Abdalgader and Al Shibli [23] proposed a variant
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5529,"of the `graph-based word sense disambiguation approach
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5530,"by exploiting all the occurrence of semantic information
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5531,"acquired using the WordNet to facilitate graph semantics
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5532,"connection for nding the anticipated meanings of words in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5533,"a specied context. In this proposed approach, the similarity
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5534,"between the graph nodes that comprised all related word
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5535,"semantic information for sentence-level disambiguation is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5536,"measured. Next, the real meanings are concurrently allocated
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5537,"to every target word by applying a graph centrality measure
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5538,"that provides the important degree between the graph nodes.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5539,"However, the experimental evaluation and comparison results
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5540,"of the approach with the benchmark dataset outperformed the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5541,"state-of-the-arts WSD approaches.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5542,"The idea of the ensemble learning approach was initially
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5543,"proposed by Fersini, et al. [24] while studying ``Detecting
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5544,"Irony and Sarcasm in Microblog: the role of Expressive
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5545,"Signals and Ensemble Classiers.'' The author considers the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5546,"`Bayesian Model averaging' and various classication algo-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5547,"rithms based on their reliabilities and marginal probability
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5548,"predictions. However, they considered Bayesian Model Aver-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5549,"aging and Majority Voting as the main ensemble approach in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5550,"the classication phase. For evaluation purposes, the author
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5551,"selected the baseline model that attained the best predictive
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5552,"performance and four congurations that include BoW, PoS,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5553,"VOLUME 9, 2021 48503C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5554,"PP, PP & PoS. However, the predictive analysis indicates
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5555,"that the proposed majority voting ensemble model performed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5556,"better than the single classier. The author also showed that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5557,"positives could enhance Sarcasm and that pragmatic features
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5558,"are discriminative in capturing ironic utterances. In another
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5559,"study, ONAN, et al. [25] proposed a Turkish news article's
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5560,"satire detection method. The authors employed linguistic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5561,"Inquiry and word count software for feature extraction by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5562,"considering the linguistic and psychology feature sets. In this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5563,"study, ensemble learning, ve deep learning architecture, and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5564,"word embeddings scheme were considered. The experimental
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5565,"analysis of the proposed approach showed that the deep learn-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5566,"ing approach outperformed other approaches, which showed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5567,"the signicance of the proposed methods. Ptá£ek, et al.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5568,"[26] made the rst attempt to study a multilingual approach
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5569,"for sarcasm identication. In their study, two different lan-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5570,"guages were considered (English and Czech). The authors
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5571,"utilised both English and Czech datasets to compare the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5572,"sarcastic occurrence in both languages. The dataset consists
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5573,"of 140,000 tweets composed in Czech and 780,000 tweets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5574,"composed in English. Twitter API was utilised to stream the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5575,"tweets. During the classication phase, two classiers (Sup-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5576,"port vector machine and Maximum entropy classier) were
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5577,"employed to evaluate the models' predictive performance.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5578,"In the testing phase, a 5-fold cross-validation approach was
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5579,"used in each classier. Thus, 0.947 and 0.924 F-measure was
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5580,"achieved on a balanced and imbalanced English dataset with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5581,"the RF classier. However, SVM produced a better result
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5582,"on the Czech dataset, attaining an F-measure of 0.582 by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5583,"enhancing features with different patterns.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5584,"In relation to affective computing and sentiment classi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5585,"cation, Esuli, et al. [27] proposed a cross-lingual sentiment
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5586,"qualication approach whereby the training data are present
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5587,"in a source language but absent in the target language required
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5588,"for performing sentiment qualication. Thus, this method
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5589,"addresses those application contexts whereby there is a pres-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5590,"ence of the training document for different source languages
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5591,"and the absence of the training document on the interested
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5592,"target language. The author utilised the distributional cor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5593,"respondence indexing (DCI) and structural correspondence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5594,"learning (SCL) approach for cross-lingual text classication.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5595,"However, the experimental analysis using the benchmark
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5596,"datasets on cross-lingual sentiment classication yielded
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5597,"promising prediction results on cross-lingual sentiment qual-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5598,"ication. In another study, Yang, et al. [28] proposed a new
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5599,"method tagged ``Segment-level joint topic-sentiment model
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5600,"(STSM)'' sentiment classication. The proposed approach
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5601,"aimed to determine the document sentiment polarity by cap-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5602,"turing the correlation of the topic sentiment. The author mod-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5603,"elled the joint topic-sentiment's correlation by inserting the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5604,"sentiment layer between the segment and topic layers. How-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5605,"ever, the sentiment classication's predictive performance
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5606,"shows that the proposed approach can enhance complex and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5607,"compound sentences' performance. Moreover, the alignment
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5608,"of sentiment and topics also indicates the signicance of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5609,"the proposed method. Agrawal, et al. [29], in their study onsarcasm identication, explored emotion categories features
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5610,"such as sadness, happiness, surprise, etc.; the authors went
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5611,"deeper by considering the sequential information encoding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5612,"among the effective features state. The comparative analysis
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5613,"of the proposed approach demonstrates the effectiveness
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5614,"of the method. In a recent study, Onan, et al. [30] exam-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5615,"ined the classication performance of conventional machine
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5616,"learning and deep learning models for sentiment analysis on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5617,"product review, and the predictive performance indicates the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5618,"effectiveness of the proposed method on sentiment
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5619,"classication.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5620,"Recently, the application of multi-tasks learning has gained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5621,"recognition and has been demonstrated in various NLP tasks,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5622,"including key phrase boundary detection [31] and implicit
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5623,"discourse relationship detection [32]. In a related study,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5624,"Majumder, et al. [33] proposed a `multitask learning frame-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5625,"work using DNN' for sentiment and sarcasm identication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5626,"study. In their research, they demonstrated that the two tasks
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5627,"are related and, as a result, modelled the two tasks using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5628,"a single neural network. However, the experimental results
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5629,"slightly performed better than the existing approach, which
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5630,"reveals that the multi-task network improves sarcasm clas-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5631,"sication in sarcasm and a polarity shift in the sentence.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5632,"In a related study, Mishra, et al. [34] proposed a method for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5633,"automatic cognitive feature extraction using a CNN variation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5634,"for sentiment and sarcasm identication tasks. The author
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5635,"utilised the gaze data present in the dataset. In the modelling
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5636,"phase, the author modelled the two tasks separately. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5637,"experimental analysis of the learned features shows that the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5638,"hybrid of automatically learned features produces a promis-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5639,"ing result, indicating that it can represent deep linguistic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5640,"subtleties in the textual data, which has remained an issue in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5641,"sentiment and sarcasm classication studies.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5642,"Riloff, et al. [14] presented an approach for detecting a spe-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5643,"cic form of Sarcasm whereby a contradiction exists between
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5644,"positive sentiment and negative situations. They proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5645,"a `bootstrapping algorithm' that utilises a single seed word
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5646,"that automatically identies and learns a phrase that shows
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5647,"positive sentiment and negative situations from the sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5648,"tweets. The authors created two baseline approaches and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5649,"employed the LIBSVM library to model SVM classiers,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5650,"using 10-fold cross-validation for model evaluation. How-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5651,"ever, a precision of 64% and 39% recall were obtained by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5652,"employing SVM on unigram and bigram features. Thus, this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5653,"method performed optimally well, but many sarcastic tweets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5654,"were not captured in the classes mentioned above of Sarcasm.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5655,"Also, the method depends on the occurrence of every possible
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5656,"``Negative situation'' on the training data, which is inefcient
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5657,"on a new tweet data Mukherjee and Bala [11] presented a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5658,"method that provides knowledge to a system, which interprets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5659,"the author's linguistic style by considering different sets of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5660,"features for sarcasm identication in a microblog. They used
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5661,"authorial style-based features for their study, and in the classi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5662,"cation phase, Naïve Bayes and fuzzy clustering algorithms
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5663,"were used. The experimental analysis indicates that the use
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5664,"of supervised and unsupervised learning, and the inclusion of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5665,"48504 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5666,"features that are independent of text produced better accuracy
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5667,"in sarcasm detection. However, the approach is only limited
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5668,"to authorial style-based features and may not work well with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5669,"other feature sets.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5670,"In the existing methods, word-level approaches require
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5671,"a couple of times in training big social data analysis.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5672,"To overcome the limitation, Hussain and Cambria [35] pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5673,"posed a novel `Semi-supervised learning model' by com-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5674,"bining `random projection scaling' as part of the vector
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5675,"space model (VSM) and a SVM to implement cognitive
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5676,"on a knowledge-based of affective common sense. How-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5677,"ever, the experimental analysis results revealed an impor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5678,"tant enhancement in both polarity identication and emotion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5679,"detection over the classication rule because both labelled
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5680,"data and unlabelled data are employed for classication learn-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5681,"ing compared to the formal method that utilises only the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5682,"labelled data. Thus, it opened the opportunity for further
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5683,"research on semi-supervised learning methods on big social
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5684,"data analysis. In a related study, Duan, et al. [36] proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5685,"a new semi-supervised learning method that considers both
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5686,"training and testing sets for sentiment classication of stock
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5687,"text messages. The method was proposed to resolve the issue
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5688,"common in short message modelling, such as data sparsity
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5689,"in mathematical representation. Moreover, the author con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5690,"structed a Generative Emotion Model with categorised words
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5691,"(GEM-CW) to extract sentiment features from both training
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5692,"and testing sets. However, the extracted features were more
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5693,"discriminative for sentiment classication than those derived
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5694,"using the conventional approach that only considers training
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5695,"sets. The analysis results indicate that the proposed learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5696,"approach and the model are signicant for sentiment clas-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5697,"sication in short text and can attain better results than the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5698,"traditional methods.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5699,"Rajadesingan, et al. [12] went deeper and looked into the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5700,"psychology involved in the sarcastic expression. Their study
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5701,"presented behavioural modelling for sarcasm detection by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5702,"identifying the various forms of Sarcasm and their existence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5703,"in tweets. The study shows the signicance of historical
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5704,"information acquired from the past tweet in identication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5705,"sarcasm. Though the approach looks very effective in such
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5706,"an instance, it cannot perform well in the absence of past
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5707,"knowledge about the user. This is because most of the features
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5708,"employed in the study were extracted from the data obtained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5709,"from the past tweet to make a decision. Thus, it is difcult
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5710,"to apply the approach for a real-time stream of tweets, where
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5711,"users are randomly posting tweets due to the fast growth in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5712,"the size of the knowledge base, which requires the repetition
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5713,"of training on data each time new tweet data is acquired.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5714,"The paradigm of the deep learning approach has recently
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5715,"attracted various researchers to combine it with the con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5716,"ventional machine learning approach for sarcasm identi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5717,"cation. For instance, Mehndiratta, et al. [37] presented a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5718,"method of automatic sarcasm identication in textual data
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5719,"using a DCNN. Their study used sentiment polarity as a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5720,"feature set and extracted feature vector using the skip-gram
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5721,"word2vec model technique. The authors further fed thefeature into the convolutional neural network. Their study
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5722,"performed optimally well but has a limitation of word sense
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5723,"not being captured separately. Ghosh and Veale [15] pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5724,"posed a DNN model for sarcasm classication in tweets.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5725,"The study integrated machine learning with a deep learn-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5726,"ing model (a hybrid of CNN, DNN, and LSTM). How-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5727,"ever, the proposed model's predictive results outperformed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5728,"the baseline approach for sarcasm detection by attaining
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5729,"an F-score of 92% [38]. Similarly, Onan [39] conducted
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5730,"a study on ``Topic-Enriched word embedding for sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5731,"Identication.'' The study employed a deep learning method
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5732,"by comparing Topic-enriched word-embedding models with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5733,"traditional word embedding variations: GloVe, Word2vec,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5734,"LDA2vec, and FastText. Besides, the author also exper-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5735,"imented with conventional features, including pragmatic,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5736,"incongruity (implicit & explicit), and lexical features. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5737,"experimental analysis was performed on a dataset by consid-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5738,"ering various subsets, ranging from 5,000 to 30,000. How-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5739,"ever, the aforementioned model's performance showed that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5740,"LDA2vec produced a better result compared with other word
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5741,"embedding schemes. Besides, the fusion of conventional
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5742,"pragmatic features, lexical, explicit, and implicit incongruity
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5743,"with the word embedding scheme enhance the model's pre-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5744,"dictive performance. Recently,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5745,"In a recent study, [40] employed multimodal features that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5746,"consist of textual, speech, and video features to recognise
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5747,"Sarcasm. The textual features in the data sets were repre-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5748,"sented using BERT (Bidirectional Encoder Representation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5749,"from Transformer) [41], a specication for sentence rep-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5750,"resentation. On the other hand, speech feature extraction
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5751,"was extracted using Libnsa, a well-known library for speech
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5752,"extraction [42], by considering only the low-level feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5753,"for audio data to exploit the audio modality information.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5754,"Also, pool ve layers of an ImageNet [43] were utilised on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5755,"each frame for visual feature extraction in video pronounce-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5756,"ment. However, the experimental analysis indicated that mul-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5757,"timodal features produced a better predictive performance
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5758,"than the unimodal features with about a 12.9% reduction
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5759,"in error rate. Recently, [44] presented an effective sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5760,"identication framework on social media data by considering
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5761,"a deep learning approach with neural language models such
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5762,"as FastText, GloVe, and word2vec. The authors introduced
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5763,"inverse gravity moment based on weighted word embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5764,"with trigram. The empirical analysis of the proposed frame-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5765,"work attained an accuracy of 95.3%, which indicates the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5766,"effectiveness of the proposed framework.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5767,"In this study, a novel context-based features technique
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5768,"is proposed for sarcasm identication in three benchmark
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5769,"datasets. Two learning models were constructed for the pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5770,"posed technique. The rst model uses semantic features based
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5771,"on word embedding using a global vector (GloVe). Word
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5772,"embedding helps in learning the representations and relation-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5773,"ships among words. The GloVe is a count-based model that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5774,"captures the relationship between words in a sentence (relat-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5775,"edness) and constructs the learned representation of a real
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5776,"value words vector. Moreover, it helps map all the tokenised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5777,"VOLUME 9, 2021 48505C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5778,"FIGURE 1. Systematic flow of a deep learning approach.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5779,"words in every tweet to its corresponding word table vector.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5780,"Thus, the generated features are employed to train and test the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5781,"model for sarcastic and non-sarcastic categories. The second
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5782,"model is constructed based on transformer learning (BERT)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5783,"and feature fusion that comprised BERT feature, hashtag fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5784,"ture, sentiment related feature, syntactic feature, and Glove
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5785,"embedding feature. BERT is the state-of-the-art model that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5786,"captures the context in sarcastic expression.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5787,"III. AUTOMATIC SARCASM IDENTIFICATION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5788,"The automatic sarcasm identication model primarily con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5789,"sists of ve major components: (i) Data acquisition (ii) Data
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5790,"preprocessing. (iii) Feature Extraction (iv) Construction of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5791,"classication model, and (v) Evaluation of the constructed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5792,"model. In this study, two learning model approaches have
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5793,"been considered to automatically identify sarcasm in tex-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5794,"tual data. They include a deep learning-based approach and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5795,"a transformer-based approach. Each of the approaches is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5796,"explained in the sections below.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5797,"A. DEEP LEARNING BASED APPROACH
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5798,"This section provides a methodological description of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5799,"deep learning methodology. The framework for the deep
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5800,"learning process is depicted in FIGURE 1. Each of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5801,"framework segments is described below.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5802,"1) DATA ACQUISITION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5803,"The sarcasm identication tasks begin with the collection
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5804,"of datasets for the study. Dataset is very crucial in any datamining studies. In this approach, two benchmarks Twitter
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5805,"dataset were utilised to construct sarcastic and non-sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5806,"datasets. Twitter, social media platform, enables users to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5807,"exchange their ideas, news, and emotion with their co-users.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5808,"With the help of Twitter API, a connection between the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5809,"Twitter server and users is provided to make the tweet archive
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5810,"easily accessible. One of the major advantages of Twitter
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5811,"data is that one can collect as many tweets as possible since
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5812,"people posts messages daily. In this approach, two benchmark
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5813,"datasets that natural language processing researchers popu-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5814,"larly use were employed, including the Riloff dataset [14]
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5815,"and Ghosh and Veale [15]. Riloff dataset is the rst public
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5816,"tweet dataset for sarcasm identication collected by [14].
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5817,"The dataset is manually annotated and validated by experts.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5818,"Ghosh and Veale obtained their tweet dataset with a hashtag
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5819,"labelled as #sarcastic, #sarcasm, #ironic [26]. The authors
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5820,"employed a feedback-based approach that enabled them to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5821,"validate the sarcasm label by consulting the authors. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5822,"statistical description of the two datasets is given in Table 1.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5823,"2) DATA PRE-PROCESSING
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5824,"The rst step in the preprocessing of tweets is tokenisation.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5825,"Each text data (tweet) is divided into smaller parts in the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5826,"tokenisation step, either into words or sentences. Tokenisa-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5827,"tion tasks can be performed using the NLTK library. Next is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5828,"the elimination of unwanted information. Most of the social
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5829,"media data, especially Twitter, come along with some noise
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5830,"and, as a result, requires a preprocessing step to eliminate
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5831,"48506 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5832,"TABLE 1. Statistical description of the dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5833,"those unwanted items. For instance, tweets may contain stop
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5834,"words, punctuations, special characters (such as @, #, etc.),
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5835,"and URL links. Thus, all the items that do not contribute
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5836,"to the classication task are eliminated before the feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5837,"extraction process. Data set can be prepared by removing
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5838,"those unwanted items from the tweet contents, such as spe-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5839,"cial characters, numbers, hashtags, tweets composed in other
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5840,"languages than English, stop words, tweets shorter than three
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5841,"words length, and URL links [45], [46]. Also, the other basic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5842,"preprocessing methods, including text normalisation (stem-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5843,"ming, lemmatising, lower case conversion, word equivalent
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5844,"number conversion, and handling of the misspellings), and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5845,"POS (parts of speech) tagging are also performed in this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5846,"stage, which can be implemented using the Natural Language
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5847,"Processing (NLP) toolkit. The processed data is required to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5848,"be transformed into an array representation of the features to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5849,"simplify the model training.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5850,"3) FEATURE EXTRACTION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5851,"In this approach, the word embedding feature (glove) feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5852,"has been utilised. Textual word is usually regarded as discrete
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5853,"and categorical features. Thus, it is required to be represented
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5854,"in a vector format. The vector representation can be carried
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5855,"out by converting the text to the vector space model (VSM).
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5856,"This process can be performed in two different stages. In the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5857,"rst stage, a dictionary of the dataset's term is created (tweets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5858,"dataset in this study). In so doing, each unique dataset term is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5859,"dened in the vector space with a unique identity.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5860,"On the other hand, in the second stage, the numerical
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5861,"representation of terms' is obtained and added to the vec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5862,"tor space. The representation can be done by employing
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5863,"some methods such as TFIDF, TF, and word embedding.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5864,"However, word embedding representation is used in this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5865,"study. Word embedding can be considered the state-of-the-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5866,"art approach for word representation in low dimensional
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5867,"vector space without compromising the contextual similar-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5868,"ity. Also, an almost similar representation can be obtained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5869,"on the words with the same meaning. In contrast, optimum
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5870,"performance can be attained by training the embedding with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5871,"a large amount of textual data. Most popular embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5872,"methods include GloVe, BERT, XLNet, Word2Vec, Fast-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5873,"Text, and ElMo [47]. However, we employed pre-trained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5874,"GloVe in this study due to its outstanding performance
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5875,"with BILSTM compared with other pre-trained embed-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5876,"ding based on our related studies' analysis. In summary,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5877,"the preprocessed tweets produce a two-dimensional vector
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5878,"space model (GloVe embedding). However, each word in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5879,"a processed tweet is a representation of each row in this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5880,"vector.4) CLASSIFICATION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5881,"Deep learning consists of the learning of deep representation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5882,"of data that helps in building an optimised solution from
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5883,"algorithm to solve conventional machine learning problem.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5884,"Deep learning is a powerful learning algorithm that surpasses
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5885,"nding a word representation of data in any particular task.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5886,"It can automatically extract novel features from the varying
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5887,"sets of features in the training data without human effort.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5888,"In other words, it extracts more features in the absence of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5889,"labels on the dataset [48]. In this study, a deep learning model
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5890,"approach that uses the Bidirectional LSTM model (a subset of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5891,"RNN) was validated on the benchmark dataset to enhance the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5892,"performance results on sarcasm analysis has been validated
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5893,"on the benchmark dataset to improve performance results of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5894,"sarcasm analysis. The model selection motivation is that the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5895,"model has obtained promising results in many NLP appli-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5896,"cations since it runs both forward and backward operations
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5897,"on the input clause information. As a result, it has a better
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5898,"understanding of the context in sarcastic expression. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5899,"semantic feature, also known as word embedding features in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5900,"this study, has been proved important in any deep learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5901,"approach for NLP tasks.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5902,"In this study, GloVe, a word embedding scheme that auto-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5903,"matically captures contextual features from the text, was
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5904,"utilised [49]. GloVe is a word embedding scheme that relies
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5905,"on the weighted least-square model and trains not only on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5906,"the local context information of the word (usually used by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5907,"word2vec) but also on the global word-to-word co-occurrence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5908,"count in a corpus to obtain a word vector. This process is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5909,"referred to as parallel implementation, and it facilitates the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5910,"GloVe in modelling on a large dataset. Thus, it integrates the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5911,"discriminative features obtained from two model families:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5912,"`Global matrix factorisation' and `Local content window'
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5913,"to construct a new one [38], [50]. In this pre-trained word
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5914,"embedding approach, the preprocessed data will be employed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5915,"to extract word vectors using word embedding (GloVe), and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5916,"these features will be utilised as a feature for modelling.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5917,"In this case, the text is usually represented in numerical form.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5918,"The deep learning model comprises machine learning and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5919,"an articial neural network that characterises the core of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5920,"network as it contains multiple hidden layers. The deep learn-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5921,"ing model consists of neural networks with various layers
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5922,"that contain a wide range of parameters. The network layers
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5923,"are situated in one of the fundamental network architectures
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5924,"such as convolutional neural network, recurrent neural net-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5925,"work, and recursive network. Convolutional neural network
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5926,"architecture consists of the input layers, convolutional layers,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5927,"pooling layers, and output layers. The input data are fed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5928,"through the input layer and then pass to the convolutional
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5929,"layer. In the convolutional layer, feature maps are extracted,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5930,"bypassing the convolutional lter on input data. However,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5931,"multiple lters are utilised to input data for multiple feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5932,"extraction. The nal decision is made by the fully connected
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5933,"layer, connected to the output layer and the previous layer.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5934,"A recurrent neural network is a standard network that uses an
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5935,"edge to feed into the next time slides instead of feeding into
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5936,"VOLUME 9, 2021 48507C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5937,"Algorithm 1 :Algorithm Representation for Deep Learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5938,"Approach for Sarcasm Identication Training Process
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5939,"Input: Training on processed data using Bi-LSTM with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5940,"pre-trained GloVe
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5941,"Output: Sarcasm prediction report
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5942,"1. Data collection and preprocessing
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5943,"Obtain benchmark dataset
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5944,"Read the dataset
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5945,"Preprocess the data
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5946,"Apply tokenisation on data using the NLTK tech-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5947,"nique
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5948,"Split data into train, test and validation set
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5949,"2. Feature Extraction
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5950,"Apply pre-trained word embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5951,"Load GloVe embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5952,"Create embedding matrix by assigning vocabulary
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5953,"with pre-trained word embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5954,"3. Set the parameter for the network
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5955,"Set the value of the hidden unit
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5956,"Minimum batch size
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5957,"Dimension of GloVe vector
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5958,"Max epoch value
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5959,"Mini batch size
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5960,"Regularisation value Optimise the Bi-LSTM network
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5961,"parameter
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5962,"4. Add callback
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5963,"6 Obtain the network output
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5964,"7 Train the model
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5965,"Train the Softmax layer using the supervised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5966,"approach
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5967,"Stack the Bi-LSTM and Softmax layer
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5968,"8 Apply the ne-tuning strategy
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5969,"9 Test the model using the pre-trained network and test
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5970,"data
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5971,"10 Obtain the prediction
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5972,"11 Output the prediction report
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5973,"the next layer in a similar time slide. It contains a cycle that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5974,"signies the existence of short memory in the network. On the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5975,"other hand, the recurrent neural network operates similarly
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5976,"to a hierarchical network that does not require time slides
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5977,"allocation to the input sequence but rather processes the input
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5978,"in a hierarchical tree structure. As stated above, this study
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5979,"will employ Bi-LSTM to construct a deep learning model for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5980,"sarcasm identication.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5981,"a: LONG-SHORT TERM MEMORY (LSTM)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5982,"LSTM was created as an enhanced form of the standard
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5983,"recurrent neural network [51], [52] to modify its state to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5984,"verify what to retain and what to discard. LSTM is created by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5985,"increasing the memory capability of RNNs [53]. The core aim
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5986,"of creating LSTM is to address the exploding and vanishing
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5987,"gradient problem found in the standard RNN. During the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5988,"FIGURE 2. LSTM representation.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5989,"training process, LSTM maintains the error to back-propagate
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5990,"using deeper layers in which learning continues over various
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5991,"steps. LSTM is created to learn long-distance dependencies
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5992,"within the sequential data. It keeps the contextual semantic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5993,"information for dependencies in a long-range context using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5994,"special memory cells. In each LSTM unit, which consists of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5995,"the input, forget, and output gate is employed to coordinate
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5996,"and decide on the fraction of information to hold, discard, and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5997,"move to the next step. It also decides when to issue read, write,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5998,"and delete permission through gates that either pass or block
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
5999,"information ow through the LSTM unit. LSTM architecture
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6000,"is depicted in FIGURE 2. To compute the input, forget,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6001,"and output gate together with the input cell state, equations
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6002,"1-6 below can be employed.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6003,"itD(WiyxtCWizht 1Cbi) (1)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6004,"ftD(WfyxtCWfzht 1Cbf) (2)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6005,"otD(WoyxtCWozht 1Cbo) (3)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6006,"dtD(WdyxtCWdzht 1Cbd) (4)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6007,"ctDft
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6008,"ct 1Cit
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6009,"dt (5)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6010,"htDtanh(C t)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6011,"Ot (6)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6012,"where
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6013,"represents element products; bibfbobdrepresents
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6014,"bias vectors. tanh represents a hyperbolic tangent function,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6015,"Dsigmoid function that represents gate activation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6016,"function. WiWfWoWdrepresents the weighing factors utilised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6017,"for mapping input cell state and three gates with the input
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6018,"hidden layers.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6019,"rhtD[ht n:::::::::::: ht 1] epresents the nal LSTM
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6020,"layer output (i.e., a vector of all output)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6021,"b: BI-DIRECTIONAL LSTM (BI-LSTM)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6022,"As indicated by [54], Bi-LSTM can capture composi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6023,"tional information in a sentence (for each input sentence).
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6024,"Bi-directional LSTM is made up of the forward operation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6025,"network that reads the clause information in the forward
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6026,"direction between word 1 and n, and the backward opera-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6027,"tion network that reads the clause information in the back-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6028,"ward direction. Thus, the generated hidden states from both
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6029,"directions (forward and backwards) are joined to form hidden
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6030,"48508 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6031,"FIGURE 3. Systematic flow of BERT model and feature fusion approach.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6032,"states for Bi-LSTM. The output of the network generates both
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6033,"future and past contexts. Thus, each output vector element
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6034,"obtained by Bi-LSTM is computed by applying equation 7
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6035,"[55]
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6036,"ytD(h!;h$t) (7)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6037,"whereis a function that outputs two sequences, the function
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6038,"can be used for summation, multiplication, average, and con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6039,"catenation function. However, a vector representation can be
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6040,"employed to represent the nal output of a Bi-LSTM layer,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6041,"as shown in the equation below.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6042,"YtD[yt n;::::::::: yt 1] (8)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6043,"Thus, concatenating the Bi-directional layer and LSTM
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6044,"layer constructs Bi-LSTM, and the LSTM results will be
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6045,"automatically concatenated.B. TRANSFORMER APPROACH AND FEATURE FUSION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6046,"This section provides a methodological description of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6047,"BERT approach and features a fusion technique for sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6048,"identication. The framework for the transformer and fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6049,"ture fusion process is depicted in FIGURE 3. Each of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6050,"framework segments is described below.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6051,"1) DATASET
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6052,"In order to provide a complete evaluation of the proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6053,"approach, the internet argument corpus version (IAC-v2)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6054,"dataset has been utilised. The IAC-v2 dataset was made
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6055,"publically available by [16]. It contains three sub-corpora,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6056,"in which the highest one is referred to as ``generic,'' contain-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6057,"ing 3260 posts per class obtained from iacv2 prepared for the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6058,"sarcasm detection dataset. Many scholars on sarcasm detec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6059,"VOLUME 9, 2021 48509C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6060,"tion study tasks have utilised the dataset. The distribution of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6061,"train, test, and validation sets are provided in table1.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6062,"2) DATA PREPROCESSING
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6063,"In this section, a similar preprocessing technique used for the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6064,"deep learning approach was employed in this section.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6065,"3) FEATURE EXTRACTION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6066,"In addition to the BERT feature, three other handcrafted
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6067,"features that comprised hashtag features (positive and neg-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6068,"ative hashtag) were proposed and extracted for feature fusion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6069,"classication. These features are described below.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6070,"a: HASHTAG FEATURE
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6071,"A hashtag is sometimes used to express some emotional con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6072,"tent. The hashtag is used to disambiguate the actual meaning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6073,"by the Twitter user to pass a message. For example, in a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6074,"tweet, ``Thank you for always sending me money, #i hate
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6075,"you.'' In this expression, the hashtag ``#i hate you'' shows
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6076,"that the user is not really expressing thanks to the intended
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6077,"but tremendously hating him for not sending him money.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6078,"We call the utterance mentioned above a negative hashtag
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6079,"token. Hashtag features can be represented as a positive or
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6080,"negative hashtag. In this study, three hashtag features are
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6081,"dened: a positive hashtag, a negative hashtag, and the posi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6082,"tive and negative hashtag co-existence. The hashtag features
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6083,"are extracted by using a sentiment lexicon that consists of a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6084,"list of negative hashtag words such as ``#hate, #pity, #waste,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6085,"#discrimination, etc.,'' and a list of a positive hashtag such as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6086,"``#happy, #perfect, #great, #goodness, etc.'' However, using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6087,"this lexicon, the number of positive hashtags and negative
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6088,"hashtags present in the tweet text is computed and added
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6089,"as a feature. The third feature is extracted by checking the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6090,"co-occurrence of positive hashtags and negative hashtags in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6091,"the same token. However, if there is co-occurrence, one is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6092,"measured; otherwise, zero (0) is measured.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6093,"b: SENTIMENT RELATED FEATURE
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6094,"Sentiment-related feature: the most common form of Sar-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6095,"casm that occurs in social media is a whimper. In whimper,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6096,"the composer of sarcastic utterance uses positive sentiment
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6097,"to describe a negative situation. In this regard, the expres-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6098,"sion of Sarcasm makes use of contradicting sentiment that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6099,"can be observed in the expression of the negative situation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6100,"using positive sentiment as found in the study conducted
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6101,"on sarcasm analysis by Riloff, et al. [14]. For example, `I
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6102,"love being always cheated.' In this study, we investigated the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6103,"contradiction between the word's sentiment and other tweets'
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6104,"components to recognise such sarcastic statements. To this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6105,"end, sentiment-related features are extracted from each tweet
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6106,"and counted. A SentiStrength [56] lexicon was utilised to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6107,"obtain the sentiment scores of the words. SentiStrength is asentiment lexicon that utilises linguistic rules and information
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6108,"to detect English text sentiment. The lexicon usually pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6109,"vides the polarity sentiment (positive and negative) of words
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6110,"like question words, emotion words, booster words, negation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6111,"words, idioms, slangs, and emoticons. The score uses an inte-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6112,"ger that ranges from  5 toC5, in which the larger absolute
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6113,"value represents the stronger sentiment. In addition, we also
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6114,"extracted more features that show the contrast between the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6115,"sentiment components that include; positive words, highly
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6116,"emotional positive words, negative words, and highly emo-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6117,"tional negative words. Finally, we dened two more fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6118,"tures that check the contrast between different components
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6119,"(co-occurrence between negative and positive components)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6120,"within the same tweet. Therefore, the sentiment-related
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6121,"feature vector contains 8 features.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6122,"c: SYNTACTIC FEATURE
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6123,"The syntactic feature plays a signicant role in offering infor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6124,"mation about the syntactic structure of tweets. This research
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6125,"denes Parts of speech feature, laughing expression, and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6126,"interjection word as syntactic features extracted from the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6127,"tweet's content. However, the NLTK tokeniser library was
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6128,"employed to perform tokenisation on the processed tweets.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6129,"Firstly, we extracted the POS feature using the parts of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6130,"speech library, and the count of its presence in the sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6131,"text is taken. This study only concentrated on POS with some
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6132,"sentimental contents such as nouns, adverbs, and adjectives.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6133,"Furthermore, POS tags are mapped with each corresponding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6134,"POS group, and only the tokenised words that correspond
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6135,"with the chosen three parts of speech groups as aforemen-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6136,"tioned were preserved in the text. This research utilised a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6137,"similar approach used in [57] and extracted adverbs, adjec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6138,"tives, and nouns. Secondly, to extract the second feature,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6139,"we identied laughter words used to express pleasures or
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6140,"joy. Thus, we added laughing features: the sum of internet
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6141,"laughs, represented with lol, hahaha, hehe, ro, and imao.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6142,"The feature is extracted by creating a list containing the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6143,"most common laughing words, and it was employed to nd
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6144,"the frequency of such words. Then, the frequency of such
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6145,"words present in the text was computed and added as a fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6146,"ture. The third feature is extracted by identifying interjection
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6147,"words such as woo, oh, wow, etc. in the tweets and the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6148,"frequency of interjection words is computed and added as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6149,"a feature.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6150,"4) CLASSIFICATION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6151,"BERT is a multi-layer bidirectional transformer encoder
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6152,"trained on BooksCorpus [58] and English Wikipedia that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6153,"contain 800M and 2,500M tokens, respectively, which can
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6154,"learn deep bi-directional representations and, in the future,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6155,"can be ne-tuned to perform various tasks like NER. How-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6156,"ever, Data is tokenised before pre-training using WordPiece
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6157,"embeddings. In the pre-training phase, two unsupervised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6158,"methods are used, Masked Language Model (MLM) and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6159,"Next Sentence Prediction (NSP). In MLM, also known as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6160,"masked language prediction task, 15% of the word in the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6161,"48510 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6162,"input sequence are marked out; thus, the whole sequence is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6163,"provided to a deep bidirectional transformer [59] encoder,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6164,"and the masked words are predicted by model learning.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6165,"In sentence prediction, on the other hand, BERT takes input
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6166,"sentences A and B for learning the relationship between
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6167,"sentences. Empirically, its established bidirectional nature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6168,"allows the model to understand the features from data ef-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6169,"ciently. Unlike the traditional sequential or recurrent models,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6170,"the entire input sequence is being preprocessed by atten-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6171,"tion architecture once, enabling the parallel processing of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6172,"input tokens. FIGURE 3 provides the visualisation of BERT
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6173,"architectural layers. The ne-tuning of the pre-trained BERT
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6174,"model by adding an extra layer could produce state-of-the-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6175,"art results in various natural language processing tasks [60].
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6176,"BERT model is comprised of two separate models called the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6177,"BERTbase model and BERTlarge model. On the one hand,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6178,"the BERTbase model consists of an encoder with 12 layers
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6179,"(also known as transformer blocks), 110 million parameters,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6180,"and 12 self-attention heads. On the other hand, the BERT-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6181,"large model is comprised of 340 million parameters,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6182,"16 attention heads, and 12 layers. In the BERTbase model,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6183,"the number of hidden dimensions extracted from embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6184,"is 768 [41].
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6185,"a: INPUT AND OUTPUT REPRESENTATION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6186,"BERT accepts the utmost length of 512 sequences of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6187,"tokens as input and represents the token sequences of a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6188,"768-dimensional vector as an output. In BERT, the maxi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6189,"mum of two-segment insertion can be made in each input
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6190,"sequence, including [SEP] and [CLS]. Special classication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6191,"token ([CLS]) embedding is usually the initial input sequence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6192,"token. It holds a special classication embedding chosen as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6193,"the rst token in the last hidden layer to represent the full
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6194,"sequence in a sarcasm classication task. However, the last
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6195,"hidden state that correlates to this token is employed as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6196,"the aggregate sequence to represent sarcasm classication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6197,"tasks. In addition, pairs of a sentence are crowded together
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6198,"in one sequence. These sentences can be differentiated into
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6199,"two methods. Firstly, a special token ([SEP]) embedding
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6200,"is employed to separate the sentences. In our classication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6201,"task, we shall employ only [CLS] embedding input sequence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6202,"tokens. Secondly, a learner embedding is added in every token
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6203,"with an indication that shows the sentence that each sentence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6204,"belongs to (either A or B).
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6205,"In our sarcasm detection task, we use social media data,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6206,"which requires rst to carry out an essential step of analysing
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6207,"the contextual information obtained from the pre-trained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6208,"BERT layer and ne-tune the model using annotated dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6209,"This is carried out because the BERT model is pre-trained on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6210,"general corpora. During ne-tuning, the weight is updated by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6211,"utilising a labelled dataset that is new to the previously trained
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6212,"model.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6213,"5) FEATURE FUSION APPROACH
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6214,"Word embedding features is not enough in capturing all the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6215,"sarcasm clue in sarcastic utterances, owing to the drawbackin word embedding feature. One of the major limitations of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6216,"word embedding is that it ignores the sentiment polarity of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6217,"words [61], [62]. Though word embedding based word vector
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6218,"captures the word's context, words having opposite polarity
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6219,"are mapped into close vectors. For example, the two different
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6220,"words ``like'' and ``unlike'' can occur in the same context as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6221,"illustrated in sentences below:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6222,"``I like that footballer'' and ``I dislike that footballer.''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6223,"Thus, the word embedding (word vector) feature lacks
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6224,"enough sentiment information in performing sarcasm clas-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6225,"sication, and it does not precisely capture the overall
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6226,"sentiment of the sarcastic expression.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6227,"Thus, a feature fusion approach is proposed that comprised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6228,"of BERT feature, hashtag feature, sentiment related feature,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6229,"syntactic features, and GloVe embedding feature for sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6230,"classication to address the sentiment polarity problem in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6231,"sarcasm utterances. To the best of our knowledge, this is the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6232,"rst study that integrates the BERT feature and Word embed-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6233,"ding with linguistic and sentiment-related features to improve
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6234,"the classication performance for sarcasm identication to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6235,"address the context and sentiment polarity issue in sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6236,"utterances.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6237,"IV. EXPERIMENTAL DESIGNS
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6238,"In this section, various empirical analysis is performed to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6239,"implement the proposed contextual feature-based technique
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6240,"for sarcasm identication using a deep learning model. Ana-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6241,"conda framework was utilised to implement the proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6242,"approach for the sarcasm detection model. The system con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6243,"guration is window ten (10) pro (64-bit operating sys-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6244,"tem), running on Intel core i7 processor with 12 GB RAM.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6245,"A detailed explanation of the parameter settings and their
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6246,"impact on the model performance and how the ne-tuning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6247,"of the parameter was carried out to enhance the proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6248,"deep learning technique's performance are provided in the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6249,"subsequent sections.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6250,"A. EXPERIMENTAL SETTINGS
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6251,"1) DEEP LEARNING APPROACH
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6252,"The rst step before experimenting is data preparation. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6253,"detail of data preparation is described in section 3. In this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6254,"study, pre-trained word embedding (GloVe) was adopted
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6255,"for the deep learning model [49]. Here, the word embed-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6256,"ding serves as an input layer. The study employed Twitter
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6257,"GloVe word embedding obtained from 2B tweets and 27B
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6258,"tokens, containing 1,2M vocabulary with 200d vectors with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6259,"the word data level. GloVe word embedding scheme with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6260,"word feature vector dimension k is xed at 200. However,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6261,"some ne-tuning of the embedding was performed during
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6262,"the training process, and a dropout of 0.2 rates was utilised
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6263,"to prevent the over-tting problem in the model. The hyper-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6264,"parameter dimension d for Bi-LSTM hidden units' layers
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6265,"is xed at 100 for the forward direction and 100 for the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6266,"backward direction (total of 200). Conversely, 50 epoch max-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6267,"imum was trained for the model. The minimum training
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6268,"batch size for both datasets is tuned to 128 mini-batch sizes.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6269,"VOLUME 9, 2021 48511C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6270,"TABLE 2. Hyperparameter settings.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6271,"The initialisation of all weight matrices was performed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6272,"by sampling in a similar direction and setting biases to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6273,"zero. Adam algorithm optimisation [63] was adopted for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6274,"model parameter optimisation with the learning rate set to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6275,"0.001 rates. Tensorow and Keras were used for the imple-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6276,"mentation of the architecture. It is crucial to select the optimal
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6277,"parameter to obtain the best predictive results. Table 2 depicts
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6278,"the summary of the Hyper-parameter setting for this study.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6279,"2) TRANSFORMER (BERT) APPROACH
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6280,"In this setting, the dataset was preprocessed by ltering some
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6281,"unwanted items that could reduce the classication perfor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6282,"mance, such as user mention, URL links, hashtags, foreign
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6283,"language characters, stop words removal, and non-English
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6284,"ASCII character. The preprocessed training set is randomly
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6285,"split into two training (70%) and validation (30%). The exper-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6286,"iment was carried out on a system running on window 10 with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6287,"64-bit operating systems. The system uses an Intel CoreTM
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6288,"i7-4770 CPU @ 3.400GHz with 16GB Random Access
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6289,"Memory (RAM) capacity.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6290,"The proposed model uses BERT Basefor encoding the input
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6291,"preprocessing text and produces a 768-dimensional hidden
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6292,"state of classication token [CLS]. The study utilised Adam
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6293,"optimiser [64] for optimisation, using the learning rate of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6294,"0.001. The model is trained for 35 epochs with a batch size
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6295,"of 16. Moreover, the maximum sequence length is set to 128,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6296,"the reprocess input data and overwrite output directory is set
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6297,"to true.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6298,"B. EVALUATION MEASURE
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6299,"Evaluation measures are the performance indicators that have
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6300,"been established to measure the output of the classica-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6301,"tion algorithms. In the evaluation phase, the constructed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6302,"model predicts the class of unlabelled text (sarcastic or
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6303,"non-sarcastic) using the training data sets. The predictive
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6304,"performance of the constructed model can be evaluated by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6305,"employing the following parameters:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6306,"Accuracy (ACC) provides the percentage ratio of the pre-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6307,"dicted instances. It measures the overall correctly classied
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6308,"instances. It is computed by dividing the overall number
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6309,"of true instances (that consists of true positive and truenegative) by all the instances. Precision (PRE) provides
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6310,"model accuracy in the existence of false positive instances.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6311,"Thus, the model accuracy provides the overall occurrence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6312,"of false positive instances with the rejection of positive
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6313,"instances. Precision is computed by nding the ratio of true
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6314,"positive over a positive result. Recall (REC) is used to mea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6315,"sure accuracy, which shows the model performance in the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6316,"existence of a false negative instance. It is the proportion of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6317,"actual positives, which are predicted positive. Thus, the false
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6318,"negative shows the wrongly predicted instance on the data.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6319,"It mathematically denotes the true positive ratio against all
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6320,"the true results. F-measure (F-M) is a cumulative factor to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6321,"test the overall effect of the recall and precision in order
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6322,"to nd the overall impact of false negative instances and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6323,"false positive instance over the whole accuracy. It denotes
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6324,"the harmonic mean of precision and recall in the presence
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6325,"of critical equality of false positive and false negative. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6326,"standard F-measure is F1-score, which provides equal impor-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6327,"tance of recall and precision. The true positive (TP)result
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6328,"is noticed when the predicted tweet is found to be sarcastic,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6329,"and the result of the classication shows exactly sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6330,"after the experimental evaluation. True negative (TN) The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6331,"true negative result is obtained when the predicted tweet is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6332,"not sarcastic, and the classication result also validates it
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6333,"as not sarcastic. False positive (FP) occurs in a situation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6334,"where a true negative result is obtained when the predicted
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6335,"tweet is not sarcastic, but the classication result indicates
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6336,"that the tweet is sarcastic. False negative (FN) occurs when
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6337,"the true positive result is obtained when the predicted tweet is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6338,"sarcastic, but the classication result shows that tweet is not
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6339,"sarcastic. Sensitivity (SEN) determines the model's appro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6340,"priateness in detecting the positive class outcome (detection
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6341,"probability). Specicity (SP) determines the exactness of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6342,"positive class assignment (True negative rate) Confusion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6343,"matrix: Also known as the error matrix, is a unique table
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6344,"representation that gives the picture of the classier's exe-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6345,"cution, especially the supervised learning classication. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6346,"confusion matrix consists of two instances (``predicted'' and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6347,"``actual'') of the same sets of classes. Basically, the negative is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6348,"discarded, whereas the positive is identied. Thus, after the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6349,"classication, true positive is the instance that is accurately
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6350,"classied, whereas false positive is not correctly classied.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6351,"False positive instances symbolise type 1 error, indicating that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6352,"the number of instances is not correctly indicated as positive.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6353,"On the other hand, true negatives are those instances that are
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6354,"correctly discarded, and false negatives denote the incorrectly
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6355,"classied instances. False negative symbolises type 2 error,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6356,"indicating that the number of instances is incorrectly clas-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6357,"sied as negative. The pictorial diagram of the confusion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6358,"matrix is depicted in Table 3
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6359,"AccDTPCTN
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6360,"TPCTNCFPCFN(9)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6361,"PREDTP
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6362,"TPCFP(10)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6363,"RECDTP
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6364,"TPCFN(11)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6365,"48512 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6366,"TABLE 3. Confusion matrix.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6367,"TABLE 4. Performance results of the deep learning model.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6368,"F MD2(PREREC
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6369,"PRECREC) (12)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6370,"SPDTN
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6371,"TNCFP(13)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6372,"SENDTP
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6373,"TPCFN(14)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6374,"Where TP represents a true positive number, TN denotes a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6375,"true negative number, FN represents a false negative number,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6376,"and FP represents a false positive number.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6377,"V. EXPERIMENTAL RESULTS AND DISCUSSIONS
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6378,"This section discusses the results obtained in the experi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6379,"mental analysis of the proposed technique. The experimen-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6380,"tal results on the three benchmark datasets are reported
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6381,"in Table 4 and Table 7, whereas the performance comparison
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6382,"of deep learning based on two Twitter datasets is represented
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6383,"in FIGURE 6. Also, both datasets' confusion matrices are
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6384,"represented in FIGURE 4 and FIGURE 5. Precision has
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6385,"been utilised as the key performance metric. However, other
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6386,"standard evaluation metrics that include accuracy, F-measure,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6387,"recall, sensitivity, and specicity were also employed as sup-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6388,"plementals to evaluate the proposed model with the baseline
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6389,"approach. The performance evaluation has been established
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6390,"with three benchmark datasets for sarcasm classication to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6391,"FIGURE 4. Confusion matrix for Riloff dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6392,"FIGURE 5. Confusion matrix for ghosh and vale dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6393,"FIGURE 6. Performance results of the deep model on two datasets.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6394,"evaluate the proposed technique's effectiveness. The evalua-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6395,"tion of the proposed approach is done based on the baseline
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6396,"study and dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6397,"A. DEEP MODEL PERFORMANCE RESULTS ON RILOFF
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6398,"DATASET
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6399,"In Table 4, the predictive results obtained on the Rillof
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6400,"data are presented. It can be observed from the table that
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6401,"VOLUME 9, 2021 48513C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6402,"TABLE 5. Comparison results of Riloff dataset with baseline.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6403,"the predictive performance attained a precision of 98%.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6404,"We can also observe that the model achieved almost perfect
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6405,"prediction with other performance measures such as recall,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6406,"f-measure, and accuracy. The result also shows a 100% pre-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6407,"diction with specicity. However, the sensitivity measure
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6408,"recorded the least result with a predictive result of 95.56%.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6409,"Our deep learning approach on the Riloff dataset was also
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6410,"evaluated with four baseline studies on sarcasm analysis.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6411,"Table 5 and FIGURE 7 present the comparison results. It can
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6412,"be observed from Table 5 that our model improved the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6413,"baseline performance substantially. Our proposed approach's
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6414,"highest result is represented in bold, whereas the highest
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6415,"performance on the baseline approaches is indicated in italic.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6416,"The results show that our approach outperformed the baseline
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6417,"in terms of precision, recall, f-measure, and accuracy by
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6418,"attaining a precision of 98%. Our proposed technique on this
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6419,"dataset nearly attains the perfect model performance with an
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6420,"accuracy of 99%, recall of 99.5%, precision of 98%, and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6421,"f-measure of 99% of our model. It also shows that the Zhang
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6422,"approach outperformed other baseline approaches in terms of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6423,"precision only, but Ghosh and Tay's approach outperformed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6424,"in terms of accuracy, recall, and F-measure. It should also
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6425,"be noted that the Riloff approach performed least in terms
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6426,"of precision, f-measure, and recall when compared to other
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6427,"baseline approaches but outperformed the Tay approach in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6428,"terms of precision. Thus, the proposed context-based feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6429,"technique using the deep learning approach attained average
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6430,"detection precision between 1.23% to 33% compared to the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6431,"existing method using the Riloff dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6432,"B. DEEP MODEL PERFORMANCE RESULTS ON GHOSH
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6433,"AND VALE DATASET
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6434,"Table 4 also depicts the predictive results obtained on the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6435,"Ghost dataset. It can be observed from the table that the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6436,"predictive performance attained a precision of 98.5%. We can
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6437,"also observe that the model achieved high-performance
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6438,"results with other performance measures such as recall,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6439,"f-measure, and accuracy. The result also shows specicity
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6440,"attained the prediction of 98.63% when compared with other
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6441,"performance measures. However, the sensitivity measureTABLE 6. Comparison results of ghosh and vale dataset with baselines.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6442,"FIGURE 7. Performance evaluation of the deep learning model compared
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6443,"to the baseline on Riloff dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6444,"recorded the least result with a prediction of 97.9%. Thence,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6445,"to evaluate the proposed technique on this dataset, two
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6446,"baseline studies have been utilised. The results of the eval-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6447,"uation are depicted in Table 6 and FIGURE 8. Our pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6448,"posed approach's highest result is indicated in bold, whereas
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6449,"the highest result obtained from the baseline is in italic.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6450,"It can be observed that our proposed deep learning approach
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6451,"outperformed all baseline approaches by attaining accuracy
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6452,"of 98.21%, precision of 98.5%, recall of 98%, and f-measure
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6453,"of 98%. Referring to the baseline studies, it can be noticed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6454,"that Ghosh's study has the least performance in terms of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6455,"precision, recall, and F-measure. However, the author did
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6456,"not determine the accuracy performance in their study. The
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6457,"evaluation of our proposed approach with the baseline shows
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6458,"that our proposed deep learning approach outperformed the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6459,"Xiong approach in terms of accuracy with 18.12%, preci-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6460,"sion with 22.11%, recall with 14.44%, and f-measure with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6461,"24.08%. Also, it supersedes the Ghosh and vale approach in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6462,"precision with 25.2%, recall with 14.44%, and f-measure with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6463,"24.08%. Thus, the proposed context-based technique using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6464,"deep learning attained average detection precision between
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6465,"22.11% to 25.2% compared to the existing method using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6466,"the Ghosh dataset. The result shows that models involving
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6467,"Bi-LSTM can learn the contextual information from the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6468,"48514 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6469,"FIGURE 8. Performance evaluation of the deep learning model compared
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6470,"to the baseline on ghosh dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6471,"TABLE 7. Comparative results of the deep learning model, Transformer
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6472,"(BERT) model, and feature fusion on IAC-v2 dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6473,"sarcastic expression and uses the information to a large
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6474,"magnitude by enhancing the model performance.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6475,"C. BERT AND FEATURE FUSION PERFORMANCE RESULTS
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6476,"ON IAC-v2 DATASET
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6477,"In Table 7, the predictive results obtained on the IAC-v2 data
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6478,"is presented. It can be observed from the table that the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6479,"comparative results of the deep learning model (Bi-LSTM),
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6480,"BERT model, and feature fusion are presented. The deep
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6481,"learning model attained the highest precision of 68.4%, trans-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6482,"former approach 80%, and feature fusion 81.2%. There-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6483,"fore the proposed feature fusion slightly outperformed the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6484,"BERT approach. We can also observe that the feature fusion
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6485,"approach achieved outperformed other performance mea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6486,"sures such as recall, f-measure, and accuracy. The feature
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6487,"fusion approach with BERT on the IAC-v2 dataset was also
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6488,"evaluated with three baseline studies on sarcasm analysis to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6489,"show the signicance of the proposed approach. Table 8 and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6490,"FIGURE 9 present the comparison results. It can be observed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6491,"from Table 5 that the proposed feature fusion with the BERT
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6492,"approach improved the baseline performance substantially.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6493,"The proposed approach's highest result is represented in bold,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6494,"whereas the highest performance on the baseline approachesTABLE 8. Comparison results of the proposed feature fusion with the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6495,"baselines.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6496,"FIGURE 9. The comparative performance of the feature fusion with BERT
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6497,"and baselines.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6498,"is indicated in italic. The results show that our approach
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6499,"outperformed the three baselines in terms of precision, recall,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6500,"and f-measure by attaining a precision of 81.2%. The pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6501,"posed feature fusion on this dataset outperformed baselines
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6502,"in terms of recall and f-measure by attaining a recall of 78.5%
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6503,"and an f-measure of 79.8%. It also shows that out of the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6504,"three baselines evaluated on the proposed approach, the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6505,"Poria, et al. [68] approach outperformed other baselines in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6506,"terms of precision only by attaining a precision of 77.5%.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6507,"However, Oraby, et al. [16] approaches outperformed the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6508,"Poria, et al. [69] and Gangi, et al. [68] in terms of recall
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6509,"and f-measure. It should also be noted that the Gangi, et al.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6510,"[68] approach performed least in terms of f-measure and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6511,"recall when compared to other baseline approaches but out-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6512,"performed the Poria approach in terms of precision. Thus,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6513,"the proposed context-based feature technique using the fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6514,"ture fusion approach with BERT attained average detection
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6515,"precision between 3.7% to 10.2% when compared to the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6516,"existing method using the IAC-v2 dataset.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6517,"VOLUME 9, 2021 48515C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6518,"Based on the two datasets' overall results, it can be
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6519,"observed that the proposed approach can address the sar-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6520,"casm identication on the tweet domain. It indicates the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6521,"efciency of the deep neural model that uses a Bidirec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6522,"tional long short term memory network. The result shows
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6523,"that models involving Bi-LSTM can learn the contextual
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6524,"information from the sarcastic expression and uses the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6525,"information to a large magnitude by enhancing the model
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6526,"performance. Also, BERT features can capture some con-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6527,"textual information and word sense in sarcasm expression,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6528,"and feature fusion with BERT features can address word
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6529,"embedding-based features commonly found in deep learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6530,"approaches.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6531,"VI. CONCLUSION
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6532,"Sarcasm identication has been a crucial challenge in nat-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6533,"ural language processing. The sarcasm identication task
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6534,"is a classication problem aimed at distinguishing sarcas-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6535,"tic utterances from the non-sarcastic counterparts. Accurate
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6536,"identication of sarcasm can enhance the sentiment analysis
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6537,"and opinion mining study. This study has focused on the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6538,"context-based feature technique for sarcasm identication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6539,"using deep learning, transformer learning, and conventional
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6540,"machine learning models. Two Twitter and Internet Argu-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6541,"ment Corpus, version two (IAC-v2) benchmark datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6542,"were utilised for classication using the three learning mod-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6543,"els. The rst model uses embedding-based representation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6544,"via deep learning model with bidirectional long short term
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6545,"memory (Bi-LSTM), a variant of recurrent neural network
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6546,"(RNN), by applying Global vector representation (GloVe)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6547,"for the construction of word embedding and context learn-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6548,"ing. The second model is based on Transformer using a
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6549,"pre-trained Bidirectional Encoder representation and Trans-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6550,"former (BERT). In contrast, the third model is based on fea-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6551,"ture fusion that comprised BERT feature, sentiment related,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6552,"syntactic, and GloVe embedding feature with conventional
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6553,"machine learning. The effectiveness of this technique is tested
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6554,"with various evaluation experiments. However, the tech-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6555,"nique's evaluation on the two Twitter benchmark datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6556,"attained 98.5% and 98.0% highest precision, and the IAC-v2
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6557,"dataset attained the highest precision of 81.2%, respectively.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6558,"When evaluated with the baselines, the obtained results on
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6559,"the three benchmark datasets outperformed all the baselines
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6560,"approaches, which shows the signicance of the proposed
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6561,"technique for sarcasm analysis. Though this technique has
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6562,"been experimented with on the benchmark dataset, it can also
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6563,"be employed on the randomly generated live tweet dataset
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6564,"to test the model's predictive performance. Besides, other
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6565,"deep learning models have produced promising results as
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6566,"reported in several NLP tasks, especially the augmented
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6567,"and attention-based neural networks. The architecture can be
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6568,"considered in further study. Accordingly, the advancement
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6569,"text-image feature engineering approach, in which text is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6570,"represented as an image, also shows the effectiveness of social
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6571,"media utterances is another promising research direction for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6572,"sarcasm classication.REFERENCES
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6573,"[1] A. Pak and P. Paroubek, ``Twitter as a corpus for sentiment analysis and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6574,"opinion mining,'' in Proc. LREc, vol. 10, 2010, pp. 13201326.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6575,"[2] G. Wang, J. Sun, J. Ma, K. Xu, and J. Gu, ``Sentiment classication:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6576,"The contribution of ensemble learning,'' Decis. Support Syst., vol. 57,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6577,"pp. 7793, Jan. 2014.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6578,"[3] V. Vyas and V. Uma, ``Approaches to sentiment analysis on product
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6579,"reviews,'' in Sentiment Analysis and Knowledge Discovery in Contempo-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6580,"rary Business. Hershey, PA, USA: IGI Global, 2019, pp. 1530.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6581,"[4] E. Fersini, E. Messina, and F. A. Pozzi, ``Sentiment analysis: Bayesian
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6582,"ensemble learning,'' Decis. Support Syst., vol. 68, pp. 2638, Dec. 2014.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6583,"[5] E. Cambria, ``Affective computing and sentiment analysis,'' IEEE Intell.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6584,"Syst., vol. 31, no. 2, pp. 102107, Mar. 2016.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6585,"[6] M. A. Walker, J. E. F. Tree, P. Anand, R. Abbott, and J. King, ``A corpus
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6586,"for research on deliberation and debate,'' in Proc. LREC, Istanbul, Turkey
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6587,"vol. 12, 2012, pp. 812817.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6588,"[7] D. G. Maynard and M. A. Greenwood, ``Who cares about sarcastic tweets?
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6589,"Investigating the impact of sarcasm on sentiment analysis,'' in Proc. LREC,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6590,"2014, pp. 17.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6591,"[8] A. Joshi, P. Bhattacharyya, and J. M. Carman, ``Automatic sarcasm detec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6592,"tion: A survey,'' ACM Comput. Surv., vol. 50, no. 5, p. 73, 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6593,"[9] N. Parde and R. Nielsen, ``Detecting sarcasm is extremely easy,'' in Proc.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6594,"Workshop Comput. Semantics Beyond Events Roles, 2018, pp. 2126.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6595,"[10] Y. Karuna and G. R. Reddy, ``Broadband subspace decomposition of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6596,"convoluted speech data using polynomial EVD algorithms,'' Multimedia
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6597,"Tools Appl., vol. 79, no. 7, pp. 52815299, 2018.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6598,"[11] S. Mukherjee and P. K. Bala, ``Sarcasm detection in microblogs using
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6599,"Naïve Bayes and fuzzy clustering,'' Technol. Soc., vol. 48, pp. 1927,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6600,"Feb. 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6601,"[12] A. Rajadesingan, R. Zafarani, and H. Liu, ``Sarcasm detection on Twitter,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6602,"presented at the 8th ACM Int. Conf. Web Search Data Mining, 2015.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6603,"[13] M. Bouazizi and T. O. Ohtsuki, ``A pattern-based approach for sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6604,"detection on Twitter,'' IEEE Access, vol. 4, pp. 54775488, 2016.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6605,"[14] E. Riloff, A. Qadir, P. Surve, L. D. Silva, N. Gilbert, and R. Huang,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6606,"``Sarcasm as contrast between a positive sentiment and negative situa-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6607,"tion,'' in Proc. Conf. Empirical Methods Natural Lang. Process., 2013,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6608,"pp. 704714.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6609,"[15] A. Ghosh and D. T. Veale, ``Fracking sarcasm using neural network,'' in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6610,"Proc. 7th Workshop Comput. Approaches Subjectivity, Sentiment Social
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6611,"Media Anal., 2016, pp. 161169.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6612,"[16] S. Oraby, V. Harrison, L. Reed, E. Hernandez, E. Riloff, and M. Walker,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6613,"``Creating and characterizing a diverse corpus of sarcasm in dialogue,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6614,"Tech. Rep., 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6615,"[17] B. G. Hb, M. A. Kumar, and K. Soman, ``Distributional semantic repre-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6616,"sentation in health care text classication,'' in Proc. FIRE (Work. Notes),
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6617,"2016, pp. 201204.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6618,"[18] C. I. Eke, A. A. Norman, L. Shuib, and H. F. Nweke, ``Sarcasm identi-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6619,"cation in textual data: Systematic review, research challenges and open
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6620,"directions,'' Artif. Intell. Rev., vol. 53, pp. 42154258, Nov. 2019.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6621,"[19] A. Joshi, S. Agrawal, P. Bhattacharyya, and M. J. Carman, ``Expect the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6622,"unexpected: Harnessing sentence completion for sarcasm detection,'' in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6623,"Proc. Int. Conf. Pacic Assoc. Comput. Linguistics. Singapore: Springer,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6624,"2017, pp. 275287.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6625,"[20] Y. Wu, M. Wang, and P. Jin, ``Disambiguating sentiment ambiguous adjec-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6626,"tives,'' in Proc. Int. Conf. Natural Lang. Process. Knowl. Eng., Oct. 2008,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6627,"pp. 11911199.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6628,"[21] Y. Xia, E. Cambria, A. Hussain, and H. Zhao, ``Word polarity disambigua-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6629,"tion using Bayesian model and opinion-level features,'' Cognit. Comput.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6630,"vol. 7, no. 3, pp. 369380, Jun. 2015.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6631,"[22] Y. Wang, M. Wang, and H. Fujita, ``Word sense disambiguation: A compre-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6632,"hensive knowledge exploitation framework,'' Knowl.-Based Syst., vol. 190,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6633,"Feb. 2020, Art. no. 105030.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6634,"[23] K. Abdalgader and A. Al Shibli, ``Context expansion approach for graph-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6635,"based word sense disambiguation,'' Expert Syst. Appl., vol. 168, Apr. 2021,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6636,"Art. no. 114313.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6637,"[24] E. Fersini, F. A. Pozzi, and E. Messina, ``Detecting irony and sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6638,"in microblogs: The role of expressive signals and ensemble classiers,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6639,"inProc. IEEE Int. Conf. Data Sci. Adv. Analytics (DSAA) , Oct. 2015,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6640,"pp. 18.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6641,"[25] A. Onan and M. A. Tocoglu, ``Satire identication in Turkish news articles
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6642,"based on ensemble of classiers,'' Turkish J. Electr. Eng. Comput. Sci.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6643,"vol. 28, no. 2, pp. 10861106, Mar. 2020.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6644,"48516 VOLUME 9, 2021C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6645,"[26] T. Ptácek, I. Habernal, and J. Hong, ``Sarcasm detection on Czech and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6646,"English Twitter,'' in Proc. 25th Int. Conf. Comput. Linguistics, Tech.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6647,"Papers, 2014, pp. 213223.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6648,"[27] A. Esuli, A. Moreo, F. Sebastiani, and E. Cambria, ``Cross-lingual sen-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6649,"timent quantication,'' IEEE Intell. Syst., vol. 35, no. 3, pp. 106114,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6650,"May 2020.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6651,"[28] Q. Yang, Y. Rao, H. Xie, J. Wang, F. L. Wang, W. H. Chan, and E. Cambria,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6652,"``Segment-level joint topic-sentiment model for online review analysis,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6653,"IEEE Intell. Syst., vol. 34, no. 1, pp. 4350, Jan. 2019.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6654,"[29] A. Agrawal, A. An, and M. Papagelis, ``Leveraging transitions of emotions
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6655,"for sarcasm detection,'' in Proc. 43rd Int. ACM SIGIR Conf. Res. Develop.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6656,"Inf. Retr., Jul. 2020, pp. 15051508.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6657,"[30] A. J. C. Onan, ``Sentiment analysis on product reviews based on weighted
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6658,"word embeddings and deep neural networks,'' Concurrency Comput.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6659,"Pract. Exper. p. e5909, 2020.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6660,"[31] I. Augenstein and A. Søgaard, ``Multi-task learning of keyphrase
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6661,"boundary classication,'' 2017, arXiv:1704.00514. [Online]. Available:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6662,"http://arxiv.org/abs/1704.00514
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6663,"[32] M. Lan, J. Wang, Y. Wu, Z.-Y. Niu, and H. Wang, ``Multi-task attention-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6664,"based neural networks for implicit discourse relationship representation
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6665,"and identication,'' in Proc. Conf. Empirical Methods Natural Lang. Pro-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6666,"cess., 2017, pp. 12991308.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6667,"[33] N. Majumder, S. Poria, H. Peng, N. Chhaya, E. Cambria, A. Gelbukh,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6668,"and E. Cambria, ``Sentiment and sarcasm classication with multitask
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6669,"learning,'' IEEE Intell. Syst., vol. 34, no. 3, pp. 3843, May 2019.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6670,"[34] A. Mishra, K. Dey, and P. Bhattacharyya, ``Learning cognitive features
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6671,"from gaze data for sentiment and sarcasm classication using convo-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6672,"lutional neural network,'' in Proc. 55th Annu. Meeting Assoc. Comput.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6673,"Linguistics, vol. 1, 2017, pp. 377387.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6674,"[35] A. Hussain and E. Cambria, ``Semi-supervised learning for big social data
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6675,"analysis,'' Neurocomputing, vol. 275, pp. 16621673, Jan. 2018.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6676,"[36] J. Duan, B. Luo, and J. Zeng, ``Semi-supervised learning with generative
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6677,"model for sentiment classication of stock messages,'' Expert Syst. Appl.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6678,"vol. 158, Nov. 2020, Art. no. 113540.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6679,"[37] P. Mehndiratta, S. Sachdevai, and D. Soni, ``Detection of sarcasm in text
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6680,"data using deep convolutional neural networks,'' Scalable Comput., Pract.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6681,"Exper., vol. 18, no. 3, pp. 219228, Sep. 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6682,"[38] R. Schifanella, P. de Juan, J. Tetreault, and L. Cao, ``Detecting sarcasm in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6683,"multimodal social platforms,'' in Proc. 24th ACM Int. Conf. Multimedia,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6684,"Oct. 2016, pp. 11361145.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6685,"[39] A. Onan, ``Topic-enriched word embeddings for sarcasm identication,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6686,"inProc. Comput. Sci. Line Conf. Cham, Switzerland: Springer, 2019,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6687,"pp. 293304.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6688,"[40] S. Castro, D. Hazarika, V. Pérez-Rosas, R. Zimmermann, R. Mihalcea,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6689,"and S. Poria, ``Towards multimodal sarcasm detection (an_obviously_
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6690,"perfect paper),'' 2019, arXiv:1906.01815. [Online]. Available: http://arxiv.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6691,"org/abs/1906.01815
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6692,"[41] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ``BERT: Pre-training
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6693,"of deep bidirectional transformers for language understanding,'' 2018,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6694,"arXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6695,"[42] C. Carr and Z. Zukowski, ``Curating generative raw audio music with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6696,"DOME,'' in Proc. Joint ACM IUI Workshops, 2019.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6697,"[43] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ``ImageNet:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6698,"A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6699,"Vis. Pattern Recognit., Jun. 2009, pp. 248255.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6700,"[44] A. Onan and M. A. Tocoglu, ``A term weighted neural language model and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6701,"stacked bidirectional LSTM based framework for sarcasm identication,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6702,"IEEE Access, vol. 9, pp. 77017722, 2021.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6703,"[45] A. Onan, ``Sarcasm identication on Twitter: A machine learning
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6704,"approach,'' in Proc. Comput. Sci. Line Conf. Cham, Switzerland: Springer,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6705,"2017, pp. 374383.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6706,"[46] R. Gonzalez-Ibanez, S. Muresan, and N. Wacholder, ``Identifying sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6707,"in Twitter: A closer look,'' in Proc. 49th Annu. Meeting Assoc. Comput.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6708,"Linguistics, Hum. Lang. Technol., vol. 2. Stroudsburg, PA, USA: Associa-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6709,"tion Computational Linguistics, 2011, pp. 581586.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6710,"[47] Z. H. Kilimci and S. Akyokus, ``The evaluation of word embedding models
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6711,"and deep learning algorithms for Turkish text classication,'' in Proc. 4th
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6712,"Int. Conf. Comput. Sci. Eng. (UBMK), Sep. 2019, pp. 548553.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6713,"[48] T. Young, D. Hazarika, S. Poria, and E. Cambria, ``Recent trends in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6714,"deep learning based natural language processing [review article],'' IEEE
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6715,"Comput. Intell. Mag., vol. 13, no. 3, pp. 5575, Aug. 2018.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6716,"[49] J. Pennington, R. Socher, and C. Manning, ``Glove: Global vectors for
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6717,"word representation,'' in Proc. Conf. Empirical Methods Natural Lang.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6718,"Process. (EMNLP), 2014, pp. 15321543.[50] C. I. Eke, A. Norman, L. Shuib, F. B. Fatokun, and I. Omame,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6719,"``The signicance of global vectors representation in sarcasm analysis,'' in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6720,"Proc. Int. Conf. Math., Comput. Eng. Comput. Sci. (ICMCECS), Mar. 2020,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6721,"pp. 17.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6722,"[51] A. Graves and J. Schmidhuber, ``Framewise phoneme classication with
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6723,"bidirectional LSTM and other neural network architectures,'' Neural Netw.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6724,"vol. 18, nos. 56, pp. 602610, Jul. 2005.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6725,"[52] Y. Agiomyrgiannakis, N. Egberts, F. Henderson, H. Zen, and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6726,"P. Szczepaniak, ``Fast, compact, and high quality LSTM-RNN based
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6727,"statistical parametric speech synthesizers for mobile devices,'' 2016,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6728,"arXiv:1606.06061. [Online]. Available: http://arxiv.org/abs/1606.06061
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6729,"[53] H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and S. Valaee,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6730,"``Recent advances in recurrent neural networks,'' 2017, arXiv:1801.01078.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6731,"[Online]. Available: http://arxiv.org/abs/1801.01078
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6732,"[54] Y. Tay, L. Anh Tuan, S. Cheung Hui, and J. Su, ``Reasoning with sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6733,"by reading in-between,'' 2018, arXiv:1805.02856. [Online]. Available:
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6734,"http://arxiv.org/abs/1805.02856
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6735,"[55] A. Graves, N. Jaitly, and A.-R. Mohamed, ``Hybrid speech recognition
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6736,"with deep bidirectional LSTM,'' in Proc. IEEE Workshop Autom. Speech
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6737,"Recognit. Understand., Dec. 2013, pp. 273278.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6738,"[56] M. Thelwall, K. Buckley, and G. Paltoglou, ``Sentiment strength detection
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6739,"for the social Web,'' J. Amer. Soc. Inf. Sci. Technol., vol. 63, no. 1,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6740,"pp. 163173, Jan. 2012.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6741,"[57] M. W. Berry and M. Castellanos, ``Survey of text mining,'' Comput. Rev.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6742,"vol. 45, no. 9, p. 548, 2004.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6743,"[58] S. Yu, J. Su, and D. Luo, ``Improving BERT-based text classication
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6744,"with auxiliary sentence and domain knowledge,'' IEEE Access, vol. 7,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6745,"pp. 176600176612, 2019.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6746,"[59] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6747,"L. Kaiser, and I. Polosukhin, ``Attention is all you need,'' Tech. Rep., 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6748,"[60] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ``BERT: Pre-training of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6749,"deep bidirectional transformers for language understanding,'' Tech. Rep.,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6750,"2018.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6751,"[61] O. Araque, I. Corcuera-Platas, J. F. Sánchez-Rada, and C. A. Iglesias,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6752,"``Enhancing deep learning sentiment analysis with ensemble techniques
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6753,"in social applications,'' Expert Syst. Appl., vol. 77, pp. 236246, Jul. 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6754,"[62] M. Giatsoglou, M. G. Vozalis, K. Diamantaras, A. Vakali, G. Sarigiannidis,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6755,"and K. C. Chatzisavvas, ``Sentiment analysis leveraging emotions and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6756,"word embeddings,'' Expert Syst. Appl., vol. 69, pp. 214224, Mar. 2017.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6757,"[63] D. P. Kingma and J. Ba, ``Adam: A method for stochastic opti-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6758,"mization,'' 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6759,"org/abs/1412.6980
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6760,"[64] D. P. Kingma and J. Ba, ``Adam: A method for stochastic optimisation,''
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6761,"Tech. Rep., 2014.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6762,"[65] D. Ghosh, W. Guo, and S. Muresan, ``Sarcastic or not: Word embeddings to
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6763,"predict the literal or sarcastic meaning of words,'' in Proc. Conf. Empirical
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6764,"Methods Natural Lang. Process., 2015, pp. 10031012.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6765,"[66] M. Zhang, Y. Zhang, and G. Fu, ``Tweet sarcasm detection using deep
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6766,"neural network,'' in Proc. COLING, 26th Int. Conf. Comput. Linguistics,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6767,"Tech. Papers, 2016, pp. 24492460.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6768,"[67] T. Xiong, P. Zhang, H. Zhu, and Y. Yang, ``Sarcasm detection with self-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6769,"matching networks and low-rank bilinear pooling,'' in Proc. World Wide
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6770,"Web Conf. (WWW), 2019, pp. 21152124.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6771,"[68] M. A. Di Gangi, G. L. Bosco, and G. Pilato, ``Effectiveness of data-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6772,"driven induction of semantic spaces and traditional classiers for sarcasm
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6773,"detection,'' Tech. Rep., 2019.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6774,"[69] S. Poria, E. Cambria, D. Hazarika, and P. Vij, ``A deeper look into sarcastic
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6775,"tweets using deep convolutional neural networks,'' Tech. Rep., 2016.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6776,"CHRISTOPHER IFEANYI EKE received the B.Sc.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6777,"degree in computer science from Ebonyi State
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6778,"University, Nigeria, and the M.Sc. degree in
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6779,"mobile computing from the University of Bed-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6780,"fordshire, Luton, U.K. He is currently pursuing
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6781,"the Ph.D. degree with the Department of Informa-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6782,"tion Systems, Faculty of Computer Science and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6783,"Information Technology, University of Malaya,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6784,"Kuala Lumpur Malaysia. His research interests
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6785,"include data science, NLP, information system and
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6786,"security, cloud computing, machine learning, big data, and social media
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6787,"analytics.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6788,"VOLUME 9, 2021 48517C. I. Eke et al.: Context-Based Feature Technique for Sarcasm Identification in Benchmark Datasets
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6789,"AZAH ANIR NORMAN received the Ph.D.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6790,"degree in information systems security. She is
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6791,"currently a Senior Lecturer with the Department
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6792,"of Information Systems, Faculty of Computer
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6793,"Science and Information Technology, Univer-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6794,"sity of Malaya, Kuala Lumpur. She had previ-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6795,"ously worked as a Security Consultant with the
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6796,"MSC Trustgate.com (subsidiary body of MDEC
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6797,"Malaysia), a certication authority in Malaysia,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6798,"for more than four years. Her research interests
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6799,"include e-commerce security, information systems security management,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6800,"security policies, and standards. She was awarded a few research grants
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6801,"that focused on social media security and cybersecurity practices. Her arti-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6802,"cles in selected research areas have been printed in local and international
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6803,"conferences and ISI/Scopus WOS journals. She actively supervises many
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6804,"students at all levels of study, from undergraduate (i.e., bachelor's degree)
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6805,"up to postgraduate (i.e., master's and Ph.D.) supervisions.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6806,"LIYANA SHUIB received the B.Comp.Sc. degree
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6807,"(Hons.) from Universiti, Teknologi Malaysia, Sku-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6808,"dai, Malaysia, the master's degree in information
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6809,"technology from Universiti Kebangsaan Malaysia,
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6810,"and the Ph.D. degree from the University of
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6811,"Malaya, Kuala Lumpur. She graduated Ph.D. stu-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6812,"dent and presently supervising several postgrad-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6813,"uate students. She is currently a Senior Lecturer
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6814,"with the Department of Information System, Fac-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6815,"ulty of Computer Science and Information Tech-
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6816,"nology, University of Malaya. She has more than 20 articles relevant to her
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6817,"research interest.
",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6818,"48518 VOLUME 9, 2021",False,Context-Based_Feature_Technique_for_Sarcasm_Identification_in_Benchmark_Datasets_Using_Deep_Learning_and_BERT_Model,False,False,False
6819,Abstract ,True,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6820,"I. I NTRODUCTION
",True,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6821,"II. R
",True,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6822,"III. M
",True,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6823,"V. C
",True,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6824,"Cross-Lingual Transfer Learning for
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6825,"Complex Word Identiﬁcation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6826,"George-Eduard Zaharia
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6827,"Computer Science Department
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6828,"University Politehnica of Bucharest
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6829,"Bucharest, Romania
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6830,"george.zaharia0806@stud.acs.upb.roDumitru-Clementin Cercel
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6831,"Computer Science Department
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6832,"University Politehnica of Bucharest
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6833,"Bucharest, Romania
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6834,"dumitru.cercel@upb.roMihai Dascalu
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6835,"Computer Science Department
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6836,"University Politehnica of Bucharest
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6837,"Bucharest, Romania
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6838,"mihai.dascalu@upb.ro
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6839,"tered on detecting hard-to-understand words, or groups of words,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6840,"in texts from different areas of expertise. The purpose of CWIis to highlight problematic structures that non-native speakerswould usually ﬁnd difﬁcult to understand. Our approach useszero-shot, one-shot, and few-shot learning techniques, alongsidestate-of-the-art solutions for Natural Language Processing (NLP)tasks (i.e., Transformers). Our aim is to provide evidence thatthe proposed models can learn the characteristics of complexwords in a multilingual environment by relying on the CWIshared task 2018 dataset available for four different languages(i.e., English, German, Spanish, and also French). Our approachsurpasses state-of-the-art cross-lingual results in terms of macroF1-score on English (0.774), German (0.782), and Spanish (0.734)languages, for the zero-shot learning scenario. At the same time,our model also outperforms the state-of-the-art monolingualresult for German (0.795 macro F1-score).
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6841,"Index Terms—Complex Word Identiﬁcation, Transformer,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6842,"Cross-Lingual Transfer Learning
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6843,"I. I NTRODUCTION
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6844,"Texts represent the main source of knowledge for our
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6845,"society. However, they can be written in various manners,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6846,"thus creating a barrier between the readers and the ideas theyintend to convey. Therefore, document comprehension is themain challenge users have to overcome, by understanding themeaning behind troublesome words and becoming familiarwith them. Complex Word Identiﬁcation (CWI) is a taskthat intends to identify hard-to-understand tokens, highlightingthem for further clariﬁcation and assisting users to graspingthe contents of the document.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6847,"Motivation. Each culture includes exclusive ideas, available
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6848,"only for the ones who can pass the obstacle of language [1].However, properly understanding language can prove to be adifﬁcult task. By identifying complex words, users can makeconsistent steps towards adapting to the culture and accessingthe knowledge it has to offer. As an example, entries like”mayoritariamente” (eng. ”mostly”) or ”gobernatura” (eng.”governance”) in the Spanish environment can create under-standing problems for non-native Spanish speakers [2], thusrequiring users to familiarize themselves with these particularterms.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6849,"Challenges. The identiﬁcation task becomes increasingly
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6850,"more difﬁcult, as proper complex word identiﬁcation is notguaranteed. For example, if we use human identiﬁcationtechniques, language learners may consider a new word tobe complex, while others might not share the same opinionby relying on their prior knowledge in that language. There-fore, universal annotation techniques are required, such that aground truth can be established and the same set of words isconsidered complex in any context.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6851,"Proposed Approach. We consider state-of-the-art solutions,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6852,"namely multilingual Transformer-based approaches, to addressthe CWI challenge. First, we apply a zero-shot learningapproach. This was performed by training Recurrent NeuralNetworks (RNNs) [3] and Transformer-based [4] models ona source language corpus, followed by validating and testingon a corpus from a target language, different from the sourcelanguage. A second experiment consists of a one-shot learningapproach that considers training on each of the three languages(i.e., English, German, Spanish), but only keeping one entryfrom the target language, and validating and testing on English,German, Spanish, and French, respectively.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6853,"In addition, we performed few-shot learning experiments by
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6854,"validating and testing on a language, and training on the others,but with the addition of a small number of training entries fromthe target language. The model learns sample structures fromthe language and, in general, performs better when applied onmultiple entries. Furthermore, this training process can helpthe model adapt to situations in which the number of traininginputs is scarce. The dataset provided by the CWI Shared Task2018 [2] was used to perform all experiments.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6855,"This paper is structured as follows. The second section de-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6856,"scribes related work and its impact on the CWI task. The thirdsection describes the corpus and outlines our method basedon multilingual embeddings and Transformer-based models,together with the corresponding experimental setup. The fourthsection details the results, alongside a discussion and an erroranalysis. The ﬁfth section concludes the paper and outlinesthe main ideas, together with potential extensions.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6857,"II. R
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6858,"ELATED WORK
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6859,"Complex word identiﬁcation was explored in various other
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6860,"studies and underlying approaches can be split into two maincategories: monolingual and cross-lingual.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6861,"Monolingual CWI. The ﬁrst category implies the usage
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6862,"of the same language for training, testing, and validation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6863,"3842020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6864,"2375-0197/20/$31.00 ©2020 IEEE
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6865,"DOI 10.1109/ICTAI50040.2020.00067
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6866,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. processes using a supervised approach. Sheang [5] proposed a
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6867,"solution based on Convolutional Neural Networks [6] trainedon both word embeddings and handcrafted features. The authorused pretrained GloVe word embeddings [7] for represent-ing words from each of the three languages in the dataset.Furthermore, the author engineered a series of morphologicalfeatures to obtain additional insights into the structure ofthe entries, features like the number of vowels, word length,and Tf-Idf. At the same time, the author considered a se-ries of linguistic features, alongside morphological ones, byidentifying syntactic dependencies between words. However,the presence of these features together with language-speciﬁcword embeddings implies a complex training and evaluationprocess, performed on each language separately and withdifferent conﬁguration setups.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6868,"Cross-lingual CWI. Cross-lingual transfer has been suc-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6869,"cessfully used in various NLP tasks, for example: machinetranslation [8], named entity recognition [9], verb sense dis-ambiguation [10], dependency parsing [11], coreference reso-lution [12], event detection [13], sentence summarization [14],document retrieval [15], irony detection [16], dialogue systems[17], domain-speciﬁc tweet classiﬁcation [18], as well abusivelanguage identiﬁcation [19].
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6870,"In addition, cross-lingual approaches were employed in
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6871,"few works on the CWI task. For example, Finnimore etal. [20] extracted cross-lingual features for each consideredlanguage (i.e. English, German, Spanish, and French). Theyconcluded that the best features for cross-lingual approachesare represented by the number of syllables, number of tokens,and number of punctuation marks. However, performing thisprocess can prove to be costly, as it requires re-running themodel for each additional language in which the user intendsto perform complex word identiﬁcation.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6872,"Another approach for cross-lingual CWI employs traditional
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6873,"classiﬁcation algorithms, such as K-Nearest Neighbors (kNN),Random Forests (RF), or Support Vector Machines (SVMs)[21]. Alongside these algorithms, the authors introduced differ-ent sets of language-independent features, ranging from lengthand frequency, to syntactic features.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6874,"Bingel and Bjerva [22] presented both a multi-task learning
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6875,"architecture and an ensemble voting approach, by using feed-forward neural networks and random-forest classiﬁers. Good-ing and Kochmar [23] proposed a sequence labeling approachfor CWI. They used 300-dimensional word embeddings forencoding the input words, and fed this input to a Bidirec-tional Long Short-Term Memory (BiLSTM) [24] network thatconsidered both word and character-level representations. Theauthors imposed a probability threshold of 0.5 for classifyinga word as complex and applied the same rules for phrase-levelclassiﬁcation. The authors used an English dataset based onnews articles written with different levels of professionalism.Their approach underlines the effectiveness of sequence label-ing models which considerably surpassed prior methods by amargin of up to 3.6% in terms of macro F1-score.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6876,"Zampieri et al. [25] developed ensemble classiﬁers to iden-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6877,"tify complex words. They used two approaches for classiﬁca-tion, namely Plurality V oting [26] and Oracle [27]. Based onmultiple subsystems, the authors concluded that the latter ap-proach performed well when integrating the top three methodsparticipating in the SemEval CWI 2016 competition [28].
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6878,"A different approach to CWI was taken by Thomas et al.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6879,"[29] who considered simplifying the entire document lexicon,thus making the text more accessible for non-native speakers.The authors introduced different algorithms for reducing thelexicon size, by combining disambiguation and lexical reduc-tion steps.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6880,"In contrast to the previous approaches, we developed a sys-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6881,"tem based on state-of-the-art NLP solutions (i.e., Transform-ers), that can efﬁciently adapt to a large number of languages,without prior setup or feature engineering. The Transformermulti-lingual models are pretrained on a large number oflanguages, with various word representations already mappedinto the same space. Unlike previous work, our models areuniversal, can be easily extended to other languages, and canbe used for transfer learning.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6882,"III. M
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6883,"ETHOD
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6884,"We consider two main multi-lingual approaches for CWI:
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6885,"a) RNN-based solutions, alongside multilingual word embed-dings, and b) multilingual Transformers specialized in tokenclassiﬁcation. Our aim is to infer cross-lingual features ofcomplex words by training or ﬁne-tuning on a labelled corpuscontaining different languages, followed by the identiﬁcationof complex words on a newly encountered language. Pre-processing is minimal and considered only the removal ofunknown characters, as well as extra spaces from the dataset.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6886,"A. Corpus
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6887,"Our analysis uses the dataset provided by the CWI Shared
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6888,"Task 2018 [2], which contains entries in four languages,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6889,"namely: English, German, Spanish, and French. The En-glish section of the dataset contains articles written atthree proﬁciency levels: professional (news), non-professional(WikiNews), and Wikipedia articles. The German and theSpanish sections contain only one category of entries, takenfrom Wikipedia pages. Quantitatively, the English sectioncontains 27,299 entries for training and 3,328 for validation.In contrast, the German section offers only 6,151 trainingelements and 795 for validation. At the same time, the Spanishsection provides 13,750 training entries and 1,622 validationentries. We note that there are no training and validation entriesfor the French language.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6890,"As expected, the number of complex words is lower when
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6891,"compared to the number of non-complex words. Table I showsthe distribution of complex words in the dataset. While theSpanish and English sections contain a relatively large amountof complex or non-complex words, the vocabulary correspond-ing to the German section is considerably smaller, with only17,462 words. The small number of German entries is causedby the general focus on English and Spanish, languages witha greater number of speakers when compared to German
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6892,"1.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6893,"1https://www.visualcapitalist.com/100-most-spoken-languages/
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6894,"385
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6895,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. Additionally, the test dataset also contains French entries, with
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6896,"a total of 4,507 words.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6897,"TABLE I
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6898,"DISTRIBUTION OF COMPLEX WORDS FOR EACH SECTION OF THE CWI
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6899,"SHARED TASK 2018 DATASET .
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6900,"Language Complex Words Non-complex Words
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6901,"English 14,100 59,944
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6902,"German 3,478 13,984
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6903,"Spanish 9,852 28,777
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6904,"French 867 3,640
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6905,"B. Multilingual Word Embeddings
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6906,"Our ﬁrst experiment consists of using a common embedding
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6907,"for all four languages. We selected pretrained FastText [30]embedding for English, German, Spanish and French. How-ever, these embedding spaces are not aligned one with another.Thus, we mapped them into a merged space by using FacebookMUSE [31], a tool that receives as inputs two embedding ﬁlesand a target vector space, and maps them into the same space.The mapping process consists of learning a rotation matrixW, that intends to align the two distributions by using anadversarial learning technique. The matrix Wis then reﬁned by
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6908,"using Procrustes transformations because the initial alignmentis rough. The transformation consists of setting frequent wordsaligned in the previous step to anchor points, followed byminimizing an energy function between the anchor points.Finally, an expansion is performed using the matrix Wand
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6909,"a distance metric for the space containing a high densityof words, such that the distance between unrelated words isincreased.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6910,"The tool requires a parallel corpus between the languages.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6911,"The corpus can be created by selecting the desired ground-truth bilingual dictionaries available on the Facebook MUSErepository
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6912,"2. The mapping was performed in two steps, as
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6913,"follows. First, we mapped the English and German vectors byusing an English-German parallel corpus. Second, we addedthe Spanish embeddings, by further using an English-Spanishparallel corpus. The obtained embeddings are then fed intoa BiLSTM [24] network, alongside a TimeDistributed layer
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6914,"3.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6915,"The experiments were performed in different scenarios: a) azero-shot approach that required training on combinations ofall the available languages, excepting the target language; b)a one-shot approach that introduces the target language (oneentry) into the training corpus; and c) a few-shot approach,introducing 100 target language entries in the training dataset.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6916,"C. Multilingual BERT
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6917,"Multilingual BERT (mBERT) [32] is a pretrained Trans-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6918,"former architecture trained on over 100 languages, which we
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6919,"selected for multi-lingual token classiﬁcation. The efﬁciency ofrepresentations generated by the model needs to be maximized
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6920,"2https://github.com/facebookresearch/MUSE
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6921,"3https://keras.io/api/layers/recurrent layers/time distributed/because we performed our experiments in a multilingual en-vironment. Fortunately, mBERT offers the possibility of split-ting its representations into two categories, language-neutralcomponents and language-speciﬁc components, thus sharingcertain features between the languages of interest. mBERT wasﬁne-tuned for the CWI task by using the previously mentionedzero-shot and one-shot learning approaches.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6922,"D. XLM-RoBERTa
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6923,"XLM-RoBERTa [33] is also a multilingual model built
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6924,"with the Masked Language Model objective, that should have
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6925,"an advantage over mBERT because it was pretrained oneven more multilingual data (approximately 2.5 TB of rawtext data). The model obtains state-of-the-art results for theGLUE benchmark tasks [34], while performing extremelywell on Named Entity Recognition and Cross-lingual NaturalLanguage Inference tasks [33].
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6926,"E. Other BERT-based Monolingual Models
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6927,"Alongside mBERT, we decided to experiment with models
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6928,"extensively pretrained on each one of our target languages,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6929,"alternatives that have shown better performance than the multi-lingual models in other NLP tasks. Thus, we used new modelsfor the German, Spanish and French languages, namely: Ger-man BERT
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6930,"4, Spanish BERT (BETO) [35], and French BERT
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6931,"(CamemBERT) [36]. Our goal was to increase performanceby speciﬁcally focusing on a certain language, instead of over100 languages (as the case of mBERT).
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6932,"F . Implementation Details
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6933,"Six experiments were conducted: a) embeddings aligned
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6934,"with MUSE fed to a BiLSTM network, b) mBERT token clas-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6935,"siﬁcation, c) XLM-RoBERTa token classiﬁcation, d) GermanBERT token classiﬁcation, e) BETO token classiﬁcation, andf) CamemBERT token classiﬁcation. Each experiment is alsodivided into sub-experiments that considered the usage of eachlanguage individually, as well as all possible combinations oflanguages in the training set. The four languages (i.e. English,German, Spanish, and French) were considered, by turn,for validation and testing. The BiLSTM-based solution wastrained for 5 epochs, while the others (i.e, the Transformer-based solutions) were trained for 4 epochs. We concludedthat this setup offers the best results considering that all oursolutions start overﬁtting after 5 and 4 epochs, respectively.Table II presents the hyperparameters used for training themodels during the experiments.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6936,"TABLE II
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6937,"EXPERIMENTAL HYPERPARAMETERS .
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6938,"Hyperparameter MUSE + BiLSTM Transformer
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6939,"Optimizer RMSprop [37] AdamW [38]
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6940,"Learning rate 5e-5 2e-5
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6941,"Weight decay - 0.01
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6942,"Adam epsilon - 1e-8
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6943,"4https://deepset.ai/german-bert
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6944,"386
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6945,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. TABLE III
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6946,"THE MACRO F1- SCORES OF DIFFERENT MODELS ON BOTH V ALIDATION AND TEST DATASETS .
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6947,"ModelTrain Dev Test
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6948,"EN DE ES EN-W EN-WN EN-N DE ES EN-W EN-WN EN-N DE ES FR
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6949,"MUSE + BiLSTM/check .606 .582 .577 .622 .609 .592 .587 .579 .625 .640 .524
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6950,"/check .487 .602 .491 .479 .474 .498 .500 .498 .483 .513 .494
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6951,"/check .610 .611 .599 .638 .635 .603 .590 .592 .602 .638 .546
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6952,"/check/check .598 .582 .571 .628 .618 .585 .588 .577 .774 .641 .516
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6953,"/check /check .603 .577 .569 .627 .619 .598 .580 .576 .626 .763 .513
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6954,"/check/check .590 .586 .609 .637 .623 .589 .595 .579 .688 .704 .519
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6955,"/check/check/check .604 .578 .570 .626 .620 .587 .581 .577 .774 .751 .512
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6956,"mBERT/check .760 .790 .734 .727 .756 .768 .746 .721 .731 .734 .653
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6957,"/check .728 .746 .670 .806 .744 .736 .696 .630 .778 .697 .691
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6958,"/check .747 .763 .703 .768 .733 .744 .702 .710 .755 .735 .671
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6959,"/check/check .750 .787 .733 .784 .758 .766 .753 .729 .766 .730 .658
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6960,"/check /check .756 .788 .751 .737 .730 .764 .754 .721 .739 .746 .649
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6961,"/check/check .736 .759 .683 .783 .734 .741 .709 .677 .746 .737 .671
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6962,"/check/check/check .755 .789 .739 .782 .740 .766 .752 .730 .752 .735 .684
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6963,"XLM-RoBERTa/check .793 .846 .780 .757 .711 .808 .811 .808 .770 .728 .647
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6964,"/check .717 .697 .695 .790 .710 .716 .701 .670 .795 .702 .702
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6965,"/check .749 .753 .717 .777 .730 .760 .720 .730 .770 .756 .701
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6966,"/check/check .795 .833 .808 .801 .720 .806 .811 .808 .801 .725 .674
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6967,"/check /check .795 .823 .791 .789 .739 .785 .801 .808 .782 .746 .688
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6968,"/check/check .750 .751 .711 .809 .744 .774 .708 .731 .802 .737 .666
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6969,"/check/check/check .800 .817 .780 .794 .748 .798 .811 .807 .534 .741 .688
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6970,"German BERT/check - - - .712 - - - - .736 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6971,"/check - - - .775 - - - - .762 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6972,"/check - - - .627 - - - - .650 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6973,"/check/check - - - .771 - - - - .770 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6974,"/check /check - - - .701 - - - - .717 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6975,"/check/check - - - .777 - - - - .764 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6976,"/check/check/check - - - .771 - - - - .775 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6977,"BETO/check - - - - .603 - - - - .656 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6978,"/check - - - - .525 - - - - .580 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6979,"/check - - - - .733 - - - - .731 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6980,"/check/check - - - - .652 - - - - .649 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6981,"/check /check - - - - .728 - - - - .738 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6982,"/check/check - - - - .730 - - - - .731 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6983,"/check/check/check - - - - .720 - - - - .733 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6984,"CamemBERT/check - - - - - - - - - - .563
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6985,"/check - - - - - - - - - - .442
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6986,"/check - - - - - - - - - - .604
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6987,"/check/check - - - - - - - - - - .592
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6988,"/check /check - - - - - - - - - - .670
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6989,"/check/check - - - - - - - - - - .669
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6990,"/check/check/check - - - - - - - - - - .683
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6991,"*We considered: EN-W = English-Wikipedia; EN-WN = English-WikiNews; EN-N = English-News; DE = German; ES = Spanish; FR = French.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6992,"IV . R ESULTS
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6993,"Table III contains the macro F1-scores obtained on the
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6994,"CWI validation and test datasets for each experiment and for
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6995,"each combination of training languages. Table III containsmonolingual and zero-shot learning experiments. The bestresults for the zero-shot approach are marked in bold, whilethe best results for the monolingual approach are underlined.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6996,"A. Zero-Shot Transfer Evaluation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6997,"The best results on both validation and test datasets for
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6998,"the zero-shot learning strategy are obtained using the XLM-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
6999,"RoBERTa model, with a single exception represented by thevalidation dataset on German. With a considerable marginwhen compared to its counterparts, XLM-RoBERTa ﬁne-tuned on English and Spanish manages to obtain a macroF1-score of 0.782 on the German test dataset, compared to0.626 (MUSE+BiLSTM), 0.739 (mBERT), and 0.717 (GermanBERT). The results are similar for the Spanish and Englishtest datasets (Wikipedia, WikiNews, News) having macro F1-values of 0.702 and 0.774, 0.720, and 0.731, respectively. Theincreased performance of XLM-RoBERTa can be attributedto the larger corpus it was pretrained on, a clear advantageover other BERT-based solutions. However, if we look at theother BERT-based monolingual models (i.e. German BERT,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7000,"387
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7001,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. TABLE IV
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7002,"RESULTS ON THE TEST DATASET USING ONE -SHOT AND FEW -SHOT LEARNING .
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7003,"ModelTrain Macro F1-score (one-shot) Macro F1-score (few-shot)
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7004,"EN DE ES EN-W EN-WN EN-N DE ES EN-W EN-WN EN-N DE ES
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7005,"mBERT/check - - - .732 .723 - - - .727 .738
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7006,"/check .730 .684 .654 - .712 .730 .688 .671 - .709
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7007,"/check .741 .711 .700 .743 - .742 .691 .690 .740 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7008,"/check/check - - - - .730 - - - - .719
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7009,"/check /check - - - .741 - - - - .768 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7010,"/check/check .751 .697 .678 - - .741 .697 .663 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7011,"XLM-RoBERTa/check - - - .769 .732 - - - .760 .730
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7012,"/check .734 .688 .643 - .693 .735 .691 .695 - .703
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7013,"/check .761 .731 .714 .779 - .761 .733 .726 .766 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7014,"/check/check - - - - .724 - - - - .722
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7015,"/check /check - - - .783 - - - - .765 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7016,"/check/check .756 .723 .679 - - .755 .703 .716 - -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7017,"German BERT/check - - - .699 - - - - .736 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7018,"/check - - - .649 - - - - .676 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7019,"/check /check - - - .734 - - - - .689 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7020,"BETO/check - - - - .650 - - - - .686
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7021,"/check - - - - .603 - - - - .545
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7022,"/check/check - - - - .693 - - - - .680
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7023,"/checkimplies the usage of the entire dataset corresponding to that language. Additionally, we randomly selected 1 (for one-shot learning) or 100
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7024,"(for few-shot learning) training entries from the language corresponding to the result for that line.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7025,"BETO, and CamemBERT), we can see that their performance
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7026,"is surpassed by both mBERT and XLM-RoBERTa. Thesemodels are pretrained on a main language, and ﬁne-tuningthem on different languages can lead to poorer results, asseen in Table III. For example, the difference in performance(macro F1) between XLM-RoBERTa and BETO is of 6.8% onthe Spanish validation dataset, a signiﬁcant discrepancy for aCWI task.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7027,"B. One-Shot Transfer Evaluation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7028,"Furthermore, the best values for the one-shot learning ap-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7029,"proach are marked with bold in Table IV, where we considered
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7030,"only one training entry corresponding to the language of theresult. We can observe that, again, the XLM-RoBERTa modeloffers the best performance. For example, XLM-RoBERTaobtains a macro F1-score of 0.731 on the WikiNews dataset,compared to 0.711 for mBERT. Moreover, the large differenceis maintained for the German language as well, with a resultof 0.783 versus 0.743. However, the scores for the Spanishlanguage are closer, with a value around 0.730 for bothmodels.C. Few-Shot Transfer Evaluation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7031,"Next, we included a small number of train entries (i.e., 100)
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7032,"from the same language as the test dataset because we intendedto further improve the scores obtained by the Transformer-based solution using the zero-shot learning scenario. Usingthis approach, the model can infer characteristics of the targetlanguage and may perform better when identifying complexwords on a wide range of different test entries.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7033,"Table IV contains the results obtained in the few-shot learn-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7034,"ing experiments. Unexpectedly, the models perform slightlyworse. This phenomenon can be attributed to the models’incapacity to grasp the main language characteristics, as wellas the representations of a complex word, given a smallnumber of training entries.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7035,"To conclude, our solution manages to outperform state-of-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7036,"the-art alternatives on ﬁve out of six cross-lingual entries,the only exception being the French language (see Table V).Furthermore, our solution manages to surpass state-of-the-artresults for German in the monolingual setup, even though itwas created for cross-lingual experiments.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7037,"TABLE V
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7038,"CROSS -LINGUAL AND MONOLINGUAL STATE -OF-THE-ART RESULT COMPARISON WITH OUR PERFORMANCE ON THE TEST DATASET .
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7039,"EN-W EN-WN EN-N DE ES FR
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7040,"Cross-lingual SotA [20] .652 .638 .659 .734 .726 .758
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7041,"Our best solution, zero-shot learning .774 .720 .731 .782 .734 .702
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7042,"Our best solution, few-shot learning .761 .733 .726 .766 .730 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7043,"Monolingual SotA [5] .811 .840 .874 .759 .797 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7044,"Our best monolingual solution .808 .811 .808 .795 .756 -
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7045,"388
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7046,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. D. Error Analysis
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7047,"Most misclassiﬁcations occurred in the English News test
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7048,"dataset, where our models yielded a maximum F1-macro score
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7049,"of 0.733 by using a few-shot learning approach with XLM-RoBERTa. The high number of wrongly categorized tokenscan be attributed to the complexity of the dataset, writtenin a more formal manner, adequate for news articles. Thiscomplexity implies the presence of more sophisticated words(e.g., ”underwriter”) that are not present in the training dataset,thus causing the model to wrongly classify them. In addition,the dataset contains news with series of location names (e.g.”Londonderry”) or composed notions (e.g. ”better-optimized”,”android-running”, ”java-related”) that, once again, are notincluded in the training set.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7050,"At the same time, another aspect that inﬂuences the classi-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7051,"ﬁcation performance is represented by the annotators’ subjec-tivity. In certain circumstances, words may not be consideredcomplex (e.g. ”with”, ”connection”, ”been”) in the trainingset, while they are marked as complex in the test dataset.Similar situations also occur in the English Wikipedia, EnglishWikiNews, German and Spanish datasets, with a series oftokens that either are not present in the training dataset, orhave different labels between them.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7052,"V. C
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7053,"ONCLUSIONS AND FUTURE WORK
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7054,"Complex Word Indentiﬁcation is a challenging task, even
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7055,"when using state-of-the-art Transformer-based solutions. Inthis work, we introduce an approach that improves the previousresults on the cross-lingual and monolingual CWI shared task2018 by using multilingual and language-speciﬁc Transformermodels, multilingual word embeddings (non-Transformer),and different ﬁne-tuning techniques. Fine-tuning a model ondata from two different languages creates the opportunity ofgrasping features that empower it to better recognize complexwords in certain contexts, even in a different language. Inaddition, zero-shot, one-shot, and few-shot learning strategiesprovide good results, surpassing strong baselines [20] andproposing an alternative to help non-native speakers to prop-erly understand the difﬁcult aspects of a certain language.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7056,"For future work, we intend to improve our results on the
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7057,"monolingual tasks by integrating additional models, such asXLNet [39] and techniques like adversarial training [40] andmulti-task learning [41]. Furthermore, we intend to experi-ment with other pretraining techniques speciﬁc to Transformermodels, such that the results for French can beneﬁt from cross-lingual transfer learning.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7058,"A
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7059,"CKNOWLEDGMENTS
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7060,"This work was supported by the Operational Programme
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7061,"Human Capital of the Ministry of European Funds through theFinancial Agreement 51675/09.07.2019, SMIS code 125125.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7062,"R
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7063,"EFERENCES
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7064,"[1] J. Liu and F. G. Fang, “Perceptions, awareness and perceived effects of
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7065,"home culture on intercultural communication: Perspectives of university
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7066,"students in china,” System, vol. 67, pp. 25–37, 2017.[2] S. M. Yimam, C. Biemann, S. Malmasi, G. Paetzold, L. Specia,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7067,"S.ˇStajner, A. Tack, and M. Zampieri, “A report on the complex word
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7068,"identiﬁcation shared task 2018,” in Proceedings of the Thirteenth Work-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7069,"shop on Innovative Use of NLP for Building Educational Applications,pp. 66–78, 2018.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7070,"[3] A. Sherstinsky, “Fundamentals of recurrent neural network (rnn) and
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7071,"long short-term memory (lstm) network,” Physica D: Nonlinear Phe-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7072,"nomena, vol. 404, p. 132306, Mar 2020.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7073,"[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7074,"Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7075,"in neural information processing systems, pp. 5998–6008, 2017.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7076,"[5] K. C. Sheang, “Multilingual complex word identiﬁcation: Convolutional
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7077,"neural networks with morphological and linguistic features,” in Proceed-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7078,"ings of the Student Research Workshop Associated with RANLP 2019,pp. 83–89, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7079,"[6] K. O’Shea and R. Nash, “An introduction to convolutional neural
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7080,"networks,” ArXiv e-prints, 11 2015.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7081,"[7] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7082,"for word representation,” in Proceedings of the 2014 Conference on
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7083,"Empirical Methods in Natural Language Processing (EMNLP), (Doha,Qatar), pp. 1532–1543, Association for Computational Linguistics, Oct.2014.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7084,"[8] Y . Kim, Y . Gao, and H. Ney, “Effective cross-lingual transfer of neural
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7085,"machine translation models without shared vocabularies,” in Proceedings
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7086,"of the 57th Annual Meeting of the Association for ComputationalLinguistics, pp. 1246–1257, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7087,"[9] Z. Liu, G. I. Winata, P. Xu, and P. Fung, “Coach: A coarse-to-ﬁne ap-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7088,"proach for cross-domain slot ﬁlling,” arXiv preprint arXiv:2004.11727,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7089,"2020.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7090,"[10] S. Gella, D. Elliott, and F. Keller, “Cross-lingual visual verb sense
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7091,"disambiguation,” in Proceedings of the 2019 Conference of the North
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7092,"American Chapter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long and Short Papers),pp. 1998–2004, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7093,"[11] W. Ahmad, Z. Zhang, X. Ma, E. Hovy, K.-W. Chang, and N. Peng,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7094,"“On difﬁculties of cross-lingual transfer with order differences: A casestudy on dependency parsing,” in Proceedings of the 2019 Conference
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7095,"of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 1 (Long and ShortPapers), pp. 2440–2452, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7096,"[12] G. Urbizu, A. Soraluze, and O. Arregi, “Deep cross-lingual coreference
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7097,"resolution for less-resourced languages: The case of basque,” in Proceed-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7098,"ings of the Second Workshop on Computational Models of Reference,Anaphora and Coreference, pp. 35–41, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7099,"[13] V . D. Lai, F. Dernoncourt, and T. H. Nguyen, “Extensively matching
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7100,"for few-shot learning event detection,” arXiv preprint arXiv:2006.10093,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7101,"2020.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7102,"[14] X. Duan, M. Yin, M. Zhang, B. Chen, and W. Luo, “Zero-shot cross-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7103,"lingual abstractive sentence summarization through teaching generationand attention,” in Proceedings of the 57th Annual Meeting of the
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7104,"Association for Computational Linguistics, pp. 3162–3172, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7105,"[15] R. Zhang, C. Westerﬁeld, S. Shim, G. Bingham, A. R. Fabbri, W. Hu,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7106,"N. Verma, and D. Radev, “Improving low-resource cross-lingual doc-ument retrieval by reranking with deep bilingual representations,” inProceedings of the 57th Annual Meeting of the Association for Compu-tational Linguistics, pp. 3173–3179, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7107,"[16] B. Ghanem, J. Karoui, F. Benamara, P. Rosso, and V . Moriceau,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7108,"“Irony detection in a multilingual context,” in European Conference on
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7109,"Information Retrieval, pp. 141–149, Springer, 2020.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7110,"[17] S. Schuster, S. Gupta, R. Shah, and M. Lewis, “Cross-lingual transfer
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7111,"learning for multilingual task oriented dialog,” in Proceedings of the
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7112,"2019 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), pp. 3795–3805, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7113,"[18] J. R. Chowdhury, C. Caragea, and D. Caragea, “Cross-lingual disaster-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7114,"related multi-label tweet classiﬁcation with manifold mixup,” in Pro-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7115,"ceedings of the 58th Annual Meeting of the Association for Computa-tional Linguistics: Student Research Workshop, pp. 292–298, 2020.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7116,"[19] E. W. Pamungkas and V . Patti, “Cross-domain and cross-lingual abu-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7117,"sive language detection: A hybrid approach with deep learning and amultilingual lexicon,” in Proceedings of the 57th Annual Meeting of the
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7118,"Association for Computational Linguistics: Student Research Workshop ,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7119,"pp. 363–370, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7120,"389
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7121,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. [20] P. Finnimore, E. Fritzsch, D. King, A. Sneyd, A. U. Rehman, F. Alva-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7122,"Manchego, and A. Vlachos, “Strong baselines for complex word
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7123,"identiﬁcation across multiple languages,” in Proceedings of the 2019
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7124,"Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), pp. 970–977, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7125,"[21] S. M. Yimam, S. ˇStajner, M. Riedl, and C. Biemann, “Multilingual and
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7126,"cross-lingual complex word identiﬁcation,” in Proceedings of the Inter-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7127,"national Conference Recent Advances in Natural Language Processing,RANLP 2017, (Varna, Bulgaria), pp. 813–822, INCOMA Ltd., Sept.2017.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7128,"[22] J. Bingel and J. Bjerva, “Cross-lingual complex word identiﬁcation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7129,"with multitask learning,” in Proceedings of the Thirteenth Workshop
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7130,"on Innovative Use of NLP for Building Educational Applications,(New Orleans, Louisiana), pp. 166–174, Association for ComputationalLinguistics, June 2018.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7131,"[23] S. Gooding and E. Kochmar, “Complex word identiﬁcation as a sequence
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7132,"labelling task,” in Proceedings of the 57th Annual Meeting of the
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7133,"Association for Computational Linguistics, pp. 1148–1153, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7134,"[24] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7135,"computation, vol. 9, no. 8, pp. 1735–1780, 1997.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7136,"[25] M. Zampieri, S. Malmasi, G. Paetzold, and L. Specia, “Complex word
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7137,"identiﬁcation: Challenges in data annotation and system performance,”inProceedings of the 4th Workshop on Natural Language Processing
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7138,"Techniques for Educational Applications (NLPTEA 2017) , pp. 59–63,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7139,"2017.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7140,"[26] R. Polikar, “Ensemble based systems in decision making,” IEEE Circuits
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7141,"and systems magazine, vol. 6, no. 3, pp. 21–45, 2006.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7142,"[27] L. I. Kuncheva, J. C. Bezdek, and R. P. Duin, “Decision templates
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7143,"for multiple classiﬁer fusion: an experimental comparison,” Pattern
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7144,"recognition, vol. 34, no. 2, pp. 299–314, 2001.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7145,"[28] G. Paetzold and L. Specia, “Semeval 2016 task 11: Complex word
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7146,"identiﬁcation,” in Proceedings of the 10th International Workshop on
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7147,"Semantic Evaluation (SemEval-2016), pp. 560–569, 2016.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7148,"[29] S. R. Thomas and S. Anderson, “Wordnet-based lexical simpliﬁcation
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7149,"of a document.,” in KONVENS, pp. 80–88, 2012.[30] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7150,"vectors with subword information,” Transactions of the Association for
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7151,"Computational Linguistics, vol. 5, pp. 135–146, 2017.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7152,"[31] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, and H. J ´egou, “Word
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7153,"translation without parallel data,” arXiv preprint arXiv:1710.04087,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7154,"2017.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7155,"[32] T. Pires, E. Schlinger, and D. Garrette, “How multilingual is multilingual
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7156,"bert?,” in Proceedings of the 57th Annual Meeting of the Association
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7157,"for Computational Linguistics, pp. 4996–5001, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7158,"[33] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7159,"F. Guzm ´an, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov, “Unsu-
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7160,"pervised cross-lingual representation learning at scale,” arXiv preprint
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7161,"arXiv:1911.02116, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7162,"[34] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7163,"“Glue: A multi-task benchmark and analysis platform for natural lan-guage understanding,” in Proceedings of the 2018 EMNLP Workshop
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7164,"BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,pp. 353–355, 2018.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7165,"[35] J. Ca ˜nete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, and J. P ´erez,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7166,"“Spanish pre-trained bert model and evaluation data,” in Practical ML
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7167,"for Developing Countries Workshop@ ICLR 2020, 2020.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7168,"[36] L. Martin, B. Muller, P. J. O. Su ´arez, Y . Dupont, L. Romary, ´E.V .
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7169,"de la Clergerie, D. Seddah, and B. Sagot, “Camembert: a tasty frenchlanguage model,” arXiv preprint arXiv:1911.03894, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7170,"[37] S. Ruder, “An overview of gradient descent optimization algorithms,”
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7171,"arXiv preprint arXiv:1609.04747, 2016.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7172,"[38] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7173,"arXiv preprint arXiv:1412.6980, 2014.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7174,"[39] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7175,"Q. V . Le, “Xlnet: Generalized autoregressive pretraining for languageunderstanding,” in Advances in neural information processing systems,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7176,"pp. 5753–5763, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7177,"[40] C. Zhu, Y . Cheng, Z. Gan, S. Sun, T. Goldstein, and J. Liu, “Freelb:
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7178,"Enhanced adversarial training for natural language understanding,” inInternational Conference on Learning Representations, 2019.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7179,"[41] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1,
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7180,"pp. 41–75, 1997.
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7181,"390
",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7182,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. ",False,Cross-Lingual Transfer Learning for Complex word Identification,False,False,False
7183,"Authorized licensed use limited to: Rutgers University. Downloaded on May 19,2021 at 06:53:02 UTC from IEEE Xplore.  Restrictions apply. ",False,Cross-Lingual Transfer Learning for Complex word Identification,True,False,False
7184,"ABSTRACT
",True,DeepSonar,False,False,True
7185,"must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
",True,DeepSonar,False,False,True
7186,"1 INTRODUCTION
",True,DeepSonar,False,False,True
7187,"2 RELATED WORK
",True,DeepSonar,False,False,True
7188,"2.1 Voice Synthesis
",True,DeepSonar,False,False,True
7189,"2.2 Fake Voice Detection
",True,DeepSonar,False,False,True
7190,"3 METHOD
",True,DeepSonar,False,False,True
7191,"3.1 Insight
",True,DeepSonar,False,False,True
7192,"3.2 Overview of DeepSonar Framework
",True,DeepSonar,False,False,True
7193,"3.3 Layer-wise Neuron Behaviors
",True,DeepSonar,False,False,True
7194,"3.4 Neuron Coverage Criteria Design
",True,DeepSonar,False,False,True
7195,"3.5 Fake Voice Detection
",True,DeepSonar,False,False,True
7196,"4 EXPERIMENTAL SETTING AND
",True,DeepSonar,False,False,True
7197,"4.1 Dataset
",True,DeepSonar,False,False,True
7198,"DeepSonar : Towards Effective and Robust Detection
",False,DeepSonar,False,False,True
7199,"of AI-Synthesized Fake Voices
",False,DeepSonar,False,False,True
7200,"Run Wang1, Felix Juefei-Xu2, Yihao Huang3, Qing Guo1,†, Xiaofei Xie1, Lei Ma4, Yang Liu1,5
",False,DeepSonar,False,False,True
7201,"1Nanyang Technological University, Singapore2Alibaba Group, USA
",False,DeepSonar,False,False,True
7202,"3East China Normal University, China4Kyushu University, Japan
",False,DeepSonar,False,False,True
7203,"5Institute of Computing Innovation, Zhejiang University, China
",False,DeepSonar,False,False,True
7204,"With the recent advances in voice synthesis, AI-synthesized fake
",False,DeepSonar,False,False,True
7205,"voices are indistinguishable to human ears and widely are applied
",False,DeepSonar,False,False,True
7206,"to produce realistic and natural DeepFakes, exhibiting real threats
",False,DeepSonar,False,False,True
7207,"to our society. However, effective and robust detectors for syn-
",False,DeepSonar,False,False,True
7208,"thesized fake voices are still in their infancy and are not ready to
",False,DeepSonar,False,False,True
7209,"fully tackle this emerging threat. In this paper, we devise a novel
",False,DeepSonar,False,False,True
7210,"approach, named DeepSonar, based on monitoring neuron behav-
",False,DeepSonar,False,False,True
7211,"iors of speaker recognition (SR) system, i.e., a deep neural network
",False,DeepSonar,False,False,True
7212,"(DNN), to discern AI-synthesized fake voices. Layer-wise neuron
",False,DeepSonar,False,False,True
7213,"behaviors provide an important insight to meticulously catch the
",False,DeepSonar,False,False,True
7214,"differences among inputs, which are widely employed for building
",False,DeepSonar,False,False,True
7215,"safety, robust, and interpretable DNNs. In this work, we leverage
",False,DeepSonar,False,False,True
7216,"the power of layer-wise neuron activation patterns with a con-
",False,DeepSonar,False,False,True
7217,"jecture that they can capture the subtle differences between real
",False,DeepSonar,False,False,True
7218,"and AI-synthesized fake voices, in providing a cleaner signal to
",False,DeepSonar,False,False,True
7219,"classifiers than raw inputs. Experiments are conducted on three
",False,DeepSonar,False,False,True
7220,"datasets (including commercial products from Google, Baidu, etc.)
",False,DeepSonar,False,False,True
7221,"containing both English and Chinese languages to corroborate the
",False,DeepSonar,False,False,True
7222,"high detection rates (98.1% average accuracy) and low false alarm
",False,DeepSonar,False,False,True
7223,"rates (about 2% error rate) of DeepSonar in discerning fake voices.
",False,DeepSonar,False,False,True
7224,"Furthermore, extensive experimental results also demonstrate its
",False,DeepSonar,False,False,True
7225,"robustness against manipulation attacks (e.g ., voice conversion and
",False,DeepSonar,False,False,True
7226,"additive real-world noises). Our work further poses a new insight
",False,DeepSonar,False,False,True
7227,"into adopting neuron behaviors for effective and robust AI aided
",False,DeepSonar,False,False,True
7228,"multimedia fakes forensics as an inside-out approach instead of
",False,DeepSonar,False,False,True
7229,"being motivated and swayed by various artifacts introduced in
",False,DeepSonar,False,False,True
7230,"synthesizing fakes.
",False,DeepSonar,False,False,True
7231,"CCS CONCEPTS
",False,DeepSonar,False,False,True
7232,"•Security and privacy →Human and societal aspects of se-
",False,DeepSonar,False,False,True
7233,"curity and privacy ;•Information systems →Multimedia in-
",False,DeepSonar,False,False,True
7234,"formation systems ;•Computing methodologies →Artificial
",False,DeepSonar,False,False,True
7235,"intelligence.
",False,DeepSonar,False,False,True
7236,"Run Wang’s email: runwang1991@gmail.com
",False,DeepSonar,False,False,True
7237,"†Qing Guo is the corresponding author (tsingqguo@gmail.com).
",False,DeepSonar,False,False,True
7238,"Permission to make digital or hard copies of all or part of this work for personal or
",False,DeepSonar,False,False,True
7239,"classroom use is granted without fee provided that copies are not made or distributed
",False,DeepSonar,False,False,True
7240,"for profit or commercial advantage and that copies bear this notice and the full citation
",False,DeepSonar,False,False,True
7241,"on the first page. Copyrights for components of this work owned by others than ACM
",False,DeepSonar,False,False,True
7242,"to post on servers or to redistribute to lists, requires prior specific permission and/or a
",False,DeepSonar,False,False,True
7243,"fee. Request permissions from permissions@acm.org.
",False,DeepSonar,False,False,True
7244,"MM ’20, October 12–16, 2020, Seattle, WA, USA
",False,DeepSonar,False,False,True
7245,"©2020 Association for Computing Machinery.
",False,DeepSonar,False,False,True
7246,"ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00
",False,DeepSonar,False,False,True
7247,"https://doi.org/10.1145/3394171.3413716
",False,DeepSonar,False,False,True
7248,"Source Target
",False,DeepSonar,False,False,True
7249,"Encoder
",False,DeepSonar,False,False,True
7250," Decoder
",False,DeepSonar,False,False,True
7251,"Voice Cloning
",False,DeepSonar,False,False,True
7252,"Text-to-Speech
",False,DeepSonar,False,False,True
7253,"Encoder
",False,DeepSonar,False,False,True
7254," DecoderReal Voices
",False,DeepSonar,False,False,True
7255,"Fake VoicesFake Voices
",False,DeepSonar,False,False,True
7256,"Degradations
",False,DeepSonar,False,False,True
7257,"Real Fake
",False,DeepSonar,False,False,True
7258,"DeepSonarFigure 1: Two types of fake voices, voice cloning and text-to-speech. Voice
",False,DeepSonar,False,False,True
7259,"cloning is more likely a voice style transfer by giving a ""source voice"" and
",False,DeepSonar,False,False,True
7260,"output a cloned style similar ""synthesized voice"". Text-to-speech can generate
",False,DeepSonar,False,False,True
7261,"a new voices by any given texts having specific timbre. Degradation indicates
",False,DeepSonar,False,False,True
7262,"our proposed approach DeepSonar can handle voices that are manipulated by
",False,DeepSonar,False,False,True
7263,"voice conversions and additive real-world noises.
",False,DeepSonar,False,False,True
7264,"KEYWORDS
",False,DeepSonar,False,False,True
7265,"DeepFake, fake voice, neuron behavior
",False,DeepSonar,False,False,True
7266,"ACM Reference Format:
",False,DeepSonar,False,False,True
7267,"Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, Lei Ma,
",False,DeepSonar,False,False,True
7268,"Yang Liu. 2020. DeepSonar : Towards Effective and Robust Detection of
",False,DeepSonar,False,False,True
7269,"AI-Synthesized Fake Voices. In Proceedings of the 28th ACM International
",False,DeepSonar,False,False,True
7270,"Conference on Multimedia (MM ’20), October 12–16, 2020, Seattle, WA, USA.
",False,DeepSonar,False,False,True
7271,"ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3394171.3413716
",False,DeepSonar,False,False,True
7272,"1 INTRODUCTION
",False,DeepSonar,False,False,True
7273,"In August 2019, the wall street journal reported the news titled
",False,DeepSonar,False,False,True
7274,"""Fraudsters Used AI to Mimic CEO’s Voice in Unusual Cybercrime
",False,DeepSonar,False,False,True
7275,"Case"" [17]. In this report, criminals used AI-based software to imper-
",False,DeepSonar,False,False,True
7276,"sonate a CEO’s voice and successfully swindled more than $243,000
",False,DeepSonar,False,False,True
7277,"by speaking on the phone. Recently, advances in AI-synthesized
",False,DeepSonar,False,False,True
7278,"techniques have shown its powerful capabilities in creating highly
",False,DeepSonar,False,False,True
7279,"realistically sounded voices [ 12,49], indistinguishable images [ 14,
",False,DeepSonar,False,False,True
7280,"19,57], and natural videos [ 5,15,50]. Human eyes and ears could be
",False,DeepSonar,False,False,True
7281,"easily fooled by these realistic DeepFakes [ 41,42]. Furthermore, pro-
",False,DeepSonar,False,False,True
7282,"ducing DeepFakes is easy with tools like FaceApp, ZAO, etc. Thus,
",False,DeepSonar,False,False,True
7283,"it also raises security and privacy concerns to everyone while we
",False,DeepSonar,False,False,True
7284,"are enjoying the fun of these synthesized fakes. Powerful detection
",False,DeepSonar,False,False,True
7285,"and defense mechanisms should be developed by the community
",False,DeepSonar,False,False,True
7286,"for fighting against such DeepFakes [34, 55].
",False,DeepSonar,False,False,True
7287,"Voice/speech synthesis steps into a new era since DeepMind
",False,DeepSonar,False,False,True
7288,"developed WaveNet [ 10,44] that could generate realistic and con-
",False,DeepSonar,False,False,True
7289,"vincing voices. Improving the interaction experiences between
",False,DeepSonar,False,False,True
7290,"Oral Session E3: Music, Speech and Audio Processing 
",False,DeepSonar,False,False,True
7291,"in Multimedia & Social Media
",False,DeepSonar,False,False,True
7292,"MM '20, October 12–16, 2020, Seattle, WA, USA 
",False,DeepSonar,False,False,True
7293,"1207machines and humans is the initial idea for developing voice syn-
",False,DeepSonar,False,False,True
7294,"thesis techniques. Based on this idea, some commercial products
",False,DeepSonar,False,False,True
7295,"like intelligent customer service are created by using voice synthe-
",False,DeepSonar,False,False,True
7296,"sis techniques. Unfortunately, some attackers and criminals misuse
",False,DeepSonar,False,False,True
7297,"them for illegal purposes like a politician giving an unreal statement,
",False,DeepSonar,False,False,True
7298,"which may cause a regional crisis or someone imitating the victim’s
",False,DeepSonar,False,False,True
7299,"voice for fraud intentions. All of these can be easily performed with-
",False,DeepSonar,False,False,True
7300,"out any effort by merely giving texts and a clip of the victim’s real
",False,DeepSonar,False,False,True
7301,"voice using some open-sourced tools [ 20] or commercially available
",False,DeepSonar,False,False,True
7302,"text-to-speech (TTS) systems. Thus, discerning whether a clip of
",False,DeepSonar,False,False,True
7303,"voice is synthesized with AI or spoken by humans is extremely
",False,DeepSonar,False,False,True
7304,"important in this era when hearing is not believing anymore.
",False,DeepSonar,False,False,True
7305,"TTS synthesis, voice cloning (VC), and replay attack (RA) are the
",False,DeepSonar,False,False,True
7306,"three different modalities for synthesizing fake voices [ 16]. TTS and
",False,DeepSonar,False,False,True
7307,"VC involve the content regeneration, thus they are more realistic
",False,DeepSonar,False,False,True
7308,"than RA and are difficult for human ears to distinguish. Therefore,
",False,DeepSonar,False,False,True
7309,"they are especially worrisome and pose high risks. Figure 1 shows a
",False,DeepSonar,False,False,True
7310,"more detailed description of the two types of fake voices. Recently,
",False,DeepSonar,False,False,True
7311,"AI-synthesized fake voices have already drawn attention from the
",False,DeepSonar,False,False,True
7312,"community. Google launched a challenge competition dedicated to
",False,DeepSonar,False,False,True
7313,"spoofed voice detection [ 54]. Farid et al. proposed the first bispectral
",False,DeepSonar,False,False,True
7314,"analysis method to distinguish human voices and AI-synthesized
",False,DeepSonar,False,False,True
7315,"voices based on the observation of the bispectral artifacts in fake
",False,DeepSonar,False,False,True
7316,"voices [ 1]. However, existing works on discerning AI-synthesized
",False,DeepSonar,False,False,True
7317,"fake voices all failed in fully tackling the aforementioned TTS and
",False,DeepSonar,False,False,True
7318,"VC fake voices and thoroughly evaluating their robustness against
",False,DeepSonar,False,False,True
7319,"manipulation attacks, which is extremely important for a detector
",False,DeepSonar,False,False,True
7320,"deployed in the wild. Here, manipulation attacks indicate that the
",False,DeepSonar,False,False,True
7321,"voices are corrupted with real-world noises (e.g ., rain, laughing)
",False,DeepSonar,False,False,True
7322,"or converted by manipulating their signals without altering its
",False,DeepSonar,False,False,True
7323,"linguistic contents, such as resampling, and shifting of pitch.
",False,DeepSonar,False,False,True
7324,"Voice synthesis and image synthesis are regularly combined
",False,DeepSonar,False,False,True
7325,"for producing audio-visual consistent video DeepFakes. Compared
",False,DeepSonar,False,False,True
7326,"to image synthesis, voice synthesis exhibits some differences and
",False,DeepSonar,False,False,True
7327,"brings new challenges to detection. Firstly, artifacts in fake voices
",False,DeepSonar,False,False,True
7328,"could be hardly sounded and provide sufficient clues for forensics.
",False,DeepSonar,False,False,True
7329,"They are vastly different from artifacts in fake images that are easily
",False,DeepSonar,False,False,True
7330,"noticed by eyes. Secondly, voice signals are one dimension signals.
",False,DeepSonar,False,False,True
7331,"It is not as simple to introduce artifacts into the voice synthesis
",False,DeepSonar,False,False,True
7332,"procedure as in images that have multiple channels spanning two
",False,DeepSonar,False,False,True
7333,"dimensions spatially. Lastly, for voices recorded indoors or outdoors
",False,DeepSonar,False,False,True
7334,"where noises are abundant, it is easy for the attackers to fool the
",False,DeepSonar,False,False,True
7335,"detectors by adding real-world noises in such circumstances, thus
",False,DeepSonar,False,False,True
7336,"robustness is essential for fake voice detectors.
",False,DeepSonar,False,False,True
7337,"In this paper, we propose a novel approach, named DeepSonar1
",False,DeepSonar,False,False,True
7338,"as presented in Figure 1, based on monitoring neuron behaviors of
",False,DeepSonar,False,False,True
7339,"a DNN-based SR system with a simple binary-classifier to discern
",False,DeepSonar,False,False,True
7340,"AI-synthesized fake voices. We conjecture that the layer-by-layer
",False,DeepSonar,False,False,True
7341,"neuron behaviors in DNNs could provide more subtle features and
",False,DeepSonar,False,False,True
7342,"cleaner signals for the classifiers than raw voice inputs, which
",False,DeepSonar,False,False,True
7343,"served as an important asset for differentiating real human voices
",False,DeepSonar,False,False,True
7344,"and fake voices. In this work, we are dedicated to the TTS andVC
",False,DeepSonar,False,False,True
7345,"fake voices since they are AI-synthesized with content regenerated
",False,DeepSonar,False,False,True
7346,"that are more indistinguishable than RA to our ears. To the best of
",False,DeepSonar,False,False,True
7347,"1Sonar is known as its powerful capabilities in sniffing and probing electronic devices
",False,DeepSonar,False,False,True
7348,"underwater based on sound signals. We hope that our approach is a sonar in discerning
",False,DeepSonar,False,False,True
7349,"AI-synthesized fake voices.our knowledge, this is the first work employing layer-wise neuron
",False,DeepSonar,False,False,True
7350,"behaviors to discern AI-synthesized voices and conducting a com-
",False,DeepSonar,False,False,True
7351,"prehensive evaluation on its robustness against two manipulation
",False,DeepSonar,False,False,True
7352,"attacks, 1) voice conversions, and 2) additive real-world noises.
",False,DeepSonar,False,False,True
7353,"To comprehensively evaluate the effectiveness and robustness of
",False,DeepSonar,False,False,True
7354,"our approach in discerning AI-synthesized fake voices, our experi-
",False,DeepSonar,False,False,True
7355,"ments are conducted on three datasets including publicly available
",False,DeepSonar,False,False,True
7356,"datasets, in which voices are synthesized with commercial prod-
",False,DeepSonar,False,False,True
7357,"ucts and self-built dataset with available open-sourced tools. In
",False,DeepSonar,False,False,True
7358,"the experiments, we aim to evaluate the effectiveness of Deep-
",False,DeepSonar,False,False,True
7359,"Sonar in distinguishing fake voices synthesized with different lan-
",False,DeepSonar,False,False,True
7360,"guages, synthetic techniques, etc., and investigate the robustness
",False,DeepSonar,False,False,True
7361,"of DeepSonar in tackling two manipulation attacks (including voice
",False,DeepSonar,False,False,True
7362,"conversion andadditive real-world noises ). Experimental re-
",False,DeepSonar,False,False,True
7363,"sults have demonstrated that DeepSonar gives an average accuracy
",False,DeepSonar,False,False,True
7364,"higher than 98.1%and an equal error rate (EER) lower than 2%on
",False,DeepSonar,False,False,True
7365,"the three datasets. DeepSonar also outperforms prior work lever-
",False,DeepSonar,False,False,True
7366,"aging bispectral artifacts to differentiate fake voices [ 1] in both
",False,DeepSonar,False,False,True
7367,"effectiveness and robustness. Our main contributions are summa-
",False,DeepSonar,False,False,True
7368,"rized as follows.
",False,DeepSonar,False,False,True
7369,"•New observation of layer-wise neuron behaviors for dis-
",False,DeepSonar,False,False,True
7370,"cerning fake voices. We observe that the layer-wise neuron
",False,DeepSonar,False,False,True
7371,"behaviors capture more subtle features that provide cleaner sig-
",False,DeepSonar,False,False,True
7372,"nals for the classifiers than raw voice inputs for building effective
",False,DeepSonar,False,False,True
7373,"and robust fake detectors. Thus, we propose DeepSonar based on
",False,DeepSonar,False,False,True
7374,"this observation by monitoring neuron behaviors to reveal the
",False,DeepSonar,False,False,True
7375,"differences between real voices and AI-synthesized fake voices.
",False,DeepSonar,False,False,True
7376,"•Performing a comprehensive evaluation of the effective-
",False,DeepSonar,False,False,True
7377,"ness and robustness against manipulations attacks. Exper-
",False,DeepSonar,False,False,True
7378,"iments are conducted on three datasets where voices are synthe-
",False,DeepSonar,False,False,True
7379,"sized with various techniques, containing English and mandarin
",False,DeepSonar,False,False,True
7380,"Chinese languages spoken by males and females with different
",False,DeepSonar,False,False,True
7381,"accents. Experimental results illustrated its effectiveness in dis-
",False,DeepSonar,False,False,True
7382,"cerning fake voices and robustness against two manipulation
",False,DeepSonar,False,False,True
7383,"attacks, voice conversions and additive real-world noises.
",False,DeepSonar,False,False,True
7384,"•New insights for fighting against AI aided multimedia fakes.
",False,DeepSonar,False,False,True
7385,"Instead of investigating the artifacts introduced by various syn-
",False,DeepSonar,False,False,True
7386,"thetic techniques, our approach presents a new insight by leverag-
",False,DeepSonar,False,False,True
7387,"ing the power of layer-wise neuron behaviors for differentiating
",False,DeepSonar,False,False,True
7388,"real and fake in a generic manner. Furthermore, it also demon-
",False,DeepSonar,False,False,True
7389,"strates the potentials for building robust detectors and evasion
",False,DeepSonar,False,False,True
7390,"attacks, which are important to be deployed in the wild.
",False,DeepSonar,False,False,True
7391,"2 RELATED WORK
",False,DeepSonar,False,False,True
7392,"2.1 Voice Synthesis
",False,DeepSonar,False,False,True
7393,"Voice synthesis can be divided into two categories: 1) non-DNN
",False,DeepSonar,False,False,True
7394,"based, such as using hidden Markov models (HMMs) and Gaussian
",False,DeepSonar,False,False,True
7395,"mixture models (GMMs) to learn speech features and replicate them,
",False,DeepSonar,False,False,True
7396,"and 2) DNN based for synthesizing naturalness speech and even
",False,DeepSonar,False,False,True
7397,"on unseen words.
",False,DeepSonar,False,False,True
7398,"The first technique is speech concatenation that concatenates
",False,DeepSonar,False,False,True
7399,"some pre-recorded speech segments to synthesize a new clip voice
",False,DeepSonar,False,False,True
7400,"[62]. The other technique on format analysis uses acoustic mod-
",False,DeepSonar,False,False,True
7401,"els without a human voice as input to generate robotic-sounding
",False,DeepSonar,False,False,True
7402,"speech [ 52]. Modeling the human vocal tract and vocal biomechan-
",False,DeepSonar,False,False,True
7403,"ics is another technique for synthesizing speech, which is known as
",False,DeepSonar,False,False,True
7404,"Oral Session E3: Music, Speech and Audio Processing 
",False,DeepSonar,False,False,True
7405,"in Multimedia & Social Media
",False,DeepSonar,False,False,True
7406,"MM '20, October 12–16, 2020, Seattle, WA, USA 
",False,DeepSonar,False,False,True
7407,"1208articulatory speech synthesis [ 26]. Some studies explore leveraging
",False,DeepSonar,False,False,True
7408,"HMM to modulate speech proprietaries like fundamental frequency
",False,DeepSonar,False,False,True
7409,"and duration [ 64]. These techniques are widely employed in the
",False,DeepSonar,False,False,True
7410,"early years for speech synthesis, but suffer from naturalness issues,
",False,DeepSonar,False,False,True
7411,"which could be easily sounded by human ears.
",False,DeepSonar,False,False,True
7412,"DNN based . DNN-based speech synthesis techniques directly
",False,DeepSonar,False,False,True
7413,"map linguistic features to acoustic features by leveraging the power
",False,DeepSonar,False,False,True
7414,"of DNNs in representation. Various models (e.g ., Boltzmann ma-
",False,DeepSonar,False,False,True
7415,"chines [24], deep belief network [18], mixed density networks [2],
",False,DeepSonar,False,False,True
7416,"Bidirectional LSTM [ 22]) are proposed based on DNNs for synthe-
",False,DeepSonar,False,False,True
7417,"sizing high quality and natural speech. Some synthesized samples
",False,DeepSonar,False,False,True
7418,"are available online [39].
",False,DeepSonar,False,False,True
7419,"WaveNet [ 44] developed by DeepMind in 2016 and Tacotron [ 58]
",False,DeepSonar,False,False,True
7420,"created by Google in 2017 are two milestones in speech synthesis.
",False,DeepSonar,False,False,True
7421,"The two models significantly promote the progress of speech syn-
",False,DeepSonar,False,False,True
7422,"thesis, which enables large scale commercial applications for build-
",False,DeepSonar,False,False,True
7423,"ing TTS and VC systems. WaveNet originates from PixelCNN [ 56]
",False,DeepSonar,False,False,True
7424,"or PixelRNN [ 45] and shown its powerful capabilities in modeling
",False,DeepSonar,False,False,True
7425,"waveforms with a generative model that is trained on a real audio
",False,DeepSonar,False,False,True
7426,"dataset. Tacotron [ 58] is an end-to-end speech synthesis model
",False,DeepSonar,False,False,True
7427,"that can be trained on <text, audio> pairs to avoid large human
",False,DeepSonar,False,False,True
7428,"annotation efforts. Due to the powerful capabilities of WaveNet and
",False,DeepSonar,False,False,True
7429,"Tacotron, some commercial products are developed based on them,
",False,DeepSonar,False,False,True
7430,"such as Baidu TTS [ 33], Amazon AWS Polly [ 7], and Google Cloud
",False,DeepSonar,False,False,True
7431,"TTS [ 9]. Unfortunately, some attackers can maliciously use speech
",False,DeepSonar,False,False,True
7432,"synthesis techniques and develop fake voices for fraud intentions,
",False,DeepSonar,False,False,True
7433,"bringing potential security concerns.
",False,DeepSonar,False,False,True
7434,"2.2 Fake Voice Detection
",False,DeepSonar,False,False,True
7435,"In the past decades, some digital audio forensic studies are working
",False,DeepSonar,False,False,True
7436,"on detecting various forms of audio spoofing [ 63]. These approaches
",False,DeepSonar,False,False,True
7437,"examine metadata of audio files and investigate their actual bytes.
",False,DeepSonar,False,False,True
7438,"Douglas et al. [21] examine the eleven audio recordings from three
",False,DeepSonar,False,False,True
7439,"Olympus recorders in the digital header data for audio authentica-
",False,DeepSonar,False,False,True
7440,"tion. Malik et al. [65] propose using acoustic environment signature
",False,DeepSonar,False,False,True
7441,"as an important feature for detecting audio forgery by verifying the
",False,DeepSonar,False,False,True
7442,"integrity of digital audio. These studies failed in addressing audio
",False,DeepSonar,False,False,True
7443,"content that is synthesized.
",False,DeepSonar,False,False,True
7444,"The most similar work to ours is [ 1] that is the first study dedi-
",False,DeepSonar,False,False,True
7445,"cated to AI-synthesized fake voices. In their work, they propose a
",False,DeepSonar,False,False,True
7446,"bispectral analysis method for detecting AI-synthesized fake voices.
",False,DeepSonar,False,False,True
7447,"They observe that specific and unusual spectral correlation exhib-
",False,DeepSonar,False,False,True
7448,"ited in the fake voices synthesized with DNNs, which are called bis-
",False,DeepSonar,False,False,True
7449,"pectral artifacts. Thus, they explore to use higher-order polyspectral
",False,DeepSonar,False,False,True
7450,"features for discriminating fake voices. This work is also motivated
",False,DeepSonar,False,False,True
7451,"by investigating artifacts introduced in fake voices like some recent
",False,DeepSonar,False,False,True
7452,"studies on detecting fake images [ 23,61]. Artifact-based detectors
",False,DeepSonar,False,False,True
7453,"will be invalid when the artifacts are fixed with some optimization
",False,DeepSonar,False,False,True
7454,"methods or new synthetic techniques are proposed.
",False,DeepSonar,False,False,True
7455,"In this paper, instead of investigating the artifacts in raw voices
",False,DeepSonar,False,False,True
7456,"introduced in synthesis, we explore a new way by monitoring
",False,DeepSonar,False,False,True
7457,"neuron behaviors of DNN-based SR systems with a simple binary-
",False,DeepSonar,False,False,True
7458,"classifier to distinguish real and fake voices. The layer-wise neuron
",False,DeepSonar,False,False,True
7459,"behaviors can capture more subtle features in differentiating real
",False,DeepSonar,False,False,True
7460,"and fake voices. Experimental results show that our approach out-
",False,DeepSonar,False,False,True
7461,"performs previous work (by investigating bispectral artifacts [ 1])
",False,DeepSonar,False,False,True
7462,"in terms of both effectiveness and robustness.
",False,DeepSonar,False,False,True
7463,"Raw Neurons
",False,DeepSonar,False,False,True
7464,"TextFake Voices
",False,DeepSonar,False,False,True
7465,"Audio
",False,DeepSonar,False,False,True
7466,"TTS
",False,DeepSonar,False,False,True
7467,"VCSynthesis
",False,DeepSonar,False,False,True
7468,"Real VoicesSR System
",False,DeepSonar,False,False,True
7469,"DNN-based ModelActivated Neurons
",False,DeepSonar,False,False,True
7470,"Neuron Coverage 
",False,DeepSonar,False,False,True
7471,"Criteria
",False,DeepSonar,False,False,True
7472,"ACN
",False,DeepSonar,False,False,True
7473," TKAN ...
",False,DeepSonar,False,False,True
7474,"Layer-wise Neuron Behaviors
",False,DeepSonar,False,False,True
7475," Data Collection
",False,DeepSonar,False,False,True
7476,"Classifier
",False,DeepSonar,False,False,True
7477,"Real FakeFigure 2: The framework of our proposed DeepSonar. We collect numerous
",False,DeepSonar,False,False,True
7478,"real human speeches and fake voices synthesized with VC and TTS tech-
",False,DeepSonar,False,False,True
7479,"niques as inputs, then a DNN-based SR system is adopted to capture the raw
",False,DeepSonar,False,False,True
7480,"layer-wise neuron behaviors of inputs and the designed neuron coverage cri-
",False,DeepSonar,False,False,True
7481,"teria (e.g ., ACN and TKAN) is employed to determine the activated neurons
",False,DeepSonar,False,False,True
7482,"which are more valuable in hunting the subtle differences between inputs, fi-
",False,DeepSonar,False,False,True
7483,"nally a binary-classifier is trained based on the activated layer-wise behaviors
",False,DeepSonar,False,False,True
7484,"of inputs to predict if a clip of voice is real or fake.
",False,DeepSonar,False,False,True
7485,"3 METHOD
",False,DeepSonar,False,False,True
7486,"We first introduce our basic insight in discerning fake voices, and
",False,DeepSonar,False,False,True
7487,"then present the overview framework of DeepSonar, after which we
",False,DeepSonar,False,False,True
7488,"detail how to capture the layer-wise neuron behaviors and detect
",False,DeepSonar,False,False,True
7489,"fake voices with binary-classifier in the following subsections.
",False,DeepSonar,False,False,True
7490,"3.1 Insight
",False,DeepSonar,False,False,True
7491,"Monitoring neuron behaviors is an important technique for hunting
",False,DeepSonar,False,False,True
7492,"the differences among a set of inputs to DNNs and investigating the
",False,DeepSonar,False,False,True
7493,"internal behaviors of DNNs, which is widely employed in assuring
",False,DeepSonar,False,False,True
7494,"the quality of DNNs [ 28,35,46,60], protecting the safety of DNNs
",False,DeepSonar,False,False,True
7495,"like fighting adversarial examples attack [ 30,31], and providing
",False,DeepSonar,False,False,True
7496,"interpretation for DNNs [51], etc.
",False,DeepSonar,False,False,True
7497,"For quality assurance of DNNs, both DeepXplore [ 46] and Deep-
",False,DeepSonar,False,False,True
7498,"Gauge [ 28] introduce neuron coverage as testing criteria to explore
",False,DeepSonar,False,False,True
7499,"the amount of DNN logic covered by given a set of inputs. Neu-
",False,DeepSonar,False,False,True
7500,"ron coverage is similar to code coverage in traditional software
",False,DeepSonar,False,False,True
7501,"testing and used to explore the vulnerabilities of DNNs, which are
",False,DeepSonar,False,False,True
7502,"susceptible to adversarial examples [ 11]. In ensuring the safety of
",False,DeepSonar,False,False,True
7503,"DNNs, NIC [ 30] and MODE [ 31] exploit the critical neurons in
",False,DeepSonar,False,False,True
7504,"DNNs for detecting adversarial examples and fixing issues that lead
",False,DeepSonar,False,False,True
7505,"to misclassification in DNNs. In providing interpretation for DNNs,
",False,DeepSonar,False,False,True
7506,"AMI [ 51] explores the correlation between important neurons and
",False,DeepSonar,False,False,True
7507,"human perceptible face attributes. Furthermore, the visualization
",False,DeepSonar,False,False,True
7508,"techniques are also proposed [ 4,36] to facilitate the understanding
",False,DeepSonar,False,False,True
7509,"on the roles of neurons.
",False,DeepSonar,False,False,True
7510,"According to recent studies, neuron behaviors have demon-
",False,DeepSonar,False,False,True
7511,"strated their powerful capabilities in investigating the internal
",False,DeepSonar,False,False,True
7512,"behaviors of DNNs and revealing the minor differences among
",False,DeepSonar,False,False,True
7513,"inputs like adversarial examples and legitimate inputs. In this work,
",False,DeepSonar,False,False,True
7514,"we conjecture that layer-wise neuron behaviors could capture more
",False,DeepSonar,False,False,True
7515,"subtle features and produce cleaner signals to a classifier than raw
",False,DeepSonar,False,False,True
7516,"voice inputs in distinguishing the differences between inputs. Thus,
",False,DeepSonar,False,False,True
7517,"we propose DeepSonar by monitoring layer-wise neuron behav-
",False,DeepSonar,False,False,True
7518,"iors of the DNN-based SR system with a simple binary-classifier to
",False,DeepSonar,False,False,True
7519,"discern human speeches and AI-synthesized fake voices.
",False,DeepSonar,False,False,True
7520,"3.2 Overview of DeepSonar Framework
",False,DeepSonar,False,False,True
7521,"We present the overview of DeepSonar framework in Figure 2. In
",False,DeepSonar,False,False,True
7522,"general, we first collect numerous real and synthesized fake voices
",False,DeepSonar,False,False,True
7523,"Oral Session E3: Music, Speech and Audio Processing 
",False,DeepSonar,False,False,True
7524,"in Multimedia & Social Media
",False,DeepSonar,False,False,True
7525,"MM '20, October 12–16, 2020, Seattle, WA, USA 
",False,DeepSonar,False,False,True
7526,"1209with good diversity in languages, accents, genders, and synthetic
",False,DeepSonar,False,False,True
7527,"techniques. Real voices are collected from public datasets and avail-
",False,DeepSonar,False,False,True
7528,"able free videos from the internet, which are spoken by humans
",False,DeepSonar,False,False,True
7529,"in different languages, accents by males or females. In fake voice
",False,DeepSonar,False,False,True
7530,"collection, we 1) use TTS techniques to synthesize new voices with
",False,DeepSonar,False,False,True
7531,"merely given texts, and 2) utilize VC techniques to produce a clip of
",False,DeepSonar,False,False,True
7532,"fake voices having similar timbre to real voices. Then, we adopt a
",False,DeepSonar,False,False,True
7533,"DNN-based SR system to capture the layer-wise neuron behaviors
",False,DeepSonar,False,False,True
7534,"for both real and fake voices and determine the activated neurons
",False,DeepSonar,False,False,True
7535,"with designed neuron coverage criteria. Finally, the captured neuron
",False,DeepSonar,False,False,True
7536,"behaviors are formed as input feature vectors for training a simple
",False,DeepSonar,False,False,True
7537,"supervised binary-classifier based on shallow neural networks to
",False,DeepSonar,False,False,True
7538,"predict whether a clip of voice is a human speech or synthesized.
",False,DeepSonar,False,False,True
7539,"3.3 Layer-wise Neuron Behaviors
",False,DeepSonar,False,False,True
7540,"Layer and neuron are the basic components in a DNN model. Each
",False,DeepSonar,False,False,True
7541,"layer in a DNN has its own distinct role in learning the input
",False,DeepSonar,False,False,True
7542,"representations [ 32]. A neuron xis the basic unit for representing
",False,DeepSonar,False,False,True
7543,"the inputs in each layer, whose output is calculated by the activation
",False,DeepSonar,False,False,True
7544,"functionφ, previous layer neurons X′, weights matrix W, and bias
",False,DeepSonar,False,False,True
7545,"b,i.e.,φ(W·X′+b).
",False,DeepSonar,False,False,True
7546,"Neurons can be classified as activated neurons and inactivated
",False,DeepSonar,False,False,True
7547,"neurons on a given input, according to recent studies in DNN
",False,DeepSonar,False,False,True
7548,"testing [ 46]. Here, an activated neuron means that its output value
",False,DeepSonar,False,False,True
7549,"is large than a predefined threshold δ, and vice versa. According
",False,DeepSonar,False,False,True
7550,"to recent studies, activated neurons could carry more information
",False,DeepSonar,False,False,True
7551,"than inactivate neurons and have a large influence on its following
",False,DeepSonar,False,False,True
7552,"consecutive layers [ 27–29,46]. Thus, we monitor the activated
",False,DeepSonar,False,False,True
7553,"neurons to discern the differences among inputs.
",False,DeepSonar,False,False,True
7554,"In monitoring the layer-wise neuron behaviors, we need to ad-
",False,DeepSonar,False,False,True
7555,"dress the following three issues, 1) which DNN-based model is
",False,DeepSonar,False,False,True
7556,"more suitable for monitoring neuron behaviors? 2) which layers
",False,DeepSonar,False,False,True
7557,"in the model are elected to monitor neuron behaviors? 3) how to
",False,DeepSonar,False,False,True
7558,"determine the threshold δusing neuron coverage criteria?
",False,DeepSonar,False,False,True
7559,"Model selection. In this paper, we monitor the layer-wise neu-
",False,DeepSonar,False,False,True
7560,"ron behaviors of a third-party DNN-based SR system. Speaker recog-
",False,DeepSonar,False,False,True
7561,"nition systems aim at determining the identity of speakers by learn-
",False,DeepSonar,False,False,True
7562,"ing the acoustic features mostly with DNN-based models. In this
",False,DeepSonar,False,False,True
7563,"work, we exploit the DNN-based SR system to serve as a third-party
",False,DeepSonar,False,False,True
7564,"model for capturing the layer-wise neuron behaviors by leveraging
",False,DeepSonar,False,False,True
7565,"its power in representing speech in a layer-wise manner.
",False,DeepSonar,False,False,True
7566,"Layer selection. We select the layers that learn and preserve
",False,DeepSonar,False,False,True
7567,"valuable representation information of inputs, such as convolutional
",False,DeepSonar,False,False,True
7568,"and fully-connected layers in typical convolutional neural networks
",False,DeepSonar,False,False,True
7569,"(CNNs). Here, other layers like pooling without learning substantial
",False,DeepSonar,False,False,True
7570,"representation information can be seen as redundant layers. It might
",False,DeepSonar,False,False,True
7571,"be interesting to explore layers that specifically learn the differences
",False,DeepSonar,False,False,True
7572,"between real and fake voices in future work.
",False,DeepSonar,False,False,True
7573,"Neuron coverage criteria. We introduce two different neuron
",False,DeepSonar,False,False,True
7574,"coverage criteria to figure out the threshold δfor determining the
",False,DeepSonar,False,False,True
7575,"activated neurons. Then, the determined activated neurons in each
",False,DeepSonar,False,False,True
7576,"selected layer are applied to represent the layer-wise behaviors of
",False,DeepSonar,False,False,True
7577,"voices. Previous work [ 46] uses a global threshold to determine if the
",False,DeepSonar,False,False,True
7578,"neuron is activated or not, which is too coarse [ 28]. Here, we specify
",False,DeepSonar,False,False,True
7579,"each layer with a particular threshold. More details on calculating
",False,DeepSonar,False,False,True
7580,"the threshold δare presented in the following subsection.
",False,DeepSonar,False,False,True
7581,"−40 −20 0 20 40 60 −40 −20 020 40 (a) MFCC
",False,DeepSonar,False,False,True
7582,"−20 0 20 40 −40 −30 −20 −10 010 20 30 (b) Raw Neurons
",False,DeepSonar,False,False,True
7583,"−20 0 20 40 −30 −20 −10 010 20 30 (c) Activated Neurons
",False,DeepSonar,False,False,True
7584,"Figure 3: Visualization of three different features in representing real and
",False,DeepSonar,False,False,True
7585,"fake voices. From the left to right, features are represented by MFCC, raw
",False,DeepSonar,False,False,True
7586,"layer-wise neuron behaviors, and activated neuron behaviors with designed
",False,DeepSonar,False,False,True
7587,"neuron coverage criteria, respectively. Here, we select the neuron coverage
",False,DeepSonar,False,False,True
7588,"criteria TKAN as a presentation example.
",False,DeepSonar,False,False,True
7589,"3.4 Neuron Coverage Criteria Design
",False,DeepSonar,False,False,True
7590,"In this paper, we introduce two different neuron coverage criteria
",False,DeepSonar,False,False,True
7591,"for determining the activated neurons to capture layer-wise neuron
",False,DeepSonar,False,False,True
7592,"behaviors. The first one counts the number of activated neurons
",False,DeepSonar,False,False,True
7593,"in each layer, called average count neuron (ACN). The other one
",False,DeepSonar,False,False,True
7594,"selects neurons having top kvalues in each layer, named Top- k
",False,DeepSonar,False,False,True
7595,"activated neuron (TKAN).
",False,DeepSonar,False,False,True
7596,"ACN. Motivated by the weakness of the global threshold defined
",False,DeepSonar,False,False,True
7597,"in previous DNN testing studies, we specify each layer lwith a
",False,DeepSonar,False,False,True
7598,"particular threshold δlthat is calculated from the training dataset.
",False,DeepSonar,False,False,True
7599,"The threshold δlis an average value of all the neuron output values
",False,DeepSonar,False,False,True
7600,"in the layer lof all inputs in the training dataset. We calculate the
",False,DeepSonar,False,False,True
7601,"thresholdδlwith the following formula:
",False,DeepSonar,False,False,True
7602,"δl=Í
",False,DeepSonar,False,False,True
7603,"x∈X,i∈Iφ(x,i;θ)
",False,DeepSonar,False,False,True
7604,"|I|·|X|(1)
",False,DeepSonar,False,False,True
7605,"where xis the neuron in l-th layer, Xis the set of neurons in layer
",False,DeepSonar,False,False,True
7606,"l,iis an input in the training dataset I,φis the activation function
",False,DeepSonar,False,False,True
7607,"for calculating the neuron output value of input iwith trained
",False,DeepSonar,False,False,True
7608,"parameterθ,|X|and|I|represent the number of neurons in layer l
",False,DeepSonar,False,False,True
7609,"and the number of inputs in training dataset I, respectively. Here,
",False,DeepSonar,False,False,True
7610,"we define the ACN as follows:
",False,DeepSonar,False,False,True
7611,"ACN( l,i)=|{x|∀x∈l,φ(x,i;θ)>δl}| (2)
",False,DeepSonar,False,False,True
7612,"where irepresents the input, xis the neuron in layer l,φis an
",False,DeepSonar,False,False,True
7613,"activation function for computing the neuron output value, and δl
",False,DeepSonar,False,False,True
7614,"is the threshold of the l-th layer calculated by formula (1).
",False,DeepSonar,False,False,True
7615,"TKAN. Instead of learning a threshold from training datasets to
",False,DeepSonar,False,False,True
7616,"determine whether a neuron is activated or not, we explore another
",False,DeepSonar,False,False,True
7617,"neuron coverage criterion by simply selecting neurons whose out-
",False,DeepSonar,False,False,True
7618,"put value is ranked as top kin its layer. Here, we conjecture that
",False,DeepSonar,False,False,True
7619,"neurons with large output value are critical neurons that have high
",False,DeepSonar,False,False,True
7620,"influences in representing inputs for a DNN model. We define the
",False,DeepSonar,False,False,True
7621,"TKAN as follows:
",False,DeepSonar,False,False,True
7622,"TKAN( l,i)={arg max
",False,DeepSonar,False,False,True
7623,"k(φ(x,i;θ),k):x∈X} (3)
",False,DeepSonar,False,False,True
7624,"where the function arg max returns kneuron output values calcu-
",False,DeepSonar,False,False,True
7625,"lated withφ. Here, the kis applied for all the layers in the model.
",False,DeepSonar,False,False,True
7626,"Figure 3 adopts t-distributed stochastic neighbor embedding
",False,DeepSonar,False,False,True
7627,"(T-SNE), an algorithm for high-dimensional data visualization, to
",False,DeepSonar,False,False,True
7628,"visualize the effectiveness of neuron behaviors in hunting the dif-
",False,DeepSonar,False,False,True
7629,"ferences between real and fake voices compared with Mel-scale
",False,DeepSonar,False,False,True
7630,"frequency cepstral coefficients (MFCC), a popular feature in speech
",False,DeepSonar,False,False,True
7631,"Oral Session E3: Music, Speech and Audio Processing 
",False,DeepSonar,False,False,True
7632,"in Multimedia & Social Media
",False,DeepSonar,False,False,True
7633,"MM '20, October 12–16, 2020, Seattle, WA, USA 
",False,DeepSonar,False,False,True
7634,"1210analysis. From L-R, voices are represented with MFCC, raw layer-
",False,DeepSonar,False,False,True
7635,"wise neuron behaviors, and activated neurons with designed neuron
",False,DeepSonar,False,False,True
7636,"coverage criteria, respectively. We can easily find that compared
",False,DeepSonar,False,False,True
7637,"with MFCC, raw layer-wise neurons can capture the differences
",False,DeepSonar,False,False,True
7638,"between real and fake in a coarse manners, where the voices are
",False,DeepSonar,False,False,True
7639,"separated into several relatively independent clusters. Furthermore,
",False,DeepSonar,False,False,True
7640,"the subtle differences between real and fake voices can be easily
",False,DeepSonar,False,False,True
7641,"distinguished by applying our designed neuron coverage criteria,
",False,DeepSonar,False,False,True
7642,"where real and fake are separated into two independent clusters.
",False,DeepSonar,False,False,True
7643,"3.5 Fake Voice Detection
",False,DeepSonar,False,False,True
7644,"We train a binary-classifier with a shallow neural network to pre-
",False,DeepSonar,False,False,True
7645,"dict whether a clip of voice is human speech or AI-synthesized
",False,DeepSonar,False,False,True
7646,"fake voice. The inputs of our binary-classifier are the vectorized
",False,DeepSonar,False,False,True
7647,"captured layer-wise neuron behaviors rather than the raw input
",False,DeepSonar,False,False,True
7648,"of voices, which are better for a simple classifier to learn the dif-
",False,DeepSonar,False,False,True
7649,"ferences between real and fake voices. Additionally, the neuron
",False,DeepSonar,False,False,True
7650,"behavior inputs are insensitive to manipulations on voices, thus
",False,DeepSonar,False,False,True
7651,"are robust against various manipulations, such as voice conversion
",False,DeepSonar,False,False,True
7652,"and additive real-world noises.
",False,DeepSonar,False,False,True
7653,"Algorithm 1 describes our basic ideas of capturing layer-wise
",False,DeepSonar,False,False,True
7654,"neurons behaviors for discerning real and fake voices. We train two
",False,DeepSonar,False,False,True
7655,"supervised binary-classifiers with the same architecture based on
",False,DeepSonar,False,False,True
7656,"the two different strategies, namely ACN and TKAN. In predicting
",False,DeepSonar,False,False,True
7657,"an input, we first obtain the layer-wise neuron behaviors with ACN
",False,DeepSonar,False,False,True
7658,"and TKAN, respectively. Then, the neuron behaviors are formed as
",False,DeepSonar,False,False,True
7659,"input features into the binary-classifier for prediction. For ACN, the
",False,DeepSonar,False,False,True
7660,"number of activated neurons in each layer is formed as a feature
",False,DeepSonar,False,False,True
7661,"vector. For TKAN, the raw value of neuron output, which ranked the
",False,DeepSonar,False,False,True
7662,"topkin its layer is formed as a feature vector. Finally, the classifier
",False,DeepSonar,False,False,True
7663,"predicts the voice based on the classifier’s final output score.
",False,DeepSonar,False,False,True
7664,"4 EXPERIMENTAL SETTING AND
",False,DeepSonar,False,False,True
7665,"IMPLEMENTATION
",False,DeepSonar,False,False,True
7666,"4.1 Dataset
",False,DeepSonar,False,False,True
7667,"In our experiments, fake voices are collected from three different
",False,DeepSonar,False,False,True
7668,"datasets including TTS and VC synthesized with various techniques.
",False,DeepSonar,False,False,True
7669,"To ensure its diversity in languages and genders, English and Man-
",False,DeepSonar,False,False,True
7670,"darin Chinese languages are spoken by males and females contain-
",False,DeepSonar,False,False,True
7671,"ing different accents. The first dataset is a public dataset, called FoR,
",False,DeepSonar,False,False,True
7672,"created by APTLY lab [ 43] with the latest open-sourced tools and
",False,DeepSonar,False,False,True
7673,"commercial speech synthesis products (e.g ., Amazon AWS Polly,
",False,DeepSonar,False,False,True
7674,"Google Cloud TTS, and Microsoft Azure TTS). The real voices in
",False,DeepSonar,False,False,True
7675,"FoR are collected from open-sourced speech datasets and free avail-
",False,DeepSonar,False,False,True
7676,"able videos on internet like TED talks and YouTube videos, which
",False,DeepSonar,False,False,True
7677,"cover a good variety of genders, speaker ages, and accents, etc. All
",False,DeepSonar,False,False,True
7678,"the fake voices are synthesized with latest deep learning-based
",False,DeepSonar,False,False,True
7679,"techniques, which own high qualities. However, the dataset FoR
",False,DeepSonar,False,False,True
7680,"only contains the first type TTS fake voices that are synthesized by
",False,DeepSonar,False,False,True
7681,"given texts.
",False,DeepSonar,False,False,True
7682,"Therefore, we build the second dataset, a VC fake voice dataset.
",False,DeepSonar,False,False,True
7683,"The dataset is built by ourselves with an open-sourced tool sprocket
",False,DeepSonar,False,False,True
7684,"[20], which allows to clone the source speaker’s identity into the
",False,DeepSonar,False,False,True
7685,"target speaker. Sprocket also served as a baseline system in voiceTable 1: Statistics of the three datasets for evaluating the effectiveness and
",False,DeepSonar,False,False,True
7686,"robustness of DeepSonar. Column Language indicates the language spoken
",False,DeepSonar,False,False,True
7687,"in the voice samples. Column Real Voice Collection means the sources of real
",False,DeepSonar,False,False,True
7688,"voices collected in the dataset. All the real and fake voices in FoR are collected
",False,DeepSonar,False,False,True
7689,"from the second version for-norm in the original dataset where three different
",False,DeepSonar,False,False,True
7690,"versions are included. Column Model represents the number of techniques for
",False,DeepSonar,False,False,True
7691,"synthesizing voices. Last two columns Real(#) andFake(#) denote the number
",False,DeepSonar,False,False,True
7692,"of real and fake voices in each dataset.
",False,DeepSonar,False,False,True
7693,"Dataset Type Language Real Voice Collection Model Real(#) Fake(#)
",False,DeepSonar,False,False,True
7694,"FoR TTS English multi-sources 7 26,941 26,927
",False,DeepSonar,False,False,True
7695,"MC-TTS TTS Chinese lecture_tts [37] unknown 6,000 6,026
",False,DeepSonar,False,False,True
7696,"Sprocket-VC VC English VCC16&VCC18 1 3,132 3,456
",False,DeepSonar,False,False,True
7697,"(c) ActivatedNeurons 
",False,DeepSonar,False,False,True
7698,"e 3: Visualization of three diﬀerent features in representing real and 
",False,DeepSonar,False,False,True
7699,"voices. From the left to right, features are represented by MFCC, raw 
",False,DeepSonar,False,False,True
7700,"on behaviors, and activated neuron behaviors with designed 
",False,DeepSonar,False,False,True
7701,"on coverage criteria, respectively. Here, we select the neuron coverage 
",False,DeepSonar,False,False,True
7702,"dintoseveralrelativelyindependentclusters.Furthermore,
",False,DeepSonar,False,False,True
7703,"betweenrealandfakevoicescanbeeasily 
",False,DeepSonar,False,False,True
7704,"dbyapplyingourdesignedneuroncoveragecriteria,
",False,DeepSonar,False,False,True
7705,"erealandfakeareseparatedintotwoindependentclusters.
",False,DeepSonar,False,False,True
7706,"Wetrainbinary-classiﬁerwithashallowneuralnetworktopredict 
",False,DeepSonar,False,False,True
7707,"a clip of voice is human speech or AI-synthesized fake 
",False,DeepSonar,False,False,True
7708,"voice. The inputs of our binary-classiﬁerare the vectorized cap- 
",False,DeepSonar,False,False,True
7709,"ed layer-wise neuron behaviors rather than the raw input of 
",False,DeepSonar,False,False,True
7710,"voices,whichisbetterforasimpleclassiﬁertolearnthediﬀerences 
",False,DeepSonar,False,False,True
7711,"betweenrealandfakevoices.Additionally,theneuronbehavior 
",False,DeepSonar,False,False,True
7712,"are insensitive to manipulations on voices, thus they are 
",False,DeepSonar,False,False,True
7713,"robustagainstvariousmanipulations,suchasvoiceconversionand 
",False,DeepSonar,False,False,True
7714,"1describesourbasicideasforcapturinglayer-wise Algorithm 1: Algorithmfordiscerningfakevoiceswith 
",False,DeepSonar,False,False,True
7715,"twodiﬀerentlayer-wiseneuronbehaviors.
",False,DeepSonar,False,False,True
7716,"Input :Trainingandtestingdatasetoffakeandrealvoices Iand
",False,DeepSonar,False,False,True
7717,"D,DNN-basedSRmodel /tildewide/u1D440,topvalue /u1D458
",False,DeepSonar,False,False,True
7718,"Output:Label/u1D453/u1D459/u1D44E/u1D454 
",False,DeepSonar,False,False,True
7719,"1⊲Selectlayersfrom /tildewide/u1D440tomonitorneuronbehaviors.
",False,DeepSonar,False,False,True
7720,"2/u1D43F←/u1D43F/u1D44E/u1D466.alt/u1D452/u1D45F/u1D446/u1D452/u1D459/u1D452/u1D450/u1D461/u1D456/u1D45C/u1D45B (/tildewide/u1D440)
",False,DeepSonar,False,False,True
7721,"3⊲Capturelayer-wiseneuronbehaviorswithACN.
",False,DeepSonar,False,False,True
7722,"4/u1D44B/u1D459isasetofneuronsinlayer /u1D459of /tildewide/u1D440.
",False,DeepSonar,False,False,True
7723,"5V/u1D459countsactivatedneuronsinlayer /u1D459of /tildewide/u1D440.
",False,DeepSonar,False,False,True
7724,"6for/u1D456∈I do 
",False,DeepSonar,False,False,True
7725,"6for/u1D456∈I do 
",False,DeepSonar,False,False,True
7726,Abstract,True,DennisVlaar_Paper,False,False,False
7727,"I. INTRODUCTION  
",True,DennisVlaar_Paper,False,False,False
7728,"II. RELATED WORK  
",True,DennisVlaar_Paper,False,False,False
7729,"III. METHODS  
",True,DennisVlaar_Paper,False,False,False
7730,"IV. RESULTS  
",True,DennisVlaar_Paper,False,False,False
7731,"V. DISCUSSION  
",True,DennisVlaar_Paper,False,False,False
7732,"Customer sentiment toward the use of cameras at 
",False,DennisVlaar_Paper,False,False,False
7733,"Dutch supermarket self -scan checkouts  
",False,DennisVlaar_Paper,False,False,False
7734,"Dennis S. Vlaar  
",False,DennisVlaar_Paper,False,False,False
7735,"Master Applied AI  
",False,DennisVlaar_Paper,False,False,False
7736,"University of Applied Sciences  
",False,DennisVlaar_Paper,False,False,False
7737,"Amsterdam, Netherlands  
",False,DennisVlaar_Paper,False,False,False
7738,"dennisvlaar@gmail.com  
",False,DennisVlaar_Paper,False,False,False
7739,"the payment process in supermarkets tremendously. The use of 
",False,DennisVlaar_Paper,False,False,False
7740,"cameras and facial recognition could p otentially speed up this 
",False,DennisVlaar_Paper,False,False,False
7741,"process more by facilitating as an age checker for alcohol 
",False,DennisVlaar_Paper,False,False,False
7742,"control. The use of facial recognition in public spaces is a 
",False,DennisVlaar_Paper,False,False,False
7743,"privacy sensitive subject, and the question is if customers are 
",False,DennisVlaar_Paper,False,False,False
7744,"comfortable with the presence of these technolo gies in the 
",False,DennisVlaar_Paper,False,False,False
7745,"supermarket. A survey has been conducted and the results show 
",False,DennisVlaar_Paper,False,False,False
7746,"that a majority of Dutch supermarket customers is not 
",False,DennisVlaar_Paper,False,False,False
7747,"comfortable with these technologies. There is no correlation 
",False,DennisVlaar_Paper,False,False,False
7748,"found between age and sentiment toward these technologies.  
",False,DennisVlaar_Paper,False,False,False
7749,"Keywords —Machine vision, Face -recognition, Customer 
",False,DennisVlaar_Paper,False,False,False
7750,"sentiment  
",False,DennisVlaar_Paper,False,False,False
7751,"I. INTRODUCTION  
",False,DennisVlaar_Paper,False,False,False
7752," According to Liftin and Wolfram s elf-scan checkouts have 
",False,DennisVlaar_Paper,False,False,False
7753,"increased the speed of the payment process for supermarket 
",False,DennisVlaar_Paper,False,False,False
7754,"customers tremendously  (Liftin & Wolfram, 2009) . Customer 
",False,DennisVlaar_Paper,False,False,False
7755,"can now scan products with a portable hand -scanner while 
",False,DennisVlaar_Paper,False,False,False
7756,"shopping. The use of facial recognition can speed up this 
",False,DennisVlaar_Paper,False,False,False
7757,"process even more by  facilitating age -verification for when 
",False,DennisVlaar_Paper,False,False,False
7758,"customers buy alcohol. This technology also has the potential 
",False,DennisVlaar_Paper,False,False,False
7759,"of impr oving the quality of age -verification and mitigate false -
",False,DennisVlaar_Paper,False,False,False
7760,"positives. The use of cameras in public spaces, and the use of 
",False,DennisVlaar_Paper,False,False,False
7761,"facial recognition software is a loaded topic and raises 
",False,DennisVlaar_Paper,False,False,False
7762,"concerns regarding privacy.  This research is focused on 
",False,DennisVlaar_Paper,False,False,False
7763,"customer sentiment towa rds the use of cameras at self -scan 
",False,DennisVlaar_Paper,False,False,False
7764,"checkouts in supermarkets.  
",False,DennisVlaar_Paper,False,False,False
7765,"This paper stems from a machine vision project at the 
",False,DennisVlaar_Paper,False,False,False
7766,"University of Applied Sciences.   During this project two ideas 
",False,DennisVlaar_Paper,False,False,False
7767,"where proposed which use facial recognition software at the 
",False,DennisVlaar_Paper,False,False,False
7768,"self-scan checkouts in supermarkets. The first idea consists of 
",False,DennisVlaar_Paper,False,False,False
7769,"using facial recognition software to estimate a customer’s age. 
",False,DennisVlaar_Paper,False,False,False
7770,"Using this technology no image data needs to be saved, the 
",False,DennisVlaar_Paper,False,False,False
7771,"age-estimation will be done real time using a video feed. This 
",False,DennisVlaar_Paper,False,False,False
7772,"first idea raises the first research question:  
",False,DennisVlaar_Paper,False,False,False
7773,"Research question 1:  
",False,DennisVlaar_Paper,False,False,False
7774,"“What is the sentiment of  Dutch supermarket customers 
",False,DennisVlaar_Paper,False,False,False
7775,"toward supermarkets using face recognition for age 
",False,DennisVlaar_Paper,False,False,False
7776,"estimation?”  
",False,DennisVlaar_Paper,False,False,False
7777,"The second idea consists of using facial recognition software 
",False,DennisVlaar_Paper,False,False,False
7778,"to identify a person. This technology requires image data of a 
",False,DennisVlaar_Paper,False,False,False
7779,"person. A person will put his face in a database once, then the 
",False,DennisVlaar_Paper,False,False,False
7780,"age of the customer will be verified manually and will be 
",False,DennisVlaar_Paper,False,False,False
7781,"linked to his biometric face. This second idea raises the second 
",False,DennisVlaar_Paper,False,False,False
7782,"research question:  
",False,DennisVlaar_Paper,False,False,False
7783,"Research question 2:  
",False,DennisVlaar_Paper,False,False,False
7784,"“What is the sentiment of Dutch supermarket customers 
",False,DennisVlaar_Paper,False,False,False
7785,"toward supermarkets saving faces for age verification?”  
",False,DennisVlaar_Paper,False,False,False
7786,"The goal of this research is to get insights in the opinion of the 
",False,DennisVlaar_Paper,False,False,False
7787,"Dutch public toward the use of above mentioned technologies. This will help Dutch supermarkets/retailers to make an 
",False,DennisVlaar_Paper,False,False,False
7788,"informed decision to either implement or not implement th ese 
",False,DennisVlaar_Paper,False,False,False
7789,"technologies . 
",False,DennisVlaar_Paper,False,False,False
7790,"II. RELATED WORK  
",False,DennisVlaar_Paper,False,False,False
7791,"A. Beyond face value: public attitudes to facial 
",False,DennisVlaar_Paper,False,False,False
7792,"recognition technology. (Ada Lovelace institute)  
",False,DennisVlaar_Paper,False,False,False
7793,"Facial recognition technology is now widely employed in 
",False,DennisVlaar_Paper,False,False,False
7794,"several industries in the UK. It has been used at airports since 
",False,DennisVlaar_Paper,False,False,False
7795,"2008 at ePassport border security gates, and Heathrow Airport 
",False,DennisVlaar_Paper,False,False,False
7796,"has ambitions to replace check -in and passport checks with it. 
",False,DennisVlaar_Paper,False,False,False
7797,"The financial services hub Canary Wharf is exploring 
",False,DennisVlaar_Paper,False,False,False
7798,"implementing facial recognition technology throughout its 97 -
",False,DennisVlaar_Paper,False,False,False
7799,"acre property, and central London's Kings Cross shopping and 
",False,DennisVlaar_Paper,False,False,False
7800,"commercial sector has been monitored using CCTV cameras 
",False,DennisVlaar_Paper,False,False,False
7801,"with facial recognition capabilities.  
",False,DennisVlaar_Paper,False,False,False
7802,"The morality of facial recognition technology is a hot topic in 
",False,DennisVlaar_Paper,False,False,False
7803,"British society right now. While reports concerning the use of 
",False,DennisVlaar_Paper,False,False,False
7804,"face recognition  in key London neighborhoods and shopping 
",False,DennisVlaar_Paper,False,False,False
7805,"centers drew regulatory investigation and political censure, 
",False,DennisVlaar_Paper,False,False,False
7806,"police trials of the technology in public locations in London 
",False,DennisVlaar_Paper,False,False,False
7807,"and Wales were met with outcry and legal challenge.  
",False,DennisVlaar_Paper,False,False,False
7808,"In light of this, the Ada Lovelace Inst itute engaged YouGov 
",False,DennisVlaar_Paper,False,False,False
7809,"to conduct the first survey of its kind to ascertain public 
",False,DennisVlaar_Paper,False,False,False
7810,"opinion in the UK about the burgeoning use of face 
",False,DennisVlaar_Paper,False,False,False
7811,"recognition technology in both the public and private sectors. 
",False,DennisVlaar_Paper,False,False,False
7812,"A nationwide representative sample of 4109 persons from the 
",False,DennisVlaar_Paper,False,False,False
7813,"UK are included in the poll. It records the initial reactions of 
",False,DennisVlaar_Paper,False,False,False
7814,"the UK public to a variety of scenarios explaining particular 
",False,DennisVlaar_Paper,False,False,False
7815,"uses of face recognition technology in various fields and for a 
",False,DennisVlaar_Paper,False,False,False
7816,"range of objectives.  
",False,DennisVlaar_Paper,False,False,False
7817,"The report summarizes six key findings from t he survey, the 
",False,DennisVlaar_Paper,False,False,False
7818,"top four findings that are related to the research questions of 
",False,DennisVlaar_Paper,False,False,False
7819,"this paper the most are:  
",False,DennisVlaar_Paper,False,False,False
7820,"1. Although there is a high level of awareness about 
",False,DennisVlaar_Paper,False,False,False
7821,"facial recognition technology, little is actually known 
",False,DennisVlaar_Paper,False,False,False
7822,"about it, especially in terms of the technolog y's 
",False,DennisVlaar_Paper,False,False,False
7823,"limits.  
",False,DennisVlaar_Paper,False,False,False
7824,"2. Nearly half of the population expresses the opinion 
",False,DennisVlaar_Paper,False,False,False
7825,"that individuals should be able to refuse to use or 
",False,DennisVlaar_Paper,False,False,False
7826,"consent to facial recognition technology, indicating 
",False,DennisVlaar_Paper,False,False,False
7827,"that consent is a crucial safety measure for many 
",False,DennisVlaar_Paper,False,False,False
7828,"people.  
",False,DennisVlaar_Paper,False,False,False
7829,"3. The majority of people suppor t facial recognition 
",False,DennisVlaar_Paper,False,False,False
7830,"technology when there is a demonstrable public 
",False,DennisVlaar_Paper,False,False,False
7831,"benefit and appropriate safeguards in place, which 
",False,DennisVlaar_Paper,False,False,False
7832,"justifies greater investment in testing and articulating 
",False,DennisVlaar_Paper,False,False,False
7833,"the potential public benefits of such technologies.  
",False,DennisVlaar_Paper,False,False,False
7834,"4. Further discussion between t he public, business 
",False,DennisVlaar_Paper,False,False,False
7835,"sector, and policymakers is required in order to 
",False,DennisVlaar_Paper,False,False,False
7836,"comprehend and address the public's lack of faith in the private sector's ethical use of face recognition 
",False,DennisVlaar_Paper,False,False,False
7837,"technology.  
",False,DennisVlaar_Paper,False,False,False
7838,"B. Has facial recognition technology been misused? 
",False,DennisVlaar_Paper,False,False,False
7839,"A public perception m odel of facial recognition 
",False,DennisVlaar_Paper,False,False,False
7840,"scenarios.  (Lai, X. & Patrick Rau, P. L. ) 
",False,DennisVlaar_Paper,False,False,False
7841,"In this study, a model of the public's perception of facial 
",False,DennisVlaar_Paper,False,False,False
7842,"recognition technology (FRT) is constructed. This study used 
",False,DennisVlaar_Paper,False,False,False
7843,"a questionnaire with 704 participants to examine people's 
",False,DennisVlaar_Paper,False,False,False
7844,"familiarity with, trust in, and attitudes concerning FRT 
",False,DennisVlaar_Paper,False,False,False
7845,"scenarios. The research then used hierarchical cluster analysis 
",False,DennisVlaar_Paper,False,False,False
7846,"to segment public opinions of several FRT scenarios and 
",False,DennisVlaar_Paper,False,False,False
7847,"looked at how familiarity, trust, and attitude related to each 
",False,DennisVlaar_Paper,False,False,False
7848,"FRT scenario. Fo ur major conclusions were found, the two 
",False,DennisVlaar_Paper,False,False,False
7849,"conclusions that are related to  the research questions of this 
",False,DennisVlaar_Paper,False,False,False
7850,"paper the most are:  
",False,DennisVlaar_Paper,False,False,False
7851,"1. Different FRT scenarios generated different public 
",False,DennisVlaar_Paper,False,False,False
7852,"views, which could be categorized into four groups 
",False,DennisVlaar_Paper,False,False,False
7853,"based on a shared level of fam iliarity, trust, and 
",False,DennisVlaar_Paper,False,False,False
7854,"attitude.  
",False,DennisVlaar_Paper,False,False,False
7855,"2. In each FRT scenario, attitude was primarily 
",False,DennisVlaar_Paper,False,False,False
7856,"influenced by trust, not familiarity, while 
",False,DennisVlaar_Paper,False,False,False
7857,"demographic factors and trust propensity had no 
",False,DennisVlaar_Paper,False,False,False
7858,"discernible influence on public perception.  
",False,DennisVlaar_Paper,False,False,False
7859,"C. Resistance to facial recognition payment in 
",False,DennisVlaar_Paper,False,False,False
7860,"China: The influence of privacy -related factors. ( 
",False,DennisVlaar_Paper,False,False,False
7861,"Liu et al.)   
",False,DennisVlaar_Paper,False,False,False
7862,"China is regarded as a pioneer in cashless payment 
",False,DennisVlaar_Paper,False,False,False
7863,"transactions as a result of the worldwide digital payment trend. 
",False,DennisVlaar_Paper,False,False,False
7864,"Because mobile payment options are so common, 40% of 
",False,DennisVlaar_Paper,False,False,False
7865,"Chinese citiz ens believe they do not need to carry cash when 
",False,DennisVlaar_Paper,False,False,False
7866,"they are on the go. However, since 2019, facial data leaks in 
",False,DennisVlaar_Paper,False,False,False
7867,"China have resulted in an increase in financial market disputes 
",False,DennisVlaar_Paper,False,False,False
7868,"and civil lawsuits, which may cause consumers to delay or 
",False,DennisVlaar_Paper,False,False,False
7869,"even refuse to adopt FRP . 
",False,DennisVlaar_Paper,False,False,False
7870,"This study investigated users' reluctance to accept facial 
",False,DennisVlaar_Paper,False,False,False
7871,"recognition payments within the context of China's innovation 
",False,DennisVlaar_Paper,False,False,False
7872,"resistance and privacy calculus. This research  uses three key 
",False,DennisVlaar_Paper,False,False,False
7873,"components of the users' own privacy perception (privacy 
",False,DennisVlaar_Paper,False,False,False
7874,"concerns, perc eived privacy risk, and privacy control) as well 
",False,DennisVlaar_Paper,False,False,False
7875,"as two essential components of the characteristics and factors 
",False,DennisVlaar_Paper,False,False,False
7876,"related to FRP (perceived benefits and the perceived 
",False,DennisVlaar_Paper,False,False,False
7877,"effectiveness of privacy policy) in order to look into potential 
",False,DennisVlaar_Paper,False,False,False
7878,"relationships between the antecedents and the outcome 
",False,DennisVlaar_Paper,False,False,False
7879,"variable of resistance.  
",False,DennisVlaar_Paper,False,False,False
7880,"A total of 1200 valid observations were gathered, including 
",False,DennisVlaar_Paper,False,False,False
7881,"both individuals who had used face recognition payment 
",False,DennisVlaar_Paper,False,False,False
7882,"systems previously and those who had not.  
",False,DennisVlaar_Paper,False,False,False
7883,"The findings demonstrate that privacy control, privacy risk, 
",False,DennisVlaar_Paper,False,False,False
7884,"and perceived advantages are just a few of the factors in the 
",False,DennisVlaar_Paper,False,False,False
7885,"privacy calculus model that are significantly impacted by how 
",False,DennisVlaar_Paper,False,False,False
7886,"effective a privacy policy is regarded. In that the perceived 
",False,DennisVlaar_Paper,False,False,False
7887,"effectiveness of privacy policies has a positive effect on 
",False,DennisVlaar_Paper,False,False,False
7888,"privacy control while having a negative effect on privacy risk, 
",False,DennisVlaar_Paper,False,False,False
7889,"they agree with a prior study.  
",False,DennisVlaar_Paper,False,False,False
7890,"This study also reveals evidence that the perceived 
",False,DennisVlaar_Paper,False,False,False
7891,"effectiveness of privacy policies has a negative influence on 
",False,DennisVlaar_Paper,False,False,False
7892,"users' resistance to facial recognition tech nology. It is 
",False,DennisVlaar_Paper,False,False,False
7893,"important to note that the perceived effectiveness of privacy 
",False,DennisVlaar_Paper,False,False,False
7894,"policies has a significant impact on all path coefficients of 
",False,DennisVlaar_Paper,False,False,False
7895,"resistance. It suggests that users' reluctance to choose FRP has lessened when the platform's bioinformation privacy p olicy 
",False,DennisVlaar_Paper,False,False,False
7896,"was made public.  
",False,DennisVlaar_Paper,False,False,False
7897,"D. A Study on the influence of customer 
",False,DennisVlaar_Paper,False,False,False
7898,"characteristics on innovation resistance and 
",False,DennisVlaar_Paper,False,False,False
7899,"intention to use in face recognition payment 
",False,DennisVlaar_Paper,False,False,False
7900,"system.  (Zhang  et al.) 
",False,DennisVlaar_Paper,False,False,False
7901,"The findings of this study indicate that consumers' 
",False,DennisVlaar_Paper,False,False,False
7902,"perceptions of risks and their views toward prior goods have 
",False,DennisVlaar_Paper,False,False,False
7903,"a favorable influence on innovation resistance, whereas 
",False,DennisVlaar_Paper,False,False,False
7904,"customers' motivation and self -efficacy have a negative 
",False,DennisVlaar_Paper,False,False,False
7905,"influence. Additionally, research demonstrates that customer 
",False,DennisVlaar_Paper,False,False,False
7906,"resistance to innovation will negatively affect usage  
",False,DennisVlaar_Paper,False,False,False
7907,"intentions.  
",False,DennisVlaar_Paper,False,False,False
7908," 
",False,DennisVlaar_Paper,False,False,False
7909,"On the basis of the findings of this study, the following 
",False,DennisVlaar_Paper,False,False,False
7910,"conclusions can be made :  
",False,DennisVlaar_Paper,False,False,False
7911,"First, the study discovered that customer innovation had no 
",False,DennisVlaar_Paper,False,False,False
7912,"discernible influence on innovation resistance during the 
",False,DennisVlaar_Paper,False,False,False
7913,"innovation and diffusion of the face recognition payment 
",False,DennisVlaar_Paper,False,False,False
7914,"system. This is due to the fact that using facial recognition to 
",False,DennisVlaar_Paper,False,False,False
7915,"confirm identity is not a novel use of biometrics. Customers 
",False,DennisVlaar_Paper,False,False,False
7916,"are used to using some physical features for identity 
",False,DennisVlaar_Paper,False,False,False
7917,"verification.  
",False,DennisVlaar_Paper,False,False,False
7918,"Secondly, according to the findings, customers' capacity for 
",False,DennisVlaar_Paper,False,False,False
7919,"risk perception has the biggest influence on innovation 
",False,DennisVlaar_Paper,False,False,False
7920,"resistance. Biometrics is specifically tied to the customer 
",False,DennisVlaar_Paper,False,False,False
7921,"themselves. Customers are concerned about how successfully 
",False,DennisVlaar_Paper,False,False,False
7922,"privacy and personal information can be maintained in the  age 
",False,DennisVlaar_Paper,False,False,False
7923,"of big data because to the vast gathering and utilization of 
",False,DennisVlaar_Paper,False,False,False
7924,"biological data. The ability to employ facial recognition and 
",False,DennisVlaar_Paper,False,False,False
7925,"simple payment for a long time will therefore depend on the 
",False,DennisVlaar_Paper,False,False,False
7926,"formulation of authoritative large data collecting rights, usage 
",False,DennisVlaar_Paper,False,False,False
7927,"rights, and forgetting rights.  
",False,DennisVlaar_Paper,False,False,False
7928,"III. METHODS  
",False,DennisVlaar_Paper,False,False,False
7929,"A. Survey  
",False,DennisVlaar_Paper,False,False,False
7930,"To answer the two research questions, a survey was 
",False,DennisVlaar_Paper,False,False,False
7931,"conducted. The survey consisted of three parts and is shown 
",False,DennisVlaar_Paper,False,False,False
7932,"in Appendix A. The first part contain s demographic  questions 
",False,DennisVlaar_Paper,False,False,False
7933,"and general question on the use of self -checkouts at the 
",False,DennisVlaar_Paper,False,False,False
7934,"supermarket . The second part focused on outlining a scenario 
",False,DennisVlaar_Paper,False,False,False
7935,"related to the first research question, and the third part focused 
",False,DennisVlaar_Paper,False,False,False
7936,"on outlining a scenario related to the second research question. 
",False,DennisVlaar_Paper,False,False,False
7937,"First the scenarios where introduce d, then the participants had 
",False,DennisVlaar_Paper,False,False,False
7938,"to indicate how comfortable they would be in these scenarios. 
",False,DennisVlaar_Paper,False,False,False
7939,"Lastly the participants had to substantiate their answer.   
",False,DennisVlaar_Paper,False,False,False
7940,"B. Reliabilit y 
",False,DennisVlaar_Paper,False,False,False
7941,"The survey was distributed in two ways. An anonymous link 
",False,DennisVlaar_Paper,False,False,False
7942,"was distributed to friends and family of me and a classmate. 
",False,DennisVlaar_Paper,False,False,False
7943,"Also a QR -code was distributed through campus. This has led 
",False,DennisVlaar_Paper,False,False,False
7944,"to more response from people aged under 25. The distribution 
",False,DennisVlaar_Paper,False,False,False
7945,"in age can be seen in figure 1 below.  The distribution of gender 
",False,DennisVlaar_Paper,False,False,False
7946,"can be  seen in figure 2.    
",False,DennisVlaar_Paper,False,False,False
7947,"Figure 1.  Age distribution of survey participants.   
",False,DennisVlaar_Paper,False,False,False
7948," 
",False,DennisVlaar_Paper,False,False,False
7949,"Figure 2. Gender distribution of survey participants.  
",False,DennisVlaar_Paper,False,False,False
7950,"C. Validity  
",False,DennisVlaar_Paper,False,False,False
7951,"To ensure the survey measures what it is intended to measure 
",False,DennisVlaar_Paper,False,False,False
7952,"the survey was made  together with a classmate who had a 
",False,DennisVlaar_Paper,False,False,False
7953,"similar research topic. Also, the  survey is peer reviewed by 
",False,DennisVlaar_Paper,False,False,False
7954,"four other classmates who provided feedback.  
",False,DennisVlaar_Paper,False,False,False
7955,"IV. RESULTS  
",False,DennisVlaar_Paper,False,False,False
7956,"This chapter discusses the results of the survey given in 
",False,DennisVlaar_Paper,False,False,False
7957,"Appendix A.  
",False,DennisVlaar_Paper,False,False,False
7958,"A. Results. Sentiment toward face re cognition for 
",False,DennisVlaar_Paper,False,False,False
7959,"age estimation.  
",False,DennisVlaar_Paper,False,False,False
7960,"Scenario 1 : “Supermarkets will be using facial recognition 
",False,DennisVlaar_Paper,False,False,False
7961,"at self -checkouts to estimate a customer’s age. This will only 
",False,DennisVlaar_Paper,False,False,False
7962,"be done when the customer gives consent.”  
",False,DennisVlaar_Paper,False,False,False
7963,"Respondents could rate their comfortability with this scenario  
",False,DennisVlaar_Paper,False,False,False
7964,"on a scale of zero to ten. The results are shown in table 1.  
",False,DennisVlaar_Paper,False,False,False
7965," 
",False,DennisVlaar_Paper,False,False,False
7966,"How comfortable are you  with this scenario ? 
",False,DennisVlaar_Paper,False,False,False
7967,"0 1 2 3 4 5 6 7 8 9 10 
",False,DennisVlaar_Paper,False,False,False
7968,"7 11 4 5 4 7 7 9 5 8 4 
",False,DennisVlaar_Paper,False,False,False
7969,"Table 1. Comfortibility of supermarket customers with scenario 1.  
",False,DennisVlaar_Paper,False,False,False
7970,"Table 1 shows that the majority of the respondents are not 
",False,DennisVlaar_Paper,False,False,False
7971,"comfortable with scenario  2. 63.4% of the respondents scores 
",False,DennisVlaar_Paper,False,False,False
7972,"a 6 or lower in comfortability with this scenario. Only 36.6% 
",False,DennisVlaar_Paper,False,False,False
7973,"of the respondents are reporting to feel a 7 or higher in 
",False,DennisVlaar_Paper,False,False,False
7974,"comfortability w ith this scenario.  
",False,DennisVlaar_Paper,False,False,False
7975,"When asked why respondents are not comfortable with this 
",False,DennisVlaar_Paper,False,False,False
7976,"scenario, the answers that were chosen the most are: It is an 
",False,DennisVlaar_Paper,False,False,False
7977,"invasion of my privacy (18.1%), I do not trust them to use the 
",False,DennisVlaar_Paper,False,False,False
7978,"technology ethically (13.7%), it will normalize surveillance 
",False,DennisVlaar_Paper,False,False,False
7979,"(12.6%), it w ill be misused or hacked (9.3%).  
",False,DennisVlaar_Paper,False,False,False
7980,"When the respondents who are comfortable with this scenario 
",False,DennisVlaar_Paper,False,False,False
7981,"were asked why, these were their most answered responses: It 
",False,DennisVlaar_Paper,False,False,False
7982,"will reduce delays (18.5%), It’s convenient (16.3%), it will not 
",False,DennisVlaar_Paper,False,False,False
7983,"affect me personally (16.3%), I trus t them to use the 
",False,DennisVlaar_Paper,False,False,False
7984,"technology ethically (13%).  
",False,DennisVlaar_Paper,False,False,False
7985,"When a test for correlation is done between age and 
",False,DennisVlaar_Paper,False,False,False
7986,"comfortability of scenario 1 the result is an r value of -0.111 with a p -value of 0.365. This means there is no significant 
",False,DennisVlaar_Paper,False,False,False
7987,"correlation between age and comfo rtability of scenario 1.  
",False,DennisVlaar_Paper,False,False,False
7988,"B. Results. Sentiment toward saving faces to 
",False,DennisVlaar_Paper,False,False,False
7989,"database.  
",False,DennisVlaar_Paper,False,False,False
7990,"Scenario 2: “Customers of supermarkets will have the option 
",False,DennisVlaar_Paper,False,False,False
7991,"to get age verification one time. A picture of their fa ce will be 
",False,DennisVlaar_Paper,False,False,False
7992,"taken and saved to a database. The next time a customer 
",False,DennisVlaar_Paper,False,False,False
7993,"doesn’t need manual ID check, but will be verified using face 
",False,DennisVlaar_Paper,False,False,False
7994,"recognition technology.”  
",False,DennisVlaar_Paper,False,False,False
7995,"Again, respondents could rate their comfortability  with this 
",False,DennisVlaar_Paper,False,False,False
7996,"scenario on a scale of zero to ten. The re sults are shown in 
",False,DennisVlaar_Paper,False,False,False
7997,"table 2.  
",False,DennisVlaar_Paper,False,False,False
7998," 
",False,DennisVlaar_Paper,False,False,False
7999," 
",False,DennisVlaar_Paper,False,False,False
8000," 
",False,DennisVlaar_Paper,False,False,False
8001,"Table 2. Comfortability of supermarket customer with scenario 2.  
",False,DennisVlaar_Paper,False,False,False
8002,"Table 2 shows that the majority of the respondents are not 
",False,DennisVlaar_Paper,False,False,False
8003,"comfortable with scenario 2. 70.2% of the respondents scores 
",False,DennisVlaar_Paper,False,False,False
8004,"a 6 or lower in comfortability with this scenario. Only 29.8% 
",False,DennisVlaar_Paper,False,False,False
8005,"of the respondents are reporting to feel a 7 or higher in 
",False,DennisVlaar_Paper,False,False,False
8006,"comfortability with this scenario.  
",False,DennisVlaar_Paper,False,False,False
8007,"When asked why respondent are not comfor table with this 
",False,DennisVlaar_Paper,False,False,False
8008,"scenario, the answers that were given the most are: It is an 
",False,DennisVlaar_Paper,False,False,False
8009,"invasion of my privacy (18.1%), I do not trust them to use the 
",False,DennisVlaar_Paper,False,False,False
8010,"technology ethically (14.3%), it will be misused or hacked 
",False,DennisVlaar_Paper,False,False,False
8011,"(14.3%), it will normalize surveillance (11.2%).  
",False,DennisVlaar_Paper,False,False,False
8012,"The resp ondents who are comfortable with this scenario report 
",False,DennisVlaar_Paper,False,False,False
8013,"the following reasons: It’s convenient (15.2%), it will reduce 
",False,DennisVlaar_Paper,False,False,False
8014,"delays (13.6%), it’s reliable (13.6%), it’s accurate (13.6%), it 
",False,DennisVlaar_Paper,False,False,False
8015,"will not affect me personally (12.1%).   
",False,DennisVlaar_Paper,False,False,False
8016,"When a test for correlation is do ne between age and 
",False,DennisVlaar_Paper,False,False,False
8017,"comfortability of scenario 2 the result is an r value of -0.104 
",False,DennisVlaar_Paper,False,False,False
8018,"with a p -value of 0.444. This means there is no significant 
",False,DennisVlaar_Paper,False,False,False
8019,"correlation between age and comfortability of scenario 2.  
",False,DennisVlaar_Paper,False,False,False
8020,"V. DISCUSSION  
",False,DennisVlaar_Paper,False,False,False
8021,"In this paper the following research questions have been 
",False,DennisVlaar_Paper,False,False,False
8022,"addressed:   
",False,DennisVlaar_Paper,False,False,False
8023,"1. “What is the sentiment of Dutch supermarket 
",False,DennisVlaar_Paper,False,False,False
8024,"customers toward supermarkets using face 
",False,DennisVlaar_Paper,False,False,False
8025,"recognition for age estimation?”  
",False,DennisVlaar_Paper,False,False,False
8026,"2. “What is the sentiment of Dutch supermarket 
",False,DennisVlaar_Paper,False,False,False
8027,"customers toward supermarkets saving faces for age 
",False,DennisVlaar_Paper,False,False,False
8028,"verification?”  
",False,DennisVlaar_Paper,False,False,False
8029,"For the first research question the conclusion can be made that 
",False,DennisVlaar_Paper,False,False,False
8030,"a majority of Dutch supermarket customers would not be 
",False,DennisVlaar_Paper,False,False,False
8031,"comfortable with supermarkets using facial recognition to 
",False,DennisVlaar_Paper,False,False,False
8032,"estimate age. The main reason for people to not feel 
",False,DennisVlaar_Paper,False,False,False
8033,"comfortable  toward the use of this technology are privacy 
",False,DennisVlaar_Paper,False,False,False
8034,"concerns.  
",False,DennisVlaar_Paper,False,False,False
8035,"For the second research question the same conclusion can be 
",False,DennisVlaar_Paper,False,False,False
8036,"made, and the results are more clear in this situation. A 
",False,DennisVlaar_Paper,False,False,False
8037,"majority of Dutch supermarket customers report to have 
",False,DennisVlaar_Paper,False,False,False
8038,"privacy issues with super markets implementing such a 
",False,DennisVlaar_Paper,False,False,False
8039,"technology.  
",False,DennisVlaar_Paper,False,False,False
8040,"There seems to be no correlation in sentiment toward the use 
",False,DennisVlaar_Paper,False,False,False
8041,"of both technologies for age. This means there is no significant 
",False,DennisVlaar_Paper,False,False,False
8042,"How comfortable are you with this scenario?  
",False,DennisVlaar_Paper,False,False,False
8043,"0 1 2 3 4 5 6 7 8 9 10 
",False,DennisVlaar_Paper,False,False,False
8044,"8 11 1 5 3 5 7 8 6 1 2 difference between how comfortable younger people are with 
",False,DennisVlaar_Paper,False,False,False
8045,"the use of these technologies versus older people.  
",False,DennisVlaar_Paper,False,False,False
8046,"Based on the results of this research, I would advice 
",False,DennisVlaar_Paper,False,False,False
8047,"supermarkets to not implement any facial recognition 
",False,DennisVlaar_Paper,False,False,False
8048,"technology in their check -out systems. The survey suggests 
",False,DennisVlaar_Paper,False,False,False
8049,"that the majority of Dutch supermarket customers are not 
",False,DennisVlaar_Paper,False,False,False
8050,"comfortable with the use of facial recognition at self -scan 
",False,DennisVlaar_Paper,False,False,False
8051,"checkouts. Implementation of these technologies will most 
",False,DennisVlaar_Paper,False,False,False
8052,"likely reduce customer experience.  
",False,DennisVlaar_Paper,False,False,False
8053,"SOURCES  
",False,DennisVlaar_Paper,False,False,False
8054,"1. Lai, X. & Patrick Rau, P. L. (2021). Has facial 
",False,DennisVlaar_Paper,False,False,False
8055,"recognition technology been misused? A public 
",False,DennisVlaar_Paper,False,False,False
8056,"perception mo del of facial recognition scenarios. 
",False,DennisVlaar_Paper,False,False,False
8057,"Computers in Human Behavior , 124, 106894. 
",False,DennisVlaar_Paper,False,False,False
8058,"https://doi.org/10.1016/j.chb.2021.106894  
",False,DennisVlaar_Paper,False,False,False
8059,"2. Litfin, T. & Wolfram, G. (2009). New Automated 
",False,DennisVlaar_Paper,False,False,False
8060,"Checkout Systems. Retailing in the 21st Century , 
",False,DennisVlaar_Paper,False,False,False
8061,"189–203. https://doi.org/10.1007/978 -3-540-
",False,DennisVlaar_Paper,False,False,False
8062,"72003 -4_12  
",False,DennisVlaar_Paper,False,False,False
8063,"3. Liu, Y. L., Yan, W. & Hu, B. (2021). Resistance to 
",False,DennisVlaar_Paper,False,False,False
8064,"facial recognition payment in China: The influence 
",False,DennisVlaar_Paper,False,False,False
8065,"of privacy -related factors. Telecommunications 
",False,DennisVlaar_Paper,False,False,False
8066,"Policy , 45(5), 102155. 
",False,DennisVlaar_Paper,False,False,False
8067,"https://doi.org/10.1016/j.telpol.2021.102155  
",False,DennisVlaar_Paper,False,False,False
8068,"4. YouGov. (2019). Beyond f ace value: public 
",False,DennisVlaar_Paper,False,False,False
8069,"attitudes to facial recognition technology. In 
",False,DennisVlaar_Paper,False,False,False
8070,"https://www.adalovelaceinstitute.org/report/beyond
",False,DennisVlaar_Paper,False,False,False
8071,"-face-value -public -attitudes -to-facial -recognition -
",False,DennisVlaar_Paper,False,False,False
8072,"technology/ . Ada Lovelace Institute.  
",False,DennisVlaar_Paper,False,False,False
8073,"5. Zhang, L. L., Zhang, Y. B. & Kim, H. K. (2021). A 
",False,DennisVlaar_Paper,False,False,False
8074,"Study on the Influence of Customer Characteristics 
",False,DennisVlaar_Paper,False,False,False
8075,"on Innovation Resistance and Intention to Use in 
",False,DennisVlaar_Paper,False,False,False
8076,"Face Recognition Payment System. Journal of 
",False,DennisVlaar_Paper,False,False,False
8077,"Advanced Researches and Reports , 1(3), 47 –54. 
",False,DennisVlaar_Paper,False,False,False
8078,"https://doi.org/10.21742/jarr.2021.1.3.07  
",False,DennisVlaar_Paper,False,False,False
8079," 
",False,DennisVlaar_Paper,False,False,False
8080,"APPENDIX A 
",False,DennisVlaar_Paper,False,False,False
8081,"General Questions:  
",False,DennisVlaar_Paper,False,False,False
8082,"Q1. How old are you?  
",False,DennisVlaar_Paper,False,False,False
8083," 
",False,DennisVlaar_Paper,False,False,False
8084,"Q2. What gender do you identify as?  
",False,DennisVlaar_Paper,False,False,False
8085,"• Male  
",False,DennisVlaar_Paper,False,False,False
8086,"• Female  
",False,DennisVlaar_Paper,False,False,False
8087,"• Nonbinary  
",False,DennisVlaar_Paper,False,False,False
8088,"• Other  
",False,DennisVlaar_Paper,False,False,False
8089," 
",False,DennisVlaar_Paper,False,False,False
8090,"Q3. Do you use the self -checkouts at the supermarket?  
",False,DennisVlaar_Paper,False,False,False
8091,"• Yes  
",False,DennisVlaar_Paper,False,False,False
8092,"• No  
",False,DennisVlaar_Paper,False,False,False
8093," 
",False,DennisVlaar_Paper,False,False,False
8094,"Q4. If your answer was yes: how often do you use the self -
",False,DennisVlaar_Paper,False,False,False
8095,"checkout?  
",False,DennisVlaar_Paper,False,False,False
8096,"• Not very often  
",False,DennisVlaar_Paper,False,False,False
8097,"• Often  
",False,DennisVlaar_Paper,False,False,False
8098,"• Always  
",False,DennisVlaar_Paper,False,False,False
8099," 
",False,DennisVlaar_Paper,False,False,False
8100,"Q5. Do you buy alcohol at the self -checkout in 
",False,DennisVlaar_Paper,False,False,False
8101,"supermarkets?  
",False,DennisVlaar_Paper,False,False,False
8102,"• Yes • No 
",False,DennisVlaar_Paper,False,False,False
8103," 
",False,DennisVlaar_Paper,False,False,False
8104,"Q6. If your answer was yes: Do you feel like the staff checks 
",False,DennisVlaar_Paper,False,False,False
8105,"your age often and correctly  
",False,DennisVlaar_Paper,False,False,False
8106,"• Both  
",False,DennisVlaar_Paper,False,False,False
8107,"• Only often  
",False,DennisVlaar_Paper,False,False,False
8108,"• Only correctly  
",False,DennisVlaar_Paper,False,False,False
8109,"• Neither  
",False,DennisVlaar_Paper,False,False,False
8110," 
",False,DennisVlaar_Paper,False,False,False
8111,"Situation 1:  
",False,DennisVlaar_Paper,False,False,False
8112,"Supermarkets will be using facial recognition at self -
",False,DennisVlaar_Paper,False,False,False
8113,"checkouts to estimate customers' age. This will only be done 
",False,DennisVlaar_Paper,False,False,False
8114,"when the customer gives consent.  
",False,DennisVlaar_Paper,False,False,False
8115," 
",False,DennisVlaar_Paper,False,False,False
8116,"Q7. On a scale of 1 to 10, where 1 is not at all comfortable 
",False,DennisVlaar_Paper,False,False,False
8117,"and 10 is very comfortable, how comfortable are you with 
",False,DennisVlaar_Paper,False,False,False
8118,"supermarkets usi ng facial recognition in this way?  
",False,DennisVlaar_Paper,False,False,False
8119," 
",False,DennisVlaar_Paper,False,False,False
8120,"Q8. [Question when Q1 is 6 to 10.] You said that you are 
",False,DennisVlaar_Paper,False,False,False
8121,"comfortable with supermarkets using facial recognition 
",False,DennisVlaar_Paper,False,False,False
8122,"technology in this way, rating your level of comfort as [insert 
",False,DennisVlaar_Paper,False,False,False
8123,"rating from Q1] out of 10. Which of the foll owing are 
",False,DennisVlaar_Paper,False,False,False
8124,"reasons why you are comfortable with this? Please select all 
",False,DennisVlaar_Paper,False,False,False
8125,"that apply  
",False,DennisVlaar_Paper,False,False,False
8126,"• It's convenient  
",False,DennisVlaar_Paper,False,False,False
8127,"• It will reduce delays  
",False,DennisVlaar_Paper,False,False,False
8128,"• I trust them to use the technology ethically  
",False,DennisVlaar_Paper,False,False,False
8129,"• It will encourage good behavior  
",False,DennisVlaar_Paper,False,False,False
8130,"• It's reliable  
",False,DennisVlaar_Paper,False,False,False
8131,"• It's accurate  
",False,DennisVlaar_Paper,False,False,False
8132,"• It's indiscriminate e.g. by rac e and by gender  
",False,DennisVlaar_Paper,False,False,False
8133,"• It will not affect me personally  
",False,DennisVlaar_Paper,False,False,False
8134,"• It enhances existing security systems (e.g CCTV)  
",False,DennisVlaar_Paper,False,False,False
8135,"• I can opt out or consent  
",False,DennisVlaar_Paper,False,False,False
8136,"• It will not be misused or hacked  
",False,DennisVlaar_Paper,False,False,False
8137,"• Other (specify)  
",False,DennisVlaar_Paper,False,False,False
8138,"• Don't know  
",False,DennisVlaar_Paper,False,False,False
8139,"Q9. [Question when Q1 is 1 to 5] You said that you are 
",False,DennisVlaar_Paper,False,False,False
8140,"uncomfortable with  supermarkets using facial recognition 
",False,DennisVlaar_Paper,False,False,False
8141,"technology in this way, rating your level of comfort as [insert 
",False,DennisVlaar_Paper,False,False,False
8142,"rating from Q1] out of 10. Which of the following are 
",False,DennisVlaar_Paper,False,False,False
8143,"reasons why you are uncomfortable with this? Please select 
",False,DennisVlaar_Paper,False,False,False
8144,"all that apply  
",False,DennisVlaar_Paper,False,False,False
8145,"• It's inconvenient  
",False,DennisVlaar_Paper,False,False,False
8146,"• It will increase delays  
",False,DennisVlaar_Paper,False,False,False
8147,"• I do not trust them to use the technology ethically  
",False,DennisVlaar_Paper,False,False,False
8148,"• It will reduce freedom of behavior  
",False,DennisVlaar_Paper,False,False,False
8149,"• It's unreliable  
",False,DennisVlaar_Paper,False,False,False
8150,"• It's inaccurate  
",False,DennisVlaar_Paper,False,False,False
8151,"• It can be used to discriminate e.g. by race or gender  
",False,DennisVlaar_Paper,False,False,False
8152,"• It will affect me personally  
",False,DennisVlaar_Paper,False,False,False
8153,"• It will normalize  surveillance  
",False,DennisVlaar_Paper,False,False,False
8154,"• I can't opt out or consent  
",False,DennisVlaar_Paper,False,False,False
8155,"• It will be misused or hacked  
",False,DennisVlaar_Paper,False,False,False
8156,"• Other (specify)  
",False,DennisVlaar_Paper,False,False,False
8157,"• Don't know  
",False,DennisVlaar_Paper,False,False,False
8158," 
",False,DennisVlaar_Paper,False,False,False
8159,"Situation 2:  
",False,DennisVlaar_Paper,False,False,False
8160,"Customers of supermarkets will have the option to get age 
",False,DennisVlaar_Paper,False,False,False
8161,"verification one time. A picture of their face will be taken and saved to a database. T he next time a customer doesn’t 
",False,DennisVlaar_Paper,False,False,False
8162,"need manual ID check, but will be verified using face -
",False,DennisVlaar_Paper,False,False,False
8163,"recognition technology.  
",False,DennisVlaar_Paper,False,False,False
8164," 
",False,DennisVlaar_Paper,False,False,False
8165,"Q10. On a scale of 1 to 10, where 1 is not at all comfortable 
",False,DennisVlaar_Paper,False,False,False
8166,"and 10 is very comfortable, how comfortable are you with 
",False,DennisVlaar_Paper,False,False,False
8167,"supermarkets using facial recognition in this way?  
",False,DennisVlaar_Paper,False,False,False
8168," 
",False,DennisVlaar_Paper,False,False,False
8169,"Q11. [Question when Q4 is 6 to 10.] You said that you are 
",False,DennisVlaar_Paper,False,False,False
8170,"comfortable with supermarkets using facial recognition 
",False,DennisVlaar_Paper,False,False,False
8171,"technology in this way, rating your level of comfort as [insert 
",False,DennisVlaar_Paper,False,False,False
8172,"rating from Q4] out of 10. Which of the following are  
",False,DennisVlaar_Paper,False,False,False
8173,"reasons why you are comfortable with this? Please select all 
",False,DennisVlaar_Paper,False,False,False
8174,"that apply  
",False,DennisVlaar_Paper,False,False,False
8175,"• It's convenient  
",False,DennisVlaar_Paper,False,False,False
8176,"• It will reduce delays  
",False,DennisVlaar_Paper,False,False,False
8177,"• I trust them to use the technology ethically  
",False,DennisVlaar_Paper,False,False,False
8178,"• It will encourage good behavior  
",False,DennisVlaar_Paper,False,False,False
8179,"• It's reliable  
",False,DennisVlaar_Paper,False,False,False
8180,"• It's accurate  
",False,DennisVlaar_Paper,False,False,False
8181,"• It's indiscriminate e.g. by race and by gender  
",False,DennisVlaar_Paper,False,False,False
8182,"• It will not affect me personally  
",False,DennisVlaar_Paper,False,False,False
8183,"• It enhances existing security systems (e.g CCTV)  
",False,DennisVlaar_Paper,False,False,False
8184,"• I can opt out or consent  • It will not be misused or hacked  
",False,DennisVlaar_Paper,False,False,False
8185,"• Other (specify)  
",False,DennisVlaar_Paper,False,False,False
8186,"• Don't know  
",False,DennisVlaar_Paper,False,False,False
8187," 
",False,DennisVlaar_Paper,False,False,False
8188,"Q12. [Question when Q4 is 1 to 5] You said that you are 
",False,DennisVlaar_Paper,False,False,False
8189,"uncomfortable with sup ermarkets using facial recognition 
",False,DennisVlaar_Paper,False,False,False
8190,"technology in this way, rating your level of comfort as [insert 
",False,DennisVlaar_Paper,False,False,False
8191,"rating from Q4] out of 10. Which of the following are 
",False,DennisVlaar_Paper,False,False,False
8192,"reasons why you are uncomfortable with this? Please select 
",False,DennisVlaar_Paper,False,False,False
8193,"all that apply  
",False,DennisVlaar_Paper,False,False,False
8194,"• It's inconvenient  
",False,DennisVlaar_Paper,False,False,False
8195,"• It will inc rease delays  
",False,DennisVlaar_Paper,False,False,False
8196,"• I do not trust them to use the technology ethically  
",False,DennisVlaar_Paper,False,False,False
8197,"• It will reduce freedom of behavior  
",False,DennisVlaar_Paper,False,False,False
8198,"• It's unreliable  
",False,DennisVlaar_Paper,False,False,False
8199,"• It's inaccurate  
",False,DennisVlaar_Paper,False,False,False
8200,"• It can be used to discriminate e.g. by race or gender  
",False,DennisVlaar_Paper,False,False,False
8201,"• It will affect me personally  
",False,DennisVlaar_Paper,False,False,False
8202,"• It will normalize surveillance  
",False,DennisVlaar_Paper,False,False,False
8203,"• I can't op t out or consent  
",False,DennisVlaar_Paper,False,False,False
8204,"• It will be misused or hacked  
",False,DennisVlaar_Paper,False,False,False
8205,"• Other (specify)  
",False,DennisVlaar_Paper,False,False,False
8206,"• Don't know  
",False,DennisVlaar_Paper,False,False,False
8207," 
",False,DennisVlaar_Paper,False,False,False
8208, ,False,DennisVlaar_Paper,False,False,False
8209,"1 Introduction
",True,dijkstra2002,False,False,True
8210,"1Third, the
",True,dijkstra2002,False,False,True
8211,"2.6 Simulating language of previous item effects
",True,dijkstra2002,False,False,True
8212,"3 Limitations and problems of the BIA model
",True,dijkstra2002,False,False,True
8213,"4 The BIA+ model for bilingual word recognition
",True,dijkstra2002,False,False,True
8214,"2The system-
",True,dijkstra2002,False,False,True
8215,"4.3 Language nodes
",True,dijkstra2002,False,False,True
8216,"3The automatically operating word identi®-
",True,dijkstra2002,False,False,True
8217,"4.5 Non-linguistic context effects
",True,dijkstra2002,False,False,True
8218,"Bilingualism: Language and Cognition
",False,dijkstra2002,False,False,True
8219,"http://journals.cambridge.org/BIL
",False,dijkstra2002,False,False,True
8220,"Additional services for Bilingualism: Language and Cognition:
",False,dijkstra2002,False,False,True
8221,"Email alerts: Click here 
",False,dijkstra2002,False,False,True
8222,"Subscriptions: Click here 
",False,dijkstra2002,False,False,True
8223,"Commercial reprints: Click here 
",False,dijkstra2002,False,False,True
8224,"Terms of use : Click here
",False,dijkstra2002,False,False,True
8225,"The architecture of the bilingual word recognition system: From
",False,dijkstra2002,False,False,True
8226,"identication to decision 
",False,dijkstra2002,False,False,True
8227,"Ton Dijkstra and Walter J.B. van Heuven 
",False,dijkstra2002,False,False,True
8228,"Bilingualism: Language and Cognition / Volume 5 / Issue 03 / December 2002, pp 175 - 197 
",False,dijkstra2002,False,False,True
8229,"DOI: 10.1017/S1366728902003012, Published online: 18 December 2002 
",False,dijkstra2002,False,False,True
8230,"Link to this article: http://journals.cambridge.org/abstract_S1366728902003012
",False,dijkstra2002,False,False,True
8231,"How to cite this article:
",False,dijkstra2002,False,False,True
8232,"Ton Dijkstra and Walter J.B. van Heuven (2002). The architecture of the bilingual word recognition system: From identication to
",False,dijkstra2002,False,False,True
8233,"decision. Bilingualism: Language and Cognition, 5, pp 175-197 doi:10.1017/S1366728902003012 
",False,dijkstra2002,False,False,True
8234,"Request Permissions : Click here
",False,dijkstra2002,False,False,True
8235,"Downloaded from http://journals.cambridge.org/BIL, IP address: 193.255.248.150 on 05 Feb 2015
",False,dijkstra2002,False,False,True
8236,"http://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150The architecture of the
",False,dijkstra2002,False,False,True
8237,"bilingual word recognitionsystem: From identi®cationto decision *TON DIJKSTRA
",False,dijkstra2002,False,False,True
8238,"WALTER J.B. VAN HEUVEN
",False,dijkstra2002,False,False,True
8239,"NICI, University of Nijmegen
",False,dijkstra2002,False,False,True
8240,"The paper opens with an evaluation of the BIA model of bilingual word recognition in the light of recent empirical
",False,dijkstra2002,False,False,True
8241,"evidence. After pointing out problems and omissions, a new model, called the BIA+, is proposed. Structurally, this newmodel extends the old one by adding phonological and semantic lexical representations to the available orthographic ones,and assigns a different role to the so-called language nodes. Furthermore, it makes a distinction between the effects ofnon-linguistic context (such as instruction and stimulus list composition) and linguistic context (such as the semantic andsyntactic effects of sentence context), based on a distinction between the word identi®cation system itself and a task/decision system that regulates control. At the end of the paper, the generalizability of the BIA+ model to different tasksand modalities is discussed.
",False,dijkstra2002,False,False,True
8242,"1 Introduction
",False,dijkstra2002,False,False,True
8243,"In the last decade, there has been a strong increase in
",False,dijkstra2002,False,False,True
8244,"the number of experimental studies investigating
",False,dijkstra2002,False,False,True
8245,"aspects of bilingualism, in particular the bilingualmental lexicon. There are various reasons for thisupsurge of interest in the process of bilingual wordrecognition and the system that takes care of it. Forinstance, it raises issues that are not present inmonolinguals, such as how bilinguals distinguishwords from the two languages while reading or
",False,dijkstra2002,False,False,True
8246,"listening. Furthermore, if monolingual and bilingual
",False,dijkstra2002,False,False,True
8247,"word recognition re¯ects a common underlying pro-cessing system, bilingual research offers new ways oftesting hypotheses derived from the monolingualdomain. At the same time, an analysis of the bilingualword recognition system may enhance our under-standing of its monolingual counterpart. In fact, ifone assumes that recognition of words in different
",False,dijkstra2002,False,False,True
8248,"languages is subserved by one system, the question
",False,dijkstra2002,False,False,True
8249,"arises whether word recognition in one language canbe appropriately studied apart from other-languageknowledge. Yet another reason for investigating bi-lingual word recognition is that there are practical
",False,dijkstra2002,False,False,True
8250,"* The authors thank Judy Kroll, David Green and Janet van Hell
",False,dijkstra2002,False,False,True
8251,"for valuable discussion. They are also grateful to Marc Brysbaert,
",False,dijkstra2002,False,False,True
8252,"Michael Thomas, Ping Li and an anonymous reviewer for theirextensive high-quality comments on an earlier version of thispaper. The proposed model was presented at the Third Inter-
",False,dijkstra2002,False,False,True
8253,"national Symposium on Bilingualism in Bristol, UK (April, 19
",False,dijkstra2002,False,False,True
8254,"2001).consequences to this research for educational pur-
",False,dijkstra2002,False,False,True
8255,"poses (e.g., how to better teach a foreign language inthe classroom). Finally, because words are the basic
",False,dijkstra2002,False,False,True
8256,"building blocks of sentences, it is important to under-
",False,dijkstra2002,False,False,True
8257,"stand how words are recognized in a bilingualcontext as a prerequisite for understanding howsentences are parsed by bilinguals. For all theseresearch issues, reaching a detailed understanding ofthe bilingual word recognition system is an importantaim.
",False,dijkstra2002,False,False,True
8258,"In 1998, we presented a model for bilingual word
",False,dijkstra2002,False,False,True
8259,"recognition that was based on the empirical evidence
",False,dijkstra2002,False,False,True
8260,"available at the time (Dijkstra and Van Heuven,1998; Dijkstra, Van Heuven and Grainger, 1998(a);Van Heuven, Dijkstra and Grainger, 1998; see alsoGrainger and Dijkstra, 1992). The model, called theBilingual Interactive Activation (BIA) model, wasconcerned with the recognition of orthographicrepresentations only. Nevertheless, because it was
",False,dijkstra2002,False,False,True
8261,"implemented on the computer, it allowed a precise
",False,dijkstra2002,False,False,True
8262,"simulation of the results of a series of experimentalstudies. Now, several years and many studies later, itis time to evaluate the model and to explore how itmay be updated and extended in the light of recentevidence.
",False,dijkstra2002,False,False,True
8263,"In the following, we will ®rst summarize the BIA
",False,dijkstra2002,False,False,True
8264,"model and discuss the major empirical phenomena
",False,dijkstra2002,False,False,True
8265,"that it can account for. Next, we will discuss prob-
",False,dijkstra2002,False,False,True
8266,"lems of the BIA model arising from limitations in itslexical and language representations, its handling ofcontext effects, and its lacking an implemented task
",False,dijkstra2002,False,False,True
8267,"Address for correspondence
",False,dijkstra2002,False,False,True
8268,"Ton Dijkstra, NICI, University of Nijmegen, P.O. Box 9104, 6500 HE Nijmegen, The Netherlands.E-mail: dijkstra@nici.kun.nlBilingualism: Language and Cognition 5 (3), 2002, 175±197 #2002 Cambridge University Press DOI: 10.1017/S1366728902003012 175http://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150structure. We will propose solutions to these prob-
",False,dijkstra2002,False,False,True
8269,"lems that can be tested and implemented in futureversions of the model. These solutions imply a majorchange in the BIA model with respect to the languagenodes, as well as the addition of representations anda task/decision component. Because the new modelincorporates, to a large extent, the old BIA model,
",False,dijkstra2002,False,False,True
8270,"we will call it the BIA+ model.
",False,dijkstra2002,False,False,True
8271,"The BIA+ model makes a clear-cut distinction
",False,dijkstra2002,False,False,True
8272,"between an encapsulated word identi®cation systemand a task/decision system, which accounts for alarger and more diverse set of empirical ®ndings.Linguistic information from input signal or (sen-tence) context may affect the word identi®cationsystem, while non-linguistic context information
",False,dijkstra2002,False,False,True
8273,"(e.g., participants' expectations and strategies) in¯u-
",False,dijkstra2002,False,False,True
8274,"ences parameter settings in the task/decision system.The new model assumes interactivity within the wordidenti®cation system and between this system andhigher-order systems such as the parser. However,the model also proposes that the lexical activationlevels within the word identi®cation system itself arenot affected by the task/decision system and, there-
",False,dijkstra2002,False,False,True
8275,"fore, not by sources of non-linguistic information
",False,dijkstra2002,False,False,True
8276,"either.
",False,dijkstra2002,False,False,True
8277,"Apart from considering how words in stimulus
",False,dijkstra2002,False,False,True
8278,"lists and sentence contexts are recognized, we willconsider to what extent assumptions of the basicmodel can be generalized to bilingual auditory wordrecognition and word production, and how a par-ticipant's performance is affected by the task at
",False,dijkstra2002,False,False,True
8279,"hand.
",False,dijkstra2002,False,False,True
8280,"2 The Bilingual Interactive Activation (BIA) model
",False,dijkstra2002,False,False,True
8281,"Visual word recognition in the monolingual domain
",False,dijkstra2002,False,False,True
8282,"has been de®ned as the retrieval of orthographicrepresentations from the mental lexicon corre-sponding to the input letter string (Grainger and
",False,dijkstra2002,False,False,True
8283,"Dijkstra, 1996). It can be de®ned likewise in the
",False,dijkstra2002,False,False,True
8284,"bilingual domain. However, two theoretical issuesarise in bilingual word recognition that do not applyin the monolingual case. First, during monolingualword recognition, word candidates become activatedthat are similar to the input string (Coltheart,Davelaar, Jonasson and Besner, 1977; Andrews,1989; Grainger and Segui, 1990). With respect to
",False,dijkstra2002,False,False,True
8285,"bilingual word recognition, one can ask if lexical
",False,dijkstra2002,False,False,True
8286,"candidates from both languages are activated in bi-lingual word recognition, or from one language only.In the empirical literature, these two positions withrespect to bilingual processing have been referred toas the language non-selective versus the languageselective access hypotheses.
",False,dijkstra2002,False,False,True
8287,"A related second issue is whether the lexical repre-sentations of their two languages are stored together
",False,dijkstra2002,False,False,True
8288,"in an integrated lexicon (irrespective of the languagethey belong to) or in different lexicons, separate foreach language. Basically, in an integrated lexicalsystem competition or selection effects may occurbetween lexical candidates of both languages,whereas in two separate lexical systems competition
",False,dijkstra2002,False,False,True
8289,"effects are limited to candidates of one language
",False,dijkstra2002,False,False,True
8290,"only.
",False,dijkstra2002,False,False,True
8291,"This distinction between structural and processing
",False,dijkstra2002,False,False,True
8292,"aspects of bilingual word recognition is not oftenmade in the bilingual literature. Generally, tworather than four theoretical view points have beencontrasted: language selective access in independentlexicons versus language non-selective access to an
",False,dijkstra2002,False,False,True
8293,"integrated lexicon. The Bilingual Interactive Activa-
",False,dijkstra2002,False,False,True
8294,"tion (BIA) model defends the second viewpoint byassuming that lexical access is basically non-selectivein nature and that the bilingual mental lexicon isintegrated across languages.
",False,dijkstra2002,False,False,True
8295,"The BIA model, illustrated in Figure 1, shares the
",False,dijkstra2002,False,False,True
8296,"basic architecture and parameter settings of themonolingual Interactive Activation model (Mc-
",False,dijkstra2002,False,False,True
8297,"Clelland and Rumelhart, 1981). Apart from intro-
",False,dijkstra2002,False,False,True
8298,"ducing an integrated Dutch and English lexicon, amajor extension is the addition of a representationallayer containing two ``language nodes'' that are con-nected to all the word nodes in both lexicons. Themodel implements top-down language-to-word in-hibition (see Figure 1). Throughout this article, twocrucial issues will be (a) whether the activity of the
",False,dijkstra2002,False,False,True
8299,"language nodes is in¯uenced by non-linguistic infor-
",False,dijkstra2002,False,False,True
8300,"mation sources from outside the word recognitionsystem, and (b) whether inside the system thelanguage nodes signi®cantly affect the activation(and therefore the recognition) of words presented instimulus lists or sentences. In the BIA model, weargued ``yes'' to both questions. For the BIA+model, we will revise our claims and argue ``no'' to
",False,dijkstra2002,False,False,True
8301,"both (but we will further argue that syntactic and
",False,dijkstra2002,False,False,True
8302,"semantic aspects of sentence context can affect the
",False,dijkstra2002,False,False,True
8303,"recognition of a following target word).
",False,dijkstra2002,False,False,True
8304,"When a string of letters is presented to the BIA
",False,dijkstra2002,False,False,True
8305,"model, this visual input affects particular features ateach letter position, which subsequently excite lettersthat contain these features and at the same timeinhibit letters for which the features are absent. The
",False,dijkstra2002,False,False,True
8306,"activated letters next excite words in both languages
",False,dijkstra2002,False,False,True
8307,"in which the activated letter occurs at the position inquestion, while all other words are inhibited. At theword level, all words inhibit each other, irrespectiveof the language to which they belong. Activated wordnodes from the same language send activation on tothe corresponding language node, while activatedlanguage nodes send inhibitory feedback to all word176 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150nodes in the other language. The language nodes
",False,dijkstra2002,False,False,True
8308,"collect activation from words in the language theyrepresent and inhibit active words of the otherlanguage. The activation of the language nodesre¯ects the amount of activity in each lexicon.
",False,dijkstra2002,False,False,True
8309,"In the BIA model, the language nodes ful®ll four
",False,dijkstra2002,False,False,True
8310,"main functions (Dijkstra and Van Heuven, 1998). In
",False,dijkstra2002,False,False,True
8311,"the ®rst two functions, language nodes serve aslinguistic representations , in the last two as non-
",False,dijkstra2002,False,False,True
8312,"linguistic functional mechanisms . First, the language
",False,dijkstra2002,False,False,True
8313,"nodes serve as language tags , or language labels,
",False,dijkstra2002,False,False,True
8314,"representations that indicate the language to whichan item belongs. These representations implement theobservation that mature language users know towhich language a particular word belongs. The need
",False,dijkstra2002,False,False,True
8315,"for this type of representation has already beenmotivated decades ago (e.g., McCormack, 1977,p. 63). Second, language nodes collect activationfrom the lexical representations within a language. Acarry-over of such global lexical activation from one
",False,dijkstra2002,False,False,True
8316,"trial to the next could account for between-trial
",False,dijkstra2002,False,False,True
8317,"language priming effects (e.g., Grainger and
",False,dijkstra2002,False,False,True
8318,"O'Regan, 1992) if one assumes that language nodesactivated by an item can affect word activation at thenext trial (top-down effect), or if decision criteria areaffected by the same/different language membershipof consecutive items (bottom-up effect).
",False,dijkstra2002,False,False,True
8319,"1Third, the
",False,dijkstra2002,False,False,True
8320,"language nodes serve as a functional mechanismmodulating relative language activation to account
",False,dijkstra2002,False,False,True
8321,"for performance differences across experiments.
",False,dijkstra2002,False,False,True
8322,"Thus, language nodes can function as a sort oflanguage ®lter (rather than an all-or-none language
",False,dijkstra2002,False,False,True
8323,"switch). Fourth, it might be assumed that thelanguage nodes can collect context activation origi-nating from outside the word recognition system.Through contextual pre-activation , the language
",False,dijkstra2002,False,False,True
8324,"nodes can account for potential top-down effects on
",False,dijkstra2002,False,False,True
8325,"the lexical identi®cation system originating from,
",False,dijkstra2002,False,False,True
8326,"e.g., the expectations of the participants with respectto the language(s) of the input materials to beprocessed.
",False,dijkstra2002,False,False,True
8327,"For the last three functions, the BIA model
",False,dijkstra2002,False,False,True
8328,"assumed that (by inhibiting non-target languagewords) the language nodes were able to facilitate theselection of words from the target language after
",False,dijkstra2002,False,False,True
8329,"language non-selective access took place, but could
",False,dijkstra2002,False,False,True
8330,"not enforce language selective lexical access from thevery beginning of word recognition. Indeed, simu-lations show that even with strong pre-activation ofthe language nodes and strong top-down effects tothe word level, word candidates from the surpressedlanguage can often be recognized.
",False,dijkstra2002,False,False,True
8331,"Before we discuss a number of limitations and
",False,dijkstra2002,False,False,True
8332,"problems of the BIA model, we will ®rst summarize
",False,dijkstra2002,False,False,True
8333,"six types of empirical ®ndings that the BIA model hasbeen able to account for. We will restrict ourselves tothe bilingual domain (also see Dijkstra et al., 1998(a);for additional monolingual simulations, see VanHeuven, 2000; Van Heuven, Dijkstra, Grainger andSchriefers, 2001).Language
",False,dijkstra2002,False,False,True
8334,"Word
",False,dijkstra2002,False,False,True
8335,"Letter
",False,dijkstra2002,False,False,True
8336,"Feature
",False,dijkstra2002,False,False,True
8337,"Visual inputDutch English
",False,dijkstra2002,False,False,True
8338,"Dutch
",False,dijkstra2002,False,False,True
8339,"wordsEnglish
",False,dijkstra2002,False,False,True
8340,"words
",False,dijkstra2002,False,False,True
8341,"pos 1
",False,dijkstra2002,False,False,True
8342,"pos 2 pos 3pos 4
",False,dijkstra2002,False,False,True
8343,"pos 1
",False,dijkstra2002,False,False,True
8344,"pos 2 pos 3pos 4
",False,dijkstra2002,False,False,True
8345,"Figure 1. The Bilingual Interactive Activation (BIA) model
",False,dijkstra2002,False,False,True
8346,"for bilingual word recognition. Arrowheads indicate
",False,dijkstra2002,False,False,True
8347,"excitatory connections; black ®lled circles indicateinhibitory connections.
",False,dijkstra2002,False,False,True
8348,"1This account implies that the speed of activation of each
",False,dijkstra2002,False,False,True
8349,"language node depends on the number of items connected to that
",False,dijkstra2002,False,False,True
8350,"node (thus on vocabulary size in the concerned language), and
",False,dijkstra2002,False,False,True
8351,"on the (subjective) frequency of the word candidates involved.Such a mechanism is likely to fail if it is applied to beginningbilinguals without making additional assumptions (consider, for
",False,dijkstra2002,False,False,True
8352,"instance, what happens if only one L2 word has been acquired).177 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.1502.1 Simulating neighborhood density effects within
",False,dijkstra2002,False,False,True
8353,"and between languages
",False,dijkstra2002,False,False,True
8354,"In neighborhood studies, it is investigated how the
",False,dijkstra2002,False,False,True
8355,"recognition of a target word is affected by thenumber of words that are similar to it. These similarwords are called ``neighbors''. An orthographic
",False,dijkstra2002,False,False,True
8356,"neighbor is any word differing by a single letter from
",False,dijkstra2002,False,False,True
8357,"the target word respecting length and letter position(Coltheart et al., 1977). For instance, CORK andWORD are both neighbors of WORK. Monolingualword identi®cation and naming turn out to besensitive to the number (density) of neighbors andtheir frequency (Andrews, 1989; Grainger, O'Regan,Jacobs and Segui, 1989; Grainger and Segui, 1990;
",False,dijkstra2002,False,False,True
8358,"Snodgrass and Mintzer, 1993). In bilingual studies,
",False,dijkstra2002,False,False,True
8359,"effects of the number of orthographic neighbors wereused as indexes of the relative in¯uence of non-targetlanguage words on target word recognition in differ-ent experimental tasks and conditions. Target wordsthemselves belonged only to one language (i.e., therewere no interlingual homographs, homophones, orcognates in the stimulus list).
",False,dijkstra2002,False,False,True
8360,"Because neighborhood density effects are assumed
",False,dijkstra2002,False,False,True
8361,"to arise during word identi®cation, cross-languagemanipulation of neighbors allows a test of theindependent versus integrated lexicon hypothesis.According to the integrated lexicon hypothesis,recognition of a target word will be affected by bothtarget and non-target language neighbors, at leastwhen lexical access is language non-selective. Accord-
",False,dijkstra2002,False,False,True
8362,"ing to an independent lexicon hypothesis, recognition
",False,dijkstra2002,False,False,True
8363,"of the target word should not be affected by inter-lexical neighborhood density, because there are nodirect interactions between the two lexicons. Thus,only a non-selective-access integrated-lexicon modelsuch as BIA predicts that target word recognition isin¯uenced by orthographic neighbors from bothlanguages. Other models predict no effects of non-
",False,dijkstra2002,False,False,True
8364,"target language neighborhood density on target word
",False,dijkstra2002,False,False,True
8365,"recognition. As such, studies involving neighborhoodmanipulations offer a way to test the structural andprocessing issues discussed above at the same time.
",False,dijkstra2002,False,False,True
8366,"In a series of progressive demasking and lexical
",False,dijkstra2002,False,False,True
8367,"decision experiments involving Dutch-English bi-linguals, Van Heuven et al. (1998) examined neigh-borhood interference effects on target word
",False,dijkstra2002,False,False,True
8368,"recognition. They manipulated the number of ortho-
",False,dijkstra2002,False,False,True
8369,"graphic neighbors of target words in the same andthe other language of the bilinguals. Cross-linguistically, increasing the number of Dutch ortho-graphic neighbors systematically slowed downresponse times (RTs) for English target words.Within the target language itself, an increase inneighbors consistently produced inhibitory effects for
",False,dijkstra2002,False,False,True
8370,"Dutch target words and facilitatory effects forEnglish target words. Monolingual English readersalso showed facilitation due to English neighbors,but no effects of Dutch neighbors. Why oppositeeffects were observed of English (facilitation) andDutch (inhibition) neighbors was not completely
",False,dijkstra2002,False,False,True
8371,"clear, but may have been a consequence of differences
",False,dijkstra2002,False,False,True
8372,"in the lexical organization of English and Dutch.Whatever explanation is favored, the results of thestudy indicate that during the presentation of a targetword, neighbors from both languages are activatedthat all affect target recognition. This is evidence thatthe lexicon of bilinguals is integrated and non-selective in nature, at least with respect to ortho-
",False,dijkstra2002,False,False,True
8373,"graphic codes.
",False,dijkstra2002,False,False,True
8374,"Van Heuven et al. (1998) and Dijkstra et al.
",False,dijkstra2002,False,False,True
8375,"(1998(a)) showed that the empirically obtained neigh-borhood density effects could be simulated by theBIA model. A language non-selective access variantof the BIA model (without cross-linguistic top-downinhibition from the language nodes to the word level)already provided a good ®t to the data of four differ-
",False,dijkstra2002,False,False,True
8376,"ent experiments when the English frequency range
",False,dijkstra2002,False,False,True
8377,"was restricted (implicitly assuming a lower subjectivefrequency for English L2 words). However, theintroduction of asymmetric cross-linguistic top-downinhibition led to an even better ®t across the fourexperiments. This provides a demonstration of thethird function of the language nodes mentionedabove, namely that they help to account for differ-
",False,dijkstra2002,False,False,True
8378,"ences across experiments. Interestingly enough, the
",False,dijkstra2002,False,False,True
8379,"BIA model was able to account for the oppositeeffects of English and Dutch neighbors by using acombination of reduced English frequencies (relativeto Dutch) and asymmetric top-down inhibition.
",False,dijkstra2002,False,False,True
8380,"2.2 Simulating shifting neighborhood effects across an
",False,dijkstra2002,False,False,True
8381,"experiment
",False,dijkstra2002,False,False,True
8382,"In one of their neighborhood density experiments,
",False,dijkstra2002,False,False,True
8383,"involving the progressive demasking technique, VanHeuven et al. (1998) observed that the pattern of RTsin their neighborhood conditions (orthogonallymanipulating the number of neighbors in Dutch andEnglish) changed for high-pro®ciency participantsacross the four parts of the experiment. In this
",False,dijkstra2002,False,False,True
8384,"blocked experiment, some participants saw the Dutch
",False,dijkstra2002,False,False,True
8385,"items ®rst, while others saw them only after theEnglish items. An inhibitory effect of English neigh-borhood size on Dutch target identi®cation wasobserved, which decreased when the English itemshad been presented earlier. Thus, the Dutch RTswere strongly inhibited by the number of English178 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150neighbors when the Dutch words were presented in
",False,dijkstra2002,False,False,True
8386,"the ®rst block. The effect was especially strong in the®rst half of the ®rst block, but was reduced in thesecond half. For the group of participants who sawthe Dutch words only after the English words, theeffect of English neighbors was reduced even furtherand turned into facilitation in the last half of the
",False,dijkstra2002,False,False,True
8387,"second block. This ®nding suggested that high-pro®-
",False,dijkstra2002,False,False,True
8388,"ciency participants in some way modulated the rela-tive contribution of the two languages. In thebeginning of the experiment, participants expectedtheir English pro®ciency to be tested and thereforekept their English lexicon in an active state(Grosjean, 1997). After being tested with Englishwords, they could focus on the Dutch task and the
",False,dijkstra2002,False,False,True
8389,"relative activation of English may have dropped.
",False,dijkstra2002,False,False,True
8390,"This account was supported by simulations with
",False,dijkstra2002,False,False,True
8391,"the BIA model in which only the settings were variedof the parameter that controlled the amount ofinhibition exerted by the language nodes on the wordlevel. Increasing the amount of asymmetric top-downinhibition across time from zero to larger values, thepattern of results could be ®tted quite well. This can
",False,dijkstra2002,False,False,True
8392,"be considered as a demonstration of the fourth
",False,dijkstra2002,False,False,True
8393,"function of the language nodes discussed above.
",False,dijkstra2002,False,False,True
8394,"2.3 Simulating masked priming effects in bilinguals
",False,dijkstra2002,False,False,True
8395,"The BIA model has also been able to simulate the
",False,dijkstra2002,False,False,True
8396,"effects of masked orthographic priming within andbetween languages observed in a study by Bijeljac-
",False,dijkstra2002,False,False,True
8397,"Babic, Biardeau and Grainger (1997). In this study,
",False,dijkstra2002,False,False,True
8398,"French-English bilinguals made lexical decisions toL1 or L2 target words preceded by masked ortho-graphic prime words from the same or a differentlanguage. In their ®rst experiment, the targetlanguage was English, in the second experiment itwas French. The target words in both experimentswere low-frequency words or nonwords that were
",False,dijkstra2002,False,False,True
8399,"orthographically legal and pronounceable. The prime
",False,dijkstra2002,False,False,True
8400,"words were high-frequency words (presented for 57ms) that either belonged to the same language as thetarget words or to the other language. In bothexperiments it was found that when prime andlanguage were from the same language, target worddecision times were inhibited for orthographicallyrelated primes relative to orthographically dissimilar
",False,dijkstra2002,False,False,True
8401,"primes. For example, when the target language was
",False,dijkstra2002,False,False,True
8402,"English, the prime-target combination ``real ±HEAL'' led to slower RTs than the combination``roof ± HEAL''. This ®nding suggests that anyfacilitation effects that might arise due to formpriming (overlap in letters) were annihilated byinhibition effects due to lexical competition. Morerelevant, however, is that when prime and target were
",False,dijkstra2002,False,False,True
8403,"words from different languages (``beau ± BEAM''),inhibition effects were found as well. This indicatesthat lexical knowledge from the other languageaffected target recognition, which provides evidencesupporting language non-selective access to the bi-lingual lexicon. The response patterns of high-
",False,dijkstra2002,False,False,True
8404,"pro®cient bilinguals showed a considerable within-
",False,dijkstra2002,False,False,True
8405,"language inhibitory priming effect (28 ms) and atrend towards even larger cross-language inhibition(43 ms).
",False,dijkstra2002,False,False,True
8406,"To simulate these results, a combined French±
",False,dijkstra2002,False,False,True
8407,"English lexicon was incorporated in the BIA model(Dijkstra et al., 1998(a)). Subsequently, standardmodel simulations were run with a ®xed recognition
",False,dijkstra2002,False,False,True
8408,"threshold. At the ®rst processing cycle, the prime
",False,dijkstra2002,False,False,True
8409,"word was presented to the model and at the thirdcycle it was replaced by the target word. In corre-spondence with the empirical data, the model pro-duced longer average RTs in the within-languageprime condition and the trend towards larger in-hibition effects in the between-language primecondition. Note that it was not necessary to assume
",False,dijkstra2002,False,False,True
8410,"top-down inhibition effects (asymmetric or other-
",False,dijkstra2002,False,False,True
8411,"wise) from the language nodes to the word level toproduce these inhibition effects.
",False,dijkstra2002,False,False,True
8412,"2.4 Simulating L2-pro®ciency differences in masked
",False,dijkstra2002,False,False,True
8413,"priming with bilinguals
",False,dijkstra2002,False,False,True
8414,"In the second masked priming experiment of their
",False,dijkstra2002,False,False,True
8415,"study, Bijeljac-Babic et al. (1997) examined the data
",False,dijkstra2002,False,False,True
8416,"patterns for three different L2-pro®ciency groups.The pattern for more advanced bilinguals was thesame as described above: clear inhibition effects inthe related prime condition relative to the unrelatedcontrol condition. Beginning bilinguals, however,showed smaller RT differences between the relatedand unrelated priming conditions, while there were
",False,dijkstra2002,False,False,True
8417,"no priming effects in monolinguals. The BIA model
",False,dijkstra2002,False,False,True
8418,"was able to simulate the three patterns of resultswhen it was assumed that the monolinguals knew afew words of the foreign language (English) after all(Dijkstra et al., 1998(a)). If the monolinguals reallydid not have any foreign knowledge, the L2 wordsshould be treated as nonwords and according to theBIA model, this should have produced a facilitation
",False,dijkstra2002,False,False,True
8419,"effect in the related condition relative to the unrelated
",False,dijkstra2002,False,False,True
8420,"condition (due to form overlap), rather than a null-effect. (Such masked priming facilitation effects formonolinguals and beginning bilinguals have indeedbeen observed in other recent work in our labora-tory). Note that again simulations were simple; notop-down inhibition was assumed.179 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.1502.5 Simulating effects for interlingual homographs in
",False,dijkstra2002,False,False,True
8421,"a go/no-go task
",False,dijkstra2002,False,False,True
8422,"Many bilingual word recognition studies investi-
",False,dijkstra2002,False,False,True
8423,"gating the issue of (non-)selective access have usedinterlingual homographs or cognates as stimulusmaterials. Interlingual homographs in a strict de®ni-
",False,dijkstra2002,False,False,True
8424,"tion are words that are identical across languages
",False,dijkstra2002,False,False,True
8425,"with respect to their orthography but not theirmeaning (nor, most often, their phonology). Otherterms used are ``interlexical homographs'' or ``falsefriends''. An example is a word like ROOM, whichmeans ``cream'' in Dutch. Cognates are de®ned hereas words from two languages that are identical inorthographic form and largely overlap in meaning,
",False,dijkstra2002,False,False,True
8426,"such as FILM in Dutch and English. The argument-
",False,dijkstra2002,False,False,True
8427,"ation for using interlingual homographs and cognatesin bilingual research has been as follows. Considerthe Dutch-English interlingual homograph LIST(meaning ``trick'' in Dutch). If the recognition of theEnglish reading of the homograph by Dutch-Englishbilinguals is affected by the Dutch reading, thenresponse latencies should be different from those to
",False,dijkstra2002,False,False,True
8428,"one-language control items that are matched to the
",False,dijkstra2002,False,False,True
8429,"interlingual homograph in frequency, length andother characteristics (e.g., MILK). For instance, therequirement of selecting either the English or theDutch reading of a homograph might induce in-hibitory effects relative to a control. However, ifrecognition proceeds in a language selective way,then no RT differences between homographs and
",False,dijkstra2002,False,False,True
8430,"controls would be expected, because the Dutch
",False,dijkstra2002,False,False,True
8431,"reading of the interlingual homograph would not beactivated at all and would not affect lexical selectionbased on the English lexicon.
",False,dijkstra2002,False,False,True
8432,"To investigate this issue, Dijkstra, Timmermans
",False,dijkstra2002,False,False,True
8433,"and Schriefers (2000(b)) made use of a go/no-goparadigm. In this paradigm, participants had to reactonly when a presented word belonged to a prespeci®ed
",False,dijkstra2002,False,False,True
8434,"target language. Participants reacted only when they
",False,dijkstra2002,False,False,True
8435,"identi®ed either an English word (English go/no-go)or a Dutch word (Dutch go/no-go), but they did notrespond if a word of the non-target language (Dutchor English, respectively) was presented. The list ofpresented words consisted of a mixture of interlingualhomographs and one- language Dutch and Englishcontrol groups. The participants were informed that
",False,dijkstra2002,False,False,True
8436,"some of the words in the list could be both English
",False,dijkstra2002,False,False,True
8437,"and Dutch: interlingual homographs. Such homo-graphs could belong to three types: high-frequent inEnglish (HE) and low-frequent in Dutch (LD); low-frequent in English (LE) and high-frequent in Dutch(HD); and low-frequent in both languages. Examplesof words in the three groups are LIST (HE/LD),BRAND (LE/HD) and GIST (LE/LD).In both go/no-go tasks, clear inhibition effects
",False,dijkstra2002,False,False,True
8438,"arose for homographs relative to one-language con-trols. Even in the Dutch go/no-go task for Dutch-English bilinguals performing in their nativelanguage, participants were unable to completelyexclude effects from the non-target language onhomograph identi®cation. Target-language homo-
",False,dijkstra2002,False,False,True
8439,"graphs were often ``overlooked'', especially if the
",False,dijkstra2002,False,False,True
8440,"frequency of their other-language competitor washigh. In the Dutch go/no-go task, participants didnot respond to low-frequency items belonging totheir native language in about 25 percent of the cases!Inspection of cumulative distributions showed that ifthey did not respond after about 1500±1600 ms, theydid not respond within the time window of two
",False,dijkstra2002,False,False,True
8441,"seconds anymore.
",False,dijkstra2002,False,False,True
8442,"To simulate these results with the BIA model, it
",False,dijkstra2002,False,False,True
8443,"was assumed that a homograph is represented in themental lexicon twice, once for each language(Dijkstra and Van Heuven, 1998). Each representa-tion has a resting level activation depending on itsfrequency of occurrence in the language that itbelongs to. Furthermore, these representations
",False,dijkstra2002,False,False,True
8444,"compete with words of both languages in a standard
",False,dijkstra2002,False,False,True
8445,"fashion. Using asymmetric top-down inhibition fromthe Dutch language node to English words, thepattern of correct responses (in percentages) forhomographs and control words in the three fre-quency conditions of the Dutch go task was repro-duced by the BIA model reasonably well. Thepercentage of correct responses was clearly dependent
",False,dijkstra2002,False,False,True
8446,"on the relative frequency of both readings of the
",False,dijkstra2002,False,False,True
8447,"interlingual homographs. However, simulating theresponse times for the different types of homographsturned out to be more dif®cult.
",False,dijkstra2002,False,False,True
8448,"2.6 Simulating language of previous item effects
",False,dijkstra2002,False,False,True
8449,"Von Studnitz and Green (1997) had German-English
",False,dijkstra2002,False,False,True
8450,"bilinguals perform a generalized lexical decision task,
",False,dijkstra2002,False,False,True
8451,"in which they gave a ``yes'' response if a presenteditem belonged to German or English, and a ``no''response if it was a nonword. Words in the list weresometimes preceded by other words from the samelanguage, and sometimes by words from the otherlanguage. Von Studnitz and Green observed that thebilinguals' RTs on switch trials were a signi®cant
",False,dijkstra2002,False,False,True
8452,"17 ms slower than on non-switch trials. Signi®cant
",False,dijkstra2002,False,False,True
8453,"language switching effects of considerable size werealso found in a reanalysis of the generalized lexicaldecision data of the Van Heuven et al. (1998) neigh-borhood study as well. Dutch words preceded bynonwords (571 ms) were responded to faster thanthose preceded by English words (586 ms) and slowerthan those preceded by Dutch words (556 ms).180 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150English words preceded by nonwords (621 ms) were
",False,dijkstra2002,False,False,True
8454,"responded to faster than those preceded by Dutchwords (641 ms) and slower than those preceded byEnglish words (606 ms).
",False,dijkstra2002,False,False,True
8455,"In a psychophysical study with themselves as the
",False,dijkstra2002,False,False,True
8456,"two participants, Grainger and O'Regan (1992) madelexical decisions to repeatedly presented words of
",False,dijkstra2002,False,False,True
8457,"four letters and nonwords that were preceded by
",False,dijkstra2002,False,False,True
8458,"unrelated French and English prime words. Atpresentation durations for which the primes could beidenti®ed, language prime effects arose that could beaccounted for by assuming that primes from a differ-ent language interfered with target processing.
",False,dijkstra2002,False,False,True
8459,"Such ®ndings can, in principle, be explained in
",False,dijkstra2002,False,False,True
8460,"terms of the BIA model by assuming that, due to the
",False,dijkstra2002,False,False,True
8461,"interaction of the language nodes with the ortho-
",False,dijkstra2002,False,False,True
8462,"graphic lexical representations, target word recog-nition is affected differently by preceding words fromthe same and from another language (Dijkstra, VanJaarsveld and Ten Brinke, 1998(b), p. 63±64). Forinstance, on trial t an English word would activatethe English language node, and on trial t + 1, thislanguage node would feed activation back to all
",False,dijkstra2002,False,False,True
8463,"English words and/or inhibit all words from the
",False,dijkstra2002,False,False,True
8464,"Dutch lexicon. This hypothesis could hold for bothtypes of studies just reviewed, including item pairsacross trials (``previous item effects'') or within-trialprime-target pairs (e.g., in unmasked or maskedpriming). However, because evidence from severalrecent studies (Von Studnitz and Green, 1997, 2002;Thomas and Allport, 2000) indicates that the major
",False,dijkstra2002,False,False,True
8465,"source of switch costs must be positioned outside the
",False,dijkstra2002,False,False,True
8466,"mental lexicon, this account is probably not correct(see also section 4.8).
",False,dijkstra2002,False,False,True
8467,"3 Limitations and problems of the BIA model
",False,dijkstra2002,False,False,True
8468,"Thus, in spite of the successful simulations of the
",False,dijkstra2002,False,False,True
8469,"BIA model described in the previous section,
",False,dijkstra2002,False,False,True
8470,"empirical studies suggest that some of the proposed
",False,dijkstra2002,False,False,True
8471,"mechanisms with respect to consecutive item effectsand language switching may be wrong. In addition,there are many aspects of bilingual word recognitionthat are not fully accounted for by the model. Tomention just a few of these:.there are no phonological or semantic representa-
",False,dijkstra2002,False,False,True
8472,"tions in the model;
",False,dijkstra2002,False,False,True
8473,".the representation of interlingual homographs and
",False,dijkstra2002,False,False,True
8474,"cognates is underspeci®ed;
",False,dijkstra2002,False,False,True
8475,".representational and functional aspects with
",False,dijkstra2002,False,False,True
8476,"respect to the language nodes are confounded;
",False,dijkstra2002,False,False,True
8477,".there is only a very limited account of how non-
",False,dijkstra2002,False,False,True
8478,"linguistic and linguistic contexts affect bilingualword recognition;
",False,dijkstra2002,False,False,True
8479,".there is no detailed description of how participantsperform a particular task, for instance lexical
",False,dijkstra2002,False,False,True
8480,"decision;
",False,dijkstra2002,False,False,True
8481,".the relationship between word identi®cation and
",False,dijkstra2002,False,False,True
8482,"task demands is underspeci®ed.The majority of points in this list call for the
",False,dijkstra2002,False,False,True
8483,"extension of the BIA model with additional repre-sentations and processing components. In addition,
",False,dijkstra2002,False,False,True
8484,"the model must be differentiated and adapted with
",False,dijkstra2002,False,False,True
8485,"respect to the language node representations. In thefollowing sections, we will describe the revised model,which we will call the BIA+ model. To the extentthat the BIA model is ``nested'' in the BIA+ model,the earlier simulations on the basis of orthographicrepresentations only are still valid for the BIA+model. However, as we shall see, implementing
",False,dijkstra2002,False,False,True
8486,"lexical phonological and semantic representations
",False,dijkstra2002,False,False,True
8487,"computationally poses serious problems, and suchimplementation problems are even more serious forthe task/decision component (in fact, at present thereis no available monolingual model that has incorpo-rated both aspects in a satisfactory way). We will,therefore, at this moment, follow Green (1998) andGrosjean (1997) in providing only a verbal analysis
",False,dijkstra2002,False,False,True
8488,"for these components of the model. We also ack-
",False,dijkstra2002,False,False,True
8489,"nowledge that there are many other aspects of bi-lingual word recognition that should be considered inthe future (for instance, how the model woulddevelop over time and during learning), because theymight affect theoretical perspective and focus.
",False,dijkstra2002,False,False,True
8490,"4 The BIA+ model for bilingual word recognition
",False,dijkstra2002,False,False,True
8491,"The BIA+ model is graphically represented in Figure
",False,dijkstra2002,False,False,True
8492,"2. It incorporates the BIA model as a special case,except that the function of the language nodes isadapted. The BIA+ model is also strongly affected byGreen's (1998) ideas on task schemas and taskcontrol. This is evident in the distinction between aword identi®cation system and a task/decision
",False,dijkstra2002,False,False,True
8493,"system. Green's (1998) Inhibitory Control (IC) model
",False,dijkstra2002,False,False,True
8494,"speci®es the control that bilinguals have over theprocessing in their lexico-semantic system in differenttask conditions. Both Green (1998) and Dijkstra(1998) have pointed out that the relationship betweenthe IC model and the BIA model is rather comple-mentary, with a larger focus on the task schema andbilingual language production in the IC model and
",False,dijkstra2002,False,False,True
8495,"on the bilingual lexico-semantic system and compre-
",False,dijkstra2002,False,False,True
8496,"hension in the BIA model.
",False,dijkstra2002,False,False,True
8497,"In separate sections, we will consider the BIA+
",False,dijkstra2002,False,False,True
8498,"model with respect to the following points: (1) Repre-sentation and processing of orthographic, phono-logical and semantic codes; (2) Representation ofinterlingual homographs and cognates; (3) Languagenodes; (4) Linguistic context effects; (5) Non-lin-181 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150guistic context effects; (6) Relationship between word
",False,dijkstra2002,False,False,True
8499,"identi®cation and task demands; (7) Stimulus-response binding in lexical decision; (8) Stimulus-response binding in language switching.
",False,dijkstra2002,False,False,True
8500,"4.1 Representation and processing of orthographic,
",False,dijkstra2002,False,False,True
8501,"phonological and semantic codes
",False,dijkstra2002,False,False,True
8502,"Just like the BIA model, the BIA+ model proposes
",False,dijkstra2002,False,False,True
8503,"that the bilingual lexicon is integrated acrosslanguages and is accessed in a language non-selectiveway. However, the BIA+ model extends theseassumptions from orthographic representations tophonological and semantic representations. In other
",False,dijkstra2002,False,False,True
8504,"words, bilingual word recognition is affected not only
",False,dijkstra2002,False,False,True
8505,"by cross-linguistic orthographic similarity effects, butalso by cross-linguistic phonological and semanticoverlap. This is in line with several recent studiesdemonstrating profoundly language non-selectiveeffects with respect to all three codes (e.g., Dijkstra,Grainger and Van Heuven, 1999; Jared and Kroll,2001).Implementing phonological and semantic repre-
",False,dijkstra2002,False,False,True
8506,"sentations in an interactive activation model,however, poses serious problems for the modeler.For instance, in English the mapping between letters(graphemes) and phonemes is context sensitive, im-plying that a sublexical representation in terms ofsuch units alone will not suf®ce. This is a well-known
",False,dijkstra2002,False,False,True
8507,"problem in the monolingual domain, for which
",False,dijkstra2002,False,False,True
8508,"several tentative solutions have been proposed(Dijkstra, in press). In a recent interactive activationmodel for monolingual word recognition, weincluded a new representational layer of orthographicand phonological representations as a solution to thisproblem, using the so-called Onset±Nucleus±Coda(ONC) scheme for both code types (Plaut, Mc-
",False,dijkstra2002,False,False,True
8509,"Clelland, Seidenberg and Patterson, 1996; Van
",False,dijkstra2002,False,False,True
8510,"Heuven, 2000). According to this scheme, a word likeSTRAND, for instance, can be represented by anonset cluster STR, a vowel nucleus A, and a codaND. Assuming this type of representation for bothorthography and phonology simpli®es the imple-mentation of a context sensitive mapping between thetwo types of codes. Preliminary simulation work
",False,dijkstra2002,False,False,True
8511,"indicates that this implementation type is successful
",False,dijkstra2002,False,False,True
8512,"in accounting for phonological effects (e.g., consist-ency effects) in monolingual word recognition, butmore work is necessary to assess the feasibility of thisapproach for implementing phonological codes in theBIA+ model.
",False,dijkstra2002,False,False,True
8513,"Activation of orthographic codes
",False,dijkstra2002,False,False,True
8514,"When an input letter string is presented to the BIA+
",False,dijkstra2002,False,False,True
8515,"model, the ®rst stages of word recognition proceed inthe same fashion as in the BIA model. A number oflexical orthographic candidates are activated inparallel depending on their similarity to the inputstring, and on the resting level activation of theindividual items (and therefore dependent on sub-jective frequency, recency of use, L2 pro®ciency,
",False,dijkstra2002,False,False,True
8516,"etc.). Because L2 representations are on average of a
",False,dijkstra2002,False,False,True
8517,"lower subjective frequency than L1 codes, they areactivated somewhat more slowly than L1 representa-tions. Next, activated orthographic word candidatesactivate their corresponding phonological, semanticand other (e.g., articulatory) representations.
",False,dijkstra2002,False,False,True
8518,"According to the model it is the similarity of the
",False,dijkstra2002,False,False,True
8519,"input word to the internal lexical representations that
",False,dijkstra2002,False,False,True
8520,"determines their activation, not the word's language
",False,dijkstra2002,False,False,True
8521,"membership. The larger the overlap between theinput string and a representation in the mentallexicon, the more the internal representation is acti-vated. As a consequence, in the case of two languageswith alphabetical writing systems, the number ofactivated orthographic candidates is determined byfactors such as the neighborhood density and fre-· Specific processing steps for task in hand
",False,dijkstra2002,False,False,True
8522,"· Receives continuous input from the
",False,dijkstra2002,False,False,True
8523,"  identification system
",False,dijkstra2002,False,False,True
8524,"· Decision criteria determine when a
",False,dijkstra2002,False,False,True
8525,"  response is made based on relevant  codesTask schema
",False,dijkstra2002,False,False,True
8526,"Identification system
",False,dijkstra2002,False,False,True
8527,"L1/L2Language nodes
",False,dijkstra2002,False,False,True
8528,"Lexical Orthography
",False,dijkstra2002,False,False,True
8529,"Sublexical OrthographySemantics
",False,dijkstra2002,False,False,True
8530,"Lexical Phonology
",False,dijkstra2002,False,False,True
8531,"Sublexical Phonology
",False,dijkstra2002,False,False,True
8532,"Figure 2. The BIA+ model for bilingual word recognition.
",False,dijkstra2002,False,False,True
8533,"Arrows indicate activation ¯ows between representational
",False,dijkstra2002,False,False,True
8534,"pools. Inhibitory connections within pools are omitted.
",False,dijkstra2002,False,False,True
8535,"Language nodes could instead be attached to lemmarepresentations between word form and meaningrepresentations. Non-linguistic context only affects the task
",False,dijkstra2002,False,False,True
8536,"schema level.182 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150quency of the target word and its within- and
",False,dijkstra2002,False,False,True
8537,"between-language neighbors. If the two languagesdiffer with respect to their input codes (e.g., lettersets), the activated set of neighbors may becomemuch smaller. For instance, assume that a presenteditem contains language speci®c bigrams or diacriticalmarkers. Due to their presence, the initial set of
",False,dijkstra2002,False,False,True
8538,"lexical candidates that is activated may become
",False,dijkstra2002,False,False,True
8539,"restricted to one language (cf. Mathey and Zagar,2000, p. 200). This is a bottom-up effect. We deem itunlikely that for words that are well-known by thebilingual this kind of information is often used in atop-down way (e.g., the bilingual noticing a par-ticular language speci®c bigram and using it forlanguage decision), because the bottom-up recog-
",False,dijkstra2002,False,False,True
8540,"nition process will usually be much faster. (Of course,
",False,dijkstra2002,False,False,True
8541,"if all German words in a language decision task beginwith a capital letter, then participants can rede®nethe task of responding to words as one in which theyrespond to capital letters.)
",False,dijkstra2002,False,False,True
8542,"Note that it follows logically that no ``ortho-
",False,dijkstra2002,False,False,True
8543,"graphically similar'' word candidates can be activatedacross language pairs that do not share orthography
",False,dijkstra2002,False,False,True
8544,"at all (e.g., Chinese and English), even though effects
",False,dijkstra2002,False,False,True
8545,"of phonological similarity might still occur for suchlanguage pairs. In other words, when particular inputaspects are language speci®c, we will (of course) ®ndevidence of language speci®c access (e.g., Chineseorthography will not induce much Latin letteractivation).
",False,dijkstra2002,False,False,True
8546,"For interlingual homographs and cognates with
",False,dijkstra2002,False,False,True
8547,"different orthographic forms (and therefore two
",False,dijkstra2002,False,False,True
8548,"representations) across languages, the degree of codeactivation of the non-target reading also depends onthe degree of cross-linguistic code overlap (seesection 4.2). This raises the question of how recog-nition of the Dutch word TOMAAT by Dutch-English bilinguals is affected by its similarity butnon-identity to the English word TOMATO. Van
",False,dijkstra2002,False,False,True
8549,"Hell and Dijkstra (in press) had trilinguals with
",False,dijkstra2002,False,False,True
8550,"Dutch as their L1, English as their L2 and French astheir L3 perform a word association task or a lexicaldecision task in their L1. Stimulus words were(mostly) non-identical cognates such as TOMAAT ornoncognates. Shorter association and lexical decisiontimes were observed for Dutch-English cognates thanfor noncognates. For trilinguals with a higher pro®-
",False,dijkstra2002,False,False,True
8551,"ciency in French, lexical decision responses were
",False,dijkstra2002,False,False,True
8552,"faster for both Dutch-English and Dutch-Frenchcognates. Thus, even when their orthographic andphonological overlap across languages is incomplete,cognates may be recognized faster than noncognates.
",False,dijkstra2002,False,False,True
8553,"For French-Spanish bilinguals, Font (2001) has
",False,dijkstra2002,False,False,True
8554,"found that in lexical decision cognates differing inone letter between languages (called by her ``neighborcognates'') are still facilitated but signi®cantly less so
",False,dijkstra2002,False,False,True
8555,"than identical cognates. Furthermore, the amount offacilitation observed depended on the position of thedeviating letter in the word. Neighbor cognatesdiffering at the end of the word (e.g., French TEXTE± Spanish TEXTO) were facilitated more than neigh-bor cognates with the different letter inside (e.g.,
",False,dijkstra2002,False,False,True
8556,"French USUEL ± Spanish USUAL). In fact, facili-
",False,dijkstra2002,False,False,True
8557,"tatory effects for the latter type of cognate dis-appeared and effects tended towards inhibition whensuch cognates were of low frequency in bothlanguages. Similar patterns of results were found inL1 and L2 processing. These results suggest that thesize of cognate and interlingual homograph effectsdepends on their degree of cross-linguistic overlap
",False,dijkstra2002,False,False,True
8558,"(cf. Cristoffanini, Kirshner and Milech, 1986), just
",False,dijkstra2002,False,False,True
8559,"like the BIA+ model predicts. However, in order toaccount for position-speci®c effects of mismatches,the letter to word connections in the BIA+ modelmust be differentiated with respect to position, or anew letter coding scheme must be introduced (alsosee Dijkstra, in press).
",False,dijkstra2002,False,False,True
8560,"Activation of phonological and semantic codes
",False,dijkstra2002,False,False,True
8561,"When (sublexical and lexical) orthographic represent-ations become active, they start to activate associatedphonological and semantic representations. As aconsequence, such phonological and semantic repre-sentations of both languages are activated slightlylater during word reading than the orthographicrepresentations of these languages (Ferrand and
",False,dijkstra2002,False,False,True
8562,"Grainger, 1993). Because their activation will
",False,dijkstra2002,False,False,True
8563,"depend, among other factors, on subjective fre-quency, this implies that L2 phonological andsemantic codes will be delayed in activation relativeto L1 codes. We will call this the ``temporal delayassumption''. However, the different codes will ofteninteract (resonate) over time and their respectiveidenti®cation will take place close in time. As a
",False,dijkstra2002,False,False,True
8564,"consequence of time course differences, the relative
",False,dijkstra2002,False,False,True
8565,"contribution of codes to the decision may bemodulated by adapting temporal deadlines ratherthan by re®guring the task schema. For instance,when pseudohomophones are present in a mono-lingual stimulus list, phonological effects could beattenuated or excluded by speeding up RTs andbasing the response on the orthographic representa-
",False,dijkstra2002,False,False,True
8566,"tion only. However, mutual activation of different
",False,dijkstra2002,False,False,True
8567,"codes may make phonological effects dif®cult toeliminate.
",False,dijkstra2002,False,False,True
8568,"Two consequences of the temporal delay assump-
",False,dijkstra2002,False,False,True
8569,"tion are that (1) cross-linguistic effects will generallybe larger from L1 to L2 than in the opposite direc-tion; (2) an absence of cross-linguistic phonologicaland semantic effects for different words could occur183 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150if task demands allow responding to faster codes (for
",False,dijkstra2002,False,False,True
8570,"instance, orthographic L1 codes), giving slower codesno chance to affect the response times.
",False,dijkstra2002,False,False,True
8571,"2The system-
",False,dijkstra2002,False,False,True
8572,"atic task dependence of cross-linguistic effects isshown clearly in a number of recent empirical studies(e.g., Dijkstra et al., 1998(b); Dijkstra, Grainger andVan Heuven, 1999; Lemho Èfer and Dijkstra, sub-
",False,dijkstra2002,False,False,True
8573,"mitted).
",False,dijkstra2002,False,False,True
8574,"We will illustrate this point by comparing the
",False,dijkstra2002,False,False,True
8575,"study by Dijkstra et al. (1999) with that by Lemho Èfer
",False,dijkstra2002,False,False,True
8576,"and Dijkstra (submitted). Dijkstra, et al. (1999)examined the effects of different types of codeoverlap in interlingual homographs and cognate pro-cessing. Dutch-English bilinguals performed anEnglish lexical decision task with English words
",False,dijkstra2002,False,False,True
8577,"varying in their degree of orthographic (O), phono-
",False,dijkstra2002,False,False,True
8578,"logical (P), and semantic (S) overlap with Dutchwords. Their six different test conditions are exempli-®ed by the following items: ``hotel'' (overlap in S, Oand P codes), ``type'' (SO), ``news'' (SP), ``step'' (OP),``stage'' (O) and ``note'' (P). The ®rst two conditions(SOP and SO conditions) may be called ``cognates'',while the last three conditions contain ``interlingual
",False,dijkstra2002,False,False,True
8579,"homographs'' (OP and O conditions) and ``inter-
",False,dijkstra2002,False,False,True
8580,"lingual homophones'' (P condition). Lexical decisionswere facilitated by cross-linguistic orthographic andsemantic similarity relative to control words thatbelonged only to English. In contrast, phonologicaloverlap produced inhibitory effects. A very similarpattern of results was found using a different task(progressive demasking), but no systematic differ-
",False,dijkstra2002,False,False,True
8581,"ences between test and control conditions arose for
",False,dijkstra2002,False,False,True
8582,"American English monolinguals.
",False,dijkstra2002,False,False,True
8583,"We conclude that in this English lexical decision
",False,dijkstra2002,False,False,True
8584,"task, where L2 (English) was the target language,cross-linguistic effects arose for L1-L2 (Dutch-English) homographs with respect to all three typesof representations. Because English was the targetlanguage in this task, a proper or safe task execution
",False,dijkstra2002,False,False,True
8585,"implied verifying the English language membership
",False,dijkstra2002,False,False,True
8586,"of possible word candidates, even when Dutch codeswould have become available faster than Englishones. In other words, Dutch codes had time toestablish themselves and exert effects on later avail-able English codes that were necessary forresponding.
",False,dijkstra2002,False,False,True
8587,"Lemho Èfer and Dijkstra (submitted) presented the
",False,dijkstra2002,False,False,True
8588,"same stimulus materials to Dutch-English bilinguals
",False,dijkstra2002,False,False,True
8589,"in a generalized lexical decision task. In this task,
",False,dijkstra2002,False,False,True
8590,"participants responded with ``yes'' to both Englishand Dutch words, but with ``no'' to nonwords. In
",False,dijkstra2002,False,False,True
8591,"contrast to English lexical decision, participants inthis task can use both Dutch and English lexicalrepresentations as a reliable basis for responding.Thus, in this task, cross-linguistic effects will ariseonly to the extent that L1 and L2 codes can affecteach other before the fastest codes (usually Dutch
",False,dijkstra2002,False,False,True
8592,"ones, according to the temporal delay assumption)
",False,dijkstra2002,False,False,True
8593,"are retrieved and responded to. The results of thisstudy were straightforward: no facilitation effectsarose for interlingual homographs relative to con-trols, while cognates were facilitated. The pattern ofresults for homographs indicates that responses werebased upon the fastest available code, usually theDutch orthographic code, while cross-linguistic
",False,dijkstra2002,False,False,True
8594,"overlap with respect to semantics in the case of
",False,dijkstra2002,False,False,True
8595,"cognates apparently can be used to speed up theresponse. This implies a special type of representationfor cognates, possibly with a strong feedback connec-tion from semantics to orthography (cf. Pecher, 2001;Reimer, Brown and Lorsbach, 2001).
",False,dijkstra2002,False,False,True
8596,"The result patterns of the two studies are
",False,dijkstra2002,False,False,True
8597,"accounted for in the BIA+ model by assuming that
",False,dijkstra2002,False,False,True
8598,"the task differences can lead to a different use of
",False,dijkstra2002,False,False,True
8599,"lexical codes while the activation pattern in the wordidenti®cation system itself is not changed. Thisassumption is more parsimonious (and easier to test)than the assumption of changes in both identi®cationand task/decision systems (an assumption that ismade by the BIA model).
",False,dijkstra2002,False,False,True
8600,"4.2 Representation of interlingual homographs and
",False,dijkstra2002,False,False,True
8601,"cognates
",False,dijkstra2002,False,False,True
8602,"Because the BIA model included only orthographic
",False,dijkstra2002,False,False,True
8603,"representations, simulations involving orthographic-ally identical interlingual homographs could only beconducted if two representations were assumed forsuch items (see section 2.5). This choice, however,
",False,dijkstra2002,False,False,True
8604,"was practically motivated and not based on solid
",False,dijkstra2002,False,False,True
8605,"empirical evidence. Note that multiple representa-tions for form-identical items are not necessarilyincompatible with an integrated lexicon account,even though they ®t more naturally within a bilingualword recognition account that assumes separatelexicons for each language. In this section, we willinvestigate this issue in some detail. As an example,
",False,dijkstra2002,False,False,True
8606,"do ROOM (a Dutch-English interlingual homo-
",False,dijkstra2002,False,False,True
8607,"graph) and FILM (a Dutch-English cognate) haveshared or distinct representations in English andDutch?
",False,dijkstra2002,False,False,True
8608,"A comparison of recent studies suggests that
",False,dijkstra2002,False,False,True
8609,"orthographically identical interlingual homographshave distinct orthographic representations in eachlanguage (Dijkstra et al., 1998(b); Dijkstra et al.,
",False,dijkstra2002,False,False,True
8610,"2There are several other ways in which these null-effects may have
",False,dijkstra2002,False,False,True
8611,"arisen. They may, for instance, be due to participant strategies orthe effects of different codes (e.g., orthography and phonology)
",False,dijkstra2002,False,False,True
8612,"may cancel each other.184 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.1501999; Lemho Èfer and Dijkstra, submitted). In the
",False,dijkstra2002,False,False,True
8613,"previous section, we discussed the study by Dijkstraet al. (1999), who found cross-linguistic effects forL1-L2 (Dutch-English) homographs with respect toorthographic, phonological and semantic codes in abilingual lexical decision task where L2 (English) wasthe target language. Dijkstra et al. (1998(b)) manipu-
",False,dijkstra2002,False,False,True
8614,"lated a different aspect of interlingual homographs,
",False,dijkstra2002,False,False,True
8615,"the relative frequency of the two readings of thehomographs. In their second experiment, Dutch-English bilinguals also performed an English lexicaldecision on interlingual homographs, exclusivelyEnglish control items, and nonwords. The experi-mental stimuli also included Dutch words requiring a``no'' response (i.e., the Dutch words had to be
",False,dijkstra2002,False,False,True
8616,"treated as ``English nonwords''). Relative to English
",False,dijkstra2002,False,False,True
8617,"control items, strong inhibitory effects were obtainedfor interlingual homographs that were dependent onthe relative frequency difference of the two readingsof the homograph. The inhibitory effect was largewhen the Dutch reading of the homographs had ahigh frequency relative to the English reading.
",False,dijkstra2002,False,False,True
8618,"The results of these two studies are paradoxical.
",False,dijkstra2002,False,False,True
8619,"On the one hand, the observed cross-linguistic facili-
",False,dijkstra2002,False,False,True
8620,"tatory effects for orthographic identity and semanticoverlap in interlingual homographs suggest that suchrepresentations are shared for items occurring in twolanguages. Shared representations for interlingualhomographs within an integrated lexicon ®t wellwithin an interactive activation account of bilingualword recognition. On the other hand, the observed
",False,dijkstra2002,False,False,True
8621,"differential contributions of the Dutch and English
",False,dijkstra2002,False,False,True
8622,"frequencies of interlingual homographs in the secondstudy suggest that these items have different lexicalrepresentations, each characterized by its frequencyin the language it belongs to.
",False,dijkstra2002,False,False,True
8623,"Is there any evidence that allows us to distinguish
",False,dijkstra2002,False,False,True
8624,"views that assume one (orthographic) representationor two (orthographic) representations for interlingual
",False,dijkstra2002,False,False,True
8625,"homographs? If there is only one orthographic repre-
",False,dijkstra2002,False,False,True
8626,"sentation, (orthographic) frequency effects should becumulative under circumstances in which both read-ings of the homograph can be used for responding.This should be the case then in a generalized lexicaldecision task, such as that of Experiment 3 in Dijk-stra et al. (1998(b)). In this task, participants canrespond to the fastest identi®ed reading of the inter-
",False,dijkstra2002,False,False,True
8627,"lingual homograph and do not need to make a
",False,dijkstra2002,False,False,True
8628,"distinction in order to perform the task. The one-representation hypothesis predicts, ®rst, that theresponses to interlingual homographs should befaster than the fastest of the control conditions.Second, the cumulative frequency effect should beespecially visible if, keeping the frequency of onereading of the homograph constant, the frequency ofthe other reading is varied. The obtained result
",False,dijkstra2002,False,False,True
8629,"pattern, however, was not in line with this view. Firstof all, the facilitation effect for the HFE-LFD con-dition is only 9 ms relative to the English controlcondition, and for the HFE-HFD condition just 12ms relative to the Dutch control condition. Further-more, for the LFE-HFD condition a 22 ms inhibition
",False,dijkstra2002,False,False,True
8630,"effect arose relative to the fast Dutch control con-
",False,dijkstra2002,False,False,True
8631,"dition, and for the LFE-LFD condition even 28 msof inhibition arose. The relatively small facilitation inthe ®rst two conditions could be accounted for byassuming that the cumulative frequency effects areonly limited because log frequency rather than fre-quency is important; however, the inhibitory resultsfor the low-frequency conditions cannot be explained
",False,dijkstra2002,False,False,True
8632,"in such a way.
",False,dijkstra2002,False,False,True
8633,"The available results are easier to account for in
",False,dijkstra2002,False,False,True
8634,"the two-representations view because, according tothis view, responses to interlingual homographscannot only be slower than to controls, but to alimited extent also faster as an effect of ``statisticalfacilitation'' (Raab, 1962). Statistical facilitation canarise if the response time is determined by the ®rst
",False,dijkstra2002,False,False,True
8635,"available reading of the homograph. In most trials
",False,dijkstra2002,False,False,True
8636,"this would be that of the high-frequency reading ofthe homograph, but sometimes the low-frequencyreading might win the race, making the resultingmean RT just somewhat faster than that of the high-frequency control (also see Lemho Èfer and Dijkstra,
",False,dijkstra2002,False,False,True
8637,"submitted).
",False,dijkstra2002,False,False,True
8638,"What can be said about the representation of
",False,dijkstra2002,False,False,True
8639,"cognates, i.e. interlingual homographs that have both
",False,dijkstra2002,False,False,True
8640,"orthographic and meaning overlap across language?The available studies suggest that cognates have aspecial representation. Apart from the studies by VanHell and Dijkstra (in press), Dijkstra et al. (1999) andLemho Èfer and Dijkstra (submitted) that we have
",False,dijkstra2002,False,False,True
8641,"already discussed, there are many earlier studiessupporting this suggestion. For instance, Kirsner and
",False,dijkstra2002,False,False,True
8642,"colleagues (e.g., Lalor and Kirsner, 2000), and
",False,dijkstra2002,False,False,True
8643,"SaÂnchez-Casas and colleagues (e.g., Sa Ânchez-Casas,
",False,dijkstra2002,False,False,True
8644,"Davis and Garcã Âa-Albea, 1992) have proposed that
",False,dijkstra2002,False,False,True
8645,"cognates may have a common morphemic represent-ation across languages. However, more detailedinformation with respect to the relationship betweentheir orthographic, phonological and semantic codesis necessary before a complete cognate representation
",False,dijkstra2002,False,False,True
8646,"can be implemented.
",False,dijkstra2002,False,False,True
8647,"To conclude, the presently available evidence
",False,dijkstra2002,False,False,True
8648,"favors the view that interlingual homographs arerepresented by two (possibly partially overlapping)representations rather than one. Thus, it seemsreasonable to continue with BIA+ simulationsassuming two representations for interlingual homo-graphs rather than one. Additional evidence about185 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150cognate representation is necessary in order to imple-
",False,dijkstra2002,False,False,True
8649,"ment these in models like BIA+.
",False,dijkstra2002,False,False,True
8650,"4.3 Language nodes
",False,dijkstra2002,False,False,True
8651,"In section 2, we discussed the various functions of the
",False,dijkstra2002,False,False,True
8652,"language nodes in the BIA model. We made two
",False,dijkstra2002,False,False,True
8653,"distinctions: between linguistic and non-linguistic
",False,dijkstra2002,False,False,True
8654,"functions of the language nodes, and betweenbottom-up (word to language) and top-down(language to word) components of these functions. Inan earlier paper on the BIA model (Dijkstra and VanHeuven, 1998), we already indicated that combiningall these aspects in one mechanism was probably tooambitious. Since that time, evidence has accrued
",False,dijkstra2002,False,False,True
8655,"suggesting that the linguistic and non-linguistic func-
",False,dijkstra2002,False,False,True
8656,"tions of the language nodes must be assigned todifferent levels of processing and that they mayoperate in a different way. More speci®cally, wepropose to restrict the language nodes' functions tolanguage membership representations within theidenti®cation system of which the activation levelalso re¯ects global lexical activity (because all acti-
",False,dijkstra2002,False,False,True
8657,"vated words of one language feed activation forward
",False,dijkstra2002,False,False,True
8658,"to a language node). Being just representations, thelanguage nodes can no longer function as language®lters dependent on experimental factors nor ascollectors of non-linguistic contextual pre-activation.
",False,dijkstra2002,False,False,True
8659,"Differences between result patterns across experi-
",False,dijkstra2002,False,False,True
8660,"ments and non-linguistic context effects now need tobe accounted for at a different level. We propose that
",False,dijkstra2002,False,False,True
8661,"these effects arise at the level of task speci®cation and
",False,dijkstra2002,False,False,True
8662,"parameter settings for the decision in a task (seeFigure 2). In other words, we propose a distinctionbetween a word identi®cation system and a task/decision system, analogous to Green (1998). Cross-experimental differences, cross-trial differences orparticipant expectations may be handled by adecision mechanism affecting the output of the word
",False,dijkstra2002,False,False,True
8663,"identi®cation system before the response is made.
",False,dijkstra2002,False,False,True
8664,"This proposal changes the nature of the language
",False,dijkstra2002,False,False,True
8665,"nodes, but they remain present as a useful constructin the model. Language users know to whichlanguage a word belongs, so there must be some sortof language tag or language membership representa-tion. In language decision, participants are assumedto retrieve this representation to decide whether they
",False,dijkstra2002,False,False,True
8666,"should push one button or another. They cannot do
",False,dijkstra2002,False,False,True
8667,"this just by identifying the item, because even thoughit may very clearly belong to one language only, it isnot clear which language that is before the tag hasbeen retrieved. The language membership infor-mation could be retrieved via the item's lexeme(orthographic or phonological) or lemma (moreabstract syntactic/semantic) representation. For thesake of simplicity, the model currently does not
",False,dijkstra2002,False,False,True
8668,"incorporate lemma representations, implying that thelanguage nodes are directly connected to lexical formrepresentations. According to this viewpoint, therelative activation of the language nodes (somewould say ``of the languages'') is completely depend-ent upon activation arriving from other linguistic
",False,dijkstra2002,False,False,True
8669,"representations, e.g., current lexical input and pre-
",False,dijkstra2002,False,False,True
8670,"vious sentence context.
",False,dijkstra2002,False,False,True
8671,"Studies indicate that language information
",False,dijkstra2002,False,False,True
8672,"becomes available rather late during (isolated) bi-lingual visual word recognition, usually too late toaffect the word selection process. An example is thestudy by Dijkstra et al. (2000(b)), discussed alreadyin section 2.5. Target-language homographs were
",False,dijkstra2002,False,False,True
8673,"often ``overlooked'' in a language go/no-go task if the
",False,dijkstra2002,False,False,True
8674,"frequency of their other-language component washigh. A ¯attening of the cumulative RT distributiontowards an asymptotic value in these experimentssuggests that recognition of the homograph readingfrom the non-target language in some way ``prohib-ited'' the subsequent recognition of the targetlanguage reading (e.g., after recognition, all other
",False,dijkstra2002,False,False,True
8675,"lexical candidates may be suppressed). Thus, selec-
",False,dijkstra2002,False,False,True
8676,"tion of one reading of the interlingual homographstook place rather late during processing. Clearly, thesystem must at some time select only one lexical item,but apparently the language of that item could notaid that selection. More likely, determination of theitem's language depended on lexical selection havingtaken place. In addition, it did not seem possible to
",False,dijkstra2002,False,False,True
8677,"focus on the target reading only and discard the non-
",False,dijkstra2002,False,False,True
8678,"target language reading. One reason for this may be atendency that the word rather than its language labeltriggers the response (see also section 4.7).
",False,dijkstra2002,False,False,True
8679,"In section 4.5 we will discuss some other studies on
",False,dijkstra2002,False,False,True
8680,"word reading that suggest that the in¯uence oflanguage membership on item recognition is rela-tively small, implying it is generally available too late
",False,dijkstra2002,False,False,True
8681,"to affect word identi®cation. If this observation
",False,dijkstra2002,False,False,True
8682,"proves to be true in further research, it means thatthe feedback parameter from language nodes to theword level must be set rather low. One may wonderhow this can come about in an interactive activationmodel such as BIA+. A possibility (borrowed fromthe monolingual language production literature) liesin the nature of the mapping between words and
",False,dijkstra2002,False,False,True
8683,"language membership. Each word is connected to
",False,dijkstra2002,False,False,True
8684,"one language node, but each language node is con-nected to thousands of words. Therefore, if theamount of activation that can be distributed betweenunits at different levels is a constant, the feedbackactivation from language node to word level will bemuch smaller per word unit than the feedforwardactivation from word level to language node.186 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.1504.4 Linguistic context effects
",False,dijkstra2002,False,False,True
8685,"The BIA+ model makes a distinction between a word
",False,dijkstra2002,False,False,True
8686,"identi®cation system incorporating linguistic repre-sentations, and a task/decision system incorporatingnon-linguistic task schema speci®cations (see Figure2). In line with this distinction, it is assumed that
",False,dijkstra2002,False,False,True
8687,"linguistic context is able in principle to directly affect
",False,dijkstra2002,False,False,True
8688,"the word identi®cation system, while non-linguisticcontext affects (settings in) the task/decision system.Linguistic context effects are de®ned here as theeffects arising from lexical, syntactic or semanticsources (e.g., sentence context), whereas non-linguistic context effects are those effects that arisefrom instruction, task demands or participant expec-
",False,dijkstra2002,False,False,True
8689,"tancies.
",False,dijkstra2002,False,False,True
8690,"3The automatically operating word identi®-
",False,dijkstra2002,False,False,True
8691,"cation system is part of a much larger Language Usersystem in which sentence parsing and language pro-duction also have a place (Dijkstra and Kempen,1984; Levelt, Roelofs and Meyer, 1999). We proposethat the word identi®cation system is interactive notonly with respect to different codes but also relativeto the sentence parsing system. As a consequence,
",False,dijkstra2002,False,False,True
8692,"BIA+ predicts that the recognition of words in
",False,dijkstra2002,False,False,True
8693,"sentence context is sensitive to syntactic and semanticcontext information from different languages, in away that is analogous to monolingual word recogni-tion in sentence context. In fact, such linguisticcontext information may exert serious constraints onthe degree of language selective access that may beobserved.
",False,dijkstra2002,False,False,True
8694,"At present, few studies have addressed the bi-
",False,dijkstra2002,False,False,True
8695,"linguals' recognition of words in sentence context(Altarriba, Kroll, Sholl and Rayner, 1996; Li, 1996;Elston-Gu Èttler and Williams, submitted), but their
",False,dijkstra2002,False,False,True
8696,"results are in line with this viewpoint. We will discusstwo of these here. In their ®rst experiment, Altarribaet al. monitored the eye movements of Spanish-English bilinguals while they were reading English
",False,dijkstra2002,False,False,True
8697,"(L2) sentences that contained either an English (L2)
",False,dijkstra2002,False,False,True
8698,"or a Spanish (L1) target word. Sentences providedeither high or low semantic constraints on the targetwords. An example sentence of the high constraintand Spanish target condition is ``He wanted todeposit all his dinero at the credit union'', where
",False,dijkstra2002,False,False,True
8699,"dinero is Spanish for ``money''. The experiment led to
",False,dijkstra2002,False,False,True
8700,"an interaction between the frequency of the target
",False,dijkstra2002,False,False,True
8701,"word and degree of sentence constraint for Spanish
",False,dijkstra2002,False,False,True
8702,"target words with respect to the ®rst ®xation dura-tion, but not for English target words. Thus, whenthe Spanish target words were of high frequency andappeared in highly constrained sentences, the partici-pants apparently experienced interference. This result
",False,dijkstra2002,False,False,True
8703,"suggests that sentence constraint in¯uences not onlythe generation of semantic feature restrictions forupcoming words, but also that of lexical features.The high-frequency Spanish word matched the gener-ated set of semantic features, but not the expectedlexical features when the word appeared in the alter-
",False,dijkstra2002,False,False,True
8704,"nate language (Altarriba et al., 1996, p. 483). The
",False,dijkstra2002,False,False,True
8705,"same pattern of results was found in a second experi-ment, where the sentences were presented word byword using the rapid serial visual presentation(RSVP) technique and participants named thecapitalized target word in each sentence.
",False,dijkstra2002,False,False,True
8706,"The ®ndings of this study are in line with the
",False,dijkstra2002,False,False,True
8707,"BIA+ model from at least two perspectives. First,
",False,dijkstra2002,False,False,True
8708,"linguistic sentence context was found to interact with
",False,dijkstra2002,False,False,True
8709,"target word recognition, as predicted by the model.Second, note that the observed data pattern showedan interaction of word frequency (a lexical infor-mation source) and the sentence constraint, and notof language membership and the sentence constraint.This suggests that ( just like for isolated words)lexical characteristics are more important than
",False,dijkstra2002,False,False,True
8710,"language characteristics in the determination of word
",False,dijkstra2002,False,False,True
8711,"recognition in sentences.
",False,dijkstra2002,False,False,True
8712,"Future studies should focus on disentangling such
",False,dijkstra2002,False,False,True
8713,"effects of lexical form features and language member-ship in sentence processing experiments. They shouldexamine, for instance, to which extent the languageitself of preceding words in the sentence can modu-late the activation of target word candidates from a
",False,dijkstra2002,False,False,True
8714,"non-target language. Two viewpoints can be con-
",False,dijkstra2002,False,False,True
8715,"trasted. One option is that the language nodes can bepre-activated by the sentence context and mayfunction as a link between sentence and lexical levels.Via this link, the language of the preceding sentencecontext could affect the recognition of a target itemindependent of any additional syntactic or semanticeffects. Another option is that such preactivation will
",False,dijkstra2002,False,False,True
8716,"not be effective, because language nodes cannot
",False,dijkstra2002,False,False,True
8717,"activate or suppress word activation to any consider-able extent. This second option currently seems to bethe most in line with the BIA+ model (and the studyby Altarriba et al.), because it ®ts the argument putforward in section 4.3 that language informationdoes not provide strong selection constraints onbilingual word recognition.
",False,dijkstra2002,False,False,True
8718,"The second study we will discuss here is that by
",False,dijkstra2002,False,False,True
8719,"Elston-Gu Èttler and Williams (submitted). It is of
",False,dijkstra2002,False,False,True
8720,"direct relevance to the BIA+ model, because theycompared the bilingual processing of interlingualhomographs in isolated word lists (see sections 2.5,2.6 and 4.2) to that of the same stimulus materialsincorporated in sentences. The authors ®rst showedthat, in word lists, visually-presented German-
",False,dijkstra2002,False,False,True
8721,"3Effects of stimulus list composition could, in principle, derive
",False,dijkstra2002,False,False,True
8722,"from both sources.187 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150English interlingual homographs primed lexical
",False,dijkstra2002,False,False,True
8723,"decisions on the English translation of their Germanreading (cf. De Moor, 1998; Van Heste, 1999). Forexample, the interlingual homograph ``gift'' primedthe target word ``poison''. Next, the prime word wasincorporated as the last word in a sentence like ``Thewoman gave her friend an expensive gift''. At a
",False,dijkstra2002,False,False,True
8724,"stimulus onset asynchrony (SOA) of 250 ms after the
",False,dijkstra2002,False,False,True
8725,"prime word, the target word ``poison'' was presented.In other conditions, the target item was a nonword.As before, the participants performed a lexicaldecision task on the target item. In contrast to thesigni®cant single word priming, no signi®cantpriming was obtained when the primes occurred insentence contexts biased towards their English
",False,dijkstra2002,False,False,True
8726,"reading. As discussed above, such a result is not
",False,dijkstra2002,False,False,True
8727,"incompatible with the assumption of the BIA+ modelthat sentence context can interact with the bilingualword identi®cation process.
",False,dijkstra2002,False,False,True
8728,"In a second experiment, Elston-Gu Èttler and
",False,dijkstra2002,False,False,True
8729,"Williams examined cases of L1-L2 relationship occur-ring at a deeper level than the form-level relationsexamined in Experiment 1. Using the same procedure
",False,dijkstra2002,False,False,True
8730,"as before, they presented German-English bilinguals
",False,dijkstra2002,False,False,True
8731,"with a sentence such as ``The lawyer tried very hardto defend his client in court'', followed by a targetword like ``meal''. In this case, the prime ``court'' canbe translated into German as ``Gericht''. TheGerman word ``Gericht'' is ambiguous, and can alsomean ``meal''. Interestingly enough, priming effectswere now obtained in both the single-word and
",False,dijkstra2002,False,False,True
8732,"sentence conditions. The authors argued that this
",False,dijkstra2002,False,False,True
8733,"suggests that L1 representations that are activated bytranslation links are less resistant to sentence andlanguage context constraints than those that dependon form-level similarity.
",False,dijkstra2002,False,False,True
8734,"The results of this study are to a considerable
",False,dijkstra2002,False,False,True
8735,"extent compatible with those obtained in the mono-lingual domain. Studies by Swinney and others (e.g.,
",False,dijkstra2002,False,False,True
8736,"Swinney, 1979; Tanenhaus, Leiman and Seidenberg,
",False,dijkstra2002,False,False,True
8737,"1979) originally suggested that even in the presenceof a strong biasing context, both readings of (intra-lingual) homographs were brie¯y activated. Laterresearch has complicated this picture by showing thatthe more frequent (dominant) meaning of an ambigu-ous word may be more accessible than the other one(e.g., Duffy, Morris and Rayner, 1988), and that
",False,dijkstra2002,False,False,True
8738,"sentential context may make the non-dominant
",False,dijkstra2002,False,False,True
8739,"meaning as accessible as the dominant one (Lucas,1999). Furthermore, under some conditions, only thecontextually appropriate meaning appears to beactivated (Tabossi and Zardon, 1993).
",False,dijkstra2002,False,False,True
8740,"BIA+ proposes to use such monolingual results to
",False,dijkstra2002,False,False,True
8741,"formulate predictions for bilingual research based onthe assumption that syntactic and semantic effectsare language non-selective, just as word recognition
",False,dijkstra2002,False,False,True
8742,"is language non-selective. For instance, syntacticconstraints will affect word recognition irrespectiveof the language of sentence context or target word,and syntactic lexical and phrasal categories may tosome extent be language independent. Thus, effectsof cross-linguistic syntactic priming between L1 and
",False,dijkstra2002,False,False,True
8743,"L2 are expected to occur.
",False,dijkstra2002,False,False,True
8744,"Thus, the BIA+ model proposes that linguistic
",False,dijkstra2002,False,False,True
8745,"context, in particular syntactic and semantic context,may directly affect bilingual word activation via theword identi®cation system. In the next section, weargue that non-linguistic information is more likelyto affect the task/decision system. In fact, we willargue that word activation is not modulated by non-
",False,dijkstra2002,False,False,True
8746,"linguistic context.
",False,dijkstra2002,False,False,True
8747,"4.5 Non-linguistic context effects
",False,dijkstra2002,False,False,True
8748,"In principle, non-linguistic context (such as the parti-
",False,dijkstra2002,False,False,True
8749,"cipants' expectations on the basis of instruction ortask demands) could affect a language non-selectiveword recognition system in several ways. One option
",False,dijkstra2002,False,False,True
8750,"is that both linguistic and non-linguistic context
",False,dijkstra2002,False,False,True
8751,"information can modulate the relative activation ofitems in the target and non-target language after theinitial stages of lexical processing. For instance,context information might inhibit (i.e., reduce theactivation of ) lexical candidates or lemmas in theirrelevant language (cf. BIA model by Dijkstra et al.,1998(a),(b); IC model by Green, 1986, 1998) or
",False,dijkstra2002,False,False,True
8752,"induce a ¯exible modulation of the relative activation
",False,dijkstra2002,False,False,True
8753,"of lexical candidates in the two languages (Grosjean,1997). A second option is that non-linguistic contextinformation does not affect the activity in the identi-®cation system itself, but leads to an adaptation ofdecision criteria only. A third option is that adapta-tions in both activation levels and decision criteriaare possible (as is the case in BIA and IC models).
",False,dijkstra2002,False,False,True
8754,"The BIA+ model proposes that (in contrast to
",False,dijkstra2002,False,False,True
8755,"linguistic effects) non-linguistic context effects inreading can be accounted for only by the secondoption. Thus, effects that are explained by relativelylate effects of top-down inhibition in the BIA modelare accounted for by decision criteria adaptations inBIA+. So far, there seems to be little pertinentevidence in favor of top-down inhibition of active
",False,dijkstra2002,False,False,True
8756,"lexical candidates on the basis of task demands or
",False,dijkstra2002,False,False,True
8757,"participant strategies. In other words, it appears thatthe bilingual word identi®cation system is encapsu-lated relative to decision level factors. Thus, forbilingual word recognition we reject the conclusiondrawn by Gerard and Scarborough (1989, p. 314)that ``bilinguals can exercise a considerable degree ofcontrol over access to lexical information''.188 Ton Dijkstra and Walter J. B. van Heuvenhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150How can the hypotheses of top-down inhibition or
",False,dijkstra2002,False,False,True
8758,"adaptation of decision criteria be contrasted? Theanswer depends in part on the speci®cation of eachview. Let us assume that top-down inhibition occursin a particular experimental situation if the suppres-sion of activation of items of one language leads to abetter overall performance. Suppression here means
",False,dijkstra2002,False,False,True
8759,"that the activation of these items is reduced and that,
",False,dijkstra2002,False,False,True
8760,"as a consequence, their in¯uence on other lexicalcandidates is minimized. We should then be able toobserve the effects of such top-down inhibition bycomparing two experimental situations that are assimilar as possible except that one of them is opti-mized if unwanted items are suppressed, while theother does not bene®t from (or is harmed by) such
",False,dijkstra2002,False,False,True
8761,"suppression.
",False,dijkstra2002,False,False,True
8762,"A few recent studies seem to allow such a com-
",False,dijkstra2002,False,False,True
8763,"parison, both within- and between-experiments. Forinstance, in the homograph studies by Dijkstra et al.(2000(b), Experiments 2 and 3) discussed earlier,participants would have given the fastest RTs tohomographs and control items if they had suppresseditems belonging to the non-target language (the same
",False,dijkstra2002,False,False,True
8764,"holds for Experiment 2 in Dijkstra et al., 1998(b)).
",False,dijkstra2002,False,False,True
8765,"Such an optimization by the participants would haveled to RTs to homographs that were just as fast as tocontrols. Instead, strong inhibition effects wereobtained indicating that suppression of the non-target language candidates was not possible. (The``inhibition effects'' that were observed re¯ectcompetition of the two readings of the interlingual
",False,dijkstra2002,False,False,True
8766,"homographs rather than the suppression of one of
",False,dijkstra2002,False,False,True
8767,"them. Thus, they are not ``top-down'' inhibitioneffects as de®ned above.) Analogously, Dutchphonology was not relevant for the English lexicaldecision to be made in the experiments in Dijkstra etal. (1999). Nevertheless, it was activated and inducedinhibitory effects on the response. In other words,parallel bottom-up activation of lexical candidates
",False,dijkstra2002,False,False,True
8768,"from both languages is so strong that suppressing it
",False,dijkstra2002,False,False,True
8769,"seems practically impossible.
",False,dijkstra2002,False,False,True
8770,"A similar argument is found in the English lexical
",False,dijkstra2002,False,False,True
8771,"decision study by Dijkstra, De Bruijn, Schriefers andTen Brinke (2000(a)). This experiment consisted oftwo parts. In part 1 of this study, which includedonly English words and nonwords, the RTs ofDutch-English bilinguals to interlingual homographs
",False,dijkstra2002,False,False,True
8772,"were just as fast as to English control items. In part 2,
",False,dijkstra2002,False,False,True
8773,"purely Dutch (non-target language) items were intro-duced (requiring a ``no'' response) that led to clearinhibition effects for interlingual homographs relativeto controls. However, RTs to English control itemswere only marginally affected by the transition(581 ms to 592 ms, signi®cant at p < .05 only in theitem analysis), indicating little change in activationgoing from part 1 to part 2. The observed small effect
",False,dijkstra2002,False,False,True
8774,"could be explained by a slight change in the decisionprocess, as a consequence of the extra caution neededto exclude purely Dutch words.
",False,dijkstra2002,False,False,True
8775,"Indeed, a comparison of the control conditions
",False,dijkstra2002,False,False,True
8776,"across experiments in Dijkstra et al. (1998(b)) and inDe Groot, Delmaar and Lupker (2000), strikingly
",False,dijkstra2002,False,False,True
8777,"con®rms this analysis. The three experiments in the
",False,dijkstra2002,False,False,True
8778,"study by Dijkstra et al. were conducted using differ-ent participants from one population and containedmostly the same test materials. Nevertheless, theexperiments showed strong and systematic effects oninterlingual homograph recognition of the inclusionof Dutch words (Experiment 2 vs. Experiment 1) andchanges in task demands (Experiments 1 and 2 vs.
",False,dijkstra2002,False,False,True
8779,"Experiment 3). In contrast, the mean RTs to the
",False,dijkstra2002,False,False,True
8780,"English control items across the three experimentswere affected much less by these changes. Similarly,De Groot et al. (2000) obtained very similar RTs forthe English control conditions in English lexicaldecision without (Experiment 2) and with Dutchwords (Experiment 3), and the same held for theDutch control conditions in Dutch lexical decision
",False,dijkstra2002,False,False,True
8781,"without and with English words. Rather than
",False,dijkstra2002,False,False,True
8782,"explaining the small differences between the controlconditions across the experiments of the two studiesin terms of relative language activation, it is moreelegant to explain them as small changes in thedecision process applied in the different experimentsor small variations in the participants' L2 pro®ciency.In sum, it does not seem to be possible to suppress
",False,dijkstra2002,False,False,True
8783,"the activation of one reading of an interlingual
",False,dijkstra2002,False,False,True
8784,"homograph even when that would undoubtedlyimprove performance.
",False,dijkstra2002,False,False,True
8785,"However, one could argue that the alternative
",False,dijkstra2002,False,False,True
8786,"reading of interlingual homographs is always comple-tely supported by input information. Perhaps thebottom-up support for these items is so strong that itoverrides any attempt at top-down inhibition. This is
",False,dijkstra2002,False,False,True
8787,"not the case for neighbors of the target item, so
",False,dijkstra2002,False,False,True
8788,"conceivably for neighborhood materials non-targetcandidates could be more easily suppressed.However, cross-experimental comparisons withrespect to neighborhood density suggest no suppres-sion either. Van Heuven et al. (1998) had Dutch-English participants of one population perform ablocked or a mixed Progressive Demasking task.
",False,dijkstra2002,False,False,True
8789,"According to a relative language activation view,
",False,dijkstra2002,False,False,True
8790,"adding words from the non-target language shouldinduce extra activation of the non-target language,which should affect especially the recognition ofitems in the weaker language (English for these bi-linguals). However, RTs in the two experimentscorrelated .99 and .98 for English and Dutch targetitems (Dijkstra and Van Heuven, 1998, Tables 6.1189 Bilingual word recognitionhttp://journals.cambridge.org Downloaded: 05 Feb 2015 IP address: 193.255.248.150and 6.2), so the result patterns were very much alike.
",False,dijkstra2002,False,False,True
8791,"The relative activation of the two languages was
",False,dijkstra2002,False,False,True
8792,"apparently not affected by mode of list presentation,
",False,dijkstra2002,False,False,True
8793,"because otherwise one would expect one of the twocorrelations to be much smaller (furthermore, regres-sion analyses of the relationship between RTs andEnglish or Dutch item frequency in the two experi-ments yielded nearly parallel lines).
",False,dijkstra2002,False,False,True
8794,"Other evidence that items of the two languages are
",False,dijkstra2002,False,False,True
8795,"always fully activated is found in a recent study by
",False,dijkstra2002,False,False,True
8796,"Schulpen, Dijkstra and Schriefers (in preparation).
",False,dijkstra2002,False,False,True
8797,"This study replicated and extended the three experi-ments from Dijkstra et al. (1998(b)) with Dutch-English high school students of 15 and 17 years ofage, students of psychology and Ph.D. students.Each experiment not only included interlingualhomographs and matched English controls of differ-ent frequency groups, but also a set of exclusively
",False,dijkstra2002,False,False,True
8798,"English words that varied across a considerable
",False,dijkstra2002,False,False,True
8799,"frequency range. As before, Experiment 1 consistedof English words only, Experiment 2 also incorpo-rated purely Dutch words that had to be rejected,and Experiment 3 contained English and Dutchwords that all had to be accepted. Figure 3 depictsregression analyses indicating how RTs to Englishitems of a considerable frequency range becamefaster in all three experiments when English fre-
",False,dijkstra2002,False,False,True
8800,"quency increased. Just like in the earlier studies(Dijkstra et al., 1998(b); De Groot et al., 2000), theRTs for English target words presented in a list thatalso contained Dutch words (Experiment 2) weresomewhat slower than when presented withoutDutch words (Experiment 1), but the slope of the two
",False,dijkstra2002,False,False,True
8801,"regression lines was statistically identical. Schulpen et
",False,dijkstra2002,False,False,True
8802,"al. argue that this is evidence that the response toEnglish words is affected only by a different decisionprocess in Experiments 1 and 2. If task demands wereto affect the relative language activation in the identi-®cation system, one would expect an interactionbetween English frequency and Experiment (1 or 2)because suppression should differentially affect the
",False,dijkstra2002,False,False,True
8803,"activation of lower and higher frequency items across
",False,dijkstra2002,False,False,True
8804,"experiments.
",False,dijkstra2002,False,False,True
8805,"To conclude, the available evidence indicates that
",False,dijkstra2002,False,False,True
8806,"differences between these experiments, involving par-ticipants from the same population and the samestimulus materials, result from changes in decisionparameter settings related to task demands ratherthan from changes in the relative activation levels of
",False,dijkstra2002,False,False,True
8807,"items of the two languages. This conclusion goes
",False,dijkstra2002,False,False,True
8808,"against such models as the original BIA model, theIC model and the theoretical view of the languagemode. It is more compatible with the positionsexpressed by De Groot et al. (2000) and Dijkstra etal. (2000(a)), and will be taken into account duringthe future implementation of the task/decision levelin the BIA+ model.
",False,dijkstra2002,False,False,True
8809,"4.6 Relationship between word identi®cation and task
",False,dijkstra2002,False,False,True
8810,"demands
",False,dijkstra2002,False,False,True
8811,"The BIA+ distinction between an identi®cation
",False,dijkstra2002,False,False,True
8812,"system and a task/decision system is in line with theview that in performing a task (such as lexicaldecision) an early preconscious, automatic level of
",False,dijkstra2002,False,False,True
8813,"processing may be followed by an attention-sensitive
",False,dijkstra2002,False,False,True
8814,"level in which percepts are selected with reference tocontextual factors of various sorts and linked toparticular responses relevant to the task at hand(cf. Altenberg and Cairns, 1983, p. 187; Dupoux andMehler, 1992; Balota, Paul and Spieler, 1999).
",False,dijkstra2002,False,False,True
8815,"The task schema speci®es the series of mental
",False,dijkstra2002,False,False,True
8816,"processing steps to be taken or operations (the ``algo-
",False,dijkstra2002,False,False,True
8817,"rithm'') to be carried out to perform the speci®c task
",False,dijkstra2002,False,False,True
8818,"at hand (Green, 1986, 1998; Norman and Shallice,1986). The task schema is set up during the practiceset or retrieved from memory to perform the task.The decision mechanism is part of the task schema. Itcontinuously reads out the activation in the identi®-cation system. At the appropriate cognitive step inthe schema, it weighs different kinds of activationExperiment
",False,dijkstra2002,False,False,True
8819,"English lex. dec.
",False,dijkstra2002,False,False,True
8820,"with D–words
",False,dijkstra2002,False,False,True
8821,"Generalized lex. dec.
",False,dijkstra2002,False,False,True
8822,"English lex. dec.
",False,dijkstra2002,False,False,True
8823,"without D–words1100
",False,dijkstra2002,False,False,True
8824,"1000
",False,dijkstra2002,False,False,True
8825,"900
",False,dijkstra2002,False,False,True
8826,"800
",False,dijkstra2002,False,False,True
8827,"700
",False,dijkstra2002,False,False,True
8828,"600
",False,dijkstra2002,False,False,True
8829,"500
",False,dijkstra2002,False,False,True
8830,"400Reaction time (ms)
",False,dijkstra2002,False,False,True
8831,"400Reaction time (ms)
",False,dijkstra2002,False,False,True
8832,"Abstract
",True,DNNvsSecondGraders,False,False,True
8833,"1. Introduction
",True,DNNvsSecondGraders,False,False,True
8834,"2The answer to the puzzle in Figure 1 is: C.
",True,DNNvsSecondGraders,False,False,True
8835,"2. Related works
",True,DNNvsSecondGraders,False,False,True
8836,"3. Proposed approach
",True,DNNvsSecondGraders,False,False,True
8837,"Are Deep Neural Networks SMARTer than Second Graders?
",False,DNNvsSecondGraders,False,False,True
8838,"Anoop Cherian1Kuan-Chuan Peng1Suhas Lohit1Kevin Smith2Joshua B. Tenenbaum2
",False,DNNvsSecondGraders,False,False,True
8839,"1Mitsubishi Electric Research Labs (MERL), Cambridge, MA
",False,DNNvsSecondGraders,False,False,True
8840,"2Massachusetts Institute of Technology (MIT), Cambridge, MA
",False,DNNvsSecondGraders,False,False,True
8841,"Recent times have witnessed an increasing number of ap-
",False,DNNvsSecondGraders,False,False,True
8842,"plications of deep neural networks towards solving tasks
",False,DNNvsSecondGraders,False,False,True
8843,"that require superior cognitive abilities, e.g., playing Go,
",False,DNNvsSecondGraders,False,False,True
8844,"generating art, question answering (such as ChatGPT), etc.
",False,DNNvsSecondGraders,False,False,True
8845,"Such a dramatic progress raises the question: how general-
",False,DNNvsSecondGraders,False,False,True
8846,"izable are neural networks in solving problems that demand
",False,DNNvsSecondGraders,False,False,True
8847,"broad skills? To answer this question, we propose SMART:
",False,DNNvsSecondGraders,False,False,True
8848,"aSimple Multimodal Algorithmic Reasoning Task and the
",False,DNNvsSecondGraders,False,False,True
8849,"associated SMART-101 dataset, for evaluating the abstrac-
",False,DNNvsSecondGraders,False,False,True
8850,"tion, deduction, and generalization abilities of neural net-
",False,DNNvsSecondGraders,False,False,True
8851,"works in solving visuo-linguistic puzzles designed speciﬁ-
",False,DNNvsSecondGraders,False,False,True
8852,"cally for children in the 6–8 age group. Our dataset con-
",False,DNNvsSecondGraders,False,False,True
8853,"sists of 101 unique puzzles; each puzzle comprises a picture
",False,DNNvsSecondGraders,False,False,True
8854,"and a question, and their solution needs a mix of several
",False,DNNvsSecondGraders,False,False,True
8855,"elementary skills, including arithmetic, algebra, and spa-
",False,DNNvsSecondGraders,False,False,True
8856,"tial reasoning, among others. To scale our dataset towards
",False,DNNvsSecondGraders,False,False,True
8857,"training deep neural networks, we programmatically gener-
",False,DNNvsSecondGraders,False,False,True
8858,"ate entirely new instances for each puzzle, while retaining
",False,DNNvsSecondGraders,False,False,True
8859,"their solution algorithm. To benchmark the performance on
",False,DNNvsSecondGraders,False,False,True
8860,"the SMART-101 dataset, we propose a vision and language
",False,DNNvsSecondGraders,False,False,True
8861,"meta-learning model using varied state-of-the-art backbone
",False,DNNvsSecondGraders,False,False,True
8862,"networks. Our experiments reveal that while powerful deep
",False,DNNvsSecondGraders,False,False,True
8863,"models offer reasonable performances on puzzles that they
",False,DNNvsSecondGraders,False,False,True
8864,"are trained on, they are not better than random accuracy
",False,DNNvsSecondGraders,False,False,True
8865,"when analyzed for generalization. We also evaluate the
",False,DNNvsSecondGraders,False,False,True
8866,"recent ChatGPT large language model on a subset of our
",False,DNNvsSecondGraders,False,False,True
8867,"dataset and ﬁnd that while ChatGPT produces convincing
",False,DNNvsSecondGraders,False,False,True
8868,"reasoning abilities, the answers are often incorrect.
",False,DNNvsSecondGraders,False,False,True
8869,"1. Introduction
",False,DNNvsSecondGraders,False,False,True
8870,"“An attempt will be made to ﬁnd how to make
",False,DNNvsSecondGraders,False,False,True
8871,"machines use language, form abstractions and
",False,DNNvsSecondGraders,False,False,True
8872,"concepts, solve kinds of problems now reserved for
",False,DNNvsSecondGraders,False,False,True
8873,"humans, and improve themselves. ”
",False,DNNvsSecondGraders,False,False,True
8874,"The Dartmouth Summer Project on AI, 1956
",False,DNNvsSecondGraders,False,False,True
8875,"Deep learning powered AI systems have been increas-
",False,DNNvsSecondGraders,False,False,True
8876,"ing in their data modeling abilities at an ever more vigor
",False,DNNvsSecondGraders,False,False,True
8877,"Question: Bird Bobbie jumps on a fence from the post on the left
",False,DNNvsSecondGraders,False,False,True
8878,"end to the other end. Each jump takes him 4 seconds. He makes 4
",False,DNNvsSecondGraders,False,False,True
8879,"jumps ahead and then 1 jump back. Then he again makes 4 jumps
",False,DNNvsSecondGraders,False,False,True
8880,"ahead and 1 jump back, and so on. In how many seconds can
",False,DNNvsSecondGraders,False,False,True
8881,"Bobbie get from one end to the other end?
",False,DNNvsSecondGraders,False,False,True
8882,"Answer Options: A: 64 B: 48 C: 56 D: 68 E: 72
",False,DNNvsSecondGraders,False,False,True
8883,"Figure 1. An example puzzle instance from our SMART-101
",False,DNNvsSecondGraders,False,False,True
8884,"dataset generated using our programmatic augmentation method.
",False,DNNvsSecondGraders,False,False,True
8885,"Solving this puzzle needs various skills such as counting the num-
",False,DNNvsSecondGraders,False,False,True
8886,"ber of posts, spatially locating Bobbie , and using the details in the
",False,DNNvsSecondGraders,False,False,True
8887,"question to derive an algorithm for the solution. At a foundational
",False,DNNvsSecondGraders,False,False,True
8888,"level, a reasoning agent needs to recognize abstracted objects such
",False,DNNvsSecondGraders,False,False,True
8889,"as posts, and identify the bird. The answer is shown below.2
",False,DNNvsSecondGraders,False,False,True
8890,"in the recent times, with compelling applications emerg-
",False,DNNvsSecondGraders,False,False,True
8891,"ing frequently, many of which may even challenge well-
",False,DNNvsSecondGraders,False,False,True
8892,"trained humans. A few notable such feats include but are
",False,DNNvsSecondGraders,False,False,True
8893,"not limited to game playing ( e.g., AlphaGo [57]), language-
",False,DNNvsSecondGraders,False,False,True
8894,"guided image generation ( e.g., the recent DALLE-2 [51]
",False,DNNvsSecondGraders,False,False,True
8895,"and ImageGen [53]), creative story writing ( e.g., using
",False,DNNvsSecondGraders,False,False,True
8896,"GPT-3 [10]), solving university level math problems [16],
",False,DNNvsSecondGraders,False,False,True
8897,"algorithmic inference [19], and general-purpose question
",False,DNNvsSecondGraders,False,False,True
8898,"answering/dialog ( e.g., ChatGPT1). Such impressive per-
",False,DNNvsSecondGraders,False,False,True
8899,"formances have prompted an introspection into the foun-
",False,DNNvsSecondGraders,False,False,True
8900,"dation of what constitutes artiﬁcial intelligence and de-
",False,DNNvsSecondGraders,False,False,True
8901,"riving novel tasks that could challenge deep models fur-
",False,DNNvsSecondGraders,False,False,True
8902,"ther [12, 35, 42, 52].
",False,DNNvsSecondGraders,False,False,True
8903,"While deep neural networks offer compelling perfor-
",False,DNNvsSecondGraders,False,False,True
8904,"mances on specialized tasks on which they are trained on, (i)
",False,DNNvsSecondGraders,False,False,True
8905,"how well do they model abstract data, attend on key entities,
",False,DNNvsSecondGraders,False,False,True
8906,"and transfer knowledge to solve new problems? (ii) how
",False,DNNvsSecondGraders,False,False,True
8907,"ﬂuid are they in acquiring new skills? and (iii) how effec-
",False,DNNvsSecondGraders,False,False,True
8908,"tive are they in the use of language for visual reasoning? We
",False,DNNvsSecondGraders,False,False,True
8909,"1https://openai.com/blog/chatgpt/
",False,DNNvsSecondGraders,False,False,True
8910,"2The answer to the puzzle in Figure 1 is: C.
",False,DNNvsSecondGraders,False,False,True
8911,"1arXiv:2212.09993v2  [cs.AI]  5 Jan 2023task ourselves to understand and seek a way to answer these
",False,DNNvsSecondGraders,False,False,True
8912,"questions for state-of-the-art (SOTA) vision and language
",False,DNNvsSecondGraders,False,False,True
8913,"deep learning models. An approach that has been taken
",False,DNNvsSecondGraders,False,False,True
8914,"several times in the past is to design specialized datasets
",False,DNNvsSecondGraders,False,False,True
8915,"that can measure the cognitive abilities of well-trained neu-
",False,DNNvsSecondGraders,False,False,True
8916,"ral networks. For example, in CLEVR [33], a diagnostic
",False,DNNvsSecondGraders,False,False,True
8917,"dataset is proposed that comprises visuo-linguistic spatial
",False,DNNvsSecondGraders,False,False,True
8918,"reasoning problems. The abstraction abilities of neural net-
",False,DNNvsSecondGraders,False,False,True
8919,"works have been explored towards solving types of Bon-
",False,DNNvsSecondGraders,False,False,True
8920,"gard problems [32, 45] and human IQ puzzles (e.g., Ravens
",False,DNNvsSecondGraders,False,False,True
8921,"progressive matrices) have been extended to evaluate neu-
",False,DNNvsSecondGraders,False,False,True
8922,"ral reasoning abilities [8,9,30,46,60,61,64,67]. However,
",False,DNNvsSecondGraders,False,False,True
8923,"while the puzzles in these prior works are often seemingly
",False,DNNvsSecondGraders,False,False,True
8924,"diverse, they are often conﬁned to a common setting and
",False,DNNvsSecondGraders,False,False,True
8925,"may need only specialized skill sets, bringing in inductive
",False,DNNvsSecondGraders,False,False,True
8926,"biases that could be exploited by well-crafted deep learn-
",False,DNNvsSecondGraders,False,False,True
8927,"ing models, thereby solving such puzzles with near perfect
",False,DNNvsSecondGraders,False,False,True
8928,"accuracy [56, 60].
",False,DNNvsSecondGraders,False,False,True
8929,"In this paper, we take a look back at the foundations
",False,DNNvsSecondGraders,False,False,True
8930,"of intelligence, by asking the question: Are state-of-the-
",False,DNNvsSecondGraders,False,False,True
8931,"art deep neural networks capable of emulating the thinking
",False,DNNvsSecondGraders,False,False,True
8932,"process of even young children? To gain insights into an-
",False,DNNvsSecondGraders,False,False,True
8933,"swering this question, we introduce the Simple Multimodal
",False,DNNvsSecondGraders,False,False,True
8934,"Algorithmic Reasoning Task (SMART) – a visuo-linguistic
",False,DNNvsSecondGraders,False,False,True
8935,"task and the associated SMART-101 dataset built from 101
",False,DNNvsSecondGraders,False,False,True
8936,"distinct children’s puzzles. As this is the ﬁrst step in this di-
",False,DNNvsSecondGraders,False,False,True
8937,"rection, we keep the puzzles simple – to ensure this, we took
",False,DNNvsSecondGraders,False,False,True
8938,"the puzzles from the Math Kangaroo USA Olympiad [4]
",False,DNNvsSecondGraders,False,False,True
8939,"with puzzle sets designed for children in the age group of
",False,DNNvsSecondGraders,False,False,True
8940,"6–8. Each puzzle in our dataset has a picture describing the
",False,DNNvsSecondGraders,False,False,True
8941,"problem setup and an associated natural language question.
",False,DNNvsSecondGraders,False,False,True
8942,"To solve the puzzle, one needs to use the question to gather
",False,DNNvsSecondGraders,False,False,True
8943,"details from the picture and infer a simple mathematical al-
",False,DNNvsSecondGraders,False,False,True
8944,"gorithm that leads to a solution to be matched against mul-
",False,DNNvsSecondGraders,False,False,True
8945,"tiple answer options. In Figure 1, we illustrate the task with
",False,DNNvsSecondGraders,False,False,True
8946,"an example puzzle from our dataset. Unlike prior datasets
",False,DNNvsSecondGraders,False,False,True
8947,"with similar goals, each of the 101 puzzles in our dataset is
",False,DNNvsSecondGraders,False,False,True
8948,"different and needs a broad range of elementary mathemat-
",False,DNNvsSecondGraders,False,False,True
8949,"ical skills for their solutions, including skills in algebra, ba-
",False,DNNvsSecondGraders,False,False,True
8950,"sic arithmetic, geometry, ordering, as well as foundational
",False,DNNvsSecondGraders,False,False,True
8951,"skills to interpret abstract images, and execute counting,
",False,DNNvsSecondGraders,False,False,True
8952,"spatial reasoning, pattern matching, and occlusion reason-
",False,DNNvsSecondGraders,False,False,True
8953,"ing. To the best of our knowledge, this is the ﬁrst dataset
",False,DNNvsSecondGraders,False,False,True
8954,"that offers such a richly diverse set of visuo-linguistic puz-
",False,DNNvsSecondGraders,False,False,True
8955,"zles in an open setting, with a psychometric control on their
",False,DNNvsSecondGraders,False,False,True
8956,"difﬁculty levels against human performance.3
",False,DNNvsSecondGraders,False,False,True
8957,"To build a large scale dataset from the 101 puzzles (for
",False,DNNvsSecondGraders,False,False,True
8958,"training deep models), we propose to augment each puzzle
",False,DNNvsSecondGraders,False,False,True
8959,"programmatically, i.e., we implement computer programs
",False,DNNvsSecondGraders,False,False,True
8960,"that replicate each puzzle into new instances, where each
",False,DNNvsSecondGraders,False,False,True
8961,"instance is distinct in its puzzle picture, as well as using new
",False,DNNvsSecondGraders,False,False,True
8962,"3This is derived from the assumption that the puzzles are professionally
",False,DNNvsSecondGraders,False,False,True
8963,"designed with a particular audience in mind.question structures, answer choices, and solutions. Such a
",False,DNNvsSecondGraders,False,False,True
8964,"major overhaul of the puzzles, we believe, would demand
",False,DNNvsSecondGraders,False,False,True
8965,"a reasoning method to learn the algorithmic skills to solve
",False,DNNvsSecondGraders,False,False,True
8966,"them. Using this approach, we created 2000 instances for
",False,DNNvsSecondGraders,False,False,True
8967,"each puzzle; SMART-101 thus having nearly 200K puzzle
",False,DNNvsSecondGraders,False,False,True
8968,"instances.
",False,DNNvsSecondGraders,False,False,True
8969,"To benchmark performances on the SMART-101 dataset,
",False,DNNvsSecondGraders,False,False,True
8970,"we propose an end-to-end meta-learning based neural net-
",False,DNNvsSecondGraders,False,False,True
8971,"work [20], where we use a SOTA pre-trained image encoder
",False,DNNvsSecondGraders,False,False,True
8972,"backbone ( e.g., ResNets/Transformers) to embed the pic-
",False,DNNvsSecondGraders,False,False,True
8973,"ture part of the puzzles, and a strong language model ( e.g.,
",False,DNNvsSecondGraders,False,False,True
8974,"word embeddings/GPT-2) to model the questions. As each
",False,DNNvsSecondGraders,False,False,True
8975,"puzzle can have a different range for their answers ( e.g., se-
",False,DNNvsSecondGraders,False,False,True
8976,"lection from a few choices, sequential answers, etc.), we
",False,DNNvsSecondGraders,False,False,True
8977,"propose to treat each puzzle as a separate task, with task-
",False,DNNvsSecondGraders,False,False,True
8978,"speciﬁc neural heads and training objectives, while a com-
",False,DNNvsSecondGraders,False,False,True
8979,"mon vision-language backbone is learned on all the puzzles.
",False,DNNvsSecondGraders,False,False,True
8980,"We provide experiments under various evaluation set-
",False,DNNvsSecondGraders,False,False,True
8981,"tings, analyzing the ability of our model for: (i) in-
",False,DNNvsSecondGraders,False,False,True
8982,"distribution generalization, when training and testing data
",False,DNNvsSecondGraders,False,False,True
8983,"are from the same distributions of puzzle instances, and
",False,DNNvsSecondGraders,False,False,True
8984,"out-of-distribution generalization, when training and testing
",False,DNNvsSecondGraders,False,False,True
8985,"data are from: (ii) distinct answer distributions, or (iii) dif-
",False,DNNvsSecondGraders,False,False,True
8986,"ferent puzzles. We ﬁnd that our model performs poorly on
",False,DNNvsSecondGraders,False,False,True
8987,"the tasks (i) and (ii), while failing entirely on (iii), suggest-
",False,DNNvsSecondGraders,False,False,True
8988,"ing that solving our dataset would demand novel research
",False,DNNvsSecondGraders,False,False,True
8989,"directions into neural abstractions and algorithmic reason-
",False,DNNvsSecondGraders,False,False,True
8990,"ing. We also evaluate the recently introduced ChatGPT
",False,DNNvsSecondGraders,False,False,True
8991,"model on a subset of our puzzles that do not need the visual
",False,DNNvsSecondGraders,False,False,True
8992,"stream for solving them. While, ChatGPT demonstrates
",False,DNNvsSecondGraders,False,False,True
8993,"human-like reasoning abilities and better out-of-distribution
",False,DNNvsSecondGraders,False,False,True
8994,"generalization, we ﬁnd that the overall performances are
",False,DNNvsSecondGraders,False,False,True
8995,"poor.
",False,DNNvsSecondGraders,False,False,True
8996,"We list the key contributions of this paper below.
",False,DNNvsSecondGraders,False,False,True
8997,"1. With the goal of making progress towards improving
",False,DNNvsSecondGraders,False,False,True
8998,"the visuo-linguistic algorithmic reasoning abilities of
",False,DNNvsSecondGraders,False,False,True
8999,"neural networks, we introduce a novel task, SMART
",False,DNNvsSecondGraders,False,False,True
9000,"and the associated large scale SMART-101 dataset.
",False,DNNvsSecondGraders,False,False,True
9001,"2. We propose a programmatic augmentation strategy for
",False,DNNvsSecondGraders,False,False,True
9002,"replicating abstract puzzles.
",False,DNNvsSecondGraders,False,False,True
9003,"3. We design a baseline meta-solver neural architecture
",False,DNNvsSecondGraders,False,False,True
9004,"for solving the puzzles in our task.
",False,DNNvsSecondGraders,False,False,True
9005,"4. We present experiments using our approach in various
",False,DNNvsSecondGraders,False,False,True
9006,"algorithmic generalization settings, bringing out key
",False,DNNvsSecondGraders,False,False,True
9007,"insights on the performance of SOTA neural networks
",False,DNNvsSecondGraders,False,False,True
9008,"on this task. We also compare our performances to
",False,DNNvsSecondGraders,False,False,True
9009,"human scores, as well as to those produced by recent
",False,DNNvsSecondGraders,False,False,True
9010,"externally-trained large language models.
",False,DNNvsSecondGraders,False,False,True
9011,"2. Related works
",False,DNNvsSecondGraders,False,False,True
9012,"To set the stage, we brieﬂy review below a few prior
",False,DNNvsSecondGraders,False,False,True
9013,"methods and datasets proposed towards understanding the
",False,DNNvsSecondGraders,False,False,True
9014,"reasoning abilities of deep neural networks.
",False,DNNvsSecondGraders,False,False,True
9015,"2Dataset Involve language Dataset size Task nature
",False,DNNvsSecondGraders,False,False,True
9016,"Bongard-LOGO [45] 7 12K few-shot concepts, abstract shape reasoning
",False,DNNvsSecondGraders,False,False,True
9017,"Bongard-HOI [32] 7 53K few-shot concepts, human-object interaction
",False,DNNvsSecondGraders,False,False,True
9018,"ARC [12] 7 800 generate image based on abstract rules
",False,DNNvsSecondGraders,False,False,True
9019,"Machine Number Sense [66] 7 280K solving arithmetic problems
",False,DNNvsSecondGraders,False,False,True
9020,"RA VEN [64] 7 70K ﬁnding next image in sequence
",False,DNNvsSecondGraders,False,False,True
9021,"Image riddles [5] 3(ﬁxed question) 3333 ﬁnding common linguistic descriptions
",False,DNNvsSecondGraders,False,False,True
9022,"VLQA [54] 3(variable questions) 9267 spatio-temporal reasoning, info lookup, mathematical, logical, causality, analogy, etc.
",False,DNNvsSecondGraders,False,False,True
9023,"PororoQA [34] 3(variable questions) 8913 reason from cartoon videos about action, person, abstract, detail, location, etc.
",False,DNNvsSecondGraders,False,False,True
9024,"CLEVR [33] 3(variable questions) 100K exist, count, query attributes, compare integers/attribute
",False,DNNvsSecondGraders,False,False,True
9025,"SMART-101 (ours) 3(variable questions) 200K 8 predominant algorithmic skills and their compositions (see Figure 2)
",False,DNNvsSecondGraders,False,False,True
9026,"Table 1. Comparison of our SMART-101 dataset with existing datasets related to visual reasoning.
",False,DNNvsSecondGraders,False,False,True
9027,"Solving IQ puzzles via creating computer programs has
",False,DNNvsSecondGraders,False,False,True
9028,"been a dream since the early days of exploration into
",False,DNNvsSecondGraders,False,False,True
9029,"AI [27, 40, 41]; Evan’s ANALOGY [18] and Hofstader’s
",False,DNNvsSecondGraders,False,False,True
9030,"CopyCat, among others [29] are famous tasks in this di-
",False,DNNvsSecondGraders,False,False,True
9031,"rection. With the resurgence of deep learning, there have
",False,DNNvsSecondGraders,False,False,True
9032,"been several attempts at re-considering such puzzles, with
",False,DNNvsSecondGraders,False,False,True
9033,"varied success. In Table 1, we brieﬂy review such tasks
",False,DNNvsSecondGraders,False,False,True
9034,"and datasets (see Małki ´nski and Ma ´ndziuk [39] for an in-
",False,DNNvsSecondGraders,False,False,True
9035,"depth survey). While, the goal of these works have been
",False,DNNvsSecondGraders,False,False,True
9036,"towards capturing human cognition through machine learn-
",False,DNNvsSecondGraders,False,False,True
9037,"ing models, their tasks are often specialized and when pro-
",False,DNNvsSecondGraders,False,False,True
9038,"vided enough data, the neural networks apparently leverage
",False,DNNvsSecondGraders,False,False,True
9039,"shortcomings in the dataset towards achieving very high ac-
",False,DNNvsSecondGraders,False,False,True
9040,"curacy [27, 60, 65], defaulting the original goals.
",False,DNNvsSecondGraders,False,False,True
9041,"Neuro-symbolic learning and program synthesis ap-
",False,DNNvsSecondGraders,False,False,True
9042,"proaches consider solving complex tasks via decomposing
",False,DNNvsSecondGraders,False,False,True
9043,"a scene into entities and synthesizing computer programs
",False,DNNvsSecondGraders,False,False,True
9044,"that operate on these entities; thereby plausibly emulat-
",False,DNNvsSecondGraders,False,False,True
9045,"ing human reasoning. The DreamCoder approach [17] for
",False,DNNvsSecondGraders,False,False,True
9046,"program synthesis to draw curves, solving Bongard prob-
",False,DNNvsSecondGraders,False,False,True
9047,"lems using program induction [59], solving Raven’s ma-
",False,DNNvsSecondGraders,False,False,True
9048,"trices using neuro-symbolic methods [28], and Bongard
",False,DNNvsSecondGraders,False,False,True
9049,"LOGO [45] are a few recent and successful approaches to-
",False,DNNvsSecondGraders,False,False,True
9050,"wards neuro-algorithmic reasoning, however, their general-
",False,DNNvsSecondGraders,False,False,True
9051,"ization to tasks beyond their domains is often unexplored.
",False,DNNvsSecondGraders,False,False,True
9052,"Visual and language tasks for understanding and reasoning
",False,DNNvsSecondGraders,False,False,True
9053,"on natural images [6, 7, 31, 33, 48] have been very success-
",False,DNNvsSecondGraders,False,False,True
9054,"ful using deep neural networks, lately [38, 48, 58]. Similar
",False,DNNvsSecondGraders,False,False,True
9055,"to such tasks, our goal in SMART-101 is to jointly interpret
",False,DNNvsSecondGraders,False,False,True
9056,"vision and language modalities for solving various reason-
",False,DNNvsSecondGraders,False,False,True
9057,"ing problems. However, differently to such approaches, our
",False,DNNvsSecondGraders,False,False,True
9058,"visual stream comprises not necessarily natural images, in-
",False,DNNvsSecondGraders,False,False,True
9059,"stead are mostly sketches without textures; thereby avoiding
",False,DNNvsSecondGraders,False,False,True
9060,"the unexpected and implicit inductive biases.
",False,DNNvsSecondGraders,False,False,True
9061,"Understanding children’s cognition for solving a variety
",False,DNNvsSecondGraders,False,False,True
9062,"of age-appropriate problems has been intensively studied
",False,DNNvsSecondGraders,False,False,True
9063,"over the years [13, 22, 35] via studying their ability to form
",False,DNNvsSecondGraders,False,False,True
9064,"abstract, hierarchical representations of the world, acquire
",False,DNNvsSecondGraders,False,False,True
9065,"language and develop a theory of mind [21]. A particu-
",False,DNNvsSecondGraders,False,False,True
9066,"larly useful and common approach to understanding chil-dren’s cognitive abilities, albeit imperfectly, is to present
",False,DNNvsSecondGraders,False,False,True
9067,"them with puzzles such as those in IQ tests [36, 44, 62]. To
",False,DNNvsSecondGraders,False,False,True
9068,"the best of our knowledge, it is the ﬁrst time that a dataset
",False,DNNvsSecondGraders,False,False,True
9069,"has been built in this direction, that can allow exploration of
",False,DNNvsSecondGraders,False,False,True
9070,"generalized reasoning abilities at a level of children’s cogni-
",False,DNNvsSecondGraders,False,False,True
9071,"tion, and that can be potentially useful not only in computer
",False,DNNvsSecondGraders,False,False,True
9072,"vision, but also for studying a breadth of abilities spanning
",False,DNNvsSecondGraders,False,False,True
9073,"psychology, neuroscience, and cognitive science.
",False,DNNvsSecondGraders,False,False,True
9074,"3. Proposed approach
",False,DNNvsSecondGraders,False,False,True
9075,"In this section, we detail our task, the associated dataset,
",False,DNNvsSecondGraders,False,False,True
9076,"and our baseline framework.
",False,DNNvsSecondGraders,False,False,True
9077,"3.1. Task and the SMART-101 dataset
",False,DNNvsSecondGraders,False,False,True
9078,"3.1. Task and the SMART-101 dataset
",False,DNNvsSecondGraders,False,False,True
9079,"Abstract. We explore possibilities for enhancing the generality, por ta-
",True,Emotion Recognition from Speech by Combining,False,False,True
9080,"1 Introduction
",True,Emotion Recognition from Speech by Combining,False,False,True
9081,"2 Methods and Materials
",True,Emotion Recognition from Speech by Combining,False,False,True
9082,"4 Experiment 2 - Fusion of Classiﬁers
",True,Emotion Recognition from Speech by Combining,False,False,True
9083,"5 Conclusions
",True,Emotion Recognition from Speech by Combining,False,False,True
9084,"Emotion Recognition from Speech by Combining
",False,Emotion Recognition from Speech by Combining,False,False,True
9085,"Databases and Fusion of Classiﬁers
",False,Emotion Recognition from Speech by Combining,False,False,True
9086,"Iulia Lefter1,2, Leon J. M. Rothkrantz1,2, Pascal Wiggers1, and David. A. van
",False,Emotion Recognition from Speech by Combining,False,False,True
9087,"Leeuwen3
",False,Emotion Recognition from Speech by Combining,False,False,True
9088,"1Delft University of Technology, The Netherlands
",False,Emotion Recognition from Speech by Combining,False,False,True
9089,"2The Netherlands Defense Academy
",False,Emotion Recognition from Speech by Combining,False,False,True
9090,"3TNO Human Factors, The Netherlands
",False,Emotion Recognition from Speech by Combining,False,False,True
9091,"bility and robustness of emotion recognition systems by com bining data-
",False,Emotion Recognition from Speech by Combining,False,False,True
9092,"bases and by fusion of classiﬁers. In a ﬁrst experiment, we in vestigate the
",False,Emotion Recognition from Speech by Combining,False,False,True
9093,"performance of an emotion detection system tested on a certa in database
",False,Emotion Recognition from Speech by Combining,False,False,True
9094,"given that it is trained on speech from either the same databa se, a dif-
",False,Emotion Recognition from Speech by Combining,False,False,True
9095,"ferent database or a mix of both. We observe that generally th ere is a
",False,Emotion Recognition from Speech by Combining,False,False,True
9096,"drop in performance when the test database does not match the training
",False,Emotion Recognition from Speech by Combining,False,False,True
9097,"material, but there are a few exceptions. Furthermore, the p erformance
",False,Emotion Recognition from Speech by Combining,False,False,True
9098,"drops when a mixed corpus of acted databases is used for train ing and
",False,Emotion Recognition from Speech by Combining,False,False,True
9099,"testing is carried out on real-life recordings. In a second e xperiment we
",False,Emotion Recognition from Speech by Combining,False,False,True
9100,"investigate the eﬀect of training multiple emotion detecto rs, and fusing
",False,Emotion Recognition from Speech by Combining,False,False,True
9101,"these into a single detection system. We observe a drop in the Equal
",False,Emotion Recognition from Speech by Combining,False,False,True
9102,"Error Rate ( eer) from 19.0 % on average for 4 individual detectors to
",False,Emotion Recognition from Speech by Combining,False,False,True
9103,"4.2 % when fused using FoCal [5].
",False,Emotion Recognition from Speech by Combining,False,False,True
9104,"1 Introduction
",False,Emotion Recognition from Speech by Combining,False,False,True
9105,"Emotion recognition from speech is a ﬁeld that gains more and more attention
",False,Emotion Recognition from Speech by Combining,False,False,True
9106,"from researchers. Typically, machine learning techniques are used to train models
",False,Emotion Recognition from Speech by Combining,False,False,True
9107,"of features extracted from databases of emotional speech [1 5]. Even though the
",False,Emotion Recognition from Speech by Combining,False,False,True
9108,"general architectures of the systems are similar, no unity c an be found within
",False,Emotion Recognition from Speech by Combining,False,False,True
9109,"the components. The results are hard to compare due to incons istencies in data,
",False,Emotion Recognition from Speech by Combining,False,False,True
9110,"task and labeling. Details of these problems are outlined in [17] where a call for
",False,Emotion Recognition from Speech by Combining,False,False,True
9111,"standardization is being made.
",False,Emotion Recognition from Speech by Combining,False,False,True
9112,"Obtaining data for training is not trivial. A recent trend is to replace acted
",False,Emotion Recognition from Speech by Combining,False,False,True
9113,"emotions by real, spontaneous ones. For this purpose, diﬀer ent emotion elicita-
",False,Emotion Recognition from Speech by Combining,False,False,True
9114,"tion methods are used, e.g. children interacting with a remo tely controlled pet
",False,Emotion Recognition from Speech by Combining,False,False,True
9115,"robot [19].
",False,Emotion Recognition from Speech by Combining,False,False,True
9116,"Recent work aims at ﬁnding the most important feature types [ 2]. The idea
",False,Emotion Recognition from Speech by Combining,False,False,True
9117,"is to extract a large number of features and then reduce this ﬁ gure, keeping
",False,Emotion Recognition from Speech by Combining,False,False,True
9118,"most relevant ones. However, the resulting feature set is hi ghly dependent on
",False,Emotion Recognition from Speech by Combining,False,False,True
9119,"the database being used. As noted by [23], diﬀerent features are relevant in the
",False,Emotion Recognition from Speech by Combining,False,False,True
9120,"case of acted and spontaneous emotions.The goal of this paper is to explore the portability of emotio n recognition
",False,Emotion Recognition from Speech by Combining,False,False,True
9121,"systems and improve the robustness. Usually experiments in volve the use of sin-
",False,Emotion Recognition from Speech by Combining,False,False,True
9122,"gle databases. As a ﬁrst experiment, we use four databases of emotional speech,
",False,Emotion Recognition from Speech by Combining,False,False,True
9123,"three with acted emotions and one with real-life recordings from call centers.
",False,Emotion Recognition from Speech by Combining,False,False,True
9124,"Our approach is to choose a ﬁxed database for testing and to us e diﬀerent data
",False,Emotion Recognition from Speech by Combining,False,False,True
9125,"combinations for training. This includes training on the sa me database, on a dif-
",False,Emotion Recognition from Speech by Combining,False,False,True
9126,"ferent database and on a merged database that includes or not the test database,
",False,Emotion Recognition from Speech by Combining,False,False,True
9127,"in a speaker independent way. Typically models are database dependent and are
",False,Emotion Recognition from Speech by Combining,False,False,True
9128,"not expected to work well on new types of data. A way to remedy t his is to
",False,Emotion Recognition from Speech by Combining,False,False,True
9129,"provide a larger amount of training data. With this experime nt we examine the
",False,Emotion Recognition from Speech by Combining,False,False,True
9130,"beneﬁts of using extended corpora as well as the portability of systems trained
",False,Emotion Recognition from Speech by Combining,False,False,True
9131,"on acted data to real life scenarios. Research using multi-c orpus training and
",False,Emotion Recognition from Speech by Combining,False,False,True
9132,"testing is presented in [18] and [21].
",False,Emotion Recognition from Speech by Combining,False,False,True
9133,"Since the performance of emotion recognition systems is sti ll far from 100%
",False,Emotion Recognition from Speech by Combining,False,False,True
9134,"accurate, especially when test data is from a diﬀerent datas et than the training
",False,Emotion Recognition from Speech by Combining,False,False,True
9135,"one, we investigate the improvements of fusing the results o f more classiﬁers
",False,Emotion Recognition from Speech by Combining,False,False,True
9136,"trained with diﬀerent feature sets on spontaneous data. We u se both utterance
",False,Emotion Recognition from Speech by Combining,False,False,True
9137,"and frame level features, whose combination is expected to e nhance the recogni-
",False,Emotion Recognition from Speech by Combining,False,False,True
9138,"tion as shown in [22]. Late fusion by linear combination of th e scores given equal
",False,Emotion Recognition from Speech by Combining,False,False,True
9139,"weights and also weights calculated with logistic regressi on are compared. Both
",False,Emotion Recognition from Speech by Combining,False,False,True
9140,"of them yield higher performance than the individual classi ﬁers.
",False,Emotion Recognition from Speech by Combining,False,False,True
9141,"This paper is organized as follows. In Section 2 we introduce the emotional
",False,Emotion Recognition from Speech by Combining,False,False,True
9142,"speech databases used in this work and the methodology for tr aining and testing.
",False,Emotion Recognition from Speech by Combining,False,False,True
9143,"Details about the setups and the results of the ﬁrst and the se cond experiment
",False,Emotion Recognition from Speech by Combining,False,False,True
9144,"are provided in Sections 3 and 4 respectively. The last secti on contains our
",False,Emotion Recognition from Speech by Combining,False,False,True
9145,"conclusions.
",False,Emotion Recognition from Speech by Combining,False,False,True
9146,"2 Methods and Materials
",False,Emotion Recognition from Speech by Combining,False,False,True
9147,"We use four databases for training and testing: the German da tabase ( berlin )
",False,Emotion Recognition from Speech by Combining,False,False,True
9148,"[6], the Danish database of emotional speech ( des) [9], the audio part of the
",False,Emotion Recognition from Speech by Combining,False,False,True
9149,"eNTERFACE’05 database ( ent) [14] and the South-African Database ( sa) [12].
",False,Emotion Recognition from Speech by Combining,False,False,True
9150,"Details about the characteristics of these databases can be found in Table 1.
",False,Emotion Recognition from Speech by Combining,False,False,True
9151,"The idea is to use subsets of the databases that contain the sa me emotions.
",False,Emotion Recognition from Speech by Combining,False,False,True
9152,"Firstly, we use only combinations of the three acted databas es and the three
",False,Emotion Recognition from Speech by Combining,False,False,True
9153,"emotions they have in common: anger, happiness and sadness ( Experiment 1.a).
",False,Emotion Recognition from Speech by Combining,False,False,True
9154,"Secondly, we include also the database of spontaneous speec h, and consider just
",False,Emotion Recognition from Speech by Combining,False,False,True
9155,"two classes: anger and neutral (Experiment 1.b). Even thoug hentdoes not
",False,Emotion Recognition from Speech by Combining,False,False,True
9156,"contain a neutral class, we have decided to use its samples fr om the anger class
",False,Emotion Recognition from Speech by Combining,False,False,True
9157,"in this experiment.
",False,Emotion Recognition from Speech by Combining,False,False,True
9158,"Given a ﬁxed test set, three training conditions are impleme nted for the ﬁrst
",False,Emotion Recognition from Speech by Combining,False,False,True
9159,"experiment: within corpus (same database is used for training and testing), cross
",False,Emotion Recognition from Speech by Combining,False,False,True
9160,"corpus (the databases for training and testing are diﬀerent), and mixed corpus
",False,Emotion Recognition from Speech by Combining,False,False,True
9161,"(samples corresponding to the same emotion but belonging to diﬀerent databases
",False,Emotion Recognition from Speech by Combining,False,False,True
9162,"are considered as one class, and speaker independent classi ﬁcation is performed).Table 1. Characteristics of the databases of emotional speech used
",False,Emotion Recognition from Speech by Combining,False,False,True
9163,"Feature berlin des ent sa
",False,Emotion Recognition from Speech by Combining,False,False,True
9164,"# anger 127 50 211 1000
",False,Emotion Recognition from Speech by Combining,False,False,True
9165,"# disgust 38 211
",False,Emotion Recognition from Speech by Combining,False,False,True
9166,"# fear 55 211
",False,Emotion Recognition from Speech by Combining,False,False,True
9167,"# happiness 64 52 208
",False,Emotion Recognition from Speech by Combining,False,False,True
9168,"# sadness 53 52 211
",False,Emotion Recognition from Speech by Combining,False,False,True
9169,"# surprise 50 211
",False,Emotion Recognition from Speech by Combining,False,False,True
9170,"# boredom 79
",False,Emotion Recognition from Speech by Combining,False,False,True
9171,"# neutral 78 52 2000
",False,Emotion Recognition from Speech by Combining,False,False,True
9172,"# speakers 10 (5 male) 4 (2 male) 46 1228
",False,Emotion Recognition from Speech by Combining,False,False,True
9173,"acted/spontaneous acted acted acted spontaneous
",False,Emotion Recognition from Speech by Combining,False,False,True
9174,"language German Danish English English/Afrikaans
",False,Emotion Recognition from Speech by Combining,False,False,True
9175,"utterance type preset preset preset free
",False,Emotion Recognition from Speech by Combining,False,False,True
9176,"mean duration (sec) 2.76 5.46 2.81 4.3
",False,Emotion Recognition from Speech by Combining,False,False,True
9177,"total duration (min) 22.8 30.68 59.06 215.02
",False,Emotion Recognition from Speech by Combining,False,False,True
9178,"recording condition mic mic mic telephone
",False,Emotion Recognition from Speech by Combining,False,False,True
9179,"Our approach is to consider one emotion as target and the othe r emotions
",False,Emotion Recognition from Speech by Combining,False,False,True
9180,"as non-target. A detector for the target emotion can make two types of errors
",False,Emotion Recognition from Speech by Combining,False,False,True
9181,"that can be traded oﬀ: misses and false alarms. We asses the pe rformance of our
",False,Emotion Recognition from Speech by Combining,False,False,True
9182,"detectors in terms of equal error rates ( eer) where false alarm and miss rates
",False,Emotion Recognition from Speech by Combining,False,False,True
9183,"are equal.
",False,Emotion Recognition from Speech by Combining,False,False,True
9184,"All experiments are implemented using speaker independent cross-validation
",False,Emotion Recognition from Speech by Combining,False,False,True
9185,"withz-normalization of features on the training set in order to ac hieveµ= 0 and
",False,Emotion Recognition from Speech by Combining,False,False,True
9186,"σ= 1. For berlin anddeswhich have a small number of speakers we use leave-
",False,Emotion Recognition from Speech by Combining,False,False,True
9187,"one-speaker-out cross-validation. For entandsawe use 10 fold cross-validation.
",False,Emotion Recognition from Speech by Combining,False,False,True
9188,"In the case of the second experiment we fuse detectors whose s cores span
",False,Emotion Recognition from Speech by Combining,False,False,True
9189,"diﬀerent ranges (some are log likelihoods, some probabilit ies). It is therefore
",False,Emotion Recognition from Speech by Combining,False,False,True
9190,"important to normalize the scores. For this reason we have us ed 10-fold speaker
",False,Emotion Recognition from Speech by Combining,False,False,True
9191,"independent double-cross validation and an adapted form of t-normalization [1].
",False,Emotion Recognition from Speech by Combining,False,False,True
9192,"The mean and standard deviation of the scores of the non-targe t development
",False,Emotion Recognition from Speech by Combining,False,False,True
9193,"set are used in order to normalize the scores of the evaluatio n set.
",False,Emotion Recognition from Speech by Combining,False,False,True
9194,"3 Experiment 1 - Multiple Corpus Training and Testing
",False,Emotion Recognition from Speech by Combining,False,False,True
9195,"In this experiment we test the ability of models trained on on e database to
",False,Emotion Recognition from Speech by Combining,False,False,True
9196,"generalize to another one. We use a prosodic, utterance leve l feature set inspired
",False,Emotion Recognition from Speech by Combining,False,False,True
9197,"from the minimum required set of features proposed by [11] an d the approach of
",False,Emotion Recognition from Speech by Combining,False,False,True
9198,"[20]. The feature set contains: pitch(mean, standard deviat ion, range, absolute
",False,Emotion Recognition from Speech by Combining,False,False,True
9199,"slope (without octave jumps), jitter), intensity (mean, st andard deviation, range,
",False,Emotion Recognition from Speech by Combining,False,False,True
9200,"absolute slope, shimmer), means of the ﬁrst 4 formants, long term averaged
",False,Emotion Recognition from Speech by Combining,False,False,True
9201,"spectrum (slope, Hammarberg index, high energy) and center of gravity and
",False,Emotion Recognition from Speech by Combining,False,False,True
9202,"skewness of the spectrum. These features were extracted usi ng Praat [3] and wewill refer to them as prosodic features. Classiﬁcation is pe rformed by Support
",False,Emotion Recognition from Speech by Combining,False,False,True
9203,"Vector Machines ( svm) with a radial basis function ( rbf) kernel by means of
",False,Emotion Recognition from Speech by Combining,False,False,True
9204,"libsvm [8]. We refer to this method as svm.
",False,Emotion Recognition from Speech by Combining,False,False,True
9205,"Table 2. Results of Experiment 1.a
",False,Emotion Recognition from Speech by Combining,False,False,True
9206,"Experiment Train corpus Test corpusEER
",False,Emotion Recognition from Speech by Combining,False,False,True
9207,"anger happiness sadness
",False,Emotion Recognition from Speech by Combining,False,False,True
9208,"within corpusberlin berlin 11.6 18.9 14.8
",False,Emotion Recognition from Speech by Combining,False,False,True
9209,"des des 31.8 33.0 25.0
",False,Emotion Recognition from Speech by Combining,False,False,True
9210,"ent ent 26.1 36.7 22.3
",False,Emotion Recognition from Speech by Combining,False,False,True
9211,"cross corpusdes berlin 31.5 53.2 44.3
",False,Emotion Recognition from Speech by Combining,False,False,True
9212,"ent berlin 44.9 45.4 19.9
",False,Emotion Recognition from Speech by Combining,False,False,True
9213,"des+ent berlin 38.4 46.8 24.0
",False,Emotion Recognition from Speech by Combining,False,False,True
9214,"berlin des 31.9 44.7 33.0
",False,Emotion Recognition from Speech by Combining,False,False,True
9215,"ent des 29.9 34.0 13.1
",False,Emotion Recognition from Speech by Combining,False,False,True
9216,"berlin +ent des 29.9* 34.5 17.5
",False,Emotion Recognition from Speech by Combining,False,False,True
9217,"berlin ent 38.8 45.6 30.2
",False,Emotion Recognition from Speech by Combining,False,False,True
9218,"des ent 33.2 36.9 16.8
",False,Emotion Recognition from Speech by Combining,False,False,True
9219,"berlin +des ent 35.7 36.2*16.4*
",False,Emotion Recognition from Speech by Combining,False,False,True
9220,"mixed corpusberlin +des+ent berlin 20.5 25.0 3.5
",False,Emotion Recognition from Speech by Combining,False,False,True
9221,"berlin +des+ent des 26.5 32.5 15.5
",False,Emotion Recognition from Speech by Combining,False,False,True
9222,"berlin +des+ent ent 30.1 36.2 16.3
",False,Emotion Recognition from Speech by Combining,False,False,True
9223,"The results for Experiment 1.a, which uses the three acted da tabases and
",False,Emotion Recognition from Speech by Combining,False,False,True
9224,"three emotions are presented in Table 2. The results for Expe riment 1.b in which
",False,Emotion Recognition from Speech by Combining,False,False,True
9225,"all four databases and two classes are used are provided in Ta ble 3. The within
",False,Emotion Recognition from Speech by Combining,False,False,True
9226,"corpus results can be considered as reference values. In gen eral the cross corpus
",False,Emotion Recognition from Speech by Combining,False,False,True
9227,"tests result in worse eers than the reference values. Interestingly, there are also
",False,Emotion Recognition from Speech by Combining,False,False,True
9228,"some exceptions which are printed in bold. Results marked wi th a star (*) in
",False,Emotion Recognition from Speech by Combining,False,False,True
9229,"the cross corpus experiment highlight that there is an impro vement by merging
",False,Emotion Recognition from Speech by Combining,False,False,True
9230,"databases for training. The mixed corpus approach gives an i mprovement to
",False,Emotion Recognition from Speech by Combining,False,False,True
9231,"both the within- and cross corpus results for most condition s.
",False,Emotion Recognition from Speech by Combining,False,False,True
9232,"Table 3. Results of Experiment 1.b
",False,Emotion Recognition from Speech by Combining,False,False,True
9233,"Experiment Train corpus Test corpus EER
",False,Emotion Recognition from Speech by Combining,False,False,True
9234,"within corpusberlin berlin 1.4
",False,Emotion Recognition from Speech by Combining,False,False,True
9235,"des des 28.4
",False,Emotion Recognition from Speech by Combining,False,False,True
9236,"sa sa 15.5
",False,Emotion Recognition from Speech by Combining,False,False,True
9237,"cross corpus berlin +des+ent sa 29.9
",False,Emotion Recognition from Speech by Combining,False,False,True
9238,"mixed corpusberlin +des ent +sa berlin 3.9
",False,Emotion Recognition from Speech by Combining,False,False,True
9239,"berlin +des+ent+sa des 25.5
",False,Emotion Recognition from Speech by Combining,False,False,True
9240,"berlin +des+ent+sa sa 16.5When all four databases are used, we are interested, for the c ross corpus
",False,Emotion Recognition from Speech by Combining,False,False,True
9241,"case, in the performance of classiﬁers trained on acted and t ested on real data.
",False,Emotion Recognition from Speech by Combining,False,False,True
9242,"In this case the eerof the cross corpus condition is twice that of the within-
",False,Emotion Recognition from Speech by Combining,False,False,True
9243,"corpus condition. The mixed approach shows an improvement o nly in the case of
",False,Emotion Recognition from Speech by Combining,False,False,True
9244,"testing on des, while for sathe result is slightly lower than the reference value.
",False,Emotion Recognition from Speech by Combining,False,False,True
9245,"Theentdatabase is only used in this experiment for training, since it does not
",False,Emotion Recognition from Speech by Combining,False,False,True
9246,"contain a neutral class.
",False,Emotion Recognition from Speech by Combining,False,False,True
9247,"4 Experiment 2 - Fusion of Classiﬁers
",False,Emotion Recognition from Speech by Combining,False,False,True
9248,"The aim of this experiment is to improve the performance of em otion detection.
",False,Emotion Recognition from Speech by Combining,False,False,True
9249,"We use only the sadatabase, which is more diﬃcult since it contains free natur al
",False,Emotion Recognition from Speech by Combining,False,False,True
9250,"speech as opposed to preset utterances and the convenient la b conditions are
",False,Emotion Recognition from Speech by Combining,False,False,True
9251,"replaced with noisy telephone speech. We are interested in t he performance of
",False,Emotion Recognition from Speech by Combining,False,False,True
9252,"diﬀerent classiﬁcation methods, as well as their fusion.
",False,Emotion Recognition from Speech by Combining,False,False,True
9253,"A ﬁrst detection approach uses svmand the prosodic feature set described
",False,Emotion Recognition from Speech by Combining,False,False,True
9254,"in Section 3. Further, we use three spectral feature based cl assiﬁers popular
",False,Emotion Recognition from Speech by Combining,False,False,True
9255,"in speaker recognition. They are based on Relative Spectral Perceptual Linear
",False,Emotion Recognition from Speech by Combining,False,False,True
9256,"Predictive ( rasta plp ) coding of speech [10]. In order to extract the features
",False,Emotion Recognition from Speech by Combining,False,False,True
9257,"from the sound signal, voice activity detection is performe d based on energy
",False,Emotion Recognition from Speech by Combining,False,False,True
9258,"levels. Every 16 ms, 26 coeﬃcients are extracted for a frame o f 32 ms : 12 plp
",False,Emotion Recognition from Speech by Combining,False,False,True
9259,"coeﬃcients plus log energy and their derivatives.
",False,Emotion Recognition from Speech by Combining,False,False,True
9260,"The Universal Background Model - Gaussian Mixture Model ( ubm-gmm )
",False,Emotion Recognition from Speech by Combining,False,False,True
9261,"[16] approach models each class by a mixture of Gaussians bas ed on the rasta
",False,Emotion Recognition from Speech by Combining,False,False,True
9262,"plpfeatures. We use a 512 mixtures precomputed ( ubm) trained on NIST SRE
",False,Emotion Recognition from Speech by Combining,False,False,True
9263,"2008 data. This is mapadapted using either emotion or neutral speech data. We
",False,Emotion Recognition from Speech by Combining,False,False,True
9264,"refer to this method as gmm.
",False,Emotion Recognition from Speech by Combining,False,False,True
9265,"The third technique is a ubm-gmm-svm detector [7]. The feature supervectors
",False,Emotion Recognition from Speech by Combining,False,False,True
9266,"are the means of the ubm-gmm model. These feature sets are used for svm
",False,Emotion Recognition from Speech by Combining,False,False,True
9267,"classiﬁcation.
",False,Emotion Recognition from Speech by Combining,False,False,True
9268,"The ﬁnal classiﬁer is known as dot-scoring ( ds) [4], and it is a linear approx-
",False,Emotion Recognition from Speech by Combining,False,False,True
9269,"imation of ubm-gmm . It uses suﬃcient ﬁxed size zero and ﬁrst order statistics
",False,Emotion Recognition from Speech by Combining,False,False,True
9270,"of these features. The method includes channel compensatio n, meaning that the
",False,Emotion Recognition from Speech by Combining,False,False,True
9271,"impact of the communication medium is reduced.
",False,Emotion Recognition from Speech by Combining,False,False,True
9272,"Two types of score level fusion are applied on the scores of th ese four clas-
",False,Emotion Recognition from Speech by Combining,False,False,True
9273,"siﬁcation methods: a linear combination of the t-normalized scores with equal
",False,Emotion Recognition from Speech by Combining,False,False,True
9274,"weights, and fusion by calculating the weights using linear logistic regression
",False,Emotion Recognition from Speech by Combining,False,False,True
9275,"using FoCal [5]. For the second fusion type, a constant is add ed to the formula
",False,Emotion Recognition from Speech by Combining,False,False,True
9276,"for calibration. This approach provides simultaneous fusi on and calibration in a
",False,Emotion Recognition from Speech by Combining,False,False,True
9277,"way that optimizes discrimination and calibration. The fus ed scores tend to be
",False,Emotion Recognition from Speech by Combining,False,False,True
9278,"well-calibrated detection log-likelihood-ratios.
",False,Emotion Recognition from Speech by Combining,False,False,True
9279,"As we expect diﬀerent classiﬁers based on diﬀerent features to complement
",False,Emotion Recognition from Speech by Combining,False,False,True
9280,"each other, we fuse in turn the svmwith prosodic features with each of the gmm-
",False,Emotion Recognition from Speech by Combining,False,False,True
9281,"like approaches which are based on rasta plp features. Finally, we fuse all four
",False,Emotion Recognition from Speech by Combining,False,False,True
9282,"classiﬁers by logistic regression. The results are shown in Table 4 for diﬀerentweights of each classiﬁer to the ﬁnal result. The weights diﬀ erent than 1 are
",False,Emotion Recognition from Speech by Combining,False,False,True
9283,"calculated with FoCal. svmgives the highest performance from the individual
",False,Emotion Recognition from Speech by Combining,False,False,True
9284,"classiﬁers and is always assigned high weights for fusion. H owever, ugswhich
",False,Emotion Recognition from Speech by Combining,False,False,True
9285,"has a lower performance by itself is assigned slightly highe r weights.
",False,Emotion Recognition from Speech by Combining,False,False,True
9286,"Table 4. eers for individual classiﬁers and various combinations with d iﬀerent weights
",False,Emotion Recognition from Speech by Combining,False,False,True
9287,"Classiﬁer Weight
",False,Emotion Recognition from Speech by Combining,False,False,True
9288,"gmm 1 13.03 3.35
",False,Emotion Recognition from Speech by Combining,False,False,True
9289,"ugs 1 15.62 5.92
",False,Emotion Recognition from Speech by Combining,False,False,True
9290,"ds 1 11.811.72
",False,Emotion Recognition from Speech by Combining,False,False,True
9291,"svm 115.77 15.44 15.275.13
",False,Emotion Recognition from Speech by Combining,False,False,True
9292,"eer(%) 21.219.819.615.511.39.910.510.111.311.04.2
",False,Emotion Recognition from Speech by Combining,False,False,True
9293,"The detection error tradeoﬀ ( det) curves [13] of the individual detectors and
",False,Emotion Recognition from Speech by Combining,False,False,True
9294,"their fusion are presented in Figure 1. The results show clea rly that fusion leads
",False,Emotion Recognition from Speech by Combining,False,False,True
9295,"to great improvements.
",False,Emotion Recognition from Speech by Combining,False,False,True
9296,"Fig. 1. detcurves for the fusion of Dot Scoring, svm,gmm andubm-gmm-svm by
",False,Emotion Recognition from Speech by Combining,False,False,True
9297,"logistic regression
",False,Emotion Recognition from Speech by Combining,False,False,True
9298,"5 Conclusions
",False,Emotion Recognition from Speech by Combining,False,False,True
9299,"In this paper we have investigated several aspects related t o emotion recogni-
",False,Emotion Recognition from Speech by Combining,False,False,True
9300,"tion in speech. First, we have investigated the ability of an emotion detector togeneralize to a diﬀerent data set. For this we use the common e motions found
",False,Emotion Recognition from Speech by Combining,False,False,True
9301,"in three widely used emotion databases: berlin ,des, andentwith emotions
",False,Emotion Recognition from Speech by Combining,False,False,True
9302,"anger, happiness and sadness. Surprisingly, we found that f or the destest data
",False,Emotion Recognition from Speech by Combining,False,False,True
9303,"we obtained better performance for detectors trained on dat a including the ent
",False,Emotion Recognition from Speech by Combining,False,False,True
9304,"database, than using training on desalone. This may be due to the fact that
",False,Emotion Recognition from Speech by Combining,False,False,True
9305,"when using desfor training, only 3 actors are available, of which only 1 has the
",False,Emotion Recognition from Speech by Combining,False,False,True
9306,"same gender as the test speaker. Here, the classiﬁer obvious ly can beneﬁt from a
",False,Emotion Recognition from Speech by Combining,False,False,True
9307,"wider variability in speakers, even if the recording protoc ol, way of eliciting the
",False,Emotion Recognition from Speech by Combining,False,False,True
9308,"emotions, or even the language of the speech used are diﬀeren t. Another aspect
",False,Emotion Recognition from Speech by Combining,False,False,True
9309,"of emotion in speech is whether it results from acted or real e motion. In order
",False,Emotion Recognition from Speech by Combining,False,False,True
9310,"to study this, we used data collected from a call center, wher e two emotions
",False,Emotion Recognition from Speech by Combining,False,False,True
9311,"dominate calls from clients: anger and neutral. Here, we obs erve that mixing in
",False,Emotion Recognition from Speech by Combining,False,False,True
9312,"acted emotions does not lead to additional performance with our baseline classi-
",False,Emotion Recognition from Speech by Combining,False,False,True
9313,"ﬁer. We may presume that the emotion cause (real versus acted ) is too diﬀerent
",False,Emotion Recognition from Speech by Combining,False,False,True
9314,"between the test and additional training data, although we c annot exclude that
",False,Emotion Recognition from Speech by Combining,False,False,True
9315,"other sources of variation (channel, language) also preven t the emotion models
",False,Emotion Recognition from Speech by Combining,False,False,True
9316,"from improving.
",False,Emotion Recognition from Speech by Combining,False,False,True
9317,"As a ﬁnal experiment we have looked into methods for improvin g a single
",False,Emotion Recognition from Speech by Combining,False,False,True
9318,"emotion detector tested on the natural sadata. By using additional features
",False,Emotion Recognition from Speech by Combining,False,False,True
9319,"and several frame-based classiﬁers borrowed from speaker r ecognition, we could
",False,Emotion Recognition from Speech by Combining,False,False,True
9320,"show a very strong improvement in performance from 19 % on ave rage for the
",False,Emotion Recognition from Speech by Combining,False,False,True
9321,"4 individual detectors to 4.2 % for the fused detectors. This is consistent with
",False,Emotion Recognition from Speech by Combining,False,False,True
9322,"what has been observed in speaker recognition [5], but it is i nteresting to note
",False,Emotion Recognition from Speech by Combining,False,False,True
9323,"that the fusion still works so well for very short duration ut terances (2 seconds,
",False,Emotion Recognition from Speech by Combining,False,False,True
9324,"compared to the 2 minutes we are used to in speaker recognitio n) and for three
",False,Emotion Recognition from Speech by Combining,False,False,True
9325,"classiﬁers that are based on the same spectral features. Alt hough there still is a
",False,Emotion Recognition from Speech by Combining,False,False,True
9326,"long way to go before we have robust emotion detectors that ar e not sensitive
",False,Emotion Recognition from Speech by Combining,False,False,True
9327,"to spontaneity, language or recording channel, we do believ e that the methods
",False,Emotion Recognition from Speech by Combining,False,False,True
9328,"and experiments presented in this paper give some insight in to what can be
",False,Emotion Recognition from Speech by Combining,False,False,True
9329,"promising approaches in both technology and data collectio n.
",False,Emotion Recognition from Speech by Combining,False,False,True
9330,"References
",False,Emotion Recognition from Speech by Combining,False,False,True
9331,"1. R. Auckenthaler, M. Carey, and H. Lloyd-Thomas. Score Nor malization for Text-
",False,Emotion Recognition from Speech by Combining,False,False,True
9332,"Independent Speaker Veriﬁcation Systems. Digital Signal Processing , 10:42–54,
",False,Emotion Recognition from Speech by Combining,False,False,True
9333,"2000.
",False,Emotion Recognition from Speech by Combining,False,False,True
9334,"2. A. Batliner, S. Steidl, B. Schuller, D. Seppi, T. Vogt, J. W agner, L. Devillers,
",False,Emotion Recognition from Speech by Combining,False,False,True
9335,"L. Vidrascu, V. Aharonson, L. Kessous, and N. Amir. Whodunni t - Searching
",False,Emotion Recognition from Speech by Combining,False,False,True
9336,"for the Most Important Feature Types Signalling Emotion-Re lated User States in
",False,Emotion Recognition from Speech by Combining,False,False,True
9337,"Speech. Computer Speech and Language , 2010.
",False,Emotion Recognition from Speech by Combining,False,False,True
9338,"3. P. Boersma. Praat, a System for Doing Phonetics by Compute r.Glot International ,
",False,Emotion Recognition from Speech by Combining,False,False,True
9339,"5(9/10):341–345, 2001.
",False,Emotion Recognition from Speech by Combining,False,False,True
9340,"5(9/10):341–345, 2001.
",False,Emotion Recognition from Speech by Combining,False,False,True
9341,"availableattheendofthearticleAbstract
",True,Evaluating effects of focal length and viewing angle,False,False,True
9342,"1 Introduction
",True,Evaluating effects of focal length and viewing angle,False,False,True
9343,"2 Methodofcomparison
",True,Evaluating effects of focal length and viewing angle,False,False,True
9344,"2.1 Landmarkschemes
",True,Evaluating effects of focal length and viewing angle,False,False,True
9345,"2.2 Evaluationmetrics
",True,Evaluating effects of focal length and viewing angle,False,False,True
9346,"2.3 Camerapositionandfocallength
",True,Evaluating effects of focal length and viewing angle,False,False,True
9347,"2.4 Facelandmarkandalignmentmethods
",True,Evaluating effects of focal length and viewing angle,False,False,True
9348,"3 Procedure
",True,Evaluating effects of focal length and viewing angle,False,False,True
9349,"4 Resultsanddiscussion
",True,Evaluating effects of focal length and viewing angle,False,False,True
9350,"3DSTN,e3D-Fan,and faverageresultforallalgorithms
",True,Evaluating effects of focal length and viewing angle,False,False,True
9351,"5C o n c l u s i o n
",True,Evaluating effects of focal length and viewing angle,False,False,True
9352,"EURASIP Journal on Image
",False,Evaluating effects of focal length and viewing angle,False,False,True
9353,"and Video ProcessingLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 
",False,Evaluating effects of focal length and viewing angle,False,False,True
9354,"https://doi.org/10.1186/s13640-021-00549-3
",False,Evaluating effects of focal length and viewing angle,False,False,True
9355,"REVIEW OpenAccess
",False,Evaluating effects of focal length and viewing angle,False,False,True
9356,"Evaluatingeffectsoffocallengthand
",False,Evaluating effects of focal length and viewing angle,False,False,True
9357,"viewingangleinacomparisonofrecentface
",False,Evaluating effects of focal length and viewing angle,False,False,True
9358,"landmarkandalignmentmethods
",False,Evaluating effects of focal length and viewing angle,False,False,True
9359,"XiangLi1*,JianzhengLiu2,JessicaBaron1,KhoaLuu3andEricPatterson1
",False,Evaluating effects of focal length and viewing angle,False,False,True
9360,"*Correspondence:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9361,"xiang5@clemson.edu
",False,Evaluating effects of focal length and viewing angle,False,False,True
9362,"1SchoolofComputing,Clemson
",False,Evaluating effects of focal length and viewing angle,False,False,True
9363,"University,304McAdamsHall,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9364,"29630Clemson,SC,USA
",False,Evaluating effects of focal length and viewing angle,False,False,True
9365,"Fulllistofauthorinformationis
",False,Evaluating effects of focal length and viewing angle,False,False,True
9366,"Recentattentiontofacialalignmentandlandmarkdetectionmethods,particularlywith
",False,Evaluating effects of focal length and viewing angle,False,False,True
9367,"applicationofdeepconvolutionalneuralnetworks,haveyieldednotable
",False,Evaluating effects of focal length and viewing angle,False,False,True
9368,"improvements.Neithertheseneural-networknormoretraditionalmethods,though,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9369,"havebeentesteddirectlyregardingperformancedifferencesduetocamera-lensfocal
",False,Evaluating effects of focal length and viewing angle,False,False,True
9370,"lengthnorcameraviewingangleofsubjectssystematicallyacrosstheviewing
",False,Evaluating effects of focal length and viewing angle,False,False,True
9371,"hemisphere.Thisworkusesphoto-realistic,synthesizedfacialimageswithvarying
",False,Evaluating effects of focal length and viewing angle,False,False,True
9372,"parametersandcorrespondingground-truthlandmarkstoenablecomparisonof
",False,Evaluating effects of focal length and viewing angle,False,False,True
9373,"alignmentandlandmarkdetectiontechniquesrelativetogeneralperformance,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9374,"performanceacrossfocallength,andperformanceacrossviewingangle.Recently
",False,Evaluating effects of focal length and viewing angle,False,False,True
9375,"publishedhigh-performingmethodsalongwithtraditionaltechniquesarecompared
",False,Evaluating effects of focal length and viewing angle,False,False,True
9376,"inregardstotheseaspects.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9377,"Keywords: Facialalignmentandlandmarking,Convolutionalneuralnetworks,Focal
",False,Evaluating effects of focal length and viewing angle,False,False,True
9378,"length,Viewangle,Comparison,Evaluation,Review
",False,Evaluating effects of focal length and viewing angle,False,False,True
9379,"1 Introduction
",False,Evaluating effects of focal length and viewing angle,False,False,True
9380,"Face detection, tracking, and recognition continue to be employed in a variety of ever
",False,Evaluating effects of focal length and viewing angle,False,False,True
9381,"more common-place biometric applications, particularly with recent integrations in
",False,Evaluating effects of focal length and viewing angle,False,False,True
9382,"mobile-device security and communication. Most of these applications, such as identity
",False,Evaluating effects of focal length and viewing angle,False,False,True
9383,"verification, pose tracking, expression analysis, and age or gender estimation, make use
",False,Evaluating effects of focal length and viewing angle,False,False,True
9384,"oflandmarkpointsaroundfacialcomponents.Correctlylocatingthesekeypointsiscru-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9385,"cial as they often are used to abstract main features such as the jaw, eye-brows, eyes,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9386,"nose shape, nostrils, and mouth [ 1]. Due to the complexity of head gestures , automatic
",False,Evaluating effects of focal length and viewing angle,False,False,True
9387,"localizingofcanonicallandmarksusuallyfirstinvolvesfacealignmenttoaccountforrota-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9388,"tion,translation,andscaleduetoposeorview-directiondifferences[ 2–5].Furthermore,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9389,"2D images photographically captured by cameras are affected by perspective and lens
",False,Evaluating effects of focal length and viewing angle,False,False,True
9390,"distortion,animportantaspectconsideredinthiswork.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9391,"Thisreviewaimstocompareperformanceoffivenotablefaciallandmarkandalignment
",False,Evaluating effects of focal length and viewing angle,False,False,True
9392,"methods under the effects of different camera focal lengths and positions, particularly
",False,Evaluating effects of focal length and viewing angle,False,False,True
9393,"under conditions that have been ignored or difficult to test. Previously, Çeliktutan et
",False,Evaluating effects of focal length and viewing angle,False,False,True
9394,"©TheAuthor(s). 2021 OpenAccess ThisarticleislicensedunderaCreativeCommonsAttribution4.0InternationalLicense,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9395,"whichpermitsuse,sharing,adaptation,distributionandreproductioninanymediumorformat,aslongasyougiveappropriate
",False,Evaluating effects of focal length and viewing angle,False,False,True
9396,"credittotheoriginalauthor(s)andthesource,providealinktotheCreativeCommonslicence,andindicateifchangeswere
",False,Evaluating effects of focal length and viewing angle,False,False,True
9397,"made. Theimagesorotherthirdpartymaterialinthisarticleareincludedinthearticle’sCreativeCommonslicence,unless
",False,Evaluating effects of focal length and viewing angle,False,False,True
9398,"indicatedotherwiseinacreditlinetothematerial. Ifmaterialisnotincludedinthearticle’sCreativeCommonslicenceandyour
",False,Evaluating effects of focal length and viewing angle,False,False,True
9399,"intendeduseisnotpermittedbystatutoryregulationorexceedsthepermitteduse,youwillneedtoobtainpermissiondirectly
",False,Evaluating effects of focal length and viewing angle,False,False,True
9400,"fromthecopyrightholder. Toviewacopyofthislicence,visit http://creativecommons.org/licenses/by/4.0/ .Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page2of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9401,"al. completed a thorough survey of facial landmark detection algorithms and compara-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9402,"tive performance in 2013, which at the time primarily focused on 2D techniques such as
",False,Evaluating effects of focal length and viewing angle,False,False,True
9403,"ActiveShapeModel(ASM)andActiveAppearanceModel(AAM)variations[ 6].In2018,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9404,"Johnston and Chazal published work that built on the earlier survey, noting the shift of
",False,Evaluating effects of focal length and viewing angle,False,False,True
9405,"interesttodeep-learningmethodsduetopotentialperformanceincreasesaswellastech-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9406,"niques that also perform 3D alignment [ 7]. Several strong-performing neural-network
",False,Evaluating effects of focal length and viewing angle,False,False,True
9407,"methods have been published since; however, and in general, no performance compar-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9408,"isonshaveincludedlens-perspectiveeffectsnorsystematicevaluationacrosstherangeof
",False,Evaluating effects of focal length and viewing angle,False,False,True
9409,"viewing angles. This study is not an exhaustive survey of recent methods but rather an
",False,Evaluating effects of focal length and viewing angle,False,False,True
9410,"investigationintheeffectsoffocallengthandviewingangleonbothtraditionalandmore
",False,Evaluating effects of focal length and viewing angle,False,False,True
9411,"recent neural methods (published after the 2018 article). Focal-length-based perspective
",False,Evaluating effects of focal length and viewing angle,False,False,True
9412,"and viewing angle are both important considerations if designing a biometric or other
",False,Evaluating effects of focal length and viewing angle,False,False,True
9413,"system in order to account for the lens chosen, viewing angle, and proximity necessary
",False,Evaluating effects of focal length and viewing angle,False,False,True
9414,"forthesystem.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9415,"The effects a lens imparts on acquisition have often been ignored in face-related
",False,Evaluating effects of focal length and viewing angle,False,False,True
9416,"research.A fundamental techniquein computervision isestimatingacameraprojection
",False,Evaluating effects of focal length and viewing angle,False,False,True
9417,"matrixandhasbeenregardedinmanystudies;however,thedatasetsusedtotrainandtest
",False,Evaluating effects of focal length and viewing angle,False,False,True
9418,"landmark detection do not usually include camera meta-data (particularly large datasets
",False,Evaluating effects of focal length and viewing angle,False,False,True
9419,"gleanedfromtheInternetfordeep-learningapproaches),ordatasetshavebeencaptured
",False,Evaluating effects of focal length and viewing angle,False,False,True
9420,"inverycontrolledsituationswithasinglelens.Themostwidelyuseddatabasesintraining
",False,Evaluating effects of focal length and viewing angle,False,False,True
9421,"recentdeepnetworksare300W[ 8],COFW[ 3],WFLW[ 9],andAFLW[ 10].Thosecover
",False,Evaluating effects of focal length and viewing angle,False,False,True
9422,"large variation over age, ethnicity, skin color, expression, and pose and have been used
",False,Evaluating effects of focal length and viewing angle,False,False,True
9423,"by top-performing deep neural networks [ 11–15]. None of them explicitly note focal-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9424,"length as a parameter. In short, there is no dataset published online that has considered
",False,Evaluating effects of focal length and viewing angle,False,False,True
9425,"focal length/field of view versus proximity for training alignment or landmark detection
",False,Evaluating effects of focal length and viewing angle,False,False,True
9426,"methods. We assume perspective distortions caused by focal length will likely affect the
",False,Evaluating effects of focal length and viewing angle,False,False,True
9427,"final annotation results. If so, training sets including camera and lens parameters could
",False,Evaluating effects of focal length and viewing angle,False,False,True
9428,"increaseaccuracyofasystemoratleastaidindesigningsystems.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9429,"A few researchers have considered aspects of image distortions relative to face images
",False,Evaluating effects of focal length and viewing angle,False,False,True
9430,"forparticularapplications,butnotwhatwepresenthere.Dameretal.investigatedstate-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9431,"of-the-art deep neural networks for facial landmark detection, but their main focus was
",False,Evaluating effects of focal length and viewing angle,False,False,True
9432,"perspective distortion due to distances between cameras and captured faces and did
",False,Evaluating effects of focal length and viewing angle,False,False,True
9433,"not consider the effects due to lenses and associated field of view [ 16]. Valente et al.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9434,"investigatedbasiclenseffects;however,theyonlyanalyzedtheserelativetosimplemath-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9435,"ematical algorithms for facial recognition (EIGENDETECT and SRC) and not those for
",False,Evaluating effects of focal length and viewing angle,False,False,True
9436,"facial alignment nor their effects on facial landmark detection [ 17]. Flores et al. also
",False,Evaluating effects of focal length and viewing angle,False,False,True
9437,"focused on perspective distortion caused by distance [ 18]. They estimated camera pose
",False,Evaluating effects of focal length and viewing angle,False,False,True
9438,"fromfacialimagesusingEfficientPerspectiven-Point(EPnP)ratherthanevaluatingland-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9439,"mark location. In this work, we consider the effects of lens focal length and viewing
",False,Evaluating effects of focal length and viewing angle,False,False,True
9440,"angleinregardstosomeofthehighestperformingrecentfacial-landmarkingtechniques.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9441,"Although the method of evaluation uses synthetic images, the question of performance
",False,Evaluating effects of focal length and viewing angle,False,False,True
9442,"relative to lens and viewing angle is also relative, and the goal is to demonstrate that all
",False,Evaluating effects of focal length and viewing angle,False,False,True
9443,"methods are affected to varying degree. Studying such effects without large datasets that
",False,Evaluating effects of focal length and viewing angle,False,False,True
9444,"include camera and lens meta-data would not currently be possible without either col-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9445,"lecting such a dataset or creating test images synthetically as we have done. Future work
",False,Evaluating effects of focal length and viewing angle,False,False,True
9446,"could include design of a dataset, although it could be prohibitive to collect data on theLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page3of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9447,"size order of Internet-driven datasets used for deep-learning training. Furture work also
",False,Evaluating effects of focal length and viewing angle,False,False,True
9448,"couldconsiderimprovingsyntheticallyrenderedimagesforhigherfidelity,style-transfer,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9449,"orin-environmentplacement,etc.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9450,"Thecontributionsofthisworkincludeevaluationoffivedifferentfaciallandmarkdetec-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9451,"tionmethodsinregardstovaryinglenschoiceandviewingangle.Threeofthemarefrom
",False,Evaluating effects of focal length and viewing angle,False,False,True
9452,"recently published deep-learning 3D facial annotating methods, and the remaining two
",False,Evaluating effects of focal length and viewing angle,False,False,True
9453,"are AAM implementations. We evaluate the performance of these methods across view
",False,Evaluating effects of focal length and viewing angle,False,False,True
9454,"anglesandfocallengthsbyusingfaceimagessynthesizedfromdetailed3Dscansofindi-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9455,"viduals.We demonstrate that all are subject to particular performance degradation with
",False,Evaluating effects of focal length and viewing angle,False,False,True
9456,"lens-perspective distortion and viewing angle. This information may be used to guide
",False,Evaluating effects of focal length and viewing angle,False,False,True
9457,"designchoicesinbiometricorotherimagingsystemsaswellasdeveloponmethodsthat
",False,Evaluating effects of focal length and viewing angle,False,False,True
9458,"aremorerobusttolenschoiceandangle.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9459,"2 Methodofcomparison
",False,Evaluating effects of focal length and viewing angle,False,False,True
9460,"2.1 Landmarkschemes
",False,Evaluating effects of focal length and viewing angle,False,False,True
9461,"There have been a variety of landmark schemes used in related projects, but a few have
",False,Evaluating effects of focal length and viewing angle,False,False,True
9462,"been most used in recent work and make a logical choice for comparative evaluation.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9463,"Following the categories in [ 6], there are two major groups of facial landmarks schemes:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9464,"primary landmarks and secondary landmarks. Primary landmarks usually define the eye
",False,Evaluating effects of focal length and viewing angle,False,False,True
9465,"corners,themouthcorners,andthenosetip.Thoselandmarksarelocatedat“T”sections
",False,Evaluating effects of focal length and viewing angle,False,False,True
9466,"between boundaries or at high curvatures on a face which may be detected by image
",False,Evaluating effects of focal length and viewing angle,False,False,True
9467,"processingalgorithms,e.g.,multi-resolutionshapemodels[ 19],HarrisCornerDetection
",False,Evaluating effects of focal length and viewing angle,False,False,True
9468,"model[20],orImageGradientOrientation(IGO)model[ 21].Secondarylandmarksout-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9469,"line the contour of main features that are guided by primary landmarks, such as the jaw
",False,Evaluating effects of focal length and viewing angle,False,False,True
9470,"line, eyebrows, and nostrils. Wu et al. [ 1] provide a thorough survey on facial landmark
",False,Evaluating effects of focal length and viewing angle,False,False,True
9471,"databases and their corresponding landmark schemes. A common 68-point landmark is
",False,Evaluating effects of focal length and viewing angle,False,False,True
9472,"supportedbymanyfacedatabases,e.g.,AFLW[ 10],BU-4DFE[ 22],Helen[ 8,23],etc.For
",False,Evaluating effects of focal length and viewing angle,False,False,True
9473,"easiestconsistency,the68-pointschemefromMulti-PIE[ 24],andfurtherpopularizedby
",False,Evaluating effects of focal length and viewing angle,False,False,True
9474,"iBUG’s300W[ 8],waschosenforthisstudy.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9475,"Sagonas et al. and Johnston et al. [ 7,8] state that primary landmarks are more easily
",False,Evaluating effects of focal length and viewing angle,False,False,True
9476,"detected than secondary landmarks while annotating the ground-truth reference. The
",False,Evaluating effects of focal length and viewing angle,False,False,True
9477,"“m7 landmarks” including the 4 eye corners, 1 nose tip, and 2 mouth corners are also
",False,Evaluating effects of focal length and viewing angle,False,False,True
9478,"included here in some comparisons with the idea that they provide higher importance
",False,Evaluating effects of focal length and viewing angle,False,False,True
9479,"information.Figure 1showsthetwolandmarksschemesusedinthispaper.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9480,"In order to generate face images at controlled focal lengths and precise angle selec-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9481,"tions, we synthesized photo-realistic images using detailed 3D meshes captured from a
",False,Evaluating effects of focal length and viewing angle,False,False,True
9482,"structured-light 3dMD system. Our facial capture participants were asked to make dif-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9483,"ferent expressions following the Facial Action Coding System (FACS). FACS was created
",False,Evaluating effects of focal length and viewing angle,False,False,True
9484,"by the anatomist Carl-Herman Hjortsjö [ 25] and further developed by Ekman etc. [ 26]
",False,Evaluating effects of focal length and viewing angle,False,False,True
9485,"It provides a coding system which describes how to categorize facial expressions into
",False,Evaluating effects of focal length and viewing angle,False,False,True
9486,"Action Units (AUs) with muscle movements. We manually annotated the ground-truth
",False,Evaluating effects of focal length and viewing angle,False,False,True
9487,"landmarksin3Dfor84facesfromourparticipants,64fromasetofFACS-captureexpres-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9488,"sions of two individuals and 20 of unique individuals with a range of ethnicity, age, and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9489,"gender where the pose was neutral or a slight smile. Figure 2shows an example of FACS
",False,Evaluating effects of focal length and viewing angle,False,False,True
9490,"and neutral faces in our dataset. Landmark variation often occurs between in datasets,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9491,"particularly for areas such as the jawline or eyebrows. For consistency, we keep jawlineLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page4of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9492,"Fig.1Twodifferentlandmarksschemes. aiBUG-68. bM7
",False,Evaluating effects of focal length and viewing angle,False,False,True
9493,"points evenly distributed along the chin. In some projects, eyebrow points are placed at
",False,Evaluating effects of focal length and viewing angle,False,False,True
9494,"thecenter,bottom,ortopofbrowarcs.Goodchoicesforlandmarkspointsincludethose
",False,Evaluating effects of focal length and viewing angle,False,False,True
9495,"near high curvature or boundaries on objects. Here, eyebrows are marked anatomically
",False,Evaluating effects of focal length and viewing angle,False,False,True
9496,"atthesupraorbitalridgeoreyebrowridge.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9497,"2.2 Evaluationmetrics
",False,Evaluating effects of focal length and viewing angle,False,False,True
9498,"We use ground-truth based localization error to evaluate performance in each case via
",False,Evaluating effects of focal length and viewing angle,False,False,True
9499,"root mean squared error (RMSE). Accurate landmarks are generated for each synthetic
",False,Evaluating effects of focal length and viewing angle,False,False,True
9500,"imagebyprojectingmanual3Dlandmarkstomatchtherenderedangleandfieldofview.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9501,"WeusethemethodproposedbyJohnstonetal.[ 7]forcalculatingtheRMSE:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9502,"Fig.2FACSandNFACSfaces. aItisalippuckerexpressionanditscorrespondingactionunitis18[ 26].bThe
",False,Evaluating effects of focal length and viewing angle,False,False,True
9503,"participantprovidesaneutralexpressionLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page5of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9504,"RMSE =1
",False,Evaluating effects of focal length and viewing angle,False,False,True
9505,"KK/summationdisplay
",False,Evaluating effects of focal length and viewing angle,False,False,True
9506,"k=1/radicalBig/parenleftbig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9507,"xk−˜xk/parenrightbig2+/parenleftbig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9508,"yk−˜yk/parenrightbig2(1)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9509,"wherexk,ykdenote each of the K predicted landmark kin an image, and ˜xk,˜ykindicate
",False,Evaluating effects of focal length and viewing angle,False,False,True
9510,"the corresponding ground-truth landmark. Normalizing for face size in pixels is useful
",False,Evaluating effects of focal length and viewing angle,False,False,True
9511,"due to the variance across images. Previously, RMSE is normalized by the ground-truth
",False,Evaluating effects of focal length and viewing angle,False,False,True
9512,"outercornersofthelefteyeandrighteyelandmarks(Eq. 3)[8].Theerrorperlandmarkin
",False,Evaluating effects of focal length and viewing angle,False,False,True
9513,"imageiisgivenas:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9514,"/epsilon1k
",False,Evaluating effects of focal length and viewing angle,False,False,True
9515,"i =/radicalbigg/parenleftBig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9516,"xk
",False,Evaluating effects of focal length and viewing angle,False,False,True
9517,"i−˜xk
",False,Evaluating effects of focal length and viewing angle,False,False,True
9518,"i/parenrightBig2
",False,Evaluating effects of focal length and viewing angle,False,False,True
9519,"+/parenleftBig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9520,"yk
",False,Evaluating effects of focal length and viewing angle,False,False,True
9521,"i−˜yk
",False,Evaluating effects of focal length and viewing angle,False,False,True
9522,"i/parenrightBig2
",False,Evaluating effects of focal length and viewing angle,False,False,True
9523,"dnormi (2)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9524,"dnormi=/radicalBig/parenleftbig˜xle−˜xre/parenrightbig2+(˜yle−˜yre)2(3)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9525,"where (˜xle,˜yle)and(˜xre,˜yre)are the ground-truth outer corners of the left eye and right
",False,Evaluating effects of focal length and viewing angle,False,False,True
9526,"eyeintheimage i.Inourcase,however,oursyntheticimagesvarywithcamerapositions.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9527,"Thedistancesofouter-eyecornersmayhavesmallimpactsatsideanglesduetoperspec-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9528,"tive projection. Hence, we calculate Normalized Root Mean Squared Error (NRMSE) by
",False,Evaluating effects of focal length and viewing angle,False,False,True
9529,"normalizingperwidthoftheheadboundingbox.Wecalculatethepercentageofaccepted
",False,Evaluating effects of focal length and viewing angle,False,False,True
9530,"pointsamongallpointstoshowtheperformanceforeachalgorithm:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9531,"P(k)=1001
",False,Evaluating effects of focal length and viewing angle,False,False,True
9532,"II/summationdisplay
",False,Evaluating effects of focal length and viewing angle,False,False,True
9533,"i=1/bracketleftBig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9534,"i:/epsilon1k
",False,Evaluating effects of focal length and viewing angle,False,False,True
9535,"i<Th/bracketrightBig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9536,"(4)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9537,"where [i:/epsilon1k
",False,Evaluating effects of focal length and viewing angle,False,False,True
9538,"i<Th] is a mask function that if the normalized distance /epsilon1is less than Th,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9539,"itisacceptable,andiissetto1.Otherwise,theresultisnotacceptable,and i’svalueisset
",False,Evaluating effects of focal length and viewing angle,False,False,True
9540,"to0.So,theoverallperformanceoverKlandmarksineachimageforIimagesetis:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9541,"P=1001
",False,Evaluating effects of focal length and viewing angle,False,False,True
9542,"K×IK/summationdisplay
",False,Evaluating effects of focal length and viewing angle,False,False,True
9543,"k=1I/summationdisplay
",False,Evaluating effects of focal length and viewing angle,False,False,True
9544,"i=1/bracketleftBig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9545,"i:/epsilon1k
",False,Evaluating effects of focal length and viewing angle,False,False,True
9546,"i<Th/bracketrightBig
",False,Evaluating effects of focal length and viewing angle,False,False,True
9547,"(5)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9548,"2.3 Camerapositionandfocallength
",False,Evaluating effects of focal length and viewing angle,False,False,True
9549,"Our coordinate system follows the typical computer-graphics right-handed coordinate
",False,Evaluating effects of focal length and viewing angle,False,False,True
9550,"system convention, where the X-axis points to horizontal right, Y-axis points to vertical
",False,Evaluating effects of focal length and viewing angle,False,False,True
9551,"up, andZ-axis perpendicular to both XandYpoints outward from the screen. In order
",False,Evaluating effects of focal length and viewing angle,False,False,True
9552,"to track the camera around each face, we use spherical coordinates to represent camera
",False,Evaluating effects of focal length and viewing angle,False,False,True
9553,"positions. Our interests are analyzing multiple viewing angles at a wide range of specific
",False,Evaluating effects of focal length and viewing angle,False,False,True
9554,"viewing angles. We define camera positions in spherical coordinates at (r,φ,θ),w h e r e
",False,Evaluating effects of focal length and viewing angle,False,False,True
9555,"φis the polar angle (also known as zenith angle) from the positive Y-axis with 45◦≤
",False,Evaluating effects of focal length and viewing angle,False,False,True
9556,"φ≤135◦,a t1 5◦each. We define θto be the azimuthal angle in the xy-plane from the
",False,Evaluating effects of focal length and viewing angle,False,False,True
9557,"positiveX-axis with 180◦≤θ≤0◦at intervals 30◦.L a s t l y ,rvaries for simulated focal
",False,Evaluating effects of focal length and viewing angle,False,False,True
9558,"length. Overall, we have 49 camera positions so that various front views of the face and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9559,"some extreme camera positions could be tested. Figure 3shows the position of spherical
",False,Evaluating effects of focal length and viewing angle,False,False,True
9560,"coordinatesandsamplesoffaceimageswithdifferentviewingangles.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9561,"Focallength,relativetothedimensionsofthefilmordigitalsensor,determinesthefield
",False,Evaluating effects of focal length and viewing angle,False,False,True
9562,"of view on a physical camera, and there are also radial distortion issues relative to physi-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9563,"cal lenses and typical of certain optical designs such as pincushion and barrel distortion
",False,Evaluating effects of focal length and viewing angle,False,False,True
9564,"(thesearenotspecificallyincludedherebutcouldwarrantafollow-upstudy).Inphotog-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9565,"raphy,acommonstandardofcomparisonoffocallengthtoexpressfieldofviewisrelative
",False,Evaluating effects of focal length and viewing angle,False,False,True
9566,"tothestandardofthe35-mm-filmframesizeusedformuchofthetwentiethcenturyandLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page6of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9567,"Fig.3Camerapositions. aRight-handedsystemsphericalcoordinates. φisthepolarangleand θisthe
",False,Evaluating effects of focal length and viewing angle,False,False,True
9568,"azimuthalangle. bSamplesforrealfaceimagestakenfromdifferentviewingangles
",False,Evaluating effects of focal length and viewing angle,False,False,True
9569,"carriedforwardintodigitalsensors.This“35-mm”framesizeof36mmacrossby24mm
",False,Evaluating effects of focal length and viewing angle,False,False,True
9570,"downcametobeastandardforstillphotographywhenOskarBarnackdoubledtheindi-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9571,"vidual frame from motion-picture film (standardized by Thomas Edison) to use in still
",False,Evaluating effects of focal length and viewing angle,False,False,True
9572,"cameras.Therelationbetweenangleofviewandfocallengthisgivenby:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9573,"α=2arctand
",False,Evaluating effects of focal length and viewing angle,False,False,True
9574,"2f(6)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9575,"where αis the angle of view, ddenotes the size of film, and fis the focal length t. As can
",False,Evaluating effects of focal length and viewing angle,False,False,True
9576,"beseenbytherelationship,shorterfocallengthswidenthefieldofviewandvice-versa.To
",False,Evaluating effects of focal length and viewing angle,False,False,True
9577,"maintain a face of a relative size in images captured with different focal lengths, the dis-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9578,"tancetothecameraneedstobechanged.Perspectiveeffectsaremodifiedasthisoccurs,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9579,"ascanbenotedinFig. 4. Short focal lengths (wide-angle lenses) introduce a fair amount
",False,Evaluating effects of focal length and viewing angle,False,False,True
9580,"of facial distortion whereas longer lengths begin to approximate an orthographic projec-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9581,"tionthatmaintainsrelativedistancesamonglandmarksbetter.Althoughnottestedhere,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9582,"theseeffectscanbemorepronouncedneartheedgesofacaptureframe.Asmobilephone
",False,Evaluating effects of focal length and viewing angle,False,False,True
9583,"photography increases, some of the most common focal lengths relative to the standard
",False,Evaluating effects of focal length and viewing angle,False,False,True
9584,"of comparison noted would equate to the 28-mm to 35-mm range of focal lengths, or a
",False,Evaluating effects of focal length and viewing angle,False,False,True
9585,"relatively wide field of view. Interchangeable lens cameras or cameras with zoom lenses
",False,Evaluating effects of focal length and viewing angle,False,False,True
9586,"can vary the focal length. As can be seen from formula 6, a larger focal length lens has
",False,Evaluating effects of focal length and viewing angle,False,False,True
9587,"a narrower angle of view at the same camera-to-object distance which offers magnified,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9588,"detailed photos. Focal lengths greater than 50 mm are often used in longer range pho-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9589,"tography, long range biometric acquisition, and especially in head-and-shoulder portrait
",False,Evaluating effects of focal length and viewing angle,False,False,True
9590,"photography.Forthisstudy,commonfocallengthsofprimelensesusedinstillphotogra-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9591,"phy were chosen as the range, from 24 mm (wide-angle on a 35-mm system) to 135-mm
",False,Evaluating effects of focal length and viewing angle,False,False,True
9592,"(slighttelephotoona35-mmsystem),withtherangecoveringtypicalfocallengthsusedin
",False,Evaluating effects of focal length and viewing angle,False,False,True
9593,"photography and not including extreme wide-angle lenses nor extreme telephoto lenses.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9594,"We choose six different types of common lens focal lengths (24 mm, 28 mm, 35 mm,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9595,"50mm,85mm,and135mm)asourtestdomainsforcomparison.Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page7of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9596,"Fig.4Camerafocallengths:Firstrowfromlefttoright:24mm,28mm,and35mm;secondrowfromleftto
",False,Evaluating effects of focal length and viewing angle,False,False,True
9597,"right:50mm,85mm,and135mm
",False,Evaluating effects of focal length and viewing angle,False,False,True
9598,"2.4 Facelandmarkandalignmentmethods
",False,Evaluating effects of focal length and viewing angle,False,False,True
9599,"Wu et al.[ 1]mentionclassifyingtechnologyasholisticmethods,constrainedlocalmodel
",False,Evaluating effects of focal length and viewing angle,False,False,True
9600,"methods,andregression-basedmethods.Holisticmethodstreatawholefaceimageasthe
",False,Evaluating effects of focal length and viewing angle,False,False,True
9601,"entireappearanceandshapetotrainmodels.Constrainedlocalmodelslocatelandmarks
",False,Evaluating effects of focal length and viewing angle,False,False,True
9602,"based on the global face but emphasizing local features around landmarks. Regression-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9603,"based methods mostly are adopted for deep-learning, using regression analysis to map
",False,Evaluating effects of focal length and viewing angle,False,False,True
9604,"landmarks to images directly. Johnston et al. [ 7] believe that facial landmark detection
",False,Evaluating effects of focal length and viewing angle,False,False,True
9605,"methods can be divided into generative methods, discriminate methods, and statistical
",False,Evaluating effects of focal length and viewing angle,False,False,True
9606,"methods. Generative methods minimize the error between models and facial recon-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9607,"structions.Discriminatemethodsuseadatasettotraintheregressionmodels.Statistical
",False,Evaluating effects of focal length and viewing angle,False,False,True
9608,"modelsareacombinationofgenerativemethodsanddiscriminatemethods.Çeliktutanet
",False,Evaluating effects of focal length and viewing angle,False,False,True
9609,"al. classify facial landmark detection into model-based (using the entire face region) and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9610,"texture-based (matching landmarks to local features) [ 6]. Here we consider landmark-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9611,"ing algorithms based on either statistical methods or deep-learning methods. Statistical
",False,Evaluating effects of focal length and viewing angle,False,False,True
9612,"methodscalculatethepositionsoflandmarksusingmathematicalalgorithms.Mostofthe
",False,Evaluating effects of focal length and viewing angle,False,False,True
9613,"traditionalmethods(e.g.,AAMandASM)canfallintothisgroup.Deeplearningmethods
",False,Evaluating effects of focal length and viewing angle,False,False,True
9614,"feedfacialimagestotraindeepneuralnetworkstolocatelandmarks.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9615,"ASM and AAM models have performed among some of the best landmark-detection
",False,Evaluating effects of focal length and viewing angle,False,False,True
9616,"algorithms for nearly two decades. ASM, first introduced by Cootes et al., attempts to
",False,Evaluating effects of focal length and viewing angle,False,False,True
9617,"detect and measure the expected shape of a target in an image. ASM requires a set of
",False,Evaluating effects of focal length and viewing angle,False,False,True
9618,"landmarked images for training the model. The first step is using Procrustes Analysis
",False,Evaluating effects of focal length and viewing angle,False,False,True
9619,"to align all object images. A mean shape is calculated by Principle Component AnalysisLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page8of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9620,"(PCA) which applied to find eigen vectors and eigen values [ 27]. All the objects’ shapes
",False,Evaluating effects of focal length and viewing angle,False,False,True
9621,"canbeapproximatedas:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9622,"x=¯x+Pb (7)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9623,"where ¯xis the mean shape calculated over all overall training data. Pis a set of eigen
",False,Evaluating effects of focal length and viewing angle,False,False,True
9624,"vectors derived from the covariance matrix calculated via PCA, and bis a set of shape
",False,Evaluating effects of focal length and viewing angle,False,False,True
9625,"parametersgivenby:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9626,"b=PT(x−¯x) (8)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9627,"As an improvement of ASM, an active appearance model matches both shape and tex-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9628,"ture simultaneously and gives an optimal parameterized model. PCA is also applied for
",False,Evaluating effects of focal length and viewing angle,False,False,True
9629,"textureandonceagainforfindingcombinedappearanceparametersandvectors.Menpo
",False,Evaluating effects of focal length and viewing angle,False,False,True
9630,"provides five different AAM versions with two main groups: Holistic AAM (HAAM)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9631,"and Patch AAM (PAAM) [ 28]. HAAM warps appearance information using a nonlin-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9632,"ear function, such as Thin Plate Spline (TPS), and takes the whole texture into account
",False,Evaluating effects of focal length and viewing angle,False,False,True
9633,"when fitting, while the PAAM uses rectangular patches around each landmark as tex-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9634,"tureappearance.WetestbothHAAMandPAAMasseparatetechniquesforcomparison
",False,Evaluating effects of focal length and viewing angle,False,False,True
9635,"here. For building the AAM, we chose the widely used Helen Dataset which provides a
",False,Evaluating effects of focal length and viewing angle,False,False,True
9636,"high-resolution set of annotated facial images containing different ethnicities, ages, gen-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9637,"ders, head poses, facial expressions, and skin colors, similarly used by Johnston et al. [ 7].
",False,Evaluating effects of focal length and viewing angle,False,False,True
9638,"In order to reduce error caused by facial detection, we extract faces from image using
",False,Evaluating effects of focal length and viewing angle,False,False,True
9639,"boundingboxescalculatedfromground-truthlandmarksanddilatedby5%.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9640,"Inthepastfewyears,deep-learningbasedneural-networkmethodshaveleveragedvery
",False,Evaluating effects of focal length and viewing angle,False,False,True
9641,"large datasets for training and recently outperformed statistical shape and appearance
",False,Evaluating effects of focal length and viewing angle,False,False,True
9642,"modelsinmanyareas.Wegatheredthreerecenthigh-performingmethodswhereimple-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9643,"mentations were available to compare in our various cases. The first method is called
",False,Evaluating effects of focal length and viewing angle,False,False,True
9644,"the Position Map Regression Network [ 29]. The main idea of PRNet is creating a 2D UV
",False,Evaluating effects of focal length and viewing angle,False,False,True
9645,"Position Map which contains the shape of an entire face to predict 3D positions. PRNet
",False,Evaluating effects of focal length and viewing angle,False,False,True
9646,"employs a convolutional neural network (CNN) trained 2D images along with ground
",False,Evaluating effects of focal length and viewing angle,False,False,True
9647,"truth 3D dense position clouds created via 3D morphable model (3DMM). 3D positions
",False,Evaluating effects of focal length and viewing angle,False,False,True
9648,"are projected to the UV texture-map format and used in training the CNN. The UV
",False,Evaluating effects of focal length and viewing angle,False,False,True
9649,"texturemappreserves3Dinformation,evenposedwithocclusions.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9650,"The second method is the 3D Face Alignment Network (3D-FAN). Bulat and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9651,"Tzimiropoulos use a 2D-to-3D Face Alignment Network combined with a stacked
",False,Evaluating effects of focal length and viewing angle,False,False,True
9652,"heat-mapsub-networktopredict Zcoordinatesalongwith2Dlandmarks[ 30].
",False,Evaluating effects of focal length and viewing angle,False,False,True
9653,"The third method from Bahagavatula et al. uses a 3D Spatial Transformer Network
",False,Evaluating effects of focal length and viewing angle,False,False,True
9654,"(3DSTN)toestimateacameraprojectionmatrixinordertoreconstruct3Dfacialgeom-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9655,"etry. The method forms occluded faces with 2D landmark regression and predicts 3D
",False,Evaluating effects of focal length and viewing angle,False,False,True
9656,"landmarklocations[ 31].
",False,Evaluating effects of focal length and viewing angle,False,False,True
9657,"These methods were trained on 300W-LP except for 3D-FAN which was trained on
",False,Evaluating effects of focal length and viewing angle,False,False,True
9658,"the 230,000 + 300W-LP. It would be prohibitive to attempt to include all recent deep-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9659,"learningmethodsinthiscomparison,butthesewerechosenbasedonstrongperformance
",False,Evaluating effects of focal length and viewing angle,False,False,True
9660,"in recent publications, and we believe other recent methods would very likely perform
",False,Evaluating effects of focal length and viewing angle,False,False,True
9661,"similarlybasedonsimilaroverallperformanceonthesamedatasets.Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page9of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9662,"Fig.5Flowchartofthemainprocessformeasurement
",False,Evaluating effects of focal length and viewing angle,False,False,True
9663,"3 Procedure
",False,Evaluating effects of focal length and viewing angle,False,False,True
9664,"Figure5illustrates the main work flow of our approach to evaluate facial landmark and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9665,"alignmentalgorithms.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9666,"TocalculatetheRMSE( 1)andNRMSE( 2),(3)onlandmarks,allmeasurementsrequire
",False,Evaluating effects of focal length and viewing angle,False,False,True
9667,"ground-truth as references. All facial meshes with texture were manually marked using
",False,Evaluating effects of focal length and viewing angle,False,False,True
9668,"landmarker.io to create these ground-truth landmarks. Figure 6shows an example of 3D
",False,Evaluating effects of focal length and viewing angle,False,False,True
9669,"facialannotationinlandmarker.ioasperformedonourdataset[ 28].
",False,Evaluating effects of focal length and viewing angle,False,False,True
9670,"Using our own Python-, Qt-, and OpenGL-based lab application, Countenance Tool,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9671,"we render 3D facial positions given varying angles and focal lengths. Since we compare
",False,Evaluating effects of focal length and viewing angle,False,False,True
9672,"howviewanglesandfocallengthsaffectlandmarkmethods,wemovethevirtualcamera
",False,Evaluating effects of focal length and viewing angle,False,False,True
9673,"to 49 different locations shown in Fig. 3. At each location, we rasterize faces with 6 dif-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9674,"ferent synthesized focal lengths (24 mm, 28 mm, 35 mm, 50 mm, 85 mm, and 135 mm)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9675,"by changing the focal length parameter shown in equation 6before rendering. Overall,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9676,"there are images at 49 angles and 6 focal lengths for each face. At the same time, we use
",False,Evaluating effects of focal length and viewing angle,False,False,True
9677,"the same camera matrices (varying with view of angles and focal length parameters) to
",False,Evaluating effects of focal length and viewing angle,False,False,True
9678,"projectthe3Dground-truthlandmarkstoyieldtheground-truth2Dlandmarksatimage
",False,Evaluating effects of focal length and viewing angle,False,False,True
9679,"coordinates. Figure 7shows a set of images with ground-truth landmarks of different
",False,Evaluating effects of focal length and viewing angle,False,False,True
9680,"focallengthsandviewingangles.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9681,"TosummarizetheworkflowdemonstratedinFig. 5,wefirstperformedfacialgeometry
",False,Evaluating effects of focal length and viewing angle,False,False,True
9682,"capturewitha3dMDsystem.The3dMDsystemprovided3Dmeshesalongwithtexture
",False,Evaluating effects of focal length and viewing angle,False,False,True
9683,"information. We then imported those into landmarker.io to annotate each face manually
",False,Evaluating effects of focal length and viewing angle,False,False,True
9684,"to generate 3D ground-truth landmarks. After getting the ground-truth, we rasterized
",False,Evaluating effects of focal length and viewing angle,False,False,True
9685,"each face at 49 angles and 6 focal lengths and calculated the ground-truth 2D landmark
",False,Evaluating effects of focal length and viewing angle,False,False,True
9686,"locations.Finally,weanalyzedperformanceofeachmethodbycalculatingNRMSEerror
",False,Evaluating effects of focal length and viewing angle,False,False,True
9687,"betweenamethod’spredictedlandmarksandthe2Dground-truthlocations.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9688,"Fig.63Dground-truthlandmarkingusinglandmarker.ioLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page10of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9689,"Fig.7Converted2Dground-truthonimages. aandbareground-truthlandmarkswith24mmand135mm
",False,Evaluating effects of focal length and viewing angle,False,False,True
9690,"focallengthatthecenterview,respectively. canddarerenderedlandmarksfromtopviewanddownview
",False,Evaluating effects of focal length and viewing angle,False,False,True
9691,"with135mmfocallength
",False,Evaluating effects of focal length and viewing angle,False,False,True
9692,"4 Resultsanddiscussion
",False,Evaluating effects of focal length and viewing angle,False,False,True
9693,"In this section, we compare the RMSE performance of the five methods with the full
",False,Evaluating effects of focal length and viewing angle,False,False,True
9694,"68-points scheme and the reduced m7 scheme against 6 threshold levels. Figure 8plots
",False,Evaluating effects of focal length and viewing angle,False,False,True
9695,"the percentage correctly accepted for each facial landmark and alignment method with
",False,Evaluating effects of focal length and viewing angle,False,False,True
9696,"both schemes. Generally speaking, as expected, the overall acceptance performance for
",False,Evaluating effects of focal length and viewing angle,False,False,True
9697,"each algorithm increases as the threshold widens. The m7 landmarking scheme tends
",False,Evaluating effects of focal length and viewing angle,False,False,True
9698,"to show better performance as a smaller set located at distinct “corners.” In general, the
",False,Evaluating effects of focal length and viewing angle,False,False,True
9699,"CNNmethodsperformbetter,butallarestillsubjecttoperformanceeffectsduetofocal
",False,Evaluating effects of focal length and viewing angle,False,False,True
9700,"lengths and viewing angles. It would be remiss to declare one method particularly better
",False,Evaluating effects of focal length and viewing angle,False,False,True
9701,"thananotherhere,particularlysince3D-FANwastrainedonanaugmenteddatasetversus
",False,Evaluating effects of focal length and viewing angle,False,False,True
9702,"theothers;weusedthepubliclyavailablepre-trainednetworks.Comparedtotheneural-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9703,"network techniques,the performance oftraditionalstatisticalmethodsistypicallylower.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9704,"AsCootesexplains[ 32],theperformanceofASMandAAMisdependentonthestartingLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page11of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9705,"Fig.8ResultswithM7(left)andiBUG(right)landmarksarebothplottedforcomparison.Performance
",False,Evaluating effects of focal length and viewing angle,False,False,True
9706,"respecttothresholdsusing68pts
",False,Evaluating effects of focal length and viewing angle,False,False,True
9707,"position of landmark displacement. Higher accuracy of face detection tends to improve
",False,Evaluating effects of focal length and viewing angle,False,False,True
9708,"landmarkdetection.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9709,"One of the main contributions of this paper is demonstrating the effect of focal length
",False,Evaluating effects of focal length and viewing angle,False,False,True
9710,"on landmarking accuracy. Figure 9demonstrates lower performance with a wider field-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9711,"of-view,associatedwithstrongperspectiveeffects,andbetterperformanceasfocallength
",False,Evaluating effects of focal length and viewing angle,False,False,True
9712,"increases.Thereisexpectedlevelinginperformancewithfocallengthincrease.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9713,"In order to visualize effects on specific landmarks at different focal lengths, we drew
",False,Evaluating effects of focal length and viewing angle,False,False,True
9714,"the 68-point landmarks located by each method and the average of the frontal view for
",False,Evaluating effects of focal length and viewing angle,False,False,True
9715,"theextremes(135mmlensinbluecirclesbasedonRMSEand24mminred).Thisshows
",False,Evaluating effects of focal length and viewing angle,False,False,True
9716,"which landmarks are most affected by the focal-length perspective warping. Figure 10
",False,Evaluating effects of focal length and viewing angle,False,False,True
9717,"alsoreflectsthedatadepictedintheFig. 9.TheradiusoftheRMSEpresentshowfareach
",False,Evaluating effects of focal length and viewing angle,False,False,True
9718,"predicted landmark is from the ground-truth. The result shows that all of the landmarks
",False,Evaluating effects of focal length and viewing angle,False,False,True
9719,"thatareclosetothecenteroffaceshavemoreaccuratepredictions,whilelandmarksalong
",False,Evaluating effects of focal length and viewing angle,False,False,True
9720,"facial edges have lower accuracy predictions due to projective distortions; particularly,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9721,"cornersofeyesandlipsseemaffected.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9722,"The last consideration for this paper is systematic adjustment of the camera’s viewing
",False,Evaluating effects of focal length and viewing angle,False,False,True
9723,"angle across the viewing hemisphere. We place the camera at 49 different positions with
",False,Evaluating effects of focal length and viewing angle,False,False,True
9724,"extreme poses included. When the camera views from the center ( θ∼=90◦andφ∼=
",False,Evaluating effects of focal length and viewing angle,False,False,True
9725,"90◦), the performance results are better than when the camera view from the sides. The
",False,Evaluating effects of focal length and viewing angle,False,False,True
9726,"landmark predictions at φaround 45◦and 135◦have the lowest performances due to
",False,Evaluating effects of focal length and viewing angle,False,False,True
9727,"extreme viewing angles. As expected, performance drops as the view moves to the more
",False,Evaluating effects of focal length and viewing angle,False,False,True
9728,"extremeangles,andtherateofeffectforeachmethodareshowninFigs. 11,12,13,14,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9729,"and15.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9730,"Fig.9FocalLengthVarying:LefthalfofthechartusesM7,righthalfuses68pointsLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page12of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9731,"Fig.10RNSEdistance.BluecirclesareRMSEat135mm,redcirclesare24mm. aHAAM,bPAAM,cPRNet,d
",False,Evaluating effects of focal length and viewing angle,False,False,True
9732,"3DSTN,e3D-Fan,and faverageresultforallalgorithms
",False,Evaluating effects of focal length and viewing angle,False,False,True
9733,"Most of the facial landmark and alignment algorithms perform well at frontal views,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9734,"and the detection precision relies on the training set variability. Attempting to delineate
",False,Evaluating effects of focal length and viewing angle,False,False,True
9735,"prediction differences between extreme-view cases and center view cases, we chose the
",False,Evaluating effects of focal length and viewing angle,False,False,True
9736,"most centered view image ( φ=90◦andθ=90◦), as well as 8 images surrounding by
",False,Evaluating effects of focal length and viewing angle,False,False,True
9737,"it, to be the frontal group (Fig. 16). The rest of the images are the outer group (Fig. 17).
",False,Evaluating effects of focal length and viewing angle,False,False,True
9738,"Front view detection can approach almost 100% accuracy especially at center view for
",False,Evaluating effects of focal length and viewing angle,False,False,True
9739,"deep neural networking methods. The precision rate drops more than 50% approaching
",False,Evaluating effects of focal length and viewing angle,False,False,True
9740,"extremeangles( θ=0◦andθ=180◦).
",False,Evaluating effects of focal length and viewing angle,False,False,True
9741,"Part of the set of the images used were also based on 3D captures of action units
",False,Evaluating effects of focal length and viewing angle,False,False,True
9742,"from FACS which taxonomizes individual physical expression of emotions. The results
",False,Evaluating effects of focal length and viewing angle,False,False,True
9743,"s h o w ni nF i g . 18illustrate that in general the landmark-prediction methods work better
",False,Evaluating effects of focal length and viewing angle,False,False,True
9744,"on neutral faces due to FACS faces having more facial expressions which increase pre-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9745,"diction difficulties. Performance decreases across wide field of view and view angle are
",False,Evaluating effects of focal length and viewing angle,False,False,True
9746,"consistent.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9747,"Fig.11PRNet:camerapositionvaryingacross φ(from45◦to135◦)andθ(from0◦to180◦)Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page13of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9748,"Fig.123DFan:camerapositionvaryingacross φ(from45◦to135◦)andθ(from0◦to180◦)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9749,"Fig.133DSTN:camerapositionvaryingacross φ(from45◦to135◦)andθ(from0◦to180◦)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9750,"Fig.14HAAM:camerapositionvaryingacross φ(from45◦to135◦)andθ(from0◦to180◦)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9751,"Fig.15PAAM:camerapositionvaryingacross φ(from45◦to135◦)andθ(from0◦to180◦)Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page14of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9752,"Fig.16Camerapositionwithgroupoffrontalview.Thefrontalviewpositionswhere φ=75◦,90◦,and105◦;
",False,Evaluating effects of focal length and viewing angle,False,False,True
9753,"θ=60◦,90◦,and120◦
",False,Evaluating effects of focal length and viewing angle,False,False,True
9754,"5C o n c l u s i o n
",False,Evaluating effects of focal length and viewing angle,False,False,True
9755,"In conclusion 3DSTN, PRNet, and 3D-FAN methods generally work better than tradi-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9756,"tional statistical methods. Deep-learning methods have become the prevalent research
",False,Evaluating effects of focal length and viewing angle,False,False,True
9757,"directionforthetimebeing,buttheyarestillsubjecttoviewinganglesandalso,particu-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9758,"larly,lenseffectsthathaverarelybeenconsideredduringanyperformanceevaluations.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9759,"Increasingfocallengthtendstoimprovethelandmarkandalignmentperformancesdue
",False,Evaluating effects of focal length and viewing angle,False,False,True
9760,"to less projection distortion. This could inform design decisions for camera system and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9761,"lenschosenforabiometricsystem,oritcouldbeusedtoinformfuturealgorithmdesign.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9762,"Given experimental results, all methods, as expected, work best from frontal-viewing
",False,Evaluating effects of focal length and viewing angle,False,False,True
9763,"angles.Itisalsointerestingtonotethattheslopeoffall-offfortheperformancedecrease
",False,Evaluating effects of focal length and viewing angle,False,False,True
9764,"introduced by shorter focal lengths (wider field of view) is less for the AAM based
",False,Evaluating effects of focal length and viewing angle,False,False,True
9765,"methods and the 3DSTN approach. This is likely due to the AAM methods being based
",False,Evaluating effects of focal length and viewing angle,False,False,True
9766,"on image features, and the PAAM more specifically emphasizing local image features.Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page15of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9767,"Fig.17Camerapositionwithgroupofouterview.Theouterviewpositionwhere φ=45◦,60◦,120◦,and
",False,Evaluating effects of focal length and viewing angle,False,False,True
9768,"135◦;θ=0◦,30◦,150◦,and180◦
",False,Evaluating effects of focal length and viewing angle,False,False,True
9769,"Fig.18FACSandnon-FACScomparison. aFACSNRMSEandnon-FACSNRMSEwithM7and bisiBUG-68
",False,Evaluating effects of focal length and viewing angle,False,False,True
9770,"landmarkschemesLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page16of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9771,"3DSTN likely does well as part of the method specifically estimates a camera projection
",False,Evaluating effects of focal length and viewing angle,False,False,True
9772,"matrix, which in some sense should help counteract some of the focal length introduced
",False,Evaluating effects of focal length and viewing angle,False,False,True
9773,"perspective issues. PRNET and 3D-FAN methods using more general 3D data are likely
",False,Evaluating effects of focal length and viewing angle,False,False,True
9774,"moreaffected,andthelargertrainingsetfor3D-FANlikelyassistsitsperformancehere.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9775,"Onelimitationofstatisticalalgorithmsisthelandmarkdetectionperformanceistiedto
",False,Evaluating effects of focal length and viewing angle,False,False,True
9776,"theheadposevariationinthetrainingset.WhenapplyingPCA,thefirstNeigenvectors
",False,Evaluating effects of focal length and viewing angle,False,False,True
9777,"are chosen as the main components. Typically, these are chosen based on representing
",False,Evaluating effects of focal length and viewing angle,False,False,True
9778,"±3 standard deviations from the mean value. Based on this limitation, the landmarking
",False,Evaluating effects of focal length and viewing angle,False,False,True
9779,"performanceforextremeviewangles,asoftenshown,drops.However,theCNNmethods
",False,Evaluating effects of focal length and viewing angle,False,False,True
9780,"thatallincorporatesomesystemof3Dreferencetendtodobetterasviewinganglesmove
",False,Evaluating effects of focal length and viewing angle,False,False,True
9781,"from the center; however, they still suffer performance drops and are still affected by
",False,Evaluating effects of focal length and viewing angle,False,False,True
9782,"shorterfocallengths.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9783,"Sincefocallengthvariancedoesaffectfinalfacelandmarkandalignmentperformance,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9784,"future work could include use of this to augment training data. This could be done
",False,Evaluating effects of focal length and viewing angle,False,False,True
9785,"throughdatacollectionoruseofsyntheticdata.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9786,"Meta-data from capture lenses stored in digital photographs is often removed by the
",False,Evaluating effects of focal length and viewing angle,False,False,True
9787,"time images reach large datasets, but it would be interesting to note such effects from
",False,Evaluating effects of focal length and viewing angle,False,False,True
9788,"in-the-wild photographs. In the meantime, training with synthetic data that includes
",False,Evaluating effects of focal length and viewing angle,False,False,True
9789,"controlled variance of viewing angle ranges as well as varying focal length, added to
",False,Evaluating effects of focal length and viewing angle,False,False,True
9790,"photographicdatasets,shouldlikelyimproveresults.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9791,"In the future, image acquisition should not only cover pose, illumination, expression,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9792,"ethnicity,skincolor,etc.,butalsoincludeconsiderationoffullcameraandlensparameters
",False,Evaluating effects of focal length and viewing angle,False,False,True
9793,"whenpossible.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9794,"Abbreviations
",False,Evaluating effects of focal length and viewing angle,False,False,True
9795,"ASM:Activeshapemodel;AAM:Activeappearancemodel;FACS:FacialActionCodingSystem;EfficientPerspective
",False,Evaluating effects of focal length and viewing angle,False,False,True
9796,"n-Point(EPnP);NFACS:NonFacialActionCodingSystem;AUs:ActionUnits;RMSE:Rootmeansquarederror;NRMSE:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9797,"Normalizedrootmeansquarederror;PCA:Principalcomponentanalysis;HAAM:Holisticactiveappearancemodel;PAAM:
",False,Evaluating effects of focal length and viewing angle,False,False,True
9798,"Patchactiveappearancemodel;TPS:Thinplatespline;PRNet:Positionmapregressionnetwork;CNN:convolutionalneural
",False,Evaluating effects of focal length and viewing angle,False,False,True
9799,"network;3DMM:3Dmorphablemodel;3D-FAN:3Dfacealignmentnetwork;3DSTN:3dspatialtransformernetwork
",False,Evaluating effects of focal length and viewing angle,False,False,True
9800,"Acknowledgements
",False,Evaluating effects of focal length and viewing angle,False,False,True
9801,"Noadditionalacknowledgements.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9802,"Authors’contributions
",False,Evaluating effects of focal length and viewing angle,False,False,True
9803,"XiangLiwasprimaryauthoroftheexperimentalrenderinganddataanalysisaswellasalignmentperformedbyAAMin
",False,Evaluating effects of focal length and viewing angle,False,False,True
9804,"Menpo.MarcusLiuwasresponsiblefortrainingandimplementationofthePRNetand3D-FANmodelsusedfor
",False,Evaluating effects of focal length and viewing angle,False,False,True
9805,"alignment.KhoaLuawasresponsibleforalignmentofthedatausingthe3DSTNmethod.JessicaBaronwasresponsible
",False,Evaluating effects of focal length and viewing angle,False,False,True
9806,"forportionsofthesoftwareusedtocreatetherenderingaswellassomeofthenumericalanalysis.EricPattersonwasthe
",False,Evaluating effects of focal length and viewing angle,False,False,True
9807,"coordinatoranddesigneroftheexperimentandfinaleditorforthepaperafterprimarywritingbyXiangLi.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9808,"Funding
",False,Evaluating effects of focal length and viewing angle,False,False,True
9809,"Thisworkwasnotfundedbyanyexternalbodyandthuswasnotaffectedbyanyaspectsofsuch.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9810,"Availabilityofdataandmaterials
",False,Evaluating effects of focal length and viewing angle,False,False,True
9811,"Pleasecontactauthorsfordatarequests.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9812,"Consentforpublication
",False,Evaluating effects of focal length and viewing angle,False,False,True
9813,"Wehaveconsenttopublishforimagesofindividualsfrom3Dscans.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9814,"Competinginterests
",False,Evaluating effects of focal length and viewing angle,False,False,True
9815,"Theauthorsdeclarethattheyhavenocompetinginterests.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9816,"Authordetails
",False,Evaluating effects of focal length and viewing angle,False,False,True
9817,"1SchoolofComputing,ClemsonUniversity,304McAdamsHall,29630Clemson,SC,USA.2CollegeofComputerScience
",False,Evaluating effects of focal length and viewing angle,False,False,True
9818,"&InformationEngineering,TianjinUniversityofScience&Technology,13thSt,BinhaiXinqu,300457Tianjin,China.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9819,"3DepartmentofComputerScienceandComputerEngineering,UniversityofArkansas,JBHT#521,72701Fayetteville,AR,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9820,"USA.Lietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page17of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9821,"Received:28February2020 Accepted:2February2021
",False,Evaluating effects of focal length and viewing angle,False,False,True
9822,"References
",False,Evaluating effects of focal length and viewing angle,False,False,True
9823,"1. Y.Wu,Q.Ji,Faciallandmarkdetection:Aliteraturesurvey.Int.J.Comput.Vis. 127(2),115–142(2018). https://doi.org/
",False,Evaluating effects of focal length and viewing angle,False,False,True
9824,"10.1007/s11263-018-1097-z
",False,Evaluating effects of focal length and viewing angle,False,False,True
9825,"2. X.P.Burgos-Artizzu,P.Perona,P.Dollár,in Proceedingsofthe2013IEEEInternationalConferenceonComputerVision,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9826,"ICCV’13,RobustFaceLandmarkEstimationunderOcclusion(IEEEComputerSociety,USA,2013),pp.1513–1520.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9827,"https://doi.org/10.1109/ICCV.2013.191
",False,Evaluating effects of focal length and viewing angle,False,False,True
9828,"3. J.Shi,A.Samal,D.Marx,Howeffectivearelandmarksandtheirgeometryforfacerecognition?.Comp.VisionImage
",False,Evaluating effects of focal length and viewing angle,False,False,True
9829,"Underst.102(2),117–133(2006). https://doi.org/10.1016/j.cviu.2005.10.002
",False,Evaluating effects of focal length and viewing angle,False,False,True
9830,"4. A.Kae,IncorporatingBoltzmannMachinePriorsforSemanticLabelinginImagesandVideos(2014). https://doi.org/
",False,Evaluating effects of focal length and viewing angle,False,False,True
9831,"10.7275/37zj-rc94
",False,Evaluating effects of focal length and viewing angle,False,False,True
9832,"5. X.Cao,Y.Wei,F.Wen,J.Sun,Facealignmentbyexplicitshaperegression.Int.J.Comput.Vis. 107(2),177–190(2014).
",False,Evaluating effects of focal length and viewing angle,False,False,True
9833,"https://doi.org/10.1007/s11263-013-0667-3
",False,Evaluating effects of focal length and viewing angle,False,False,True
9834,"6. O.Çeliktutan,S.Ulukaya,B.Sankur,Acomparativestudyoffacelandmarkingtechniques.EURASIPJ.ImageVideo
",False,Evaluating effects of focal length and viewing angle,False,False,True
9835,"Process.2013(1),13(2013). https://doi.org/10.1186/1687-5281-2013-13
",False,Evaluating effects of focal length and viewing angle,False,False,True
9836,"7. B.Johnston,P.d.Chazal,Areviewofimage-basedautomaticfaciallandmarkidentificationtechniques.EURASIPJ.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9837,"ImageVideoProcess. 2018(1),86(2018). https://doi.org/10.1186/s13640-018-0324-4
",False,Evaluating effects of focal length and viewing angle,False,False,True
9838,"8. C.Sagonas,E.Antonakos,G.Tzimiropoulos,S.Zafeiriou,M.Pantic,300facesin-the-wildchallenge:Databaseand
",False,Evaluating effects of focal length and viewing angle,False,False,True
9839,"results.ImagevisionComput. 47,3–18(2016)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9840,"9. W.Wu,C.Qian,S.Yang,Q.Wang,Y.Cai,Q.Zhou,LookatBoundary:ABoundary-AwareFaceAlignmentAlgorithm.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9841,"arXiv(2018). https://arxiv.org/abs/1805.10483
",False,Evaluating effects of focal length and viewing angle,False,False,True
9842,"10. M.Köstinger,P.Wohlhart,P.M.Roth,H.Bischof,in 2011IEEEInternationalConferenceonComputerVisionWorkshops
",False,Evaluating effects of focal length and viewing angle,False,False,True
9843,"(ICCVWorkshops) ,AnnotatedFacialLandmarksintheWild:Alarge-scale,real-worlddatabaseforfaciallandmark
",False,Evaluating effects of focal length and viewing angle,False,False,True
9844,"localization,(2011),pp.2144–2151. https://doi.org/10.1109/ICCVW.2011.6130513 .https://ieeexplore.ieee.org/
",False,Evaluating effects of focal length and viewing angle,False,False,True
9845,"document/6130513
",False,Evaluating effects of focal length and viewing angle,False,False,True
9846,"11. X.Wang,L.Bo,L.Fuxin,in 2019IEEE/CVFInternationalConferenceonComputerVision(ICCV) ,AdaptiveWingLossfor
",False,Evaluating effects of focal length and viewing angle,False,False,True
9847,"RobustFaceAlignmentviaHeatmapRegression,(2019),pp.6970–6980. https://doi.org/10.1109/ICCV.2019.00707 .
",False,Evaluating effects of focal length and viewing angle,False,False,True
9848,"https://ieeexplore.ieee.org/document/9010657
",False,Evaluating effects of focal length and viewing angle,False,False,True
9849,"12. R.Valle,J.M.Buenaposada,A.Valdes,L.Baumela,in ProceedingsoftheEuropeanConferenceonComputerVision(ECCV) ,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9850,"ADeeply-initializedCoarse-to-fineEnsembleofRegressionTreesforFaceAlignment,(2018). https://openaccess.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9851,"thecvf.com/content_ECCV_2018/html/Roberto_Valle_A_Deeply-initialized_Coarse-to-fine_ECCV_2018_paper.html
",False,Evaluating effects of focal length and viewing angle,False,False,True
9852,"13. J.Su,Z.Wang,C.Liao,H.Ling,in 2019IEEE/CVFConferenceonComputerVisionandPatternRecognitionWorkshops
",False,Evaluating effects of focal length and viewing angle,False,False,True
9853,"(CVPRW),EfficientandAccurateFaceAlignmentbyGlobalRegressionandCascadedLocalRefinement,(2019),
",False,Evaluating effects of focal length and viewing angle,False,False,True
9854,"pp.267–276. https://doi.org/10.1109/CVPRW.2019.00036 .https://ieeexplore.ieee.org/document/9025428
",False,Evaluating effects of focal length and viewing angle,False,False,True
9855,"14. M.Kowalski,J.Naruniec,T.Trzcinski,in 2017IEEEConferenceonComputerVisionandPatternRecognitionWorkshops
",False,Evaluating effects of focal length and viewing angle,False,False,True
9856,"(CVPRW),DeepAlignmentNetwork:AConvolutionalNeuralNetworkforRobustFaceAlignment,(2017),
",False,Evaluating effects of focal length and viewing angle,False,False,True
9857,"pp.2034–2043. https://doi.org/10.1109/CVPRW.2017.254 .https://ieeexplore.ieee.org/document/8014988
",False,Evaluating effects of focal length and viewing angle,False,False,True
9858,"15. J.Lv,X.Shao,J.Xing,C.Cheng,X.Zhou,in 2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR) ,A
",False,Evaluating effects of focal length and viewing angle,False,False,True
9859,"DeepRegressionArchitecturewithTwo-StageRe-initializationforHighPerformanceFacialLandmarkDetection,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9860,"(2017),pp.3691–3700. https://doi.org/10.1109/CVPR.2017.393 .https://ieeexplore.ieee.org/document/8099876
",False,Evaluating effects of focal length and viewing angle,False,False,True
9861,"16. N.Damer,Y.Wainakh,O.Henniger,C.Croll,B.Berthe,A.Braun,A.Kuijper,in 201824thInternationalConferenceon
",False,Evaluating effects of focal length and viewing angle,False,False,True
9862,"PatternRecognition(ICPR) ,DeepLearning-basedFaceRecognitionandtheRobustnesstoPerspectiveDistortion,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9863,"(2018),pp.3445–3450. https://doi.org/10.1109/ICPR.2018.8545037 .https://ieeexplore.ieee.org/document/8545037
",False,Evaluating effects of focal length and viewing angle,False,False,True
9864,"17. J.Valente,S.Soatto,in 2015IEEEConferenceonComputerVisionandPatternRecognitionWorkshops(CVPRW) ,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9865,"Perspectivedistortionmodeling,learningandcompensation,(2015),pp.9–16. https://doi.org/10.1109/CVPRW.2015.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9866,"7301314.https://ieeexplore.ieee.org/document/7301314
",False,Evaluating effects of focal length and viewing angle,False,False,True
9867,"18. A.Flores,E.Christiansen,D.Kriegman,S.Belongie,in AdvancesinVisualComputing .ed.byG.Bebis,R.Boyle,B.Parvin,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9868,"D.Koracin,B.Li,F.Porikli,V.Zordan,J.Klosowski,S.Coquillart,X.Luo,M.Chen,andD.Gotz,Cameradistancefrom
",False,Evaluating effects of focal length and viewing angle,False,False,True
9869,"faceimages(Springer,Berlin,Heidelberg,2013),pp.513–522
",False,Evaluating effects of focal length and viewing angle,False,False,True
9870,"19. P.J.Burt, ThePyramidasaStructureforEfficientComputation .(A.Rosenfeld,ed.)(SpringerBerlinHeidelberg,Berlin,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9871,"1984),pp.6–35. https://doi.org/10.1007/978-3-642-51590-3_2 .https://link.springer.com/chapter/10.1007/978-3-
",False,Evaluating effects of focal length and viewing angle,False,False,True
9872,"642-51590-3_2
",False,Evaluating effects of focal length and viewing angle,False,False,True
9873,"20. C.Harris,M.Stephens,in ProceedingsoftheAlveyVisionConference1988 ,ACombinedCornerandEdgeDetector
",False,Evaluating effects of focal length and viewing angle,False,False,True
9874,"(AlveyVisionClub,1988). https://doi.org/10.5244%2Fc.2.23 .https://core.ac.uk/display/21892060
",False,Evaluating effects of focal length and viewing angle,False,False,True
9875,"21. G.Tzimiropoulos,S.Zafeiriou,M.Pantic,in 2011InternationalConferenceonComputerVision ,Robustandefficient
",False,Evaluating effects of focal length and viewing angle,False,False,True
9876,"parametricfacealignment,(2011),pp.1847–1854. https://doi.org/10.1109/ICCV.2011.6126452 .https://ieeexplore.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9877,"ieee.org/document/6126452
",False,Evaluating effects of focal length and viewing angle,False,False,True
9878,"22. X.Zhang,L.Yin,J.F.Cohn,S.Canavan,M.Reale,A.Horowitz,P.Liu,J.M.Girard,BP4D-Spontaneous:ahigh-resolution
",False,Evaluating effects of focal length and viewing angle,False,False,True
9879,"spontaneous3Ddynamicfacialexpressiondatabase.ImageVis.Comput. 32(10),692–706(2014). https://doi.org/10.
",False,Evaluating effects of focal length and viewing angle,False,False,True
9880,"1016/j.imavis.2014.06.002 .https://www.sciencedirect.com/science/article/pii/S0262885614001012?via%3Dihub
",False,Evaluating effects of focal length and viewing angle,False,False,True
9881,"23. V.Le,J.Brandt,Z.Lin,L.Bourdev,T.S.Huang,S.Lazebnik,P.Perona,Y.Sato,C.Schmid,in ComputerVision–ECCV
",False,Evaluating effects of focal length and viewing angle,False,False,True
9882,"2012.ed.byA.Fitzgibbon,InteractiveFacialFeatureLocalization(SpringerBerlinHeidelberg,Berlin,2012),
",False,Evaluating effects of focal length and viewing angle,False,False,True
9883,"pp.679–692. https://link.springer.com/chapter/10.1007/978-3-642-33712-3_49
",False,Evaluating effects of focal length and viewing angle,False,False,True
9884,"24. R.Gross,I.Matthews,J.Cohn,T.Kanade,S.Baker,Multi-pie.ImageVisionComput. 28(5),807–813(2010)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9885,"25. C.H.Hjortsjö, Man’sFaceandMimicLanguage .(Studentlitteratur,1969). https://books.google.com/books?id=
",False,Evaluating effects of focal length and viewing angle,False,False,True
9886,"BakQAQAAIAAJ .https://books.google.com/books/about/Man_s_Face_and_Mimic_Language.html?id=
",False,Evaluating effects of focal length and viewing angle,False,False,True
9887,"BakQAQAAIAAJ
",False,Evaluating effects of focal length and viewing angle,False,False,True
9888,"26. CMU, FACS-FacialActionCodingSystem ,(2002).https://www.cs.cmu.edu/~face/facs.htmLietal. EURASIPJournalonImageandVideoProcessing          (2021) 2021:9 Page18of18
",False,Evaluating effects of focal length and viewing angle,False,False,True
9889,"27. T.F.Cootes,C.J.Taylor,D.H.Cooper,J.Graham,Activeshapemodels-theirtrainingandapplication.Comput.vision
",False,Evaluating effects of focal length and viewing angle,False,False,True
9890,"imageUnderst. 61(1),38–59(1995)
",False,Evaluating effects of focal length and viewing angle,False,False,True
9891,"28. J.Alabort-i-Medina,E.Antonakos,J.Booth,P.Snape,S.Zafeiriou,in Proceedingsofthe22ndACMInternational
",False,Evaluating effects of focal length and viewing angle,False,False,True
9892,"ConferenceonMultimedia ,Menpo:AComprehensivePlatformforParametricImageAlignmentandVisual
",False,Evaluating effects of focal length and viewing angle,False,False,True
9893,"DeformableModels(AssociationforComputingMachinery,NewYork,2014),pp.679–682. https://doi.org/10.1145/
",False,Evaluating effects of focal length and viewing angle,False,False,True
9894,"2647868.2654890
",False,Evaluating effects of focal length and viewing angle,False,False,True
9895,"29. Y.Feng,F.Wu,X.Shao,Y.Wang,X.Zhou,in ComputerVision–ECCV2018 .ed.byV.Ferrari,M.Hebert,C.Sminchisescu,
",False,Evaluating effects of focal length and viewing angle,False,False,True
9896,"andY.Weiss,Joint3DFaceReconstructionandDenseAlignmentwithPositionMapRegressionNetwork(Springer
",False,Evaluating effects of focal length and viewing angle,False,False,True
9897,"InternationalPublishing,Cham,2018),pp.557–574. https://github.com/YadiraF/PRNet .https://link.springer.com/
",False,Evaluating effects of focal length and viewing angle,False,False,True
9898,"chapter/10.1007%2F978-3-030-01264-9_33
",False,Evaluating effects of focal length and viewing angle,False,False,True
9899,"30. A.Bulat,G.Tzimiropoulos,in 2017IEEEInternationalConferenceonComputerVision(ICCV) ,HowFarareWefrom
",False,Evaluating effects of focal length and viewing angle,False,False,True
9900,"Solvingthe2D&3DFaceAlignmentProblem?(andaDatasetof230,0003DFacialLandmarks)(IEEE. https://doi.org/
",False,Evaluating effects of focal length and viewing angle,False,False,True
9901,"10.1109/iccv.2017.116 .https://arxiv.org/abs/1703.07332
",False,Evaluating effects of focal length and viewing angle,False,False,True
9902,"31. C.Bhagavatula,C.Zhu,K.Luu,M.Savvides,in 2017IEEEInternationalConferenceonComputerVision(ICCV) ,Fasterthan
",False,Evaluating effects of focal length and viewing angle,False,False,True
9903,"Real-TimeFacialAlignment:A3DSpatialTransformerNetworkApproachinUnconstrainedPoses,(2017),
",False,Evaluating effects of focal length and viewing angle,False,False,True
9904,"pp.4000–4009. https://doi.org/10.1109/ICCV.2017.429
",False,Evaluating effects of focal length and viewing angle,False,False,True
9905,"32. T.F.Cootes,G.Edwards,C.J.Taylor,in Proc.BritishMachineVisionConf ,ComparingActiveShapeModelswithActive
",False,Evaluating effects of focal length and viewing angle,False,False,True
9906,"AppearanceModels(BMVAPress,Durham,1999),pp.173–182. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=
",False,Evaluating effects of focal length and viewing angle,False,False,True
9907,"10.1.1.16.524
",False,Evaluating effects of focal length and viewing angle,False,False,True
9908,"Publisher’sNote
",False,Evaluating effects of focal length and viewing angle,False,False,True
9909,SpringerNatureremainsneutralwithregardtojurisdictionalclaimsinpublishedmapsandinstitutionalaffiliations.,False,Evaluating effects of focal length and viewing angle,False,False,True
9910,Abstract ,True,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9911,"I. I NTRODUCTION
",True,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9912,"II. B ACKGROUND AND RELATED WORK
",True,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9913,"III. T ACOTRON WITHFRAME AND STYLE RECONSTRUCTION
",True,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9914,"V. C ONCLUSION
",True,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9915,"1806 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9916,"Expressive TTS Training With Frame and Style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9917,"Reconstruction Loss
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9918,"Rui Liu , Member, IEEE , Berrak Sisman , Member, IEEE , Guanglai Gao, and Haizhou Li , Fellow, IEEE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9919,"based text-to-speech (TTS) system that improves the speech styling
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9920,"at utterance level. One of the key challenges in prosody modeling
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9921,"is the lack of reference that makes explicit modeling difﬁcult.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9922,"The proposed technique doesn’t require prosody annotations from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9923,"training data. It doesn’t attempt to model prosody explicitly ei-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9924,"ther, but rather encodes the association between input text and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9925,"its prosody styles using a Tacotron-based TTS framework. This
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9926,"study marks a departure from the style token paradigm where
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9927,"prosody is explicitly modeled by a bank of prosody embeddings.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9928,"It adopts a combination of two objective functions: 1) frame level
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9929,"reconstruction loss, that is calculated between the synthesized and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9930,"target spectral features; 2) utterance level style reconstruction loss,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9931,"that is calculated between the deep style features of synthesized
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9932,"and target speech. The style reconstruction loss is formulated as
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9933,"a perceptual loss to ensure that utterance level speech style is
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9934,"taken into consideration during training. Experiments show that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9935,"the proposed training strategy achieves remarkable performance
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9936,"and outperforms the state-of-the-art baseline in both naturalness
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9937,"and expressiveness. To our best knowledge, this is the ﬁrst study
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9938,"to incorporate utterance level perceptual quality as a loss function
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9939,"into Tacotron training for improved expressiveness.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9940,"Index Terms —Expressive speech synthesis, tacotron, frame and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9941,"style reconstruction loss, emotion recognition.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9942,"I. I NTRODUCTION
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9943,"WITH the advent of deep learning, neural TTS has
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9944,"shown many advantages over the conventional TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9945,"Manuscript received July 19, 2020; revised February 7, 2021 and April 12,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9946,"2021; accepted April 24, 2021. Date of publication April 30, 2021; date of current
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9947,"version June 1, 2021. This work was supported in part by SUTD Start-up Grant
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9948,"Artiﬁcial Intelligence for Human V oice Conversion (SRG ISTD 2020 158) and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9949,"SUTD AI Grant, titled ‘The Understanding and Synthesis of Expressive Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9950,"by AI’. The work of Haizhou Li is supported by the National Research Founda-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9951,"tion, Singapore under its AI Singapore Programme Award AISG-GC-2019-002
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9952,"and Award AISG-100E-2018-006, and its National Robotics Programme under
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9953,"Grant 1922500054, and in part by RIE2020 Advanced Manufacturing and En-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9954,"gineering Programmatic Grants A1687b0033 and A18A2b0046. The associate
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9955,"editor coordinating the review of this manuscript and approving it for publication
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9956,"was Prof. Lei Xie.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9957,"Rui Liu is with the Singapore University of Technology and Design (SUTD)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9958,"and National University of Singapore, Singapore 117583, Singapore (e-mail:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9959,"liurui_imu@163.com).
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9960,"Berrak Sisman is with the Singapore University of Technology and Design
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9961,"(SUTD), Singapore 117583, Singapore (e-mail: berraksisman@u.nus.edu).
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9962,"Guanglai Gao is with the Department of Computer Science, Inner Mongolia
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9963,"University, Huhhot 010021, China (e-mail: csggl@imu.edu.cn).
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9964,"Haizhou Li is with the Department of Electrical and Computer Engi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9965,"neering, National University of Singapore, Singapore 119077, Singapore
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9966,"and also with Faculty 3 Computer Science/Mathematics, Enrique-Schmidt-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9967,"Str. 5 Cartesium, University of Bremen, 28359 Bremen, Germany (e-mail:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9968,"haizhou.li@nus.edu.sg).
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9969,"Digital Object Identiﬁer 10.1109/TASLP.2021.3076369techniques [1]–[3]. For example, encoder-decoder architecture
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9970,"with attention mechanism, such as Tacotron [4]–[7], has consis-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9971,"tently achieved high voice quality. The key idea is to integrate the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9972,"conventional TTS pipeline [8], [9] into an uniﬁed framework that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9973,"learns sequence-to-sequence mapping from text to a sequence
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9974,"of acoustic features [7], [10]–[15]. Furthermore, together with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9975,"a neural vocoder [5], [16]–[21], neural TTS generates natural-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9976,"sounding and human-like speech which achieves state-of-the-art
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9977,"performance. Despite the progress, the expressiveness of the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9978,"synthesized speech remains to be improved.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9979,"Speech conveys information not only through phonetic con-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9980,"tent, but also through its prosody. Speech prosody can affect
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9981,"syntactic and semantic interpretation of an utterance [22], [23],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9982,"that is called linguistic prosody. Speech prosody is also used
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9983,"to display one’s emotional state, that is referred to as affec-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9984,"tive prosody. Both linguistic prosody and affective prosody are
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9985,"manifested over a segment of speech beyond short-time speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9986,"frame. Linguistically, speech prosody in general refers to stress,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9987,"intonation, and rhythm in spoken words, phrases, and sentences.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9988,"As speech prosody is the result of the interplay of multiple
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9989,"speech properties, it is not easy to deﬁne speech prosody by
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9990,"a simple labeling scheme [24]–[28]. Even if a labeling scheme
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9991,"is possible [29], [30], a set of discrete labels may not be sufﬁcient
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9992,"to describe the entire continuum of speech prosody.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9993,"Besides naturalness, one of the factors that differentiate hu-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9994,"man speech from today’s synthesized speech is their expressive-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9995,"ness. Prosody is one of the deﬁning features of expressiveness
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9996,"that makes speech lively. Several recent studies successfully
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9997,"improve the expressiveness of Tacotron TTS framework [31]–
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9998,"[35]. The idea is to learn latent prosody embedding, i.e. style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
9999,"token, from training data [31], [36], [37]. At run-time, the style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10000,"token can be used to predict the speech style from text [32],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10001,"or to transfer the speech style from a reference utterance to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10002,"target [33]. It is observed that such speech styling is effective
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10003,"and consistently improves speech quality. Sun et al. [34], [35]
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10004,"further study a hierarchical, ﬁne-grained and interpretable latent
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10005,"variable model for prosody rendering. The studies show that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10006,"precise control of the prosody style leads to improvement of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10007,"prosody expressiveness in the Tacotron TTS framework. How-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10008,"ever, several issues have hindered the effectiveness of above
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10009,"prosody modeling techniques.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10010,"First, the latent embedding space of prosody is learnt in an
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10011,"unsupervised manner, where the style is deﬁned as anything
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10012,"but speaker identity and phonetic content in speech. We note
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10013,"that many different styles co-exist in speech. Some are speaker
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10014,"dependent, such as accent and idiolect, others are speaker
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10015,"This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/LIU et al. : EXPRESSIVE TTS TRAINING WITH FRAME AND STYLE RECONSTRUCTION LOSS 1807
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10016,"independent such as prosodic phrasing, lexical stress and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10017,"prosodic stress. There is no guarantee that such latent embedding
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10018,"space of style represents only the intended prosody. Second,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10019,"while the techniques don’t require the prosody annotations
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10020,"on training data, they require a reference speech or a manual
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10021,"selection of style token [31] in order to explicitly control the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10022,"style of output speech during run-time inference. While it is
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10023,"possible to automate the style token selection [32], a correct
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10024,"prediction of style token is subject to both the design of the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10025,"style token dictionary, and the run-time style token prediction
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10026,"algorithm. Third, the style token dictionary in Tacotron is trained
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10027,"from a collection of speech utterances to represent a large range
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10028,"of acoustic expressiveness for a speaker or an audiobook [31]. It
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10029,"is not intended to provide differential prosodic details at phrase
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10030,"or utterance level. It is desirable for Tacotron system to learn
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10031,"to automate the prosody styling in response to input text at
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10032,"run-time, that will be the focus of this paper.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10033,"To address the above issues, we believe that Tacotron training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10034,"should minimize frame level reconstruction loss [4], [5] and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10035,"utterance level perceptual loss at the same time. Perceptual loss
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10036,"is ﬁrst proposed for image stylization and synthesis [37]–[40],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10037,"where feature activation patterns, or deep features, derived from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10038,"pre-trained auxiliary networks are used to optimize the percep-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10039,"tual quality of output image. Several computational models have
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10040,"been proposed to approximate human perception of audio qual-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10041,"ity, such as Perceptual Evaluation of Audio Quality (PEAQ) [41],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10042,"Perceptual Evaluation of Speech Quality (PESQ) [42], and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10043,"Perceptual Evaluation of Audio methods for Source Separation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10044,"(PEASS) [43]. However, such models are not differentiable,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10045,"hence cannot be directly employed during TTS training. We
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10046,"believe that utterance level perceptual loss based on deep fea-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10047,"tures that reﬂects global speech style would be useful to improve
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10048,"overall speech quality.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10049,"We are motivated to study a novel training strategy for TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10050,"systems, that learns to associate prosody styles with input text
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10051,"implicitly. We would like to avoid the use of prosody annotations.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10052,"We don’t attempt to model prosody explicitly either, but rather
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10053,"learn the association between prosody styles and input text using
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10054,"existing neural TTS system, such as Tacotron. As the training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10055,"strategy is only involved during training, it doesn’t change the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10056,"run-time inference process for neural TTS system. At run-time,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10057,"we don’t require any reference signal nor manual selection of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10058,"prosody style.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10059,"The main contributions of this paper include: 1) we propose a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10060,"novel training strategy for Tacotron TTS that improves utterance
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10061,"level expressiveness of speech; 2) we propose to supervise
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10062,"the training of Tacotron with a fully differentiable perceptual
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10063,"loss, which is derived from a pre-trained auxiliary network, in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10064,"addition to frame reconstruction loss; and 3) we successfully
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10065,"implement a system that doesn’t require any reference speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10066,"nor manual selection of prosody style at run-time. To our best
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10067,"knowledge, this is the ﬁrst study to incorporate perceptual loss
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10068,"into Tacotron training for improved expressiveness.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10069,"This paper is organized as follows: In Section II, we present
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10070,"the research background and related work to motivate our study.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10071,"In Section III, we propose a novel training strategy for TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10072,"system with frame and style reconstruction loss. In Section IV,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10073,"Fig. 1. Block diagram of Tacotron2-based TTS reference baseline [5].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10074,"we report the subjective and objective evaluations. Section V
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10075,"concludes the discussion.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10076,"II. B ACKGROUND AND RELATED WORK
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10077,"This work is built on several previous studies on neural TTS,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10078,"prosody modeling, perceptual loss, and speech emotion recog-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10079,"nition. Here we brieﬂy summarize the related previous work to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10080,"set the stage for our study, and to place our novel contributions
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10081,"in a proper context.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10082,"A. Tacotron2-Based TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10083,"In this paper, we adopt the Tacotron2-based [5] TTS model
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10084,"as a reference baseline, which is also referred to as Tacotron
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10085,"baseline for brevity.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10086,"The overall architecture of the reference baseline includes en-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10087,"coder, attention-based decoder and waveform generation mod-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10088,"ule [44]–[46] as illustrated in Fig. 1. The encoder consists of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10089,"two components, a convolutional neural network (CNN) mod-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10090,"ule [47], [48] that has 3 convolutional layers, and a bidirec-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10091,"tional LSTM (BLSTM) [49] layer. The decoder consists of four
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10092,"components: a 2-layer pre-net, 2 LSTM layers, a linear projec-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10093,"tion layer and a 5-convolution-layer post-net. The decoder is a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10094,"standard autoregressive recurrent neural network that generates
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10095,"mel-spectrum features and stop tokens frame by frame. There are
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10096,"two common techniques to generate the audio waveform from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10097,"mel-spectrum features. One is the Grifﬁn Lim [44] algorithm,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10098,"another is via a neural vocoder [5], [45], [46], [50].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10099,"Just like other TTS systems, Tacotron [4], [5] TTS system
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10100,"predicts mel-spectrum features from input sequence of charac-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10101,"ters by minimizing a frame level reconstruction loss. Such frame
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10102,"level objective function focuses on the distance between spectral
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10103,"features. It does not seek to optimize the perceptual quality at
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10104,"utterance level. To improve the suprasegmental expressiveness,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10105,"there have been studies [32], [35], [51] on latent prosody repre-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10106,"sentations, that make possible prosody styling in Tacotron TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10107,"framework. However, most of the studies rely on the style tokens
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10108,"mechanism to explicitly model the prosody. Simply speaking,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10109,"they build a Tacotron TTS system that synthesizes speech, and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10110,"learns the global style tokens (GST) at the same time. At run-time
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10111,"inference, they apply the style tokens to control the expressive ef-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10112,"fect [31], [33], that is referred to as the GST-Tacotron paradigm.1808 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10113,"In this paper, we advocate a new way of addressing the ex-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10114,"pressiveness issue by integrating a perceptual quality motivated
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10115,"objective function into the training process, in addition to the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10116,"frame level reconstruction loss function. We no longer require
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10117,"any dedicated prosody control mechanism during run-time in-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10118,"ference, such as style tokens in Tacotron system.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10119,"B. Prosody Modeling in TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10120,"Prosody conveys linguistic, para-linguistic and various types
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10121,"of non-linguistic information, such as speaker identity, intention,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10122,"attitude and mood [52], [53]. It is inherently supra-segmental [1],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10123,"[54] due to the fact that prosody patterns cannot be derived
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10124,"solely from short-time segments [55]. Prosody is hierarchical
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10125,"in nature [55]–[58] and affected by long-term dependencies at
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10126,"different levels such as word, phrase and utterance level [59].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10127,"Studies on hierarchical modeling of F0 in speech synthesis [1],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10128,"[60], [61] suggest that utterance-level prosody modeling is more
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10129,"effective. Similar studies, such as continuous wavelet transform,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10130,"can be found in many speech synthesis related applications [59],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10131,"[62]–[65]. In this paper, we will study a novel technique to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10132,"observe utterance-level prosody quality during Tacotron training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10133,"to achieve expressive synthesis.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10134,"The early studies of modeling speaking styles are carried out
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10135,"on Hidden Markov Models (HMM) [9], [66], where we can
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10136,"synthesize speech with an intermediate speaking style between
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10137,"two speakers through model interpolation [67]. To improve
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10138,"the HMM-based TTS model, there have been studies to in-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10139,"corporate unsupervised expression cluster information during
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10140,"training [68]. Deep learning opens up many possibilities for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10141,"expressive speech synthesis, where speaker, gender, and age
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10142,"codes can be used as control vectors to change TTS output in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10143,"different ways [69]. The style tokens, or prosody embeddings,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10144,"represent one type of such control vectors, that is derived from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10145,"a representation learning network. The success of prosody em-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10146,"bedding motivates us to further develop the idea.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10147,"Tacotron TTS framework has achieved remarkable perfor-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10148,"mance in terms of spectral feature generation. With a large
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10149,"training corpus, it may be able to generate natural prosody and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10150,"expression by remembering the training data using a large num-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10151,"ber of network parameters. However, its training process doesn’t
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10152,"aim to optimize the system for expressive prosody rendering. As
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10153,"a result, Tacotron TTS system tends to generate speech outputs
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10154,"that represent model average, rather than the intended prosody.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10155,"The idea of global style tokens [31], [32] represents a success
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10156,"in controlling prosody style of Tacotron output. Style tokens
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10157,"learn to represent high level styles, such as speaker style, pitch
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10158,"range, and speaking rate across a collection of utterances or a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10159,"speech database. We argue that they neither necessarily represent
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10160,"the useful styles to describe the continuum of prosodic expres-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10161,"sions [70], nor provide the dynamic and differential prosodic
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10162,"details with the right level of granularity at utterance level. Sun et
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10163,"al.[34], [35] study a way to include a hierarchical, ﬁne-grained
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10164,"prosody representation, that represents the recent attempts to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10165,"address the problems in GST-Tacotron paradigm.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10166,"We would like to address three issues in the existing prosody
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10167,"modeling in Tacotron framework, 1) lack of prosodic supervisionduring training; 2) limitation of explicit prosody modeling, such
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10168,"as style tokens, in describing the continuum of prosodic expres-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10169,"sions; 3) lack of dynamic and differential prosody at utterance
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10170,"level.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10171,"C. Perceptual Loss for Style Reconstruction
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10172,"It is noted that frame-level reconstruction loss, denoted as
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10173,"frame reconstruction loss in short, is not always consistent
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10174,"with human perception because it doesn’t take into account
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10175,"human sensitivities to temporal and spectral information, such
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10176,"as prosody and temporal structure of the utterance. For example,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10177,"if one repeatedly asks the same question two times, despite
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10178,"the perceptual similarity of two utterances, they would be very
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10179,"different as measured by frame-level losses.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10180,"Perceptual loss refers to the training loss derived from a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10181,"pre-trained auxiliary network [38]. The auxiliary network is usu-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10182,"ally trained on a different task that provides perceptual quality
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10183,"evaluation of an input at a higher level than a speech frame. The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10184,"intermediate feature representations, generated by the auxiliary
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10185,"network in form of hidden layer activations, are usually referred
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10186,"to as deep features. They are used as the high level abstraction
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10187,"to measure the training loss between reconstructed signals and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10188,"reference signals. Such training loss is also called deep feature
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10189,"loss [71], [72].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10190,"In speech enhancement, perceptual loss has been used suc-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10191,"cessfully in end-to-end speech denoising pipeline, with an aux-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10192,"iliary network pre-trained on audio classiﬁcation task [73].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10193,"Kataria et al. [71] propose to use perceptual loss which op-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10194,"timizes the enhancement network with an auxiliary network
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10195,"pre-trained on speaker recognition task. In voice conversion,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10196,"Loet al. [74] propose deep learning-based assessment models
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10197,"to predict human ratings of converted speech. Lee [75] propose a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10198,"perceptually meaningful criterion where human auditory system
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10199,"was taken into consideration in measuring the distances between
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10200,"the converted speech and the reference.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10201,"In speech synthesis, Oord et al. propose to train a WaveNet-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10202,"like classiﬁer with perceptual loss for phone recognition [76].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10203,"As the classiﬁer extracts high-level features that are relevant
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10204,"for phone recognition, this loss term supervises the training of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10205,"WaveNet to look after temporal dynamics, and penalize bad
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10206,"pronunciations. Cai et al. [77] study to use a pre-trained speaker
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10207,"embedding network to provide feedback constraint, that serves
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10208,"as the perceptual loss for the training of a multi-speaker TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10209,"system.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10210,"In the context of prosody modeling, the perceptual loss in the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10211,"above studies can be generally described as style reconstruc-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10212,"tion loss [38]. Following the same principle, we would like
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10213,"to propose a novel auxiliary network, that is pre-trained on a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10214,"speech emotion recognition (SER) task, to extract high level
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10215,"prosody representations. By comparing prosody representations
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10216,"in a continuous space, we measure perceptual loss between
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10217,"two utterances. While perceptual loss is not new in speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10218,"reconstruction, the idea of using a pre-trained emotion recog-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10219,"nition network for perceptual loss is a novel attempt in speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10220,"synthesis.LIU et al. : EXPRESSIVE TTS TRAINING WITH FRAME AND STYLE RECONSTRUCTION LOSS 1809
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10221,"D. Deep Features for Perceptual Loss
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10222,"Now the question is which deep features could be suitable
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10223,"for measuring perceptual loss. We beneﬁt from the prior work
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10224,"in prosody modeling. Prosody embedding in Tacotron is a type
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10225,"of feature learning, that learns the representation for prediction
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10226,"or classiﬁcation tasks. With deep learning algorithms, auto-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10227,"matic feature learning can be achieved in either supervised,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10228,"such as multilayer perceptron [78], or unsupervised manner,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10229,"such as variational autoencoder [79]. Deep features are usually
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10230,"more generalizable, and easier to manage than hand-crafted or
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10231,"manually designed features [80]. There have been studies on
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10232,"representation learning for prosody patterns, such as speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10233,"emotion [81], and speech styles [31].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10234,"Affective prosody refers to the expression of emotion in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10235,"speech [82], [83]. It is prominently exhibited in emotion speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10236,"database. Therefore, the studies in speech emotion recognition
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10237,"provide valuable insights into prosodic modeling. Emotion are
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10238,"usually characterized by discrete categories, such as happy,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10239,"angry, and sad, and continuous attributes, such as activation,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10240,"valence and dominance [84], [85]. Recent studies show that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10241,"latent representations of deep neural networks also characterize
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10242,"well emotion in a continuous space [78].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10243,"There have been studies to leverage emotion speech modeling
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10244,"for expressive TTS [33], [68], [86]–[88]. Eyben et al. [68]
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10245,"incorporate unsupervised expression cluster information into an
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10246,"HMM-based TTS system. Skerry-Ryan et al. [33] study learning
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10247,"prosody representation from animated and emotive storytelling
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10248,"speech corpus. Wu et al. [86] propose a semi-supervised training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10249,"of Tacotron TTS framework for emotional speech synthesis,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10250,"where style tokens are deﬁned to represent emotion categories.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10251,"Gao et al. [87] propose to use an emotion recognizer to extract
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10252,"the style embedding for speech style transfer. Um et al. [88]
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10253,"study a technique to apply style embedding to Tacotron system
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10254,"to generate emotional speech, and to control the intensity of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10255,"emotional expressiveness.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10256,"All the studies point to the fact that emotion-related deep
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10257,"features serve as the excellent descriptors of speech prosody
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10258,"and speech styles. In this paper, instead of using the style tokens
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10259,"to control the TTS outputs, we would like to study how to use
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10260,"deep style features to measure perceptual loss for the training of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10261,"neural TTS system in general.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10262,"III. T ACOTRON WITHFRAME AND STYLE RECONSTRUCTION
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10263,"LOSS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10264,"We propose a novel training strategy for Tacotron with both
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10265,"frame and style reconstruction loss. As the style reconstruction
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10266,"loss is formulated as a perceptual loss (PL) [38], the proposed
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10267,"frame and style training strategy is called Tacotron-PL in short.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10268,"It seeks to optimize both frame-level spectral loss, that is frame
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10269,"reconstruction loss , as well as utterance-level style loss, that is
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10270,"style reconstruction loss , at the same time.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10271,"The overall framework is illustrated in Fig. 2, that has three
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10272,"stages: 1) training of style descriptor, 2) the proposed frame and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10273,"style training for Tacotron-PL model, and 3) run-time inference.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10274,"In Stage I, we train an auxiliary network to serve as the style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10275,"descriptor for input speech utterances. In Stage II, the proposed
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10276,"Fig. 2. Overall framework of a Tacotron-PL system in three stages: Stage I
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10277,"for training of style descriptor; Stage II for training of Tacotron-PL ; Stage III
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10278,"for run-time inference.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10279,"frame and style training strategy is implemented to associate
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10280,"input text with acoustic features, as well as prosody style of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10281,"natural speech, that is assisted by the style descriptor obtained
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10282,"from Stage I. In Stage III, the Tacotron-PL system takes input
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10283,"text and generates expressive speech in the same way as a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10284,"standard Tacotron does. Unlike other Tacotron variants [31],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10285,"Tacotron-PL doesn’t require any add-on module or process for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10286,"run-time inference.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10287,"As discussed in Section II-A, traditional Tacotron archi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10288,"tecture contains a text encoder and an attention-based de-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10289,"coder. We ﬁrst encode input character embedding into hidden
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10290,"state, from which the decoder generates mel-spectrum fea-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10291,"tures. During training, we adopt a frame-level mel-spectrum
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10292,"loss as in [5], which is a L2loss between the synthesized
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10293,"mel-spectrum ˆY={ˆy1,...ˆyt,...ˆyT}and target mel-spectrum
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10294,"Y={y1,...yt,...yT}.W eh a v e Loss frame as follows,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10295,"Loss frame(Y,ˆY)=T/summationdisplay
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10296,"t=1L2(yt,ˆyt) (1)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10297,"which is designed to minimize frame level distortion. As it
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10298,"doesn’t guarantee utterance level similarity concerning speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10299,"expressions, such as speech prosody and speech styles. We will
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10300,"study a new loss function Loss style next, that measures the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10301,"utterance-level style reconstruction loss.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10302,"A. Stage I: Training of Style Descriptor
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10303,"One of the great difﬁculties of prosody modeling is the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10304,"lack of reference samples. In linguistics, we usually describe
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10305,"prosody styles qualitatively. However, precise annotation of1810 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10306,"Fig. 3. Block diagram of the proposed training strategy, Tacotron-PL . A speech emotion recognition (SER) model is trained separately to serve as an auxiliary
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10307,"model to extract deep style features. A style reconstruction loss ,Loss style , is computed between the deep style features of the generated and reference speech at
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10308,"utterance-level.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10309,"speech prosody is not straightforward. One of the ways to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10310,"describe a prosody style is to show by example. The idea of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10311,"style token [31] shows a way to compare two prosody styles
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10312,"quantitatively using deep features.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10313,"Manual prosodic annotations of recorded speech [29] provide
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10314,"quantiﬁable prosodic labels that allow us to associate speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10315,"styles with actual acoustic features. Prosody labeling schemes
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10316,"often attempt to describe prosodic phenomena, such as the supra-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10317,"segmental features of intonation, stress, rhythm and speech rate,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10318,"in discrete categories. Categorical labels of speech emotion [89]
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10319,"also seek to achieve a similar goal. The prosody labeling schemes
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10320,"serve as a type of style descriptor. With deep neural network, one
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10321,"is able to learn the feature representation of the data at different
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10322,"level of abstraction in a continuous space [90]. As speech styles
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10323,"naturally spread over a continuum rather than forced-ﬁtting into
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10324,"a ﬁnite set of categorical labels, we believe that deep neural
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10325,"network learned from animated and emotive speech serves as a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10326,"more suitable style descriptor.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10327,"We propose to use a speech emotion recognizer (SER) [82],
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10328,"[83] as a style descriptor F(·), which extracts deep style features
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10329,"Ψfrom an utterance Y,o rΨ=F(Y). We use neuronal acti-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10330,"vations of hidden units in a deep neural network as the deep
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10331,"style features to represent high level prosodic abstraction at
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10332,"utterance level. In practice, we ﬁrst train an SER network with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10333,"highly animated and emotive speech with supervised learning.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10334,"We then derive deep style features from a small intermediate
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10335,"layer. As the intermediate layer is small relative to the size of the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10336,"other layers, it creates a constriction in the network that forces
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10337,"the information pertinent to emotion classiﬁcation into a low
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10338,"dimensional prosody representation [91]. Such low dimensional
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10339,"prosody representation is expected to describe the prosody style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10340,"of speech signals as the SER network relies on the prosody
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10341,"representation for accurate emotion classiﬁcation.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10342,"We follow the SER implementation in [36], [92] as illustrated
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10343,"in Fig. 3, that forms part of Fig. 2. The SER network includes 1)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10344,"a three-dimensional (3-D) CNN layer; 2) a BLSTM layer [93];
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10345,"3) an attention layer; and 4) a fully connected (FC) layer. The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10346,"3-D CNN [92] ﬁrst extracts a latent representation from mel-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10347,"spectrum, its delta and delta-delta values from input utterance,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10348,"converting the input utterance of variable length into a ﬁxed size
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10349,"latent representation, denoted as deep features sequence Ψlow,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10350,"that reﬂects the semantics of emotion. The BLSTM summarizesTABLE I
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10351,"THEMCD, RMSE AND FD R ESULTS OF DIFFERENT SYSTEMS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10352,"TABLE II
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10353,"THEAB P REFERENCE TEST FOR EXPRESSIVENESS AND NATURALNESS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10354,"EVA L UAT I O N B Y 15 L ISTENERS ,W ITH95% C ONFIDENCE INTERV ALS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10355,"COMPUTED FROM THE T-T EST
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10356,"TABLE III
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10357,"BESTWORST SCALING (BWS) L ISTENING EXPERIMENTS THAT COMPARE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10358,"FOUR DEEPSTYLE FEATURES IN FOUR TACOTRON -PLMODELS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10359,"the temporal information of Ψlowinto another latent represen-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10360,"tationΨmiddle . Finally, the attention layer assigns weights to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10361,"Ψmiddle and generates Ψhigh for emotion prediction.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10362,"The question is which of the latent representations, Ψlow,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10363,"Ψmiddle , andΨhigh, is suitable to be the deep style features. To
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10364,"validate the descriptiveness of deep style features, we perform
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10365,"an analysis on LJ-Speech corpus [94]. Speciﬁcally, we randomly
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10366,"select ﬁve utterances from each of the six style groups from the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10367,"database, each group having a distinctive speech style, namely,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10368,"1) Short question; 2) Long question; 3) Short answer; 4) Short
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10369,"statement; 5) Long statement and 6) Digit string. The complete
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10370,"list of utterances can be found at Table V in Appendix A.LIU et al. : EXPRESSIVE TTS TRAINING WITH FRAME AND STYLE RECONSTRUCTION LOSS 1811
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10371,"TABLE IV
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10372,"THEAB P REFERENCE TEST FOR EXPRESSIVENESS AND NATURALNESS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10373,"EVA L UAT I O N B Y 15 L ISTENERS ,W ITH95% C ONFIDENCE INTERV ALS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10374,"COMPUTED FROM THE T-T EST
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10375,"Fig. 4. t-SNE plot of the distributions of deep style features Ψlow,Ψmiddle
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10376,"andΨhigh for six groups of utterances in LJ-Speech corpus. The list of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10377,"utterances can be found at Table V in Appendix A.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10378,"We visualize the Ψlow,Ψmiddle andΨhigh of utterances using
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10379,"the t-SNE algorithm in a two dimensional plane [95], as shown in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10380,"Fig. 4. Please note that the distributions of digits 1 to 6 represent
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10381,"those of groups 1 to 6 in the two dimensional space. As illustrated
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10382,"in Table V , the utterances within the same group form a cluster,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10383,"while the utterances between groups distance from one another.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10384,"To visualize, we color the clusters to highlight their distributions.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10385,"It is observed that Ψlow,Ψmiddle andΨhigh of utterances
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10386,"form clear style groups in terms of feature distributions, that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10387,"correspond to the six different utterance styles summarized in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10388,"Table V . Furthermore, it is clear that Fig. 4(a) shows a better
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10389,"clustering than Fig. 4(b) and Fig. 4(c). We will further compare
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10390,"the performance of different deep style features through TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10391,"experiments in Section IV.B. Stage II: Tacotron-PL Training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10392,"During the training of Tacotron-PL , the SER-based style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10393,"descriptor F(·)is used to extract the deep style features Ψ.W e
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10394,"deﬁne a style reconstruction loss that compares the prosody style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10395,"between the reference speech Yand the generated speech ˆY.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10396,"Loss style(Y,ˆY)=L2(Ψ,ˆΨ) (2)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10397,"whereΨ=F(Y)andˆΨ=F(ˆY). As illustrated in Fig. 3,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10398,"the proposed training strategy involves two loss functions: 1)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10399,"Loss frame that minimizes the loss between synthesized and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10400,"original mel-spectrum at frame level; and 2) Loss style that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10401,"minimizes the style differences between the synthesized and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10402,"reference speeches at utterance level.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10403,"Loss total(Y,ˆY)=Loss frame(Y,ˆY)+Loss style(Y,ˆY)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10404,"(3)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10405,"whereLoss frame is also the loss function of a traditional
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10406,"Tacotron [5] system.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10407,"Style reconstruction loss can be seen as perceptual quality
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10408,"feedback at utterance level to supervise the training of prosody
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10409,"style. All parameters in the TTS model are updated with the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10410,"gradients of the total loss through back-propagation. We expect
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10411,"that mel-spectrum generation will learn from local and global
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10412,"viewpoint through the frame and style reconstruction loss.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10413,"C. Stage III: Run-Time Inference
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10414,"The inference stage follows exactly the same Tacotron work-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10415,"ﬂow, that only involves the TTS Model in Fig. 3. The difference
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10416,"between Tacotron-PL and other global style tokens variation of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10417,"Tacotron is that Tacotron-PL encodes prosody styling inside the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10418,"standard Tacotron architecture. It doesn’t require any add-on
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10419,"module.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10420,"At run-time, the Tacotron architecture takes text as input
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10421,"and generate expressive mel-spectrum features as output, that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10422,"is followed by Grifﬁn-Lim algorithm [44] and WaveRNN
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10423,"vocoder [45] in this paper to generates audio signals.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10424,"IV . E XPERIMENTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10425,"We train a SER as the style descriptor on IEMOCAP
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10426,"dataset [89], which consists of ﬁve sessions. The dataset contains
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10427,"a total of 10039 utterances, with an average duration of 4.5
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10428,"seconds at a sampling rate of 16 kHz. We only use a subset of the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10429,"improvised data with four emotional categories, namely, happy,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10430,"angry, sad, and neutral, which are recorded in the hypothetical
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10431,"scenarios designed to elicit speciﬁc types of emotions.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10432,"With the style descriptor, we further train a Tacotron system
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10433,"on LJ-Speech database [94], which consists of 13100 short clips
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10434,"with a total of nearly 24 hours of speech from one single speaker
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10435,"reading 7 non-ﬁction books. The speech samples are available
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10436,"from the demo link.1
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10437,"1Speech Samples: https://ttslr.github.io/Expressive-TTS-Training-with-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10438,"Frame-and-Style-Reconstruction-Loss/1812 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10439,"Fig. 5. Three level (low, middle and high) of deep style features extracted
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10440,"from SER-based style descriptors for computing style construction loss.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10441,"A. Comparative Study
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10442,"We develop ﬁve Tacotron-based TTS systems for a compara-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10443,"tive study, that includes the Tacotron baseline, and four variants
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10444,"of Tacotron with the proposed training strategy, Tacotron-PL .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10445,"To study the effect of different style descriptors, we compare
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10446,"the use of four deep style features, which includes three single
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10447,"features and a combination of them, in Loss style , as illustrated
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10448,"in Fig. 5, and summarized as follows:rTacotron: Tacotron [5] trained with Loss frame as in Eq.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10449,"(1), that doesn’t explicitly model speech style.rTacotron-PL(L): Tacotron-PL which uses Ψlow in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10450,"Loss style .rTacotron-PL(M): Tacotron-PL which uses Ψmiddle in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10451,"Loss style .rTacotron-PL(H): Tacotron-PL which uses Ψhigh in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10452,"Loss style .rTacotron-PL(LMH): Tacotron-PL which uses
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10453,"{Ψlow,Ψmiddle,Ψhigh}inLoss style .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10454,"B. Experimental Setup
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10455,"For SER training, we ﬁrst split the speech signals into seg-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10456,"ments of 3 seconds as in [92]. We then extract 40-channel
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10457,"mel-spectrum features with a frame size of 50 ms and 12.5 ms
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10458,"frame shift. The ﬁrst convolution layer has 128 feature maps,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10459,"while the remaining convolution layers have 256 feature maps.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10460,"The ﬁlter size for all convolution layers is 5 ×3, with 5 along the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10461,"time axis, and 3 along the frequency axis, and the pooling size
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10462,"for the max pooling layer is 2 ×2. We add a linear layer with 200
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10463,"output units after 3-D CNN for dimension reduction.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10464,"In this way, the 3-D CNN extracts a ﬁxed size of latent
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10465,"representation with 150×200 dimension from the input ut-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10466,"terance, that we use as the deep style features Ψlow=Flow(·)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10467,"to represent a temporal sequence of 150 segment, each having
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10468,"an embedding of 200 elements. As each direction of BLSTM
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10469,"layer contains 128 cells, in two directions, we obtain 256 output
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10470,"activations for each input segment, that are further mapped to 200
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10471,"output units via a linear layer. BLSTM summarizes the temporal
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10472,"information of Ψlowinto another ﬁxed size latent representation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10473,"Ψmiddle=Fmiddle(·)of150×200 dimension. The attention
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10474,"layer assigns the weights to Ψmiddle and generate a new la-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10475,"tent representation Ψhigh=Fhigh(·). All latent representation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10476,"Ψlow,Ψmiddle ,Ψhigh have the same dimension.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10477,"The fully connected layer contains 64 output units. Batch
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10478,"normalization [96] is applied to the fully connected layer to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10479,"accelerate training and improve the generalization performance.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10480,"The parameters of the SER model were optimized by minimizingthe cross-entropy objective function, with a minibatch of 40
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10481,"samples, using the Adam optimizer with Nestorov momentum.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10482,"The initial learning rate is set to 10−4and the momentum is
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10483,"set to 0.9. In this way, we obtain a SER style descriptor that is
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10484,"reported with an average classiﬁcation accuracy of 73.2% for all
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10485,"emotions on the test set.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10486,"The SER-based style descriptor is used to extract deep style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10487,"features for the computing of Loss style . For TTS training, the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10488,"encoder takes a 256-dimensions character sequence as input
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10489,"and the decoder generates the 40-channel mel-spectrum. The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10490,"training utterances from LJ-Speech database are of variable
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10491,"length. Mel-spectrum features are also extracted with a frame
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10492,"size of 50 ms and 12.5 ms frame shift. They are normalized to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10493,"zero-mean and unit-variance to serve as the reference target. The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10494,"decoder predicts only one non-overlapping output frame at each
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10495,"decoding step. We use the Adam optimizer with β1=0.9,β2
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10496,"=0.999 and a learning rate of 10−3exponentially decaying to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10497,"10−5starting at 50 k iterations. We also apply L2regularization
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10498,"with weight 10−6. All models are trained with a batch size of 32
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10499,"and 150 k steps.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10500,"C. Objective Evaluation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10501,"We conduct objective evaluation experiments to compare the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10502,"systems in a comparative study. The results are summarized in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10503,"Table I.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10504,"1) Performance Evaluation Metrics: Mel-cepstral distortion
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10505,"(MCD) [97] is used to measure the spectral distance between the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10506,"synthesized and reference mel-spectrum features that is known
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10507,"to correlate well with human perception [97]. MCD is calculated
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10508,"as:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10509,"MCD =10√
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10510,"2
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10511,"ln101
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10512,"N/radicaltp/radicalvertex/radicalvertex/radicalbtN/summationdisplay
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10513,"k=1(yt,k−ˆyt,k)2(4)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10514,"whereNrepresents the dimension of the mel-spectrum, yt,k
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10515,"denotes the kthmel-spectrum component in tthframe for the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10516,"reference target mel-spectrum, and ˆyt,kfor the synthesized mel-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10517,"spectrum. Lower MCD value indicates smaller distortion.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10518,"We use Root Mean Squared Error (RMSE) as the evaluation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10519,"metrics for F0 modeling, that is calculated as:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10520,"RMSE =/radicaltp/radicalvertex/radicalvertex/radicalbt1
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10521,"TT/summationdisplay
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10522,"t=1/parenleftBig
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10523,"F0t−/hatwiderF0t/parenrightBig2
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10524,"(5)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10525,"whereF0tand/hatwidestF0tdenote the reference and synthesized F0 at
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10526,"tthframe. We note that lower RMSE value suggests that the two
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10527,"F0 contours are more similar.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10528,"Moreover, we propose to use frame disturbance, denoted as
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10529,"FD, to calculate the deviation in the dynamic time warping
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10530,"(DTW) alignment path [98]–[100]. FD is calculated as:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10531,"FD =/radicaltp/radicalvertex/radicalvertex/radicalbt1
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10532,"TT/summationdisplay
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10533,"t=1(at,x−at,y)2(6)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10534,"whereat,xandat,ydenote the x-coordinate and the y-coordinate
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10535,"of thetthframe in the DTW alignment path. As FD representsLIU et al. : EXPRESSIVE TTS TRAINING WITH FRAME AND STYLE RECONSTRUCTION LOSS 1813
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10536,"Fig. 6. Spectrogram (left) and F0 contour (right) of an utterance “ The design
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10537,"of the letters of this modern ‘old style’ leaves a good deal to be desired. ” from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10538,"LJ-Speech database between the reference natural speech, labelled as Ground
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10539,"Truth, and ﬁve Tacotron systems. It is observed that Tacotron-PL models produce
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10540,"ﬁner spectral details, prosodic phrasing and F0 contour that are closer to those
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10541,"of the reference than Tacotron baseline.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10542,"the duration deviation of the synthesized speech from the target,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10543,"it is a proxy to show the duration distortion. A larger value
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10544,"indicates poor duration modeling performance and a smaller
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10545,"value indicates otherwise.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10546,"2) Spectral Modeling: We observe that all implementations
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10547,"ofTacotron-PL model consistently provide lower MCD values
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10548,"than Tacotron baseline, with Tacotron-PL(L) representing the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10549,"lowest MCD, as can be seen in Table I. We also visualize the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10550,"spectrograms of same speech content synthesized by ﬁve differ-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10551,"ent models, together with that of the reference natural speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10552,"in Fig. 6. A visual inspection of the spectrograms suggests that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10553,"Tacotron-PL models consistently provide ﬁner spectral details
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10554,"than Tacotron baseline.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10555,"3) F0 Modeling: Fundamental frequency, or F0, is an es-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10556,"sential prosodic feature of speech [32], [35]. As there is no
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10557,"guarantee that synthesized speech and reference speech have
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10558,"the same length, we apply DTW [101] to align speech pairs and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10559,"calculate RMSE between the F0 contour of them. The resultsare reported in Table I. It is observed that Tacotron-PL models
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10560,"consistently generate F0 contours which are closer to reference
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10561,"speech than Tacotron baseline.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10562,"We note that both F0 and prosody style contributes to RMSE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10563,"measurement. To show the effect of various deep style features
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10564,"on the F0 contours, we also plot the F0 contours of the utterances
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10565,"in Fig. 6. A visual inspection suggests that the Tacotron-PL
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10566,"models beneﬁt from the perceptual loss training, and produce
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10567,"F0 contour with a better ﬁt to that of the reference speech, with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10568,"Tacotron-PL(L) producing the best ﬁt (see Fig. 6(c)).
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10569,"4) Duration Modeling: Frame disturbance is a proxy to the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10570,"duration difference [100] between synthesized speech and ref-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10571,"erence natural speech. We report frame disturbance of ﬁve
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10572,"systems in Table I. As shown in Table I, Tacotron-PL models
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10573,"obtain signiﬁcantly lower FD value than Tacotron baseline, with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10574,"Tacotron-PL(L) giving the lowest FD. From Fig. 6, we can
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10575,"also observe that Tacotron-PL(L) example clearly provides a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10576,"better duration prediction than other models. We can conclude
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10577,"that perceptual loss training with style reconstruction loss helps
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10578,"Tacotron to achieve a more accurate rendering of prosodic
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10579,"patterns.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10580,"5) Deep Style Features: We compare four different deep style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10581,"features by evaluating the performance of their use in Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10582,"PLmodels, namely Tacotron-PL(L) ,Tacotron-PL(M) ,Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10583,"PL(H) andTacotron-PL(LMH) .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10584,"In supervised feature learning, the features that are near the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10585,"input layer are related to the low level features, while those that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10586,"are near the output are related to the supervision target, that are
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10587,"the categorical labels of the emotion. While we expect the style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10588,"descriptors to capture utterance level prosody style, we don’t
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10589,"want the style reconstruction loss function to directly relate to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10590,"emotion categories. Hence, the lower level deep features, Ψlow,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10591,"as illustrated in Fig. 4, would be more appropriate than the higher
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10592,"level deep features, such as Ψmiddle andΨhigh.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10593,"We observe that Ψlowis more descriptive than other deep style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10594,"features for perceptual loss evaluation, as reported in spectral
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10595,"modeling (MCD), F0 modeling (RMSE), duration modeling
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10596,"(FD) for Tacotron-PL experiment in Table I. The observations
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10597,"conﬁrm our intuition and the analysis in Fig. 4.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10598,"D. Subjective Evaluation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10599,"We conduct listening experiments to evaluate several aspects
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10600,"of the synthesized speech, and the choice of deep style features
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10601,"forLoss style . Grifﬁn-Lim algorithm [44] and neural vocoder
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10602,"are employed to generate the speech waveform. We choose
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10603,"WaveRNN vocoder which follows the same parameter settings
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10604,"as [45] since it’s the ﬁrst sequential neural model for real-time
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10605,"audio synthesis [45].
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10606,"1) Voice Quality: Each audio is listened by 15 subjects, each
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10607,"of which listens to 150 synthesized speech samples. We ﬁrst
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10608,"evaluate the voice quality in terms of mean opinion score (MOS)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10609,"among Tacotron ,Tacotron-PL(L) ,Tacotron-PL(M) ,Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10610,"PL(H) , and Tacotron-PL(LMH) . As shown in Fig. 7, Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10611,"PLmodels consistently outperforms Tacotron baseline with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10612,"either Grifﬁn-Lim algorithm or WaveRNN vocoder, while
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10613,"Tacotron-PL(L) achieves the best result. Note that WaveRNN1814 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10614,"Fig. 7. The mean opinion scores (MOS) of ﬁve systems evaluated by 15
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10615,"listeners, with 95% conﬁdence intervals computed from the t-test.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10616,"vocoder achieves better speech quality than Grifﬁn-Lim algo-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10617,"rithm, we conduct the subsequent listening experiments only
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10618,"with the speech samples generated by WaveRNN vocoder.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10619,"2) Expressiveness: In the objective evaluations and MOS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10620,"listening tests, Tacotron-PL(L) andTacotron-PL(LHM) consis-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10621,"tently offer better results. We next focus on comparing Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10622,"PL(L) andTacotron-PL(LHM) with Tacotron baseline. We ﬁrst
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10623,"conduct the AB preference test to assess speech expressiveness
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10624,"of the systems. Each audio is listened by 15 subjects, each
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10625,"of which listens to 120 synthesized speech samples. Table II
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10626,"reports the speech expressiveness evaluation results. We note
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10627,"that Tacotron-PL(L) outperforms both Tacotron baseline and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10628,"Tacotron-PL(LMH) in the preference test. The results suggest
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10629,"thatΨlowis more effective than other deep style features to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10630,"inform the speech style.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10631,"3) Naturalness: We further conduct the AB preference test
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10632,"to assess the naturalness of the systems. Each audio is listened by
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10633,"15 subjects, each of which listens to 120 synthesized speech sam-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10634,"ples. Table II reports the naturalness evaluation results. Just like
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10635,"in the expressiveness evaluation, we note that Tacotron-PL(L)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10636,"outperforms both Tacotron baseline and Tacotron-PL(LMH)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10637,"in the preference test. The results conﬁrm that Ψlowis more
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10638,"effective to inform the speech style.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10639,"4) Deep Style Features: We ﬁnally conduct Best Worst Scal-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10640,"ing (BWS) listening experiments to compare the four different
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10641,"Tacotron-PL systems with different deep style features. The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10642,"subjects are invited to evaluate multiple samples derived from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10643,"the different models, and choose the best and the worst sample.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10644,"We perform this experiment for 18 different utterances, and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10645,"each subject listens to 72 speech samples in total. Each audio is
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10646,"listened by 15 subjects.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10647,"Table III summarizes the results. We can see that Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10648,"PL(L) is selected for 80% of time as the best model and only 5%
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10649,"of time as the worst model, that shows Ψlowis the most effective
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10650,"deep style features.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10651,"E. Comparison With GST-Tacotron Paradigm
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10652,"We further compare Tacotron-PL with the state-of-the-art ex-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10653,"pressive TTS framework, i.e., GST-Tacotron [31]. The original
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10654,"Fig. 8. The convergence trajectories of three loss values on LJ-Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10655,"training data over the iteration steps, namely Loss frame forTacotron base-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10656,"line, Tacotron-ST ,a n d Loss frame component as part of the Loss total for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10657,"Tacotron-PL .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10658,"GST-Tacotron model [31] is focused on style control and trans-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10659,"fer, which differs from Tacotron-PL . For a fair comparison, we
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10660,"modify the GST-Tacotron framework and build a comparative
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10661,"system, denoted as Tacotron-ST . Speciﬁcally, the reference en-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10662,"coder of the GST-Tacotron model is replaced with a pre-trained
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10663,"SER-based style descriptor as described in Sec. III-A. The style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10664,"featuresΨextracted by the reference encoder informs Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10665,"STthe style information as GST-Tacotron does [31]. We then
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10666,"jointly train the whole Tacotron-ST framework including the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10667,"pre-trained SER-based reference encoder with Loss frame .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10668,"Tacotron-ST and Tacotron-PL share a similar architecture
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10669,"with Tacotron baseline [31] except that Tacotron-ST is aug-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10670,"mented by a reference encoder derived from a pre-trained
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10671,"SER model, while Tacotron-PL is augmented by the proposed
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10672,"style reconstruction loss. In other words, both Tacotron-ST
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10673,"andTacotron-PL incorporate style representations into the TTS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10674,"training. We take Tacotron-ST under the parallel style transfer
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10675,"scenario [31] as the contrastive model for Tacotron-PL .W ea l s o
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10676,"use the Tacotron model [5] as another baseline.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10677,"We use the low-level style feature Ψlowas the style embedding
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10678,"forTacotron-ST and the deep style feature for Tacotron-PL in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10679,"this section. We then conduct a set of experiments, following the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10680,"previous experiment setup in Sec. IV-B.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10681,"1) Convergence Trajectories of Loss frame :To examine the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10682,"effect of the proposed training strategy, and the inﬂuence of and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10683,"reference encoder and perceptual loss Loss style , we would like
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10684,"to observe how Loss frame converges with different training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10685,"schemes on the same training data. We only compare the con-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10686,"vergence trajectories of Loss frame between Tacotron baseline,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10687,"Tacotron-ST and theLoss frame component of Loss total for the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10688,"training of Tacotron-PL in Fig. 8.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10689,"A lower frame-level reconstruction loss, Loss frame , indi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10690,"cates a better convergence, thus a better frame level spec-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10691,"tral prediction. We observe that the Loss frame component in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10692,"Loss total achieves a lower convergence value than Loss frame
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10693,"in traditional Tacotron andTacotron-ST training. This suggests
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10694,"that utterance-level style objective function of Tacotron-PL and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10695,"reference signal supervision of Tacotron-ST not only optimizes
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10696,"style reconstruction loss, but also red uces frame-level recon-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10697,"struction loss over the Tacotron baseline.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10698,"Finally, Tacotron-PL obtains the best convergence trajec-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10699,"tories during training, that further validates the proposedLIU et al. : EXPRESSIVE TTS TRAINING WITH FRAME AND STYLE RECONSTRUCTION LOSS 1815
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10700,"Fig. 9. The mean opinion scores (MOS) of three systems evaluated by 15
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10701,"listeners, with 95% conﬁdence intervals computed from the t-test.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10702,"frame and style training strategy. We note that the trajecto-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10703,"ries of Tacotron-PL(M) vs.Tacotron-ST(M) ,Tacotron-PL(H)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10704,"vs.Tacotron-ST(H) ,Tacotron-PL(LMH) vs.Tacotron-ST(LMH)
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10705,"follow a similar pattern as Tacotron-PL(L) vs.Tacotron-ST(L) .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10706,"2) Objective and Subjective Evaluation: We also conduct
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10707,"objective and subjective evaluation experiments to compare
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10708,"the systems. In objective evaluation of Tacotron-ST , we obtain
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10709,"6.58, 1.14 and 14.18 of MCD, RMSE and FD respectively.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10710,"The Tacotron-ST results are consistently lower than those of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10711,"Tacotron , but higher than those of Tacotron-PL(L) in Table I,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10712,"which further conﬁrms the effectiveness of the frame and style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10713,"training strategy.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10714,"In subjective evaluation, we conduct the MOS and AB prefer-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10715,"ence tests to assess the overall performance of the systems. The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10716,"MOS scores are reported in Fig. 9. Each audio is listened by 15
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10717,"subjects, each of which listens to 75 synthesized speech samples.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10718,"It is observed that Tacotron-PL outperforms the Tacotron and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10719,"Tacotron-ST baselines, that shows the clear advantage of frame
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10720,"and style training strategy. Table IV reports the AB preference
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10721,"test results. Each audio is listened by 15 subjects, each of which
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10722,"listens to 120 synthesized speech samples. All results show that
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10723,"Tacotron-PL outperforms both Tacotron baseline and Tacotron-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10724,"STsigniﬁcantly in terms of expressiveness and naturalness.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10725,"All the above experiments conﬁrm that the proposed frame
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10726,"and style training strategy is more effective in informing the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10727,"speech style than GST-Tacotron paradigm, which is encourag-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10728,"ing.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10729,"V. C ONCLUSION
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10730,"We have studied a novel training strategy for Tacotron-based
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10731,"TTS system that includes frame and style reconstruction loss.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10732,"We implement an SER model as the style descriptor to extract
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10733,"deep style features to evaluate the style reconstruction loss.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10734,"We have conducted a series of experiments and demonstrated
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10735,"that the proposed Tacotron-PL training strategy outperforms
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10736,"the start-of-the-art Tacotron and GST-Tacotron-based baselines
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10737,"without the need of any add-on mechanism at run-time. While
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10738,"we conduct the experiments only on Tacotron, the proposed idea
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10739,"is applicable to other end-to-end neural TTS systems, that will
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10740,"be the future work in our plan.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10741,"APPENDIXTABLE V
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10742,"THESCRIPTS OF UTTERANCES IN SIXDISTINCTIVE STYLE GROUPS FROM
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10743,"LJ-S PEECH DATABASE ,THE DEEPSTYLE FEATURES OF WHICH ARE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10744,"VISUALIZED IN FIG.4
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10745,"REFERENCES
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10746,"[1] K. Tokuda, Y . Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10747,"“Speech synthesis based on hidden Markov models,” IEEE Proc. IRE ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10748,"vol. 101, no. 5, pp. 1234–1252, May 2013.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10749,"[2] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech syn-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10750,"thesis using deep neural networks,” in Proc. ICASSP IEEE Int. Conf.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10751,"Acoust., Speech Signal Process. , 2013, pp. 7962–7966.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10752,"[3] R. Liu, F. Bao, G. Gao, and Y . Wang, “Mongolian text-to-speech system
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10753,"based on deep neural network,” in Proc. Nat. Conf. Man-Mach. Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10754,"Commun. , 2017, pp. 99–108.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10755,"[4] Y . Wang et al. , “Tacotron: A fully end-to-end text-to-speech synthesis
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10756,"model,” in Proc. INTERSPEECH , 2017, pp. 4006–4010.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10757,"[5] J. Shen et al. , “Natural TTS synthesis by conditioning wavenet on
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10758,"MEL spectrogram predictions,” in Proc. ICASSPIEEE Int. Conf. Acoust.,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10759,"Speech Signal Process. , 2018, pp. 4779–4783.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10760,"[6] R. Liu, B. Sisman, F. Bao, G. Gao, and H. Li, “Wavetts: Tacotron-based
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10761,"tts with joint time-frequency domain loss,” in Proc. Odyssey 2020 The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10762,"Speaker and Language Recognition Workshop , 2020, pp. 245–251.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10763,"[7] Y . Lee and T. Kim, “Robust and ﬁne-grained prosody control of end-to-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10764,"end speech synthesis,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10765,"Signal Process. , 2019, pp. 5911–5915.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10766,"[8] A. J. Hunt and A. W. Black, “Unit selection in a concatenative speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10767,"synthesis system using a large speech database,” in ICASSP IEEE Int.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10768,"Conf. Acoust., Speech, Signal Process. , 1996, pp. 373–376.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10769,"[9] K. Tokuda, H. Zen, and A. W. Black, “An HMM-based speech synthesis
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10770,"system applied to english,” in Proc. IEEE Speech Synth. Workshop , 2002,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10771,"pp. 227–230.1816 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10772,"[10] R. Liu, B. Sisman, Y . Lin, and H. Li, “Fasttalker: A neural text-to-speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10773,"architecture with shallow and group autoregression,” Neural Netw. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10774,"vol. 141, pp. 306–314, 2021.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10775,"[11] Y .-A. Chung, Y . Wang, W.-N. Hsu, Y . Zhang, and R. Skerry-Ryan,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10776,"“Semi-supervised training for improving data efﬁciency in end-to-end
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10777,"speech synthesis,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10778,"Signal Process. , 2019, pp. 6940–6944.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10779,"[12] M. He, Y . Deng, and L. He, “Robust sequence-to-sequence acoustic
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10780,"modeling with stepwise monotonic attention for neural TTS,” in Proc.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10781,"INTERSPEECH , 2019, pp. 1293–1297.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10782,"[13] H.-T. Luong, X. Wang, J. Yamagishi, and N. Nishizawa, “Training multi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10783,"speaker neural text-to-speech systems using speaker-imbalanced speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10784,"corpora,” in Proc. INTERSPEECH , 2019, pp. 1303–1307.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10785,"[14] R. Liu, B. Sisman, J. Li, F. Bao, G. Gao, and H. Li, “Teacher-student
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10786,"training for robust tacotron-based tts,” in Proc. ICASSP IEEE Int. Conf.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10787,"Acoust., Speech Signal Process. , 2020, pp. 6274–6278.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10788,"[15] R. Liu, B. Sisman, and H. Li, “Graphspeech: Syntax-aware graph atten-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10789,"tion network for neural speech synthesis,” 2020, arXiv:2010.12423 .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10790,"[16] T. Hayashi, A. Tamamori, K. Kobayashi, K. Takeda, and T. Toda,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10791,"“An investigation of multi-speaker training for wavenet vocoder,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10792,"Proc. IEEE Autom. Speech Recognit. Understanding Workshop , 2017,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10793,"pp. 712–718.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10794,"[17] K. Chen, B. Chen, J. Lai, and K. Yu, “High-quality voice conversion
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10795,"using spectrogram-based wavenet vocoder,” in Proc. INTERSPEECH ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10796,"2018, pp. 1993–1997.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10797,"[18] T. Okamoto, T. Toda, Y . Shiga, and H. Kawai, “Real-time neural text-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10798,"to-speech with sequence-to-sequence acoustic model and WaveGlow or
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10799,"single Gaussian WaveRNN vocoders,” in Proc. INTERSPEECH , 2019,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10800,"pp. 1308–1312.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10801,"[19] B. Sisman, M. Zhang, and H. Li, “A voice conversion framework with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10802,"tandem feature sparse representation and speaker-adapted wavenet
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10803,"vocoder,” in Proc. INTERSPEECH , 2018, pp. 1978–1982.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10804,"[20] B. Sisman, M. Zhang, and H. Li, “Group sparse representation with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10805,"WaveNet vocoder adaptation for spectrum and prosody conversion,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10806,"IEEE/ACM Trans. Audio ,Speech Lang. Process. , vol. 27, no. 6,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10807,"pp. 1085–1097, Jun. 2019.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10808,"[21] B. Sisman, M. Zhang, S. Sakti, H. Li, and S. Nakamura, “Adap-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10809,"tive wavenet vocoder for residual compensation in GAN-based voice
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10810,"conversion,” in Proc. IEEE Spoken Lang. Technol. Workshop , 2018,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10811,"pp. 282–289.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10812,"[22] J. Hirschberg, “Pragmatics and intonation,” The Handbook of Pragmat-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10813,"ics, pp. 515–537, 2004.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10814,"[23] R. Liu, B. Sisman, F. Bao, J. Yang, G. Gao, and H. Li, “Exploit-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10815,"ing morphological and phonological features to improve prosodic
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10816,"phrasing for mongolian speech synthesis,” in Proc. IEEE/ACM Trans.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10817,"Audio, Speech, Lang. Process. , vol. 29, pp. 274–285, 2021, doi:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10818,"10.1109/TASLP.2020.3040523 .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10819,"[24] H. Luong, S. Takaki, G. E. Henter, and J. Yamagishi, “Adapting and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10820,"controlling DNN-based speech synthesis using input codes,” in Proc.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10821,"IEEE Int. Conf. Acoust., Speech Signal Process. , 2017, pp. 4905–4909.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10822,"[25] W.-C. Lin, Y . Tsao, F. Chen, and H.-M. Wang, “Investigation of neural
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10823,"network approaches for uniﬁed spectral and prosodic feature enhance-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10824,"ment,” in Proc. Asia-Paciﬁc Signal Inf. Process. Assoc. Annu. Summit
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10825,"Conf. , 2019, pp. 1179–1184.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10826,"[26] Z. Hodari, O. Watts, and S. King, “Using generative modelling to produce
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10827,"varied intonation for speech synthesis,” in Proc. 10th ISCA Speech Synth.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10828,"Workshop , 2019, pp. 239–244.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10829,"[27] Y . Zhao, H. Li, C.-I. Lai, J. Williams, E. Cooper, and J. Yamagishi, “Im-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10830,"proved prosody from learned F0 codebook representations for VQ-V AE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10831,"speech waveform reconstruction,” in Proc. Interspeech , 2020, pp. 4417–
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10832,"4421.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10833,"[28] Z. Hodari, C. Lai, and S. King, “Perception of prosodic variation for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10834,"speech synthesis using an unsupervised discrete representation of F0,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10835,"Proc. 10th Int. Conf. Speech Prosody , 2020, pp. 965–969.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10836,"[29] K. Silverman et al. , “ToBI: A standard for labeling english prosody,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10837,"Proc. 2nd Int. Conf. Spoken Lang. Process. , 1992, pp. 867–870.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10838,"[30] P. Taylor and A. W. Black, “Assigning phrase breaks from part-of-speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10839,"sequences,” Comput. Speech Lang. , vol. 12, no. 2, pp. 99–117, 1998.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10840,"[31] Y . Wang et al. , “Style tokens: Unsupervised style modeling, control and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10841,"transfer in end-to-end speech synthesis,” in Proc. Int. Conf. Mach. Learn. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10842,"2018, pp. 5180–5189.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10843,"[32] D. Stanton, Y . Wang, and R. Skerry-Ryan, “Predicting expressive speak-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10844,"ing style from text in end-to-end speech synthesis,” in Proc. IEEE Spoken
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10845,"Lang. Technol. Workshop , 2018, pp. 595–602.[33] R. Skerry-Ryan et al. , “Towards end-to-end prosody transfer for expres-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10846,"sive speech synthesis with tacotron,” in Proc. 35th Int. Conf. Mach. Learn.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10847,"PMLR , 2018, pp. 4693–4702.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10848,"[34] G. Sun, Y . Zhang, R. J. Weiss, Y . Cao, H. Zen, and Y . Wu, “Fully-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10849,"hierarchical ﬁne-grained prosody modeling for interpretable speech syn-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10850,"thesis,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech Signal Process. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10851,"2020, pp. 6264–6268.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10852,"[35] G. Sun et al. , “Generating diverse and natural text-to-speech samples
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10853,"using a quantized ﬁne-grained vae and autoregressive prosody prior,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10854,"Proc. ICASSP IEEE Int. Conf. Acoust., Speech Signal Process. , 2020,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10855,"pp. 6699–6703.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10856,"[36] K. Zhou, B. Sisman, R. Liu, and H. Li, “Seen and unseen emotional style
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10857,"transfer for voice conversion with a new emotional speech dataset,” 2020,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10858,"864arXiv:2010.14794 .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10859,"[37] A. Dosovitskiy and T. Brox, “Generating images with perceptual simi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10860,"larity metrics based on deep networks,” in Proc. Adv. Neural Inf. Process.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10861,"Syst. , 2016, pp. 658–666.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10862,"[38] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10863,"style transfer and super-resolution,” in Proc. Eur. Conf. Comput. Vis. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10864,"2016, pp. 694–711.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10865,"[39] Q. Chen and V . Koltun, “Photographic image synthesis with cascaded
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10866,"reﬁnement networks,” in Proc. IEEE Int. Conf. Comput. Vis. , 2017,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10867,"pp. 1511–1520.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10868,"[40] A. Wright and V . Válimäki, “Perceptual loss function for neural modeling
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10869,"of audio systems,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10870,"Signal Process. , 2020, pp. 251–255.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10871,"[41] T. Thiede et al. , “Peaq-the itu standard for objective measurement of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10872,"perceived audio quality,” J. Audio Eng. Soc. , vol. 48, no. 1/2, pp. 3–29,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10873,"2000.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10874,"[42] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10875,"evaluation of speech quality (PESQ)-A new method for speech quality
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10876,"assessment of telephone networks and codecs,” in Proc. ICASSP IEEE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10877,"Int. Conf. Acoust., Speech Signal Process. , 2001, pp. 749–752.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10878,"[43] V . Emiya, E. Vincent, N. Harlander, and V . Hohmann, “The peass toolkit-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10879,"perceptual evaluation methods for audio source separation,” in Proc. 9th
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10880,"Int. Conf. on Latent Variable Analysis and Signal Separation ,2010.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10881,"[44] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-time fourier
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10882,"transform,” IEEE Trans. Acoust., Speech, Signal Process. , vol. 32, no. 2,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10883,"pp. 236–243, Apr. 1984.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10884,"[45] N. Kalchbrenner et al. , “Efﬁcient neural audio synthesis,” in Proc. Int.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10885,"Conf. Mach. Learn. , 2018, pp. 2410–2419.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10886,"[46] A. v. d. Oord et al. , “Wavenet: A generative model for raw audio,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10887,"Proc. 9th ISCA Speech Synthesis Workshop , 2016, p. 125.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10888,"[47] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10889,"with deep convolutional neural networks,” in Proc. Adv. Neural Inf.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10890,"Process. Syst. , 2012, pp. 1097–1105.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10891,"[48] K. Emir Ak, A. Kassim, J. Hwee Lim, and J. Yew Tham, “Learn-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10892,"ing attribute representations with localization for ﬂexible fashion
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10893,"search,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2018,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10894,"pp. 7708–7717.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10895,"[49] K. Emir Ak, J. Hwee Lim, J. Yew Tham, and A. Kassim, “Seman-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10896,"tically consistent hierarchical text to fashion image synthesis with an
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10897,"enhanced-attentional generative adversarial network,” in Proc. IEEE Int.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10898,"Conf. Comput. Vis. Workshops , 2019, pp. 3121–3124.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10899,"[50] B. Sisman, J. Yamagishi, S. King, and H. Li, “An overview of voice con-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10900,"version and its challenges: From statistical modeling to deep learning,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10901,"inProc. IEEE/ACM Trans. Audio ,Speech, Lang. Process. , vol. 29, 2021,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10902,"pp. 132–157, doi: 10.1109/TASLP.2020.3038524 .
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10903,"[51] Y . Yasuda, X. Wang, S. Takaki, and J. Yamagishi, “Investigation of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10904,"enhanced tacotron text-to-speech synthesis systems with self-attention
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10905,"for pitch accent language,” in Proc. ICASSP IEEE Int. Conf. Acoust.,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10906,"Speech Signal Process. , 2019, pp. 6905–6909.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10907,"[52] M. S. Ribeiro and R. A. J. Clark, “A multi-level representation of F0 using
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10908,"the continuous wavelet transform and the discrete cosine transform,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10909,"Proc. ICASSP IEEE Int. Conf. Acoust., Speech Signal Process. , 2015,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10910,"pp. 4909–4913.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10911,"[53] A. Wennerstrom, The Music of Everyday Speech Prosody and Discourse
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10912,"Analysis . London, U.K.: Oxford, 2001, pp. 153–158.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10913,"[54] D. R. Ladd, “Intonational Phonology,” Cambridge, U.K.: Cambridge,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10914,"2008, pp. 153–158.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10915,"[55] Y . XU, “Speech prosody: A methodological review,” J. Speech ,v o l .1 ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10916,"no. 1, pp. 85–115, 2011.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10917,"[56] B. ¸ Si¸sman, H. Li, and K. C. Tan, “Transformation of prosody in voice
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10918,"conversion,” in Proc. Asia-Paciﬁc Signal Inf. Process. Assoc. Annu.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10919,"Summit Conf. , 2017, pp. 1537–1546.LIU et al. : EXPRESSIVE TTS TRAINING WITH FRAME AND STYLE RECONSTRUCTION LOSS 1817
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10920,"[57] J. Latorre, “Multilevel parametric-base F0 model for speech synthesis,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10921,"inProc. Ann. Conf. Int. Speech Commun. Assoc., INTERSPEECH , 2008,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10922,"pp. 2274–2277.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10923,"[58] Z. Wu, T. Kinnunen, E. S. Chng, and H. Li, “Text-independent F0
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10924,"transformation with non-parallel data for voice conversion,” in Proc.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10925,"INTERSPEECH , 2010, pp. 1732–1735.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10926,"[59] G. Sanchez, H. Silen, J. Nurminen, and M. Gabbouj, “Hierarchical
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10927,"modeling of F0 contours for voice conversion,” in Proc. INTERSPEECH ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10928,"2014, pp. 2318–2321.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10929,"[60] M. Vainio et al. , “Continuous wavelet transform for analysis of speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10930,"prosody,” TRASP 2013-Tools and resources for the analysys of speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10931,"prosody, an interspeech 2013 satellite event, Aug. 30, 2013, Laboratoire
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10932,"Parole et Lang. , Aix-en-Provence, France, Proceedings, pp. 78–81, 2013.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10933,"[61] A. Suni, D. Aalto, T. Raitio, P. Alku, and M. Vainio, “Wavelets for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10934,"intonation modeling in HMM speech synthesis,” in Proc. 8th ISCA Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10935,"Synth. Workshop , 2014, pp. 285–290.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10936,"[62] H. Ming, D. Huang, L. Xie, S. Zhang, M. Dong, and H. Li, “Exemplar-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10937,"based sparse representation of timbre and prosody for voice conversion,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10938,"inProc. ICASSP IEEE Int. Conf. Acoust., Speech, Signal Process. Conf.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10939,"Proc. , 2016, pp. 5175–5179.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10940,"[63] Z. Luo, J. Chen, T. Takiguchi, and Y . Ariki, “Emotional voice conversion
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10941,"with adaptive scales F0 based on wavelet transform using limited amount
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10942,"of emotional data,” in Proc. INTERSPEECH , 2017, pp. 3399–3403.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10943,"[64] Z. Luo, J. Chen, T. Takiguchi, and Y . Ariki, “Emotional voice conver-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10944,"sion using neural networks with arbitrary scales F0 based on wavelet
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10945,"transform,” EURASIP J. Audio, Speech, Music Process. , vol. 2017, no. 1,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10946,"pp. 1–13, 2017.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10947,"[65] H. Ming, D. Huang, L. Xie, J. Wu, M. Dong, and H. Li, “Deep bidi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10948,"rectional LSTM modeling of timbre and prosody for emotional voice
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10949,"conversion,” in Proc. INTERSPEECH , 2016, pp. 2453–2457.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10950,"[66] J. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi, “Modeling of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10951,"various speaking styles and emotions for HMM-based speech synthesis,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10952,"inProc. 8th Eur. Conf. Speech Commun. Technol. , 2003, pp. 2461–2464.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10953,"[67] M. Tachibana, J. Yamagishi, K. Onishi, T. Masuko, and T. Kobayashi,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10954,"“Hmm-based speech synthesis with various speaking styles using model
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10955,"interpolation,” in Proc. Speech Prosody, Int. Conf. , 2004, pp. 1–4.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10956,"[68] F. Eyben et al. , “Unsupervised clustering of emotion and voice styles
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10957,"for expressive TTS,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10958,"Signal Process. , 2012, pp. 4009–4012.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10959,"[69] H.-T. Luong, S. Takaki, G. E. Henter, and J. Yamagishi, “Adapting and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10960,"controlling DNN-based speech synthesis using input codes,” in Proc.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10961,"ICASSP IEEE Int. Conf. Acoust., Speech Signal Process. , 2017, pp. 4905–
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10962,"4909.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10963,"[70] T. Kenter, V . Wan, C.-A. Chan, R. Clark, and J. Vit, “Chive: Varying
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10964,"prosody in speech synthesis with a linguistically driven dynamic hierar-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10965,"chical conditional variational network,” in Proc. Int. Conf. Mach. Learn. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10966,"2019, pp. 3331–3340.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10967,"[71] S. Kataria, P. S. Nidadavolu, J. Villalba, N. Chen, P. GarcíPerera, and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10968,"N. Dehak, “Feature enhancement with deep feature losses for speaker
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10969,"veriﬁcation,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech Signal
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10970,"Process. , 2020, pp. 7584–7588.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10971,"[72] M. Kawanaka, Y . Koizumi, R. Miyazaki, and K. Yatabe, “Stable training
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10972,"of dnn for speech enhancement based on perceptually-motivated black-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10973,"box cost function,” in Proc. ICASSP IEEE Int. Conf. Acoust., Speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10974,"Signal Process. , 2020, pp. 7524–7528.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10975,"[73] F. G. Germain, Q. Chen, and V . Koltun, “Speech denoising with deep
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10976,"feature losses,” in Proc. INTERSPEECH , 2019, pp. 2723–2727.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10977,"[74] C.-C. Lo et al. , “Mosnet: Deep learning-based objective assessment for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10978,"voice conversion,” in Proc. INTERSPEECH , 2019, pp. 1541–1545.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10979,"[75] K.-S. Lee, “V oice conversion using a perceptual criterion,” Appl. Sci. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10980,"vol. 10, no. 8, 2020, Art. no. 2884.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10981,"[76] A. van den Oord et al. , “Parallel WaveNet: Fast high-ﬁdelity speech
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10982,"synthesis,” in Proc. 35th Int. Conf. Mach. Learn., Ser. Proc. Mach. Learn.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10983,"Res., J. Dy and A. Krause, Eds., vol. 80. Stockholmsmässan, Stockholm
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10984,"Sweden: PMLR, 10-15 Jul. 2018, pp. 3918–3926.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10985,"[77] Z. Cai, C. Zhang, and M. Li, “From speaker veriﬁcation to multispeaker
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10986,"speech synthesis, deep transfer with feedback constraint,” in Proc. Inter-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10987,"speech , 2020, pp. 3974–3978.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10988,"[78] E. Kim and J. W. Shin, “DNN-based emotion recognition based on
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10989,"bottleneck acoustic features and lexical features,” in Proc. ICASSP IEEE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10990,"Int. Conf. Acoust., Speech Signal Process. , 2019, pp. 6720–6724.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10991,"[79] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10992,"Statist. , vol. 1050, p. 1, 2014.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10993,"[80] G. Zhong, L. Wang, and J. Dong, “An overview on data representation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10994,"learning: From traditional feature learning to recent deep learning,” J.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10995,"Finance Data Sci. , vol. 2, no. 4, pp. 265–278, 2016.[81] S. Latif, R. Rana, J. Qadir, and J. Epps, “Variational autoencoders for
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10996,"learning latent representations of speech emotion: A preliminary study,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10997,"inProc. INTERSPEECH , 2018, pp. 3107–3111.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10998,"[82] S. Zhang, S. Zhang, T. Huang, and W. Gao, “Speech emotion recognition
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
10999,"using deep convolutional neural network and discriminant temporal pyra-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11000,"mid matching,” IEEE Trans. Multimedia , vol. 20, no. 6, pp. 1576–1590,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11001,"Jun. 2018.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11002,"[83] R. Lotﬁan and C. Busso, “Curriculum learning for speech emotion recog-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11003,"nition from crowdsourced labels,” IEEE/ACM Trans. Audio, Speech,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11004,"Lang. Process. , vol. 27, no. 4, pp. 815–826, Apr. 2019.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11005,"[84] I. R. Murray and J. L. Arnott, “Toward the simulation of emotion in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11006,"synthetic speech: A review of the literature on human vocal emotion,” J.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11007,"Acoust. Soc. Amer. , vol. 93, no. 2, pp. 1097–1108, 1993.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11008,"[85] O. Pierre-Yves, “The production and recognition of emotions in speech:
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11009,"Features and algorithms,” Int. J. Human-Comput. Stud. , vol. 59, no. 1-2,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11010,"pp. 157–183, 2003.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11011,"[86] P. Wu, Z. Ling, L. Liu, Y . Jiang, H. Wu, and L. Dai, “End-to-end emotional
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11012,"speech synthesis using style tokens and semi-supervised training,” in
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11013,"Proc. IEEE Asia-Paciﬁc Signal Inf. Process. Assoc. Annu. Summit Conf. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11014,"2019, pp. 623–627.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11015,"[87] Y . Gao, W. Zheng, Z. Yang, T. Kohler, C. Fuegen, and Q. He, “Interactive
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11016,"Text-to-Speech System via Joint Style Analysis.,” in Proc. Interspeech ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11017,"2020, pp. 4447–4451.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11018,"[88] S.-Y. Um, S. Oh, K. Byun, I. Jang, C. Ahn, and H.-G. Kang, “Emotional
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11019,"speech synthesis with rich and granularized control,” in Proc. ICASSP
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11020,"IEEE Int. Conf. Acoust., Speech Signal Process. , 2020, pp. 7254–7258.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11021,"[89] C. Busso et al. , “Iemocap: Interactive emotional dyadic motion capture
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11022,"database,” Lang. Resour. Eval. , vol. 42, no. 4, 2008, Art. no. 335.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11023,"[90] I. Goodfellow, Y . Bengio, and A. Courville, D. Learning. Cambridge,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11024,"MA, USA: MIT Press, 2016.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11025,"[91] D. Yu and M. L. Seltzer, “Improved bottleneck features using pretrained
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11026,"deep neural networks,” in Proc. INTERSPEECH , 2011, pp. 237–240.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11027,"[92] M. Chen, X. He, J. Yang, and H. Zhang, “3-D convolutional recurrent
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11028,"neural networks with attention model for speech emotion recognition,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11029,"IEEE Signal Process. Lett. , vol. 25, no. 10, pp. 1440–1444, Oct. 2018.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11030,"[93] K. Greff, R.K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhu-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11031,"ber, “LSTM: A search space odyssey,” IEEE Trans. Neural Netw. Learn.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11032,"Syst. , vol. 28, no. 10, pp. 2222–2232, Oct. 2017.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11033,"[94] K. Ito, “The LJ speech dataset,” 2017. [Online]. Available: https://
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11034,"keithito.com/LJ-Speech-Dataset/
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11035,"[95] L. v. d. Maaten and G. Hinton, “Visualizing data using t-SNE,” J. Mach.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11036,"Learn. Res. , vol. 9, no. 11, pp. 2579–2605, 2008.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11037,"[96] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep net-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11038,"work training by reducing internal covariate shift,” in Proc. Int. Conf.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11039,"Mach. Learn. , 2015, pp. 448–456.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11040,"[97] R. Kubichek, “Mel-Cepstral distance measure for objective speech qual-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11041,"ity assessment,” in Proc. IEEE Paciﬁc Rim Conf. Commun. Comput.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11042,"Signal Process. , 1993, pp. 125–128.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11043,"[98] B. Sisman, G. Lee, H. Li, and K. C. Tan, “On the analysis and evaluation of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11044,"prosody conversion techniques,” in Proc. Int. Conf. Asian Lang. Process. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11045,"2017, pp. 44–47.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11046,"[99] A. Z. Jusoh, R. Togneri, S. Nordholm, N. Sulaiman, and M. H. Khairolan-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11047,"uar, “The investigation of frame disturbance (FD) in perceptual evaluation
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11048,"speech quality (PESQ) as a perceptual metric,” ARPN J. Eng. Appl. Sci. ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11049,"vol. 10, no. 15, pp. 6365–6369, 2015.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11050,"[100] C. Gupta, H. Li, and Y . Wang, “Perceptual evaluation of singing quality,”
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11051,"inProc. IEEE Asia-Paciﬁc Signal Inf. Process. Assoc. Annu. Summit
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11052,"Conf. , 2017, pp. 577–586.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11053,"[101] M. Müller, “Dynamic time warping,” Inf. Retrieval Music Motion ,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11054,"pp. 69–84, 2007.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11055,"Rui Liu (Member, IEEE) received the B.S. degree
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11056,"from the Department of Software, Taiyuan University
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11057,"of Technology, Taiyuan, China, in 2014, and the Ph.D.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11058,"degree in computer science and technology from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11059,"the Inner Mongolia Key Laboratory of Mongolian
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11060,"Information Processing Technology, Inner Mongolia
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11061,"University, Hohhot, China, in 2020. He is also an
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11062,"exchange Ph.D. Candidate with the Department of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11063,"Electrical and Computer Engineering, National Uni-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11064,"versity of Singapore (NUS), Singapore, funded by
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11065,"China Scholarship Council. He is currently a joint
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11066,"Postdoctoral Research Fellow with NUS and Singapore University of Tech-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11067,"nology and Design, Singapore. His research interests include prosody and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11068,"acoustic modeling for speech synthesis, machine learning, and natural language
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11069,"processing.1818 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11070,"Berrak Sisman (Member, IEEE) received the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11071,"Ph.D. degree in electrical and computer engineering
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11072,"from the National University of Singapore, Singa-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11073,"pore, in 2020, fully funded by A*STAR Graduate
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11074,"Academy under Singapore International Graduate
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11075,"Award (SINGA). She is currently an Assistant Pro-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11076,"fessor with the Singapore University of Technology
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11077,"and Design (SUTD), Singapore. She is also an Af-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11078,"ﬁliated Researcher with the National University of
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11079,"Singapore, Singapore. Prior to joining SUTD, she
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11080,"was a Postdoctoral Research Fellow with the National
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11081,"University of Singapore, and a Visiting Researcher with Columbia University,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11082,"New York City, NY , USA. She was also an exchange Ph.D. Student with the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11083,"University of Edinburgh, Edinburgh, U.K., and a Visiting Scholar with The
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11084,"Centre for Speech Technology Research, University of Edinburgh in 2019. She
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11085,"was attached to RIKEN Advanced Intelligence Project, Japan in 2018. Her
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11086,"research interests include machine learning, signal processing, speech synthesis,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11087,"and voice conversion. She was the General Coordinator of the Student Advisory
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11088,"Committee of International Speech Communication Association.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11089,"Guanglai Gao received the B.S. degree from Inner
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11090,"Mongolia University, Hohhot, China, in 1985 and the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11091,"M.S. degree from the National University of Defense
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11092,"Technology, Changsha, China, in 1988. He was a
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11093,"Visiting Researcher with the University of Montreal,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11094,"Montreal, QC, Canada. He is currently a Professor
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11095,"with the Department of Computer Science, Inner
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11096,"Mongolia University, Hohhot, China. His research
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11097,"interests include artiﬁcial intelligence and pattern
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11098,"recognition.
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11099,"Haizhou Li (Fellow, IEEE) received the B.Sc., M.Sc.,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11100,"and Ph.D. degrees in electrical and electronic engi-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11101,"neering from the South China University of Technol-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11102,"ogy, Guangzhou, China, in 1984, 1987, and 1990,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11103,"respectively. He is currently a Professor with the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11104,"Department of Electrical and Computer Engineering,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11105,"National University of Singapore, Singapore. Prior to
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11106,"joining NUS, he taught with the University of Hong
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11107,"Kong, Hong Kong, from 1988 to 1990 and South
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11108,"China University of Technology, Guangzhou, China,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11109,"from 1990 to 1994. He was a Visiting Professor with
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11110,"CRIN, France, from 1994 to 1995, the Research Manager with the Apple-ISS
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11111,"Research Centre from 1996 to 1998, the Research Director of Lernout & Hauspie
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11112,"Asia Paciﬁc from 1999 to 2001, the Vice President of InfoTalk Corp. Ltd., from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11113,"2001 to 2003, and the Principal Scientist and Department Head of Human Lan-
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11114,"guage Technology in the Institute for Infocomm Research, Singapore, from 2003
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11115,"to 2016. His research interests include automatic speech recognition, speaker and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11116,"language recognition, and natural language processing. From 2015 to 2018, he
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11117,"was the Editor-in-Chief of the IEEE/ACM T RANSACTIONS ON AUDIO ,SPEECH
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11118,"AND LANGUAGE PROCESSING and from 2012 to 2018, a Member of the Editorial
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11119,"Board of Computer Speech and Language. He was an elected Member of IEEE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11120,"Speech and Language Processing Technical Committee from 2013 to 2015,
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11121,"the President of the International Speech Communication Association from
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11122,"2015 to 2017, the President of Asia Paciﬁc Signal and Information Processing
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11123,"Association from 2015 to 2016, and the President of Asian Federation of Natural
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11124,"Language Processing from 2017 to 2018. He was the General Chair of ACL
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11125,"2012, INTERSPEECH 2014 and ASRU 2019. Dr Li is a Fellow of the IEEE
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11126,"and the ISCA. He was the recipient of the National Infocomm Award 2002 and
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11127,"the President’s Technology Award 2013 in Singapore. He was named one of the
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11128,"two Nokia Visiting Professors in 2009 by the Nokia Foundation and Bremen
",False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11129,Excellence Chair Professor in 2019.,False,Expressive_TTS_Training_With_Frame_and_Style_Reconstruction_Loss,False,False,False
11130,Abstract,True,GPT-paper,False,False,False
11131,"I. INTRODUCTION   
",True,GPT-paper,False,False,False
11132,"II. MEHODOLOGY  
",True,GPT-paper,False,False,False
11133,"III. RESULTS  
",True,GPT-paper,False,False,False
11134,"IV. CONCLUSION AND D ISCUSSION  
",True,GPT-paper,False,False,False
11135," V oice Cloning with AI: Opportunities and 
",False,GPT-paper,False,False,False
11136,"Challenges  
",False,GPT-paper,False,False,False
11137,"Chat GPT   
",False,GPT-paper,False,False,False
11138,"Open.AI  
",False,GPT-paper,False,False,False
11139,"Spanbroek , Netherlands  
",False,GPT-paper,False,False,False
11140,"intelligence (AI) research that  aims to synthesize natural -
",False,GPT-paper,False,False,False
11141,"sounding human speech from text input. This paper reviews the 
",False,GPT-paper,False,False,False
11142,"current state of the art in voice cloning technology, including the 
",False,GPT-paper,False,False,False
11143,"various techniques used and the quality of the synthesized 
",False,GPT-paper,False,False,False
11144,"speech that can be produced. The paper also discusses the 
",False,GPT-paper,False,False,False
11145,"potential applications of voice cloning, as well as the ethical and 
",False,GPT-paper,False,False,False
11146,"privacy concerns that it raises.  
",False,GPT-paper,False,False,False
11147,"Keyword s—Chat -GTP , Voice -cloning, Mind map  
",False,GPT-paper,False,False,False
11148,"I. INTRODUCTION   
",False,GPT-paper,False,False,False
11149,"Voice cloning is a form of speech synthesis that uses AI 
",False,GPT-paper,False,False,False
11150,"techniques to produce natural -sounding human speech from 
",False,GPT-paper,False,False,False
11151,"text input. This technology has the potential to revolutionize 
",False,GPT-paper,False,False,False
11152,"the way we interact with computers and other devices, 
",False,GPT-paper,False,False,False
11153,"providing a more natural and intuitive way to communicate. 
",False,GPT-paper,False,False,False
11154,"However, it also raises important ethical and privacy 
",False,GPT-paper,False,False,False
11155,"concerns.  
",False,GPT-paper,False,False,False
11156,"II. MEHODOLOGY  
",False,GPT-paper,False,False,False
11157,"This paper is a review of the existing literature on voice 
",False,GPT-paper,False,False,False
11158,"cloning with AI. A search of relevant databases was 
",False,GPT-paper,False,False,False
11159,"conducted, including PubMed and Google Sch olar, to 
",False,GPT-paper,False,False,False
11160,"identify relevant studies and articles. The search was limited 
",False,GPT-paper,False,False,False
11161,"to articles published in the past 10 years and written in 
",False,GPT-paper,False,False,False
11162,"English  
",False,GPT-paper,False,False,False
11163,"III. RESULTS  
",False,GPT-paper,False,False,False
11164,"The current state of the art in voice cloning technology 
",False,GPT-paper,False,False,False
11165,"involves the use of deep learning algorithms to train large -
",False,GPT-paper,False,False,False
11166,"scale neural networks on large datasets of human speech. 
",False,GPT-paper,False,False,False
11167,"These networks are then able to generate synthesized speech 
",False,GPT-paper,False,False,False
11168,"that is similar to the training data. The quality of the 
",False,GPT-paper,False,False,False
11169,"synthesized speech has improved significantly in recent years, 
",False,GPT-paper,False,False,False
11170,"but it still falls  short of the naturalness and expressiveness of 
",False,GPT-paper,False,False,False
11171,"human speech.  
",False,GPT-paper,False,False,False
11172,"IV. CONCLUSION AND D ISCUSSION  
",False,GPT-paper,False,False,False
11173,"Voice cloning with AI has the potential to revolutionize 
",False,GPT-paper,False,False,False
11174,"the way we interact with computers and other devices, 
",False,GPT-paper,False,False,False
11175,"providing a more natural and intuitive way to communicate. 
",False,GPT-paper,False,False,False
11176,"However, it also raise s important ethical and privacy 
",False,GPT-paper,False,False,False
11177,"concerns. These concerns need to be carefully considered as 
",False,GPT-paper,False,False,False
11178,"the technology continues to develop, and appropriate 
",False,GPT-paper,False,False,False
11179,"safeguards need to be put in place to protect individuals' 
",False,GPT-paper,False,False,False
11180,privacy and prevent the misuse of the technology.  ,False,GPT-paper,False,False,False
11181,"Abstract   
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11182,"1. Introduction  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11183,"2. Datasets  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11184,"2.1. Training/Test data  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11185,"2.2. ALeXs Dataset  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11186,"3. Methods and system description  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11187,"3.1. Pre-processing  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11188,"3.2. Supervised c lassification approach  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11189,"4. Results  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11190,"5. Conclusions and future work  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11191,"6. Acknowledgements  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11192,"7. References  
",True,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11193," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11194," 24 Hulat - ALexS CWI T ask - CWI for Language and Learning 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11195,"Disabilities A pplied to U niversity Educational Texts  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11196,"Rodrigo Alarcona, Lourdes  Morenoa and Paloma  Martíneza 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11197," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11198,"a Computer Science Department , Universidad Carlos III de Madrid, Leganés , Madrid , Spain   
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11199," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11200,"  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11201,"The number of citizens who face difficulties in reading and understanding written texts is 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11202,"growing. One of the possible cognitive accessibility barriers for cognitive, language and 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11203,"learning disabilities is when the texts contain unusual words. In this sense, there are a range of 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11204,"techniques that can be used to deal with this issue. Complex Word Identification (CWI), which aims to identify unusual words for a target audience, is one such technique. In this paper, a supervised architecture is describ ed for the identification of complex words in university 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11205,"educational texts provided by the ALexS workshop. This architecture is composed of a Linear SVM with context -aware embedding features, provided by a BERT model. Moreover, easy -
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11206,"to-read and plain language resources were used. Our system participated in the ALexS CWI 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11207,"task, obtaining the second- best recall mark of 67%. However, low precision was due to, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11208,"according to the analysis performed, having been trained with resources aimed at improving 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11209,"cognitive ac cessibility regardless of the domain. The results indicate that the level of 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11210,"readability and understanding is more demanding in informative fields, such as Wikipedia 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11211,"pages, than in the specific domain of university educational texts. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11212," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11213,"Keywords  1 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11214,"Lexical s implification, CWI, Easy to read, BERT   
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11215,"1. Introduction  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11216,"In the current era of information technology, information is abundant (education, news, social, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11217,"health, government, etc.) for individuals. However, this information is not accessible to all people. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11218,"Certa in individuals face accessibility barriers when reading texts that contain long sentences, unusual 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11219,"words, complex linguistic structures, etc. Although people with intellectual and learning disabilities are 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11220,"most directly affected, cognitive accessibility barriers affect other user groups such as the deaf, deaf -
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11221,"blind, elderly, illiterate and immigrants with a different native language [1] [2]. People with reading 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11222,"disabilities can be found even among highly- educated users with specialized knowledge of the subj ect 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11223,"matter, such as university students. It may be possible to accommodate these users by making texts 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11224,"more readable.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11225,"In order to provide universal access to information and make texts more accessible, certain resources 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11226,"exist which provide helpful document ation, such as Easy -to-Read and plain language guidelines [3]. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11227,"However, systematic compliance with these guidelines is complicated, and simplification processes, thus, become essential. Simplified versions are normally created manually. Manual simplificati on of 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11228,"written documents is quite expensive, particularly considering that information is continually being 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11229,"produced.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11230,"As a solution, Natural Language Processing (NLP) methods, such as text simplification, have been 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11231,"developed to provide systematic support and promote compliance with these cognitive accessibility 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11232,"guidelines, improving the readability and understandability of texts. There is a myriad of approaches to 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11233,"accomplish this goal, one being Complex Word Identification (CWI), which aims to identify words  that 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11234,"are perceived as difficult for a given target audience.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11235,"                                                      
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11236,"Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020)  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11237,"EMAIL: ralarcon@inf.uc3m.es (A. 1); lmoreno@inf.uc3m.es (A. 2); pmf@inf.uc3m.es (A. 3)  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11238,"ORCID: 0000 -0003 -3296 -6873 (A. 1); 0000 -0002 -9021 -2546 (A. 2); 0000 -0003 -3013 -3771 (A. 3)  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11239," ©  2020  Copyright for this paper by its authors.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11240,"Use permitted under Creative Commo ns License Attribution 4.0 International (CC BY 4.0).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11241," CEUR Workshop Proceedings (CEUR -WS.org)  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11242," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11243," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11244," 25 Considering this, a supervised CWI approach in the ALexS workshop is proposed in this paper 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11245,"which aims to identify complex words in university educational texts. The remainder of the paper is 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11246,"organized as follows. Section 2 briefly describes our training/test dataset and the ALexS dataset. In 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11247,"section 3, our system is described. Section 4 presents the task results obtained by our system, both with regards to the training/test stage and in the ALex S task. Finally, Section 5 offers conclusions.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11248,"2. Datasets  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11249,"In order to follow a supervised approach, annotated data are necessary to identify whether a word is 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11250,"complex or simple. Therefore, our system was trained and tested with the following dataset.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11251,"2.1. Training/Test data  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11252,"The data used was the annotated corpus of Spanish Wikipedia pages proposed in the BEA Workshop 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11253,"2018 for the Complex Word Identification (CWI)  (google.com/view/cwisharedtask2018 ) task. As 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11254,"shown in Table 1, 17603 instances were annotated by 54 Spanish speakers, most of whom were native 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11255,"[4]. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11256," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11257,"Table 1. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11258,"Spanish CWI datasets distribution  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11259," # Instances  # Complex  # Simple  # Uniwords  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11260,"Training set  13748  5455  8293  11931  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11261,"Development set  1622  653 969 1408  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11262,"Test set  2233  907 1326  1955  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11263," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11264,"Each instance contains a target uniword/multiword which is selected by annotators. Said target is 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11265,"marked as complex if at least one annotator designates it as complex. Moreover, each instance is represented by 11 columns which provide a range of different information. The dataset contains 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11266,"information for binary and probabilistic subtasks. For the development of this system, we focus on the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11267,"binar y classification subtask and we use the following information:  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11268," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11269,"• The Second Column shows the actual sentence where a complex phrase annotation exists.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11270,"• The Third Column shows the start of the target word in the sentence.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11271,"• The Fourth Column  shows the end of the target word in the sentence.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11272,"• The Fifth Column shows the target word.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11273,"• The Tenth Column  shows the gold- standard label for the binary task (0: simple and 1: 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11274,"complex).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11275," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11276,"2.2. ALeXs Dataset  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11277,"As shown in Table 2, the VYTEDU -CW corpus provided by the ALexS workshop (alexs -sepln -
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11278,"2020.org/ ) consists of 55 text files containing the video transcripts of classes given at the Univer sity of 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11279,"Guayaquil (Ecuador ), resulting in a corpus of more than 68000 words, with more than 1200 words per 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11280,"transcription on average and 723 words which were designated as complex.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11281," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11282," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11283," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11284," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11285," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11286,"   
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11287," 26 Table 2. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11288,"Spanish CWI datasets distribution  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11289," Number of words  Number of Paragraphs  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11290,"Min 465 5 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11291,"Max  2646  18 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11292,"Average  1241  907 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11293,"Total  68248  613 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11294," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11295,"3. Methods and system description  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11296,"A supervised approach was proposed which aimed to identify complex words in educational texts.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11297,"3.1. Pre-processing  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11298,"The VYTEDU- CW corpus texts described were pre-processed following a series of steps. First, the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11299,"texts were split into sentences and tokens using Spacy  (www. spacy.io /), an opensource library that 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11300,"provides support for texts in different languages, including Spanish. Finally, these tokens are filtere d 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11301,"according to the following POS tags:  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11302," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11303,"• ADJ: Adjective   
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11304,"• ADV: Adverb  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11305,"• NOUN: Noun  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11306,"• PROPN: Proper noun  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11307," The filtered text was then converted into the same format as that used during the training stage, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11308,"preparing it for the next step of the process.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11309,"3.2. Supervised c lassification approach  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11310,"To process the text from the previous stage, a supervised approach was followed by training an SVM 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11311,"algorithm due to its successful performance in text classification tasks. Moreover, SVM was also one of the most  used algorithms for t his task in SemEval2016 [5]. Specifically,  a Linear SVC was chosen 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11312,"as it is much faster [6], takes advantage of the fact that SVM has shown good performance in classifying 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11313,"sparse instances [7] and, finally, had better results than previous tests carried o ut with a different type 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11314,"of kernel [8].  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11315," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11316,"Using the dataset described in section 3 and in order to train the algorithm, each word (instance) 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11317,"needed to be represented as a set of features to help distinguish between complex and simple words. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11318,"The proposed fea tures used are described below:  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11319," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11320,"• Length feature: word length  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11321,"• Boolean feature:  if a word is composed of capital letters  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11322,"• E2R feature:  a new feature established by creating an Easy -to-Read (E2R) dictionary.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11323,"• Word2vec feature:  pre-trained Word2Vec model vectors.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11324,"• BERT feature:  Pytorch pre -trained BERT model vectors.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11325," In relation to the E2R feature, we proposed a new feature by creating an E2R dictionary that follows 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11326,"E2R guidelines. The goal of this feature was to optimize the de tection of simple words. If a target word 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11327,"exists in the E2R dictionary, it receives a 0, otherwise is marked with a 1. The dictionary is fed from 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11328,"different sources that provide E2R texts drafted by experts with the support of the “Plena Inclusion”  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11329," 27 organiza tion (/www.plenainclusion.org/ ). Some of these sources were: the Noticias fácil news page 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11330,"(www.noticiasfacil.es/ ) and the Easy Reading Association  (www.lecturafacil.net/es/) . Subsequently, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11331,"this text was “cleaned” in order to preserve only the content words  (noun, verbs, adjectives, adverbs). 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11332,"Currently, this dictionary contains 13400 simple words.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11333,"In the Word2vec feature, supported by the genism library, vectors were extracted for each word from 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11334,"a 300- dimension Word2vec model trained on the Spanish Billion Words Corpus [9].  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11335,"The BERT feature operated in the following manner. Vectors were extracted for each word from a 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11336,"BERT (Bidirectional Encoder Representations from Transformers) [10] model  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11337,"(www. github.com/shehzaadzd/pytorch- pretrained -BERT ). In order to do this, a 12- layer multilingual 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11338,"BERT pre -trained model was used first before word vectors were extracted by adding the last four 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11339,"layers and using the first 480 dimensions of the model. To do this first we use the stored hidden states of the model that has four dimensions: the layer number, the batch number (one sentence per instance), 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11340,"the token number and the feature number (768 features). Later, our word vectors for each word of the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11341,"sentence are created by summing the last four layers. These layers are selec ted because they’ve shown 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11342,"better results in our tests and it can show different results depending on the task.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11343,"These embeddings are useful for semantic searches and information retrieval. The main difference 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11344,"between this type of embedding and others, such as Word2Vec or FastText, is that BERT produces 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11345,"word representations that are dynamically informed by the words around them, whereas Word2Vec the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11346,"words are represented as unique indexed values. In the common word embedding models, each word is 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11347,"represented w ith one single vector, ignoring polysemy words. In a sense, with word embedding, each 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11348,"word could have several vectors, one for each of its possible meanings. Therefore, these models allow 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11349,"us to deal with the task of word disambiguation when we identify com plex words.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11350,"4. Results  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11351,"Due to the fact that a validation dataset from the workshop was not given, the BEA’s workshop test 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11352,"dataset was used to validate our features and make adjustments. Table 3 shows the results obtained as 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11353,"regards the Train and Train+Developer datasets, which were validated with the test dataset. The results 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11354,"in both cases outperformed the results obtained by other systems from the abovementioned workshop 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11355,"[11][12][13][14]. Subsequently, a model was trained with the Train+Developer+Test to process the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11356,"ALexS dataset, which, when evaluated with the test dataset, obtained a result of 0.81. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11357," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11358,"Table 3. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11359,"Results for the test dataset  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11360," Accuracy  Precision  Recall  F1 Score  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11361,"TRAIN  0.80  0.79  0.78  0.792  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11362,"TRAIN+DEV  0.80  0.80  0.79  0.794  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11363," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11364,"Additionally, to complement previous information, Table 4  shows the scores of some combinations 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11365,"between these features, helping us determine which features are more discriminatory. One of the best scores are reached with the help of vectors of the e mbedding models. Using Word2Vec and BERT 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11366,"models, a F1 -score of 0.752 is obtained. Also, evaluating F1- scores independently for each feature, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11367,"BERT feature shows a F1 -score of 0.727, being the best score between all independent features. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11368,"Likewise,  the W2V feature yields a score of 0.70, proving to be a valuable resource for this task.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11369,"  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11370," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11371," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11372," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11373," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11374," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11375," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11376,"  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11377," 28 Table 4. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11378,"Results for the test dataset  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11379," Accuracy  Precision  Recall  F1 Score  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11380,"L+BT  0.79  0.78  0.77  0.778  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11381,"L+B+BT  0.79  0.79  0.78  0.783  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11382,"L+B+E+BT  0.80 0.80 0.78 0.787  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11383,"L+B+E+W+BT  0.80 0.80 0.79 0.794  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11384,"W+BT  0.77 0.76 0.75 0.752  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11385,"L 0.73 0.74 0.70 0.702  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11386,"BT 0.74 0.74 0.72 0.727  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11387,"W 0.72 0.71 0.70 0.700  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11388," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11389,"Table 5 shows the results of the CWI task in ALexS workshop. Although it gives a lower precision 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11390,"(with a score of 0.9), the system received the second -highest rank in Recall (with a score of 0.67), 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11391,"obtaining good coverage on the detection of words. The generalization of the system seems to be good. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11392,"However, it needs to improve on specific domains when dealing with technical words.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11393," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11394,"Table 5. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11395,"Results on ALexS task  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11396,"Participants  Accuracy  Precision  Recall  F1 Score  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11397,"Antonio Rico - Method 1  0.98 0.33 0.22 0.26 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11398,"Antonio Rico - Method 2  0.98 0.34 0.23 0.27 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11399,"Antonio Rico - Method 3  0.98 0.33 0.22 0.26 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11400,"Elena Zotova - Method 1  0.91 0.10 0.60 0.17 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11401,"Elena Zotova - Method 2  0.89 0.09 0.69 0.16 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11402,"Elena Zotova - Method 3  0.91 0.10 0.59 0.17 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11403,"George Zaharia  0.91 0.02 0.08 0.03 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11404,"(*) Rodrigo Alarcón  (HULAT)  0.90 0.09 0.67 0.16 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11405,"AlexS 2020 Organizers  0.92 0.12 0.66 0.20 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11406," In addition to the previous information, Table 6 confirms the previously described information and 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11407,"shows the results on some of the texts of the VYTEDU -CW corpus. For example, in the text of video 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11408,"41, the system showed good recall on the tas k by predicting all the complex words. However, at the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11409,"same time, it presented several false positives due to the generalization issue.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11410,"To illustrate this, take, for example, the word “biodiversidad” ( biodiversity) that would be labeled 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11411,"as complex in a gen eric domain. Nevertheless, this same word, in a university educational domain, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11412,"would be labeled as simple. This can be confirmed by comparing these annotations with the dataset from the BEA Workshop comprised of annotated Wikipedia pages which contain gene ric content. In 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11413,"this instance, the word “investigaciones” ( investigations ) is labeled as simple in the ALexS dataset but 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11414,"complex in the BEA dataset. There are many examples such as this in which the reason why the system 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11415,"shows good recall, but low precision is demonstrated.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11416,"Based on the outcomes, it can be seen that the university educational texts are not easily readable 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11417,"for university students with language and learning disabilities.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11418," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11419," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11420," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11421," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11422," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11423," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11424," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11425,"  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11426," 29 Table 6. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11427,"Specific results on the ALexS t ask (Participant: Rodrigo Alarcón)  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11428," Accuracy  Precision  Recall  F1 Score  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11429,"Video 5  0.92  0.56  0.43  0.49  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11430,"Video 41  0.92  0.05  1 0.10  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11431,"Video 43  0.86  0.02  1 0.04  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11432,"Video 48  0.97 0.04 1 0.08  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11433," 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11434,"5. Conclusions and future work  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11435,"The main objective of this work is to improve cognitive accessibility by increasing the understanding 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11436,"and readability of texts. In order to accomplish this objective, a supervised algorithm that uses a more 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11437,"refined, context -aware embedding model and Easy -to-Read resources was trained. The experiments 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11438,"showed that the combinations of these features with a Linear SVM outperforms previous systems. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11439,"However, it also presented difficulties when dealing with specific domains with less of a demand for 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11440,"readability, such as educational texts at a university level. To improve the precision of our system and 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11441,"a obtain a better result in the classification, university educational domain resources should be used. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11442,"BERT and Word2Vec models with university educational texts can be trained. Additionally, students 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11443,"with language and learning disabilities should not be considered as the target audience however, at the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11444,"university there are students with language and learning disabilities.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11445," Regarding the approach followed in our com plex word detection system, one of the main 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11446,"contributions of this research work has been the use of BERT embeddings in the prediction. For future 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11447,"work, we plan to explore more features of BERT models. With the extracted vectors, we can evaluate 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11448,"the cosine distance between the target word and the surroundings in the sentence. By giving this 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11449,"additional information, provided by more detailed embedding, a better score in the CWI task can be accomplished. At the same time, we can evaluate the synergy between a w ider variety of embeddings, 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11450,"such as Sense2 Vec [ 15] and Char2Vec. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11451,"6. Acknowledgements  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11452,"This work was supported by the Research Program of the Ministry of Economy and 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11453,"Competitiveness -Government of Spain, (DeepEMR project TIN2017 -87548- C2-1-R) 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11454,"7. References  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11455,"[1] D. Ferrés, H. Saggion, and X. G. Guinovart, “An adaptable lexical simplification architecture for 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11456,"major ibero- romance languages,” in Proceedings of the first workshop on building linguistically 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11457,"generalizable NLP systems, (2017), pp. 40–47.  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11458,"[2] L. Moreno, P. Martí nez, I. Segura -Bedmar, and R. Revert, “Exploring language technologies to 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11459,"provide support to WCAG 2.0 and E2R guidelines,” Proc. XVI Int. Conf. Hum. Comput. Interact. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11460,"- Interacción ’15, pp. 1–8, (2015).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11461,"[3] L. Moreno, R. Alarcon, and P. Martínez, “Lexical simplification approach to support the 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11462,"accessibility guidelines,” Proceedings of the XX International Conference on Human Computer 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11463,"Interaction (Interacción ’19), pp. 1– 4, (2019).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11464,"[4] S. M. Yimam, S. Stajner, M. Riedl, and C. Biemann, “Multilingual and Cross  Lingua l Complex 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11465,"Word Identification,” Recent Adv. Nat. Lang. Process., pp. 813–822, (2017).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11466,"[5] G. Paetzold and L. Specia, “SemEval 2016 Task 11: Complex Word Identification,” Proc. 10th 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11467,"Int. Work. Semant. Eval., pp. 560–569, (2016).   
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11468," 30 [6] I. Segura -bedmar, C. Colón- ruíz, M. Á. Tejedor -alonso, and M. Moro- moro, “Predicting of 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11469,"anaphylaxis in big data EMR by exploring machine learning approaches,” J. Biomed. Inform., vol. 
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11470,"87, no. January, pp. 50–59, (2018).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11471,"87, no. January, pp. 50–59, (2018).  
",False,Hulat - ALexS CWI Task - CWI for Language and Learning,False,False,True
11472,Abstract ,True,i0671,False,False,True
11473,"1I NTRODUCTION
",True,i0671,False,False,True
11474,"2W IDTH-FIRST-SEARCH TREESTRUCTURE
",True,i0671,False,False,True
11475,"2.1 Related Works
",True,i0671,False,False,True
11476,"2.2 WFS Tree-Structured Detector
",True,i0671,False,False,True
11477,"3V ECTOR BOOSTING ALGORITHM
",True,i0671,False,False,True
11478,"3.1 AdaBoost Algorithm
",True,i0671,False,False,True
11479,"3.2 Vector Boosting Algorithm
",True,i0671,False,False,True
11480,"High-Performance Rotation Invariant
",False,i0671,False,False,True
11481,"Multiview Face Detection
",False,i0671,False,False,True
11482,"Chang Huang, Student Member ,IEEE , Haizhou Ai, Member ,IEEE ,
",False,i0671,False,False,True
11483,"Yuan Li, and Shihong Lao, Member ,IEEE
",False,i0671,False,False,True
11484,"off-plane (ROP) angles in still images or video sequences. MVFD is crucial as the first step in automatic face processing for general
",False,i0671,False,False,True
11485,"applications since face images are seldom upright and frontal unless they are taken cooperatively. In this paper, we propose a series of
",False,i0671,False,False,True
11486,"innovative methods to construct a high-performance rotation invariant multiview face detector, including the Width-First-Search (WFS)
",False,i0671,False,False,True
11487,"tree detector structure, the Vector Boosting algorithm for learning vector-output strong classifiers, the domain-partition-based weak
",False,i0671,False,False,True
11488,"learning method, the sparse feature in granular space , and the heuristic search for sparse feature selection. As a result of that, our
",False,i0671,False,False,True
11489,"multiview face detector achieves low computational complexity, broad detection scope, and high detection accuracy on both standard
",False,i0671,False,False,True
11490,"testing sets and real-life images.
",False,i0671,False,False,True
11491,"Index Terms —Pattern classification, AdaBoost, vector boosting, granular feature, rotation invariant, face detection.
",False,i0671,False,False,True
11492,"Ç
",False,i0671,False,False,True
11493,"1I NTRODUCTION
",False,i0671,False,False,True
11494,"INthe past several decades, we have witnessed a burst of
",False,i0671,False,False,True
11495,"activities in applying robust computer vision systems for
",False,i0671,False,False,True
11496,"Human-Computer-Interaction (HCI). The face, which con-
",False,i0671,False,False,True
11497,"tains very important biological information of human being,
",False,i0671,False,False,True
11498,"is a very interesting object in images and videos. Naturally,face detection, which locates face regions at the very
",False,i0671,False,False,True
11499,"beginning, is considered as a fundamental part of any
",False,i0671,False,False,True
11500,"automatic face processing system. Also, it is a challengingwork since the difficulties of developing a robust face detectorarise from not only the diversities in the natureof human faces
",False,i0671,False,False,True
11501,"(e.g., the variability in size, location, pose, orientation, and
",False,i0671,False,False,True
11502,"expression) but also the changes of environment conditions(e.g., illumination, exposure, occlusion, etc.) [1].
",False,i0671,False,False,True
11503,"Generally speaking, there are mainly two methodologies
",False,i0671,False,False,True
11504,"for face detection task: one is knowledge-based and the otheris learning-based. The knowledge-based methodology at-tempts to depict our prior knowledge about the face patternwith some explicit rules, such as the intensity of faces, ellipticface contour, and equilateral triangle relation between eyes
",False,i0671,False,False,True
11505,"and mouth [2], [3]. Unfortunately, it is impossible to translate
",False,i0671,False,False,True
11506,"allhumanknowledgeexactlyintothoserequiredexplicitrulesthat could be accurately comprehended by computers. As aresult, methods of this type often perform poorly when therules mismatch unusual faces or match too many backgroundpatches. On the other hand, the learning-based methodology,
",False,i0671,False,False,True
11507,"of which the representatives include Osuna et al.’s SVMmethod [4], Rowley et al.’s ANN method [5] and Schneider-
",False,i0671,False,False,True
11508,"man and Kanade’s Bayesian-rule method [6], tries to model
",False,i0671,False,False,True
11509,"the face pattern with distribution functions or discriminant
",False,i0671,False,False,True
11510,"functions under the probabilistic framework. Methods ofthis kind are not limited by our describable knowledge on
",False,i0671,False,False,True
11511,"faces but determined by the capability of learning model and
",False,i0671,False,False,True
11512,"training samples, hence being able to deal with more complexcases compared with the knowledge-based approach. Speci-fically, the breakthrough of learning-based methodology
",False,i0671,False,False,True
11513,"happened in 2001 when Viola and Jones proposed a novel
",False,i0671,False,False,True
11514,"boosted cascade framework [7]. This work showed amazingreal-time speed and high detection accuracy. People usually
",False,i0671,False,False,True
11515,"attribute the achievements of this work to the fast calculated
",False,i0671,False,False,True
11516,"Haar-like features via the integral image and the cascadestructure of classifiers learned by AdaBoost. Here, for furtheranalysis,we decompose theirframeworkinto four levelsfrom
",False,i0671,False,False,True
11517,"top to bottom as shown in Table 1.
",False,i0671,False,False,True
11518,"Based on the premise that a significant disparity of
",False,i0671,False,False,True
11519,"occurrence rate between faces and background region in
",False,i0671,False,False,True
11520,"common images exists, Viola and Jones adopted an asym-metric cascade model that connected a series of strong
",False,i0671,False,False,True
11521,"classifiers with AND logic operators and each classifier made
",False,i0671,False,False,True
11522,"unbalanced decisions for face and nonface categories. Conse-quently, most of the background region could be rejectedrapidly by the first several classifiers with very little computa-
",False,i0671,False,False,True
11523,"tion. To learn such strong classifiers, they employed the
",False,i0671,False,False,True
11524,"AdaBoost algorithm [8], which could efficiently combinemany weak classifiers, acting as a feature selection mechan-
",False,i0671,False,False,True
11525,"ism, and guarantee a strong generalization bound for final
",False,i0671,False,False,True
11526,"classification. Finally, on the bottom level, they enumerated alargenumberofHaar-likefeaturesbasedontheintegralimageand associated them with corresponding stump functions to
",False,i0671,False,False,True
11527,"form a redundant weak classifier pool, which provided
",False,i0671,False,False,True
11528,"fundamental discriminability for the AdaBoost algorithm.AlltheseactivefactorswereorganizedeffectivelybyViolaand
",False,i0671,False,False,True
11529,"Jones to yield their distinguished work on face detection [7].
",False,i0671,False,False,True
11530,"Although the frontal face detection seems to be mature so
",False,i0671,False,False,True
11531,"far, it is often inadequate to meet the rigorous requirements ofIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 4, APRIL 2007 671
",False,i0671,False,False,True
11532,".C. Huang, H. Ai, and Y. Li are with the Department of Computer Science
",False,i0671,False,False,True
11533,"and Technology, Tsinghua University, Beijing 100084, China.E-mail: {huangc99, yuan-li}@mails.tsinghua.edu.cn,ahz@mail.tsinghua.edu.cn.
",False,i0671,False,False,True
11534,".S. Lao is with Sensing and Control Technology Laboratory, OMRON
",False,i0671,False,False,True
11535,"Corporation, Kyoto 619-0283, Japan. E-mail: lao@ari.ncl.omron.co.jp.
",False,i0671,False,False,True
11536,"Manuscript received 19 Jan. 2006; revised 5 June 2006; accepted 12 June 2006;
",False,i0671,False,False,True
11537,"published online 18 Jan. 2007.Recommended for acceptance by S. Prabhakar, J. Kittler, D. Maltoni,
",False,i0671,False,False,True
11538,"L. O’Gorman, and T. Tan.
",False,i0671,False,False,True
11539,"For information on obtaining reprints of this article, please send e-mail to:tpami@computer.organd reference IEEECSLog Number TPAMISI-0029-0106.Digital Object Identifier no. 10.1109/TPAMI.2007.1011.
",False,i0671,False,False,True
11540,"0162-8828/07/$25.00 /C2232007 IEEE Published by the IEEE Computer Societygeneral applications (e.g., visual surveillance system, digital
",False,i0671,False,False,True
11541,"equipments that need autofocus on faces, etc.) as human facesin real-life images are seldom upright and frontal. Naturally,multiview face detection and rotation invariant face detectionare defined to handle faces with ROP and RIP angles,
",False,i0671,False,False,True
11542,"respectively, (Fig. 1). However, both are more formidable
",False,i0671,False,False,True
11543,"problems because of the extension of detectable face range. Inparticular, the multiview face detection is more complicatedthan rotation invariant one since compared with frontal faces,profile faces tend to be less informative, more diverse, andmore sensitive to noise. Moreover, as shown in Fig. 1, theubiquitous concomitance of ROP and RIP of faces furthercompounds the difficulties to learn a face detector.
",False,i0671,False,False,True
11544,"In recent years, there have been many works that
",False,i0671,False,False,True
11545,"developed new methods to enhance Viola and Jones’ frame-
",False,i0671,False,False,True
11546,"work in some aspect. For instance, the detector structure has
",False,i0671,False,False,True
11547,"been extended to Li et al.’s pyramid model [9], Jones andViola’s decision tree [10], and Huang et al.’s Width-First-Search (WFS) tree [11] in order to cater to the multiview face
",False,i0671,False,False,True
11548,"detection,whileXiaoetal.’sboostingchain[12]andWuetal.’s
",False,i0671,False,False,True
11549,"nesting cascade model [13] transformed Viola and Jones’loose cascade model [7] into a more compact one. On the levelof strong classifier learning, the original AdaBoost algorithm,
",False,i0671,False,False,True
11550,"which adopts binary-output predictors, was replaced by the
",False,i0671,False,False,True
11551,"superior Real AdaBoost [14] and Gentle Boost [15] thatemploy confidence-rated predictors. Moreover, a finer parti-tionofthefeaturespacewasadoptedtoalleviatetheweakness
",False,i0671,False,False,True
11552,"of stump function, e.g., Liu and Shum’s histogram method
",False,i0671,False,False,True
11553,"[16], Wu et al.’s piece-wise function [13], and Mita et al.’s jointbinarizations of Haar-like feature [17]. As for the level offeature space, there have been works of Lienhart and Maydt’s
",False,i0671,False,False,True
11554,"extended Haar-like feature set [18], Liu and Shum’s Kullback-
",False,i0671,False,False,True
11555,"Leibler features [16], Baluja’s pair-wise points [19], Wang andJi’s RNDA algorithm [20], and Abramson and Steux’s controlpoint [21]. More details about these works can be found in the
",False,i0671,False,False,True
11556,"following sections, where comparisons are made.
",False,i0671,False,False,True
11557,"In this paper, we aim at constructing a fast and accurate
",False,i0671,False,False,True
11558,"rotation invariant multiview face detector, which is capable of
",False,i0671,False,False,True
11559,"detecting faces with pose changes of /C0=þ90
",False,i0671,False,False,True
11560,"/C14ROP (Yaw) and
",False,i0671,False,False,True
11561,"360/C14RIP (Roll). Besides, the tolerance to /C0=þ30/C14up-down
",False,i0671,False,False,True
11562,"(Pitch) change is combined t oc o n f o r mt os u r v e i l l a n c e
",False,i0671,False,False,True
11563,"environment. Our main contributions include the Width-
",False,i0671,False,False,True
11564,"First-Search (WFS) tree structure, the Vector Boosting
",False,i0671,False,False,True
11565,"algorithm, the sparse features in granular space, and theweak learner based on the heuristic search method. Theremainder of this paper is organized as follows: Section 2
",False,i0671,False,False,True
11566,"focuses on the comparison of existed detector structures and
",False,i0671,False,False,True
11567,"then introduces the WFS tree structure, Section 3 describes theVector Boosting algorithm preceded by a brief review of theclassical AdaBoost algorithm, Section 4 first discusses the
",False,i0671,False,False,True
11568,"effects of different types of features, then proposes sparse
",False,i0671,False,False,True
11569,"features in granular space and finally embodies the weaklearner that adopts heuristic search method to train weak
",False,i0671,False,False,True
11570,"hypotheses for Vector Boosting, Section 5 shows related
",False,i0671,False,False,True
11571,"experiment results, and Section 6 gives the conclusion.
",False,i0671,False,False,True
11572,"2W IDTH-FIRST-SEARCH TREESTRUCTURE
",False,i0671,False,False,True
11573,"2.1 Related Works
",False,i0671,False,False,True
11574,"Viola and Jones’ cascade structure [7], as shown in Fig. 2a, hasbeen proven very efficient for dealing with rare event
",False,i0671,False,False,True
11575,"detection problems such as face detection because of its
",False,i0671,False,False,True
11576,"asymmetric decision-making process. However, such a
",False,i0671,False,False,True
11577,"succinct structure does not have enough capacity to handlemultiview or rotation invariant face detection, both of which
",False,i0671,False,False,True
11578,"involve two distinct tasks: face detection and pose estimation.
",False,i0671,False,False,True
11579,"Face detection aims to distinguish faces from nonfaces, so it is
",False,i0671,False,False,True
11580,"inclined to utilize similarities between faces of different poses
",False,i0671,False,False,True
11581,"for rapid rejection of nonfaces; on the contrary, poseestimation is to identify the probable pose of a pattern no
",False,i0671,False,False,True
11582,"matter whether it is a face or not, so it seeks for diversities
",False,i0671,False,False,True
11583,"between different face poses but ignores the nonfaces.
",False,i0671,False,False,True
11584,"Unifying orseparatingthesetwotasks willleadtodifferent
",False,i0671,False,False,True
11585,"approaches. Osadchy et al.’s manifold method [22] could betaken as an example of the unified framework. A convolu-
",False,i0671,False,False,True
11586,"tional network was trained to map the face patterns to points
",False,i0671,False,False,True
11587,"on a face manifold, whose parameters indicated the pose
",False,i0671,False,False,True
11588,"variation, while nonface points were kept far away from the
",False,i0671,False,False,True
11589,"manifold. In this way, minimizing the energy function
",False,i0671,False,False,True
11590,"defined on the manifold was essentially a synchronous
",False,i0671,False,False,True
11591,"procedure to handle both tasks of multiview face detection.On the other hand, for the separated framework, the entire
",False,i0671,False,False,True
11592,"face range is usually divided into several individual cate-
",False,i0671,False,False,True
11593,"gories according to their RIP or ROP angles. Such separated
",False,i0671,False,False,True
11594,"framework is also known as the view-based approach since
",False,i0671,False,False,True
11595,"each category usually refers to some certain view of faces.
",False,i0671,False,False,True
11596,"The most straightforward way to implement a view-based
",False,i0671,False,False,True
11597,"approach was Wu et al.’s work [13], which trained different
",False,i0671,False,False,True
11598,"cascades individually for each view and used them in parallel
",False,i0671,False,False,True
11599,"as a whole like Fig. 2b. Though such a simple parallel-cascade
",False,i0671,False,False,True
11600,"strategy could achieve rather good performance in multiviewface detection, the unexploited correlation between faces of
",False,i0671,False,False,True
11601,"different views suggested a large capacity of improvement
",False,i0671,False,False,True
11602,"through a better designed detector structure.
",False,i0671,False,False,True
11603,"One way for improvement is the coarse-to-fine strategy. In
",False,i0671,False,False,True
11604,"[23], Fleuret and Geman employed a scalar tree detector(Fig. 2c) to adapt to large variation of location and scale of
",False,i0671,False,False,True
11605,"faces, while Li et al. [9] used pyramid to handle the great672 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 4, APRIL 2007
",False,i0671,False,False,True
11606,"TABLE 1
",False,i0671,False,False,True
11607,"Hierarchy of Viola and Jones’ Detector
",False,i0671,False,False,True
11608,"Fig. 1. Faces of RIP, ROP, respectively, and concomitance of both RIP
",False,i0671,False,False,True
11609,"and ROP. In this paper, multiview means yaw varies from /C090/C14toþ90/C14,
",False,i0671,False,False,True
11610,"including composite pitch varying from /C030/C14toþ30/C14, while rotation
",False,i0671,False,False,True
11611,"invariant denotes 360/C14rolling. Pitch change is omitted in the right figure.change of appearance in MVFD (Fig. 2d). Though their
",False,i0671,False,False,True
11612,"structures were of a little difference, they both divided thecomplicated entire face space into finer and finer subspaces.In higher levels of their structure, neighboring views wereassigned to a single node and, thus, corresponding faces were
",False,i0671,False,False,True
11613,"treated as one ensemble positive class so as to be separated
",False,i0671,False,False,True
11614,"from the nonfaces. This convenient combination did help toimprove the efficiency and reusability of extracted featuresdue to the similarities exist in faces of neighboring views,whereas neglected the inherent diversities between them(though they were neighboring views). As a result, a samplethat had been identified as a face by a node would have to be
",False,i0671,False,False,True
11615,"processed by its every child-node, since it had no discrimina-
",False,i0671,False,False,True
11616,"tion in corresponding views. In other words, the decision wasuniform for child-nodes: either all active or all inactive. Suchan all-pass route selection strategy considerably delayed theentire face identification procedure of input pattern.
",False,i0671,False,False,True
11617,"On the contrary, another way for improvement, the
",False,i0671,False,False,True
11618,"decision tree method [10], put emphasis upon the diversities
",False,i0671,False,False,True
11619,"between different views. A decision tree was trained as a poseestimator to tell which view the input pattern belonged to,which was followed by individually learned cascade detec-tors for each view, respectively (Fig. 2e). A similar “poseestimation + detection” approach can be found in [29], whichemployed the support vector machine rather than thedecision tree. With the imperative judgments made by the
",False,i0671,False,False,True
11620,"pose estimator, original complicated MVFD problem was
",False,i0671,False,False,True
11621,"reduced to several simple individual-views. Nevertheless,the pose estimation results were somewhat unstable, whichweakened the generalization ability of the whole system. The
",False,i0671,False,False,True
11622,"instability should partly be attributed to the fact that face pose
",False,i0671,False,False,True
11623,"change was a continuous process rather than a discrete one. In
",False,i0671,False,False,True
11624,"fact, there must be a large number of face samples lying closeto those artificially defined category boundaries, and,
",False,i0671,False,False,True
11625,"intuitively, the training of classifier with these “hard”
",False,i0671,False,False,True
11626,"boundaries often suffered from these ambiguous samples.From another point of view, fast and robust pose estimation
",False,i0671,False,False,True
11627,"applied before face detection is probably a problem that is
",False,i0671,False,False,True
11628,"even more difficult than the face detection itself.
",False,i0671,False,False,True
11629,"To sum up, as mentioned at the beginning of this section,
",False,i0671,False,False,True
11630,"pose estimation focuses on the diversities of different viewswhereas face detection requires finding the similarities of
",False,i0671,False,False,True
11631,"different views to reject nonfaces as quickly as possible. Such
",False,i0671,False,False,True
11632,"conflict eventually leads to the dilemma that at the beginningof view-based approach, either treating all faces as a single
",False,i0671,False,False,True
11633,"class (the pyramid approach) or different individual classes
",False,i0671,False,False,True
11634,"(the decision tree approach) is unsatisfactory for the MVFDproblem. Fortunately, a moderate approach, the Width-First-
",False,i0671,False,False,True
11635,"Search (WFS) tree, could be employed to harmonize the two
",False,i0671,False,False,True
11636,"tasks (pose estimation and face detection), balancing bothaspects between different views (diversity and similarity).
",False,i0671,False,False,True
11637,"2.2 WFS Tree-Structured Detector
",False,i0671,False,False,True
11638,"In our approach to the rotation invariant multiview face
",False,i0671,False,False,True
11639,"detector described in Section 1, first a multiview face detector
",False,i0671,False,False,True
11640,"is constructed which covers the upright quarter of Roll andfull Yaw, and then three more detectors are obtained by
",False,i0671,False,False,True
11641,"rotating the upright one by 90
",False,i0671,False,False,True
11642,"/C14, 180/C14, and 270/C14(Fig. 3). Such
",False,i0671,False,False,True
11643,"reduction from one rotation invariant detector to fourHUANG ET AL.: HIGH-PERFORMANCE ROTATION INVARIANT MULTIVIEW FACE DETECTION 673
",False,i0671,False,False,True
11644,"Fig. 2. Illustrations of different detector structures. (a) is Viola and Jones’ original cascade structure for frontal face detection. Adopting vie w-based
",False,i0671,False,False,True
11645,"strategy to cope with the MVFD problem, several detector structures are developed, including (b) Wu et al.’s parallel cascade [13], (c) Fleuret andGeman’s scalar tree [23], (d) Li et al.’s pyramid [9], (e) Jones and Viola’s decision tree [10], and (f) Huang et al.’s Width-First-Search (WFS) tree [1 1].
",False,i0671,False,False,True
11646,"Each leaf node in (d), (e), and (f) is actually an individual cascade as (a). Special attention should be paid to the difference among (c) scalar tree,
",False,i0671,False,False,True
11647,"(e) decision tree, and (f) WFS tree. Although they have got the similar tree-structures, their decision-making processes are distinct. Take the rootnode that splits into three child-nodes for example, the selection of pass route is uniform in scalar tree, exclusive in decision tree, and nonexclusi ve in
",False,i0671,False,False,True
11648,"WFS tree. Moreover, the scalar tree and WFS tree are able to make rejection decision at not only the leaf nodes but also the nonleaf ones. More
",False,i0671,False,False,True
11649,"details can be found in the following section.quartered ones makes our system highly scalable (e.g., in
",False,i0671,False,False,True
11650,"applications where inverted faces are rarely encountered, the
",False,i0671,False,False,True
11651,"rotated detectors can be shut down easily). We further divide
",False,i0671,False,False,True
11652,"the upright quartered face space into 15 basic views
",False,i0671,False,False,True
11653,"according to Yaw and Roll variance (Fig. 4), and then
",False,i0671,False,False,True
11654,"empirically organize them as a tree illustrated in Fig. 5. The
",False,i0671,False,False,True
11655,"root node comprises all 15 views, which covers the entire
",False,i0671,False,False,True
11656,"quartered face space. In the coming two layers, according to
",False,i0671,False,False,True
11657,"Yaw angle, the root branching node is gradually partitioned
",False,i0671,False,False,True
11658,"into five disjointed ones. At last, in the bottom layer, these
",False,i0671,False,False,True
11659,"five branching nodes are split into 15 leaves according to
",False,i0671,False,False,True
11660,"Roll, attaining the finest 15 views. In such a tree-structured
",False,i0671,False,False,True
11661,"detector, an input pattern is identified as a face if and only if
",False,i0671,False,False,True
11662,"it passes at least one route from the root node to some certain
",False,i0671,False,False,True
11663,"leaf node. Therefore, the Width-First-Search (WFS) strategy
",False,i0671,False,False,True
11664,"is the right way to access every promising node of pass
",False,i0671,False,False,True
11665,"routes, whose pseudocode is shown as Fig. 6. Notice that for
",False,i0671,False,False,True
11666,"patterns that correlate with more than one view, the post
",False,i0671,False,False,True
11667,"pose estimation judges the final result according to the
",False,i0671,False,False,True
11668,"confidence of each view.
",False,i0671,False,False,True
11669,"An extraordinary characteristic of the WFS strategy in
",False,i0671,False,False,True
11670,"Fig. 6 is the determinative vector GðxÞ, each component of
",False,i0671,False,False,True
11671,"which decides whether the input pattern should be sent to thecorresponding child-node or not. Compared with other
",False,i0671,False,False,True
11672,"related works listed in the last section, this determinative
",False,i0671,False,False,True
11673,"vector is much more versatile, neither restricted to beexclusive as Jones and Viola’s decision tree [10] nor to beuniform as Fleuret and Geman’s scalar tree [23] and Li et al.’spyramid [9]. For instance, in the root branching node of Fig. 5,
",False,i0671,False,False,True
11674,"an input pattern making GðxÞ¼ð 1;1;0Þindicates that it may
",False,i0671,False,False,True
11675,"be a left profile face or a frontal one but cannot be a rightprofile one, so in the following layer, it will be sent only to theleft node and the middle one. Another pattern that hasGðxÞ¼ð 0;0;0Þis classified as outlying from any view of faces
",False,i0671,False,False,True
11676,"and, thus, will be rejected immediately. In fact, faces of
",False,i0671,False,False,True
11677,"different views are still considered as dissimilar categories inbranching nodes of the WFS tree. However, these categories
",False,i0671,False,False,True
11678,"are not exclusive (e.g., as in the decision tree) but compatiblewith each other, meanwhile taking nonfaces as their collectivenegative class. In this way, the WFS tree not only utilizes thesimilarities between faces of different views to recognize
",False,i0671,False,False,True
11679,"nonfaces, but also reserves their diversities for further
",False,i0671,False,False,True
11680,"separation. Again take the root branching node in Fig. 5 forexample, Table 2 compares different approaches at the aspectof pass route selection. The pose estimation made by thedecision tree [10] is equivalent to a 3D determinative vectorwith only one nonzero component, and the pyramid structure
",False,i0671,False,False,True
11681,"[9], as well as the tree structure with scalar outputs [23], can
",False,i0671,False,False,True
11682,"only give a determinative vector with either all-zerocomponents or all-one components. As for the WFS tree, ithas the most diverse selections among all these approaches.674 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 4, APRIL 2007
",False,i0671,False,False,True
11683,"Fig. 3. Reduction of the rotation invariant multiview face detection. In
",False,i0671,False,False,True
11684,"fact, sparse features in the original multiview face detector are rotatedby 90
",False,i0671,False,False,True
11685,"/C14, 180/C14, and 270/C14, which is equivalent to rotating the input image
",False,i0671,False,False,True
11686,"but more efficient to compute during detection.
",False,i0671,False,False,True
11687,"Fig. 4. View definition in the upright multiview face detector. The
",False,i0671,False,False,True
11688,"detector covers the face range of Roll from /C045/C14/C24þ 45/C14and Yaw from
",False,i0671,False,False,True
11689,"/C090/C14/C24þ 90/C14, which is partitioned into 3/C25nonoverlapped views as
",False,i0671,False,False,True
11690,"shown above. Additional tolerance to Pitch variance from /C030/C14/C24þ 30/C14
",False,i0671,False,False,True
11691,"is embedded into each view.
",False,i0671,False,False,True
11692,"Fig. 5. Coarse-to-fine partition of face range in the tree-structured
",False,i0671,False,False,True
11693,"detector. The node with a red circle is a branching node, whose face
",False,i0671,False,False,True
11694,"range is the union of its child-nodes’. Among them, the root nodecontains all 15 views that are figured in the leaf nodes, of which the tilted10 views are not displayed for clarity. The similar omissions are adoptedfor other branching nodes.
",False,i0671,False,False,True
11695,"Fig. 6. Width-First-Search process in the tree-structured detector to
",False,i0671,False,False,True
11696,"identify an input pattern whether it is a face or not.In conclusion, with the help of determinative vectors and
",False,i0671,False,False,True
11697,"nonexclusive pass route selection mechanism, the WFS treestructured detector is able to make moderate decision forthe unidentified input pattern: neither too aggressive asdecision tree nor excessively cautious as pyramid or scalartree. Capabilities of different approaches in branching node(Table 2) explain the advantage of WFS tree in the flexibilityof decision-making process.
",False,i0671,False,False,True
11698,"3V ECTOR BOOSTING ALGORITHM
",False,i0671,False,False,True
11699,"The Vector Boosting algorithm is developed to learn strongclassifiers which can output the determinative vector GðxÞ
",False,i0671,False,False,True
11700,"of the WFS tree structure. Before elaborating on this novelmethod, we give a brief review on its origin—AdaBoost.
",False,i0671,False,False,True
11701,"3.1 AdaBoost Algorithm
",False,i0671,False,False,True
11702,"Boosting algorithm [24], which linearly combines a series ofweak hypotheses to yield a superior classifier, has beenregarded as one of the most significant developments in thepattern classification field during the past decade. Itessentially employs an additive model to minimize the lossfunction of classification in a regressive manner. Conse-quently, different loss functions lead to different boostingalgorithms. For example, AdaBoost [14], [24] and GentleBoost [15] take exponential loss function as the optimizationcriterion, while LogitBoost [15] uses Bernoulli log-likelihoodfunction. Moreover, BrownBoost [25] adopts a much moresophisticated loss function to enhance the robustness againstoutliers (noises) of training data. The classical AdaBoostalgorithm can be formalized as shown in Fig. 7.
",False,i0671,False,False,True
11703,"It is easy to verify that in the Adaboost algorithm, the
",False,i0671,False,False,True
11704,"weight of a sample satisfies
",False,i0671,False,False,True
11705,"w
",False,i0671,False,False,True
11706,"t
",False,i0671,False,False,True
11707,"i¼w0
",False,i0671,False,False,True
11708,"iexp/C0yiFtðxiÞ ðÞYt
",False,i0671,False,False,True
11709,"k¼1Zk;,
",False,i0671,False,False,True
11710,"ð1Þ
",False,i0671,False,False,True
11711,"where yiFtðxiÞis usually defined as the margin of sample
",False,i0671,False,False,True
11712,"ðxi;yiÞw:r:tFtðxÞ. This weight-updating mechanism for
",False,i0671,False,False,True
11713,"samples is the kernel of AdaBoost, which makes theoptimization procedure always emphasize those incorrectlyclassified samples by increasing their weights. By this means,AdaBoost manages to minimize the expectation of exponen-tial loss of training samples
",False,i0671,False,False,True
11714,"Loss F ðxÞðÞ ¼X
",False,i0671,False,False,True
11715,"n
",False,i0671,False,False,True
11716,"i¼1w0
",False,i0671,False,False,True
11717,"iexp/C0yiFðxiÞ ðÞ ¼ Ee/C0yFðxÞ/C16/C17
",False,i0671,False,False,True
11718,":ð2ÞEssentially, (2) has a close relationship with the training
",False,i0671,False,False,True
11719,"error of the final classifier HðxÞ. It has been proved that the
",False,i0671,False,False,True
11720,"upper bound of training error is held by the loss defined in(2), which is equal to the product of every normalizationfactor [14]:
",False,i0671,False,False,True
11721,"1
",False,i0671,False,False,True
11722,"nXn
",False,i0671,False,False,True
11723,"i¼1yi6¼HðxiÞ ½/C138½/C138 /C20Xn
",False,i0671,False,False,True
11724,"i¼1w0
",False,i0671,False,False,True
11725,"iexp/C0yiFðxiÞ ðÞ ¼YT
",False,i0671,False,False,True
11726,"t¼1Zt:ð3Þ
",False,i0671,False,False,True
11727,"Therefore, AdaBoost is actually an iterative procedure to
",False,i0671,False,False,True
11728,"greedily reduce the upper bound of training error. Althoughthe generalization bound given in [14] is often too loose tomake sense in practice, this algorithm has shown satisfactory
",False,i0671,False,False,True
11729,"performance in many practical problems.
",False,i0671,False,False,True
11730,"3.2 Vector Boosting Algorithm
",False,i0671,False,False,True
11731,"As extensions of the AdaBoost algorithm, AdaBoost.MH,
",False,i0671,False,False,True
11732,"AdaBoost.MO, and AdaBoost.MR [14] deal with multiclassproblems with different definitions of loss functions. Ada-Boost.MH assigns a label set for each sample and adopts theexponential loss of symmetric difference between the label set
",False,i0671,False,False,True
11733,"and the output of the strong classifier, and AdaBoost.MO
",False,i0671,False,False,True
11734,"makes use of the output code technique to generalizeAdaBoost.MH. On the other hand, AdaBoost.MR treats themulticlass problem as a ranking problem and uses rankingloss in expectation that the correct labels could receive the
",False,i0671,False,False,True
11735,"highest ranks. Although these multiclass boosting algorithms
",False,i0671,False,False,True
11736,"have been successfully applied in many problems, they arestill not directly applicable to the problem corresponding tobranching nodes of the WFS tree, in which faces of differentviews are neither coherent nor disperse but nonexclusive.
",False,i0671,False,False,True
11737,"Therefore, the Vector Boosting algorithm, as a unified
",False,i0671,False,False,True
11738,"boosting framework, is deve loped for the learning of
",False,i0671,False,False,True
11739,"branching nodes, which manipulates different kinds ofmulticlass problems by means of the vectorization ofHUANG ET AL.: HIGH-PERFORMANCE ROTATION INVARIANT MULTIVIEW FACE DETECTION 675
",False,i0671,False,False,True
11740,"TABLE 2
",False,i0671,False,False,True
11741,"Comparison of Different Approaches on
",False,i0671,False,False,True
11742,"Determinative Vectors for Pass Route Selection
",False,i0671,False,False,True
11743,"Fig. 7. A generalized version of the AdaBoost algorithm for two-class
",False,i0671,False,False,True
11744,"problems.hypothesis output space and the flexible loss function defined
",False,i0671,False,False,True
11745,"by intrinsic projection vectors.
",False,i0671,False,False,True
11746,"3.2.1 Convex Objective Region and Exponential Loss
",False,i0671,False,False,True
11747,"Function
",False,i0671,False,False,True
11748,"The Vector Boosting algorithm originates from the motiva-
",False,i0671,False,False,True
11749,"tion of decomposing a complicated multiclass problem into
",False,i0671,False,False,True
11750,"a set of simple ones, making them share the same features
",False,i0671,False,False,True
11751,"and calculating their respective outputs. For this purpose, it
",False,i0671,False,False,True
11752,"assigns different categories with different convex objective
",False,i0671,False,False,True
11753,"regions in the vector hypothesis output space IRkso as to
",False,i0671,False,False,True
11754,"make them distinguishable. These convex objective regions
",False,i0671,False,False,True
11755,"are defined as the intersection of one or more half spaces in
",False,i0671,False,False,True
11756,"homogeneous coordinates
",False,i0671,False,False,True
11757,"C¼~z:8ev2V;~z/C1ev/C210 fg
",False,i0671,False,False,True
11758,"~z¼ðz;1Þ;ev¼ðv;bÞ;z2IRk;V¼fev1;...;evmg:ð4Þ
",False,i0671,False,False,True
11759,"The extended vector ev, composed of normal vector vand
",False,i0671,False,False,True
11760,"offset b, specifies a half space that supports the convex
",False,i0671,False,False,True
11761,"objective region C. We name it intrinsic projection vector as it
",False,i0671,False,False,True
11762,"plays an important role in the loss function introduced later.
",False,i0671,False,False,True
11763,"The first column of Fig. 8 gives a naive example to show the
",False,i0671,False,False,True
11764,"usage of intrinsic projection vectors and the consequent
",False,i0671,False,False,True
11765,"convex objective regions. Note that it is unnecessary to
",False,i0671,False,False,True
11766,"require different objective regions to be nonoverlapped or
",False,i0671,False,False,True
11767,"have their union cover the entire hypothesis output space.
",False,i0671,False,False,True
11768,"Apparently, a hypothesis output FðxÞlies in the half space
",False,i0671,False,False,True
11769,"specified by an intrinsic projection vector evif their inner
",False,i0671,False,False,True
11770,"product is nonnegative. So, the margin of a hypothesis outputwith regard to an intrinsic projection vector is defined as
",False,i0671,False,False,True
11771,"margin FðxÞ;ev ðÞ ¼ eFðxÞ/C1ev; ð5Þwhere eFðxÞ¼ðFðxÞ;1Þis the extended hypothesis output in
",False,i0671,False,False,True
11772,"homogeneous coordinate. Furthermore, a hypothesis outputlies in a convex objective region if and only if its margin withregard to every intrinsic projection vector is nonnegative.Ideally, a perfectly learned hypothesis maps every inputpattern onto its corresponding objective region. Therefore,enlightened by the exponential loss adopted in AdaBoost
",False,i0671,False,False,True
11773,"algorithm, to penalize input patterns who have not been
",False,i0671,False,False,True
11774,"mapped onto the correct objective regions, the loss function inthe Vector Boosting algorithm is defined as follows:
",False,i0671,False,False,True
11775,"Loss FðxÞðÞ ¼ EX
",False,i0671,False,False,True
11776,"e
",False,i0671,False,False,True
11777,"vj2VðxÞexp/C0margin FðxÞ;evj/C0/C1 /C0/C10
",False,i0671,False,False,True
11778,"@1
",False,i0671,False,False,True
11779,"A
",False,i0671,False,False,True
11780,"¼EX
",False,i0671,False,False,True
11781,"evj2VðxÞexp/C0eFðxÞ/C1evj/C16/C170
",False,i0671,False,False,True
11782,"@1
",False,i0671,False,False,True
11783,"A;ð6Þ
",False,i0671,False,False,True
11784,"where VðxÞis the intrinsic projection vector set of pattern x.
",False,i0671,False,False,True
11785,"The second and the third columns in Fig. 8 draw the lossfunction of each class individually. In the training process,
",False,i0671,False,False,True
11786,"the expectation in (6) becomes the sum of losses absorbed
",False,i0671,False,False,True
11787,"from training samples as follows:
",False,i0671,False,False,True
11788,"LossðFðxÞÞ ¼1
",False,i0671,False,False,True
11789,"nXn
",False,i0671,False,False,True
11790,"i¼1X
",False,i0671,False,False,True
11791,"evj2VðxiÞexp/C0evj/C1eFðxiÞhi8
",False,i0671,False,False,True
11792,"<
",False,i0671,False,False,True
11793,":9
",False,i0671,False,False,True
11794,"=
",False,i0671,False,False,True
11795,";;ð7Þ
",False,i0671,False,False,True
11796,"where nis the number of training samples. It is easy to find
",False,i0671,False,False,True
11797,"that a training sample with qintrinsic projection vectors is
",False,i0671,False,False,True
11798,"equivalent to qtraining samples each with one intrinsic
",False,i0671,False,False,True
11799,"projection vector: ðxi;fev1;...;evqgÞ Ðeq
",False,i0671,False,False,True
11800,"fðxi;ev1Þ;...;ðxi;evqÞg.676 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 29, NO. 4, APRIL 2007
",False,i0671,False,False,True
11801,"Fig. 8. A naive example of a three-class problem. Three rows correspond to three classes, respectively. The first column shows their different
",False,i0671,False,False,True
11802,"objective regions, which are convex sets and defined by intrinsic projection vectors (white arrows). According to (4), CA¼f ðx; yÞ;x/C210g,
",False,i0671,False,False,True
11803,"CB¼f ðx; yÞ;y/C210g, and CC¼f ðx; yÞ:x/C200;y/C200g. The second and the third columns draw their losses in 2D output space for each category by
",False,i0671,False,False,True
11804,"surface and contour, which are expð/C0xÞfor Class A, expð/C0yÞfor Class B, and expðxÞþexpðyÞfor Class C. The fourth and the fifth columns draw their
",False,i0671,False,False,True
11805,"posterior probabilities also by surface and contour, which are employed to explicitly calculate the decision boundaries in Section 3.2.4.Based on this observation, in practice a training sample with
",False,i0671,False,False,True
11806,"more than one intrinsic projection vector will be expanded
",False,i0671,False,False,True
11807,"into a set of samples each with only one intrinsic projection
",False,i0671,False,False,True
11808,"vector. Without loss of generality, throughout the formaliza-
",False,i0671,False,False,True
11809,"tion of optimization procedure in the next section, we employ
",False,i0671,False,False,True
11810,"a minified loss function based on the expanded training
",False,i0671,False,False,True
11811,"samples as
",False,i0671,False,False,True
11812,"dLossðFðxÞÞ ¼1
",False,i0671,False,False,True
11813,"mXm
",False,i0671,False,False,True
11814,"i¼1exp/C0evi/C1eFðxiÞ/C16/C17
",False,i0671,False,False,True
11815,"¼
",False,i0671,False,False,True
11816,"n
",False,i0671,False,False,True
11817,"m/C21
",False,i0671,False,False,True
11818,"nXm
",False,i0671,False,False,True
11819,"i¼1exp/C0evi/C1eFðxiÞ/C16/C17
",False,i0671,False,False,True
11820,"¼n
",False,i0671,False,False,True
11821,"mLossðFðxÞÞ;n /C20m;ð8Þ
",False,i0671,False,False,True
11822,"where nis the number of original training samples while m
",False,i0671,False,False,True
11823,"is the number of expanded ones. As soon as the training
",False,i0671,False,False,True
11824,"samples and their intrinsic projection vectors are given, the
",False,i0671,False,False,True
11825,"minification n=m is determined. Therefore, using the
",False,i0671,False,False,True
11826,"minified loss function in (8) instead of the original one in
",False,i0671,False,False,True
11827,"(7) will make no difference on the final optimization results.
",False,i0671,False,False,True
11828,"3.2.2 Optimization Procedure
",False,i0671,False,False,True
11829,"3.2.2 Optimization Procedure
",False,i0671,False,False,True
11830,Abstract ,True,icst,False,False,False
11831,"I.  INTRODUCTION  
",True,icst,False,False,False
11832,"II. S
",True,icst,False,False,False
11833,"III. S
",True,icst,False,False,False
11834,"IV. S
",True,icst,False,False,False
11835,"V. D
",True,icst,False,False,False
11836,"Sensor Monitoring in the Home: Giving Voice to 
",False,icst,False,False,False
11837,"Elderly People 
",False,icst,False,False,False
11838," 
",False,icst,False,False,False
11839,"Marije Kanis, Saskia Robben, Judith Hagen, Anne Bimmerman, Natasja Wagelaar & Ben Kröse 
",False,icst,False,False,False
11840,"Amsterdam University of Applied Sciences 
",False,icst,False,False,False
11841,"Amsterdam, The Netherlands 
",False,icst,False,False,False
11842,"m.kanis@hva.nl 
",False,icst,False,False,False
11843," 
",False,icst,False,False,False
11844," 
",False,icst,False,False,False
11845,"elderly people’s needs and attitudes towards applying ambient 
",False,icst,False,False,False
11846,"sensor systems for monitoring daily activities in the home. As 
",False,icst,False,False,False
11847,"elderly are typically unfamiliar with such ambient technology, 
",False,icst,False,False,False
11848,"interactive tools for explicating sensor monitoring –an interactive dollhouse and iPad applications for displaying live monitored 
",False,icst,False,False,False
11849,"sensor activity data– were developed and used for this study. 
",False,icst,False,False,False
11850,"Furthermore, four studies conducted by occupational therapists 
",False,icst,False,False,False
11851,"with more than 60 elderly particip ants –including questionnaires 
",False,icst,False,False,False
11852,"(n=41), interviews (n=6), user sessions (n=14) and field studies 
",False,icst,False,False,False
11853,"(n=2)– were conducted. The exp eriences from these studies 
",False,icst,False,False,False
11854,"suggest that this approach helped to democratically engage the 
",False,icst,False,False,False
11855,"elderly as end-user and identify acceptance issues. 
",False,icst,False,False,False
11856,"Elderly-centred design; Ambient Assisted Living; Occupational 
",False,icst,False,False,False
11857,"therapists; Sensor monitoring; Sensor displays 
",False,icst,False,False,False
11858,"I.  INTRODUCTION  
",False,icst,False,False,False
11859,"With the growing awareness of the importance of engaging 
",False,icst,False,False,False
11860,"users in the design of technology, there is an increasing 
",False,icst,False,False,False
11861,"recognition that older people should also be involved in the 
",False,icst,False,False,False
11862,"design and evaluation of technology that could assist them in 
",False,icst,False,False,False
11863,"living their life longer independently. However, engaging 
",False,icst,False,False,False
11864,"elderly by researchers and technologists is not typically a 
",False,icst,False,False,False
11865,"straightforward process. For ex ample, the technical design and 
",False,icst,False,False,False
11866,"developing party may have potential biases towards using particular technology, have difficulties explaining their 
",False,icst,False,False,False
11867,"technical ideas and not be familiar with dealing with potential 
",False,icst,False,False,False
11868,"cognitive and physical limitations of elderly users. Such issues 
",False,icst,False,False,False
11869,"can hinder the needed shared understanding of the parties 
",False,icst,False,False,False
11870,"involved. To address this, this paper describes a different 
",False,icst,False,False,False
11871,"approach that includes studies conducted by occupational 
",False,icst,False,False,False
11872,"therapists and visual displays to help older people 
",False,icst,False,False,False
11873,"democratically identify and describe the acceptance issues that 
",False,icst,False,False,False
11874,"they encounter with monitoring technology. 
",False,icst,False,False,False
11875,"A. Residential Monitoring 
",False,icst,False,False,False
11876,"The demographic change of aging populations in developed 
",False,icst,False,False,False
11877,"countries has motivated a large body of research to specifically 
",False,icst,False,False,False
11878,"focus on technologies that are target ed at elderly people. In this 
",False,icst,False,False,False
11879,"vein, the field of Ambient A ssisted Living (AAL) focuses on 
",False,icst,False,False,False
11880,"ambient technologies to support and help elderly live longer 
",False,icst,False,False,False
11881,"independently. Ambient sensor monitoring is particularly being 
",False,icst,False,False,False
11882,"explored for this purpose. Typically, it is focused on the 
",False,icst,False,False,False
11883,"residential monitoring of elderly Activities of Daily Living (ADL); a set of activities used by physicians to benchmark 
",False,icst,False,False,False
11884,"physical and cognitive decline. The use of sensor monitoring 
",False,icst,False,False,False
11885,"technology in independent living settings is promising, as it 
",False,icst,False,False,False
11886,"could support earlier detection of changes in daily activity 
",False,icst,False,False,False
11887,"patterns, like reduced activity in an elder’s home and so alert 
",False,icst,False,False,False
11888,"health care providers to intervene earlier. However, according 
",False,icst,False,False,False
11889,"to a literature review [1], more work focusing on understanding 
",False,icst,False,False,False
11890,"the needs from the perspective of the elderly is needed. 
",False,icst,False,False,False
11891,"Particularly, technology that removes control from the elderly 
",False,icst,False,False,False
11892,"user can undermine the purpose by not letting them function 
",False,icst,False,False,False
11893,"independently [2]. For example, in the case of ambient sensor 
",False,icst,False,False,False
11894,"technology, independence is not achieved by, and may even be 
",False,icst,False,False,False
11895,"undermined by, providing feedback about an individual’s status 
",False,icst,False,False,False
11896,"to someone else, such as in a sensor report about functioning 
",False,icst,False,False,False
11897,"sent to caregivers instead of reporting to the elderly individual . 
",False,icst,False,False,False
11898,"B. Occupational Therapists for Engaging Older Adults 
",False,icst,False,False,False
11899,"Mitzner and Rogers [3] state that by involving older adults 
",False,icst,False,False,False
11900,"in the design process, technologies can be developed that are 
",False,icst,False,False,False
11901,"more useful and usable for older adults and may, therefore, 
",False,icst,False,False,False
11902,"increase their acceptance rates and contribute to successful 
",False,icst,False,False,False
11903,"aging. However, democratic inclusion of older adults has been 
",False,icst,False,False,False
11904,"recognized as challenging and needing a different approach [4]. 
",False,icst,False,False,False
11905,"Involving occupational therapists in such processes could help 
",False,icst,False,False,False
11906,"to increase common understanding between the elderly and 
",False,icst,False,False,False
11907,"other parties. Although collaborations between technologists, 
",False,icst,False,False,False
11908,"and even practitioners in the field of Human Computer 
",False,icst,False,False,False
11909,"Interaction (HCI) and occupationa l therapists are not standard 
",False,icst,False,False,False
11910,"practice, these latter groups both share the basis that the needs 
",False,icst,False,False,False
11911,"of a human being are central in their activities. Occupational 
",False,icst,False,False,False
11912,"therapy is generally applied to aid individuals in recovering and 
",False,icst,False,False,False
11913,"regaining as much independence as  possible and so is also used 
",False,icst,False,False,False
11914,"to address problems that develop as a result of getting older. An 
",False,icst,False,False,False
11915,"occupational therapist can assess one’s ability to carry out 
",False,icst,False,False,False
11916,"everyday tasks, such as washing, cooking or dressing oneself, 
",False,icst,False,False,False
11917,"and offer advice or techniques to help with these activities. 
",False,icst,False,False,False
11918,"Therefore, occupational therapists can typically help to 
",False,icst,False,False,False
11919,"democratically assess acceptance issues when regarding a 
",False,icst,False,False,False
11920,"sensor system for monitoring elderly people’s daily activities.  
",False,icst,False,False,False
11921,"C. Displaying Ambient Concepts for Engaging Older Adults
",False,icst,False,False,False
11922," 
",False,icst,False,False,False
11923,"Another problem with explor ing ambient design concepts 
",False,icst,False,False,False
11924,"with elderly is adequately explaining the scope of the 
",False,icst,False,False,False
11925,"technologies involved. Resear chers [5] found that showing 
",False,icst,False,False,False
11926,"simple mappings between sensors and display in demonstration applications in an ambient-technology augmented kitchen 
",False,icst,False,False,False
11927,"This work is part of the research programs Health-lab, COMMIT and 
",False,icst,False,False,False
11928,"Smart Systems for Smart Services that has been supported by SIA. We 
",False,icst,False,False,False
11929,"acknowledge Waag Society and Vivium Naarderheem for their contributions 
",False,icst,False,False,False
11930,"to this work.  WK,QWHUQDWLRQDO&RQIHUHQFHRQ3HUYDVLYH&RPSXWLQJ7HFKQR ORJLHVIRU+HDOWKFDUHDQG:RUNVKRSV
",False,icst,False,False,False
11931,"k,&67
",False,icst,False,False,False
11932,"'2,LFVWSHUYDVLYHKHDOWK could greatly improve users' understanding of the potential 
",False,icst,False,False,False
11933,"functionality. In this line, this study explores the involvement 
",False,icst,False,False,False
11934,"of occupational therapists, and also the use of interactive visual 
",False,icst,False,False,False
11935,"tools to help to articulate and understand ambient monitoring 
",False,icst,False,False,False
11936,"technology from an elderly point of view.  
",False,icst,False,False,False
11937,"Several projects attempt to help elders maintain their 
",False,icst,False,False,False
11938,"independence. For example, the CareNet Display [6] is used for 
",False,icst,False,False,False
11939,"reassuring the care network around an elder in the home. 
",False,icst,False,False,False
11940,"However, directly targeting elderly people as end-users of 
",False,icst,False,False,False
11941,"ambient system displays in the home is not standard practice. 
",False,icst,False,False,False
11942,"Research that also targeted the elderly [7] found that elder study participants had more difficulties with interpreting and 
",False,icst,False,False,False
11943,"interacting with a sensor activity display. Such work 
",False,icst,False,False,False
11944,"demonstrates the need for studies and displays that address the 
",False,icst,False,False,False
11945,"elderly as end-user in ways that they can understand. 
",False,icst,False,False,False
11946,"II. S
",False,icst,False,False,False
11947,"TUDY APPROACH  
",False,icst,False,False,False
11948,"Different approaches were taken to assess the needs and 
",False,icst,False,False,False
11949,"attitudes of elderly people with regards to monitoring their 
",False,icst,False,False,False
11950,"daily activities with sensors in their homes. The initial Living 
",False,icst,False,False,False
11951,"Lab context concerning this st udy is on-going work, involving 
",False,icst,False,False,False
11952,"the set-up of the needed infrastructure to conduct the studies, 
",False,icst,False,False,False
11953,"including the making of the necessarily social, organizational 
",False,icst,False,False,False
11954,"and technical connections. Then, the primary stage of this 
",False,icst,False,False,False
11955,"particular study comprised the development of several interactive visual tools for explaining ambient monitoring and 
",False,icst,False,False,False
11956,"sensor data activity to elderly. In the secondary stage, these 
",False,icst,False,False,False
11957,"tools were used in several studies by occupational therapists to 
",False,icst,False,False,False
11958,"democratically assess issues with elderly that concerned the use 
",False,icst,False,False,False
11959,"of sensor monitoring. This study lasted for more than one year. 
",False,icst,False,False,False
11960,"A. Living Labs for Health Innovation 
",False,icst,False,False,False
11961,"A large part of this work was carried out in a Living Lab 
",False,icst,False,False,False
11962,"setting, an environment and infras tructure where real users can 
",False,icst,False,False,False
11963,"be exposed to applications in their daily life so to aid designers 
",False,icst,False,False,False
11964,"and developers improve their products. The Living Lab concept 
",False,icst,False,False,False
11965,"was originally coined by Mitchell at MIT, as an approach for 
",False,icst,False,False,False
11966,"sensing, prototyping, validating and refining complex solutions 
",False,icst,False,False,False
11967,"in evolving real life contexts. In The Netherlands, several Living Lab locations have been set-up as part of Health-lab , a 
",False,icst,False,False,False
11968,"program that focuses on innovative solutions for enabling 
",False,icst,False,False,False
11969,"people to live longer independently. In this program, people 
",False,icst,False,False,False
11970,"from diverse care institutions, research labs, educational 
",False,icst,False,False,False
11971,"institutes and companies closely work together with end-users to co-create (technical) soluti ons. The program’s first Living 
",False,icst,False,False,False
11972,"Lab for health innovation was Nursing home Naarderheem, and originally set in motion in 2006. The current Living Lab setting 
",False,icst,False,False,False
11973,"involves apartments in different locations that were eventually equipped as AAL environments in which sensor-monitoring 
",False,icst,False,False,False
11974,"systems have been installed (see Fig 1).  
",False,icst,False,False,False
11975,"B. Tools to Exemplify Ambient Monitoring 
",False,icst,False,False,False
11976,"For democratically engaging the elderly as end-user, 
",False,icst,False,False,False
11977,"interactive tools were developed to demonstrate and explain the 
",False,icst,False,False,False
11978,"workings of the proposed monitoring system and the sensor 
",False,icst,False,False,False
11979,"activity output (see Fig. 2). Namely, an interactive dollhouse 
",False,icst,False,False,False
11980,"and iPad applications displaying live monitored sensor activity 
",False,icst,False,False,False
11981,"data were developed that target the elderly user. 
",False,icst,False,False,False
11982," 
",False,icst,False,False,False
11983,"Figure 2.  Interactive tools for exemplifying sensor monitoring: iPad 
",False,icst,False,False,False
11984,"application and first version of the interactive dollhouse (with visible sensors) 
",False,icst,False,False,False
11985,"1) Interactive dollhouse 
",False,icst,False,False,False
11986,"Sensor-equipped dollhouses were developed for increasing 
",False,icst,False,False,False
11987,"people’s understanding of the existence and desired workings 
",False,icst,False,False,False
11988,"of ambient monitoring technology in the home by 
",False,icst,False,False,False
11989,"demonstrating its potential (see also [8]). The dollhouses, scale
",False,icst,False,False,False
11990,"models of the different sensor-equipped homes, were 
",False,icst,False,False,False
11991,"developed and used to engage elderly users in residential 
",False,icst,False,False,False
11992,"monitoring. The scale models have been equipped with simple 
",False,icst,False,False,False
11993,"sensors that are able to track movement and so simulate the actual monitoring environment. The models were used as an 
",False,icst,False,False,False
11994,"elderly-centered research and design method in different settings for increasing understandi ng of the desired workings of 
",False,icst,False,False,False
11995,"ambient monitoring in the home.  
",False,icst,False,False,False
11996,"Figure 3.  The interactive dollhouse (latest version with hidden, embedded 
",False,icst,False,False,False
11997,"sensor technology and direct display) 
",False,icst,False,False,False
11998," Figure 1.  Sensors installed in the homes of the elderly participants 
",False,icst,False,False,False
11999," 
",False,icst,False,False,False
12000," 
",False,icst,False,False,False
12001,"Figure 4.  Screenshots of the five different developed elderly-centred applications for visualizing sensor activity
",False,icst,False,False,False
12002,"2) Elderly-centered apps for demonstrating sensor activity 
",False,icst,False,False,False
12003,"As part of a third-year stude nt assignment, 25 students (in 
",False,icst,False,False,False
12004,"groups of five) from the university’s program on 
",False,icst,False,False,False
12005,"Communication and Multimedia Design (CMD) were given the 
",False,icst,False,False,False
12006,"assignment to develop an iPad application that displays 
",False,icst,False,False,False
12007,"ambient sensor activity data in relevant and meaningful ways to 
",False,icst,False,False,False
12008,"senior users. In the design of these products, elderly ( n=34) 
",False,icst,False,False,False
12009,"were actively engaged in the design of these products (and recruited by the students themselves), in order to contribute to 
",False,icst,False,False,False
12010,"the success and acceptance of such technology. This led to five elderly-centered apps (see Fig. 4), which were all able to show 
",False,icst,False,False,False
12011,"the sensor activity data, but differed in (additional) features 
",False,icst,False,False,False
12012,"(such as a weather forecast) and user interface. Another student 
",False,icst,False,False,False
12013,"from a Technical Informatics course got then involved in linking the developed apps with the actual ambient monitoring 
",False,icst,False,False,False
12014,"systems in the Living Lab environment in Naarderheem.  
",False,icst,False,False,False
12015,"Graduate students in occupational therapy continued with more 
",False,icst,False,False,False
12016,"in-depth studies concerning elder’s attitudes with regards to 
",False,icst,False,False,False
12017,"residential monitoring and also evaluated the resulting five 
",False,icst,False,False,False
12018,"apps with elderly users.  
",False,icst,False,False,False
12019,"III. S
",False,icst,False,False,False
12020,"TUDIES BY OCCUPATIONAL THERAPISTS  
",False,icst,False,False,False
12021,"Four studies were conducted by occupational therapists 
",False,icst,False,False,False
12022,"under supervision of experts in HCI and sensor monitoring for 
",False,icst,False,False,False
12023,"assessing elderly people’s attitudes and needs with regards to 
",False,icst,False,False,False
12024,"sensor monitoring systems. These were the following: 
",False,icst,False,False,False
12025,"1) Questionnaires for assessing overall views on sensor 
",False,icst,False,False,False
12026,"monitoring, data sharing needs and developed apps:  A 
",False,icst,False,False,False
12027,"questionnaire was given to elderly living in residential homes 
",False,icst,False,False,False
12028,"in the Dutch regions West-Friesland and Lelystad to gather 
",False,icst,False,False,False
12029,"views on applying sensor monitoring and the developed apps. The recruited respondents were unfamiliar with residential 
",False,icst,False,False,False
12030,"monitoring at first instance. The questionnaire explained the 
",False,icst,False,False,False
12031,"concept of sensor monitoring with a photo scenario and 
",False,icst,False,False,False
12032,"screenshots for each of the five apps were shown.  
",False,icst,False,False,False
12033,"2) Interviews for exploring attitudes towards applying 
",False,icst,False,False,False
12034,"sensor monitoring, using the interactive dollhouse:  To gain a 
",False,icst,False,False,False
12035,"deeper understanding of applying sensor monitoring, semi-structured interviews were held in the homes of the elderly 
",False,icst,False,False,False
12036,"participants who were recruite d from the first study. Similar 
",False,icst,False,False,False
12037,"subjects as in the questionnaire were discussed, but this time the interactive dollhouse with sensors was used for better 
",False,icst,False,False,False
12038,"explaining and engaging the participants in the proposed 
",False,icst,False,False,False
12039,"monitoring technology. 
",False,icst,False,False,False
12040," 3) User study sessions to evaluate the developed apps:  For 
",False,icst,False,False,False
12041,"determining the use and usability of the five apps, four user 
",False,icst,False,False,False
12042,"group sessions (including 3-4 partic ipants in each session) were 
",False,icst,False,False,False
12043,"conducted. During each session, all the five apps were actively 
",False,icst,False,False,False
12044,"tried, discussed, and compared. The sessions started with an 
",False,icst,False,False,False
12045,"initial introduction led by the occupational therapists, then 
",False,icst,False,False,False
12046,"participants were subsequently asked to “think aloud” when 
",False,icst,False,False,False
12047,"progressing through the given usage tasks. After using all the 
",False,icst,False,False,False
12048,"apps, they had to pick their favorite.  
",False,icst,False,False,False
12049,"4) Pilot field study for testing the use of the most favored 
",False,icst,False,False,False
12050,"app in real settings:  For testing the use and experience with 
",False,icst,False,False,False
12051,"the app in a real (living lab) situation in Naarderheem, two 
",False,icst,False,False,False
12052,"elders that have sensors installed in their homes used the most 
",False,icst,False,False,False
12053,"favored app for one week. Thus, the app was connected to their own sensor network so they could view their own sensor 
",False,icst,False,False,False
12054,"activity data. After that week, a semi-structured interview was 
",False,icst,False,False,False
12055,"held to gain insights in to their experience.  
",False,icst,False,False,False
12056,"IV. S
",False,icst,False,False,False
12057,"TUDY FINDINGS  
",False,icst,False,False,False
12058,"The results of the studies –(1) questionnaire; (2) interviews; 
",False,icst,False,False,False
12059,"(3) user study; and (4) field study– present multifaceted 
",False,icst,False,False,False
12060,"insights in acceptance aspects of ambient sensor monitoring. 
",False,icst,False,False,False
12061,"1) The questionnaire:  This study led to 41 respondents 
",False,icst,False,False,False
12062,"between 65 and 89 years of age. The occupational therapists 
",False,icst,False,False,False
12063,"selected and recruited the participants, obtained the necessary 
",False,icst,False,False,False
12064,"consent, and offered door-to-door assistance with filling in the 
",False,icst,False,False,False
12065,"questionnaire. Still, not all of the participants were able to 
",False,icst,False,False,False
12066,"answer every single question of  the extensive questionnaire. 
",False,icst,False,False,False
12067,"Interestingly, the majority of the questionnaire respondents 
",False,icst,False,False,False
12068,"expressed not to want to use se nsor monitoring at this moment 
",False,icst,False,False,False
12069,"in time (92%, n=38). Also, before the explanation provided by 
",False,icst,False,False,False
12070,"the questionnaire and therapists, the majority of the 
",False,icst,False,False,False
12071,"participants indicated to not understand the concept of sensor 
",False,icst,False,False,False
12072,"monitoring (65%, n=40). Interestingly, from the eight 
",False,icst,False,False,False
12073,"participants who had indicated to understand the concept of sensor monitoring, two participants still thought it concerned the use of cameras, something that is not the case. When 
",False,icst,False,False,False
12074,"participants were positive about sharing the monitored activity 
",False,icst,False,False,False
12075,"data with others, and were asked to differentiate between several parties –home care, fa mily, friends, neighbors or 
",False,icst,False,False,False
12076,"technical experts– home care ranked first and neighbors came 
",False,icst,False,False,False
12077,"out as least favorite to share their data with. 
",False,icst,False,False,False
12078,"2) Semi-structured interviews:  The interviews were held in 
",False,icst,False,False,False
12079,"the homes of six elders (and four of their partners) which had a mean age of 77 years ( SD=7,3). The results show that the 
",False,icst,False,False,False
12080,"majority of participants (83%) were positive with regards to applying sensor monitoring now or  in the future. This more 
",False,icst,False,False,False
12081,"positive outcome than in the first study could be because a 
",False,icst,False,False,False
12082,"more explanatory method was used. According to the 
",False,icst,False,False,False
12083,"participants, the dollhouse, which was used during the 
",False,icst,False,False,False
12084,"interviews, gave more clarity into sensor monitoring. During the interviews, more profound arguments were brought up whether to use sensor monitoring. Positive arguments were an 
",False,icst,False,False,False
12085,"improved sense of security and independence. As one 
",False,icst,False,False,False
12086,"participant noted: “It is rea ssuring to know that there is 
",False,icst,False,False,False
12087,"someone watching over you”. Negative brought up arguments 
",False,icst,False,False,False
12088,"were the potential costs involved and breach of privacy.  
",False,icst,False,False,False
12089,"3) User study:  From the fourteen user study participants, 
",False,icst,False,False,False
12090,"one participant was not able to finish the session, because of fatigue. Such physical limita tions are common when doing 
",False,icst,False,False,False
12091,"studies with elderly people [9]. Also holding the iPad or touch 
",False,icst,False,False,False
12092,"interactions with the device seemed strenuous for some. In 
",False,icst,False,False,False
12093,"using the tablet interface, the participants revealed a lot of 
",False,icst,False,False,False
12094,"variations in skills. Findings, that also Van der Geest [9] found 
",False,icst,False,False,False
12095,"when conducting Internet user studies with elders. The participants expressed a preference for simple applications with clear wordings and a mini mal amount of screens, as in 
",False,icst,False,False,False
12096,"the one chosen most favorite, shown at the bottom of the middle in Fig. 4. For the fina l field study, the most favored 
",False,icst,False,False,False
12097,"working app was adjusted for usage in a real situation.  
",False,icst,False,False,False
12098,"4) Field study:  For this final study, a small sample was 
",False,icst,False,False,False
12099,"recruited ( n=2), because of the diffi culty of obtaining willing 
",False,icst,False,False,False
12100,"elderly participants living in homes with sensor networks 
",False,icst,False,False,False
12101,"already installed. The two par ticipants, who used the sensor 
",False,icst,False,False,False
12102,"activity app in combination with their sensor network in their 
",False,icst,False,False,False
12103,"homes, both checked the app several times a day, suggesting an interest in their own data. Participants indicated to be 
",False,icst,False,False,False
12104,"mostly interested in seeing their data over a longer period of time and so witness potential  decline. Moreover, the 
",False,icst,False,False,False
12105,"participants indicated to appreciate to be able to have their say 
",False,icst,False,False,False
12106,"over their own sensor data. 
",False,icst,False,False,False
12107,"V. D
",False,icst,False,False,False
12108,"ISCUSSION  
",False,icst,False,False,False
12109,"Further investigations with a larger sample size to study 
",False,icst,False,False,False
12110,"sensor technology usage with elders in real settings are 
",False,icst,False,False,False
12111,"recommended. This, however, comes with many technical, 
",False,icst,False,False,False
12112,"social and practical challenges. For example, getting to the 
",False,icst,False,False,False
12113,"stage in which elderly people are willing to have sensor networks installed in their homes  is quite an undertaking. And 
",False,icst,False,False,False
12114,"when these are in place, elderly participants frequently become 
",False,icst,False,False,False
12115,"ill, may pull out of study or other unexpected things can 
",False,icst,False,False,False
12116,"happen, such as participants switching off systems to save 
",False,icst,False,False,False
12117,"electricity. Furthermore, as this research and others [9] found, older age often coincides with (more) physical and cognitive 
",False,icst,False,False,False
12118,"limitations, such as becoming hard of hearing, or having 
",False,icst,False,False,False
12119,"difficulty memorizing new material making it harder to directly 
",False,icst,False,False,False
12120,"include elderly in technology usage, study and development. 
",False,icst,False,False,False
12121,"For this purpose, the collaboration with occupational therapists 
",False,icst,False,False,False
12122,"during the study was beneficial in many ways, such as in the 
",False,icst,False,False,False
12123,"participant recruitment process. Also in the interactions and 
",False,icst,False,False,False
12124,"dealings with participants, and carrying out the studies with the 
",False,icst,False,False,False
12125,"elderly without a technology bias, this was very valuable. 
",False,icst,False,False,False
12126,"Although interdisciplinary studies, particular within the field of 
",False,icst,False,False,False
12127,"pervasive health are not uncommon, further exploring the 
",False,icst,False,False,False
12128,"relationship between occupational therapists, technologists and 
",False,icst,False,False,False
12129,"HCI researchers deserves attention. The studies conducted by 
",False,icst,False,False,False
12130,"occupational therapists offered an insight into the priorities and 
",False,icst,False,False,False
12131,"perspectives of the individuals, and particularly the elderly 
",False,icst,False,False,False
12132,"participants involved. The studies showed that engaging, 
",False,icst,False,False,False
12133,"explaining and letting the elders experience monitoring by 
",False,icst,False,False,False
12134,"using visual tools such as dollhouses and the apps visualizing 
",False,icst,False,False,False
12135,"sensor activity helped the elderly in a better and more 
",False,icst,False,False,False
12136,"democratic discussion of the technology. This can be 
",False,icst,False,False,False
12137,"considered necessary, as the study results indicate that the 
",False,icst,False,False,False
12138,"majority of the study participants were not positive and did not 
",False,icst,False,False,False
12139,"understand or want to engage in the sensor monitoring of daily 
",False,icst,False,False,False
12140,"activities at first instance. 
",False,icst,False,False,False
12141,"A
",False,icst,False,False,False
12142,"CKNOWLEDGMENT  
",False,icst,False,False,False
12143,"We thank the participants & students involved in the study. 
",False,icst,False,False,False
12144,"REFERENCES  
",False,icst,False,False,False
12145,"[1] H. B.-L. Duh, E. Y.-L. Do, M. Billi nghurst, F. Quek, and V. H.-H. Chen, 
",False,icst,False,False,False
12146,"""Senior-friendly technologies: Interaction design for senior users,"" in 
",False,icst,False,False,False
12147,"Extended abstracts of CHI 2010 Atlanta, Georgia, USA: ACM, 2010. 
",False,icst,False,False,False
12148,"[2] S. Thielke, M. Harniss, H. Thomps on, S. Patel, G. Demiris, and K. 
",False,icst,False,False,False
12149,"Johnson, ""Maslow’s hierarchy of human needs and the adoption of health-related technologies for older adults,"" Ageing international, vol. 
",False,icst,False,False,False
12150,"37, pp. 470-488, 2011. 
",False,icst,False,False,False
12151,"[3] T. Mitzner and W. Rogers, ""Understanding older adults’ limitations and 
",False,icst,False,False,False
12152,"capabilities, and involving them in the design process,"" in Extended 
",False,icst,False,False,False
12153,"abstracts of CHI'10, 2010. 
",False,icst,False,False,False
12154,"[4] A. Dickinson, J. Arnott, and S. Prior, ""Methods for human computer 
",False,icst,False,False,False
12155,"interaction research with older people,"" Behaviour & Information 
",False,icst,False,False,False
12156,"Technology, vol. 26, pp. 343-352, 2007. 
",False,icst,False,False,False
12157,"[5] P. Olivier, A. Monk, G. Xu, and J. Hoey, ""Ambient Kitchen: Designing 
",False,icst,False,False,False
12158,"situated services using a high fidelity prototyping environment,"" in 
",False,icst,False,False,False
12159,"Proceedings of PETRA'09, Corfu, Greece, 2009. 
",False,icst,False,False,False
12160,"[6] S. Consolvo, P. Roessler, B. Shelton, A. Lamarca, B. Schilit, and S. Bly, ""Technology for Care Networks of Elders,"" IEEE Pervasive Computing, 
",False,icst,False,False,False
12161,"vol. 3, pp. 22-29, 2004. 
",False,icst,False,False,False
12162,"[7] G. Alexander, B. Wakefield, M. Rantz,  M. Skubic, M. Aud, S. Erdelez, 
",False,icst,False,False,False
12163,"and S. Ghenaimi, ""Passive sensor technology interface to assess elder activity in lndependent living,"" Journal of Nursing Research, vol. 60, pp. 
",False,icst,False,False,False
12164,"318-325, 2011. 
",False,icst,False,False,False
12165,"[8] M. Kanis, S. Robben, and B. Kröse, ""Miniature play: Using an 
",False,icst,False,False,False
12166,"interactive dollhouse to demonstrate ambient interactions in the home,"" 
",False,icst,False,False,False
12167,"in Proceedings of DIS 2012, Newcastle, UK, 2012. 
",False,icst,False,False,False
12168,"[9] T. v. d. Geest, ""Conducting usability studies with users who are elderly 
",False,icst,False,False,False
12169,"or have disabilities,"" Technical Communication, vol. 53, 2006. 
",False,icst,False,False,False
12170," 
",False,icst,False,False,False
12171,,False,icst,False,False,False
12172,Abstract,True,ieee_sinefit,False,False,False
12173,"I. INTRODUCTION
",True,ieee_sinefit,False,False,False
12174,"II. T
",True,ieee_sinefit,False,False,False
12175,"III. THEESTIMATION ALGORITHM
",True,ieee_sinefit,False,False,False
12176,"IV. COMPUTER SIMULATION
",True,ieee_sinefit,False,False,False
12177,"V. CONCLUSION
",True,ieee_sinefit,False,False,False
12178,"See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/3089069
",False,ieee_sinefit,False,False,False
12179,"Sinewa ve ﬁt algorithm based on total least-squares method with application to
",False,ieee_sinefit,False,False,False
12180,"ADC effective bits measu rement
",False,ieee_sinefit,False,False,False
12181,"Article    in  IEEE T ransactions on Instrument ation and Me asur ement  · Sept ember 1997
",False,ieee_sinefit,False,False,False
12182,"DOI: 10.1109/19.650821  · Sour ce: IEEE Xplor e
",False,ieee_sinefit,False,False,False
12183,"CITATIONS
",False,ieee_sinefit,False,False,False
12184,"48READS
",False,ieee_sinefit,False,False,False
12185,"2,803
",False,ieee_sinefit,False,False,False
12186,"4 author s, including:
",False,ieee_sinefit,False,False,False
12187,"Jian Qiu Zhang
",False,ieee_sinefit,False,False,False
12188,"Fudan Univ ersity
",False,ieee_sinefit,False,False,False
12189,"144 PUBLICA TIONS    1,548  CITATIONS    
",False,ieee_sinefit,False,False,False
12190,"SEE PROFILE
",False,ieee_sinefit,False,False,False
12191,"Sun Jinw ei
",False,ieee_sinefit,False,False,False
12192,"Tsinghua Univ ersity
",False,ieee_sinefit,False,False,False
12193,"4 PUBLICA TIONS    69 CITATIONS    
",False,ieee_sinefit,False,False,False
12194,"SEE PROFILE
",False,ieee_sinefit,False,False,False
12195,"All c ontent f ollo wing this p age was uplo aded b y Jian Qiu Zhang  on 05 Dec ember 2014.
",False,ieee_sinefit,False,False,False
12196,"The user has r equest ed enhanc ement of the do wnlo aded file.1026 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 46, NO. 4, AUGUST 1997
",False,ieee_sinefit,False,False,False
12197,"Sinewave Fit Algorithm Based on Total
",False,ieee_sinefit,False,False,False
12198,"Least-Squares Method with Application
",False,ieee_sinefit,False,False,False
12199,"to ADC Effective Bits Measurement
",False,ieee_sinefit,False,False,False
12200,"Jian Qiu Zhang, Zhao Xinmin, Hu Xiao, and Sun Jinwei
",False,ieee_sinefit,False,False,False
12201,"measurement systems. The characterizations of analog–digitalconverters and digital oscilloscopes are two examples. In thispaper, we present a high-performance (i.e., high-precision andhigh-speed) algorithm to estimate the four parameters of a
",False,ieee_sinefit,False,False,False
12202,"sinewave from a sample data record. By the use of trigonometric
",False,ieee_sinefit,False,False,False
12203,"identity, we propose a frequency estimator that turns the non-linear estimation problem into a linear one. Thus, the difﬁcultyof the traditional nonlinear least-squares sinewave ﬁt method isattenuated. The total least-squares method is used to estimatefourparametersofasinewaveinordertominimizetheestimationerrors in the sense of
",False,ieee_sinefit,False,False,False
12204,"/108 /50norm. Simulation results exhibit that
",False,ieee_sinefit,False,False,False
12205,"the proposed method gives superior performance over traditionalones and achieves excellent estimation of the true resolution ofthe simulated ideal ADC. This new algorithm is noniterative andgives swift and consistent results.
",False,ieee_sinefit,False,False,False
12206,"I. INTRODUCTION
",False,ieee_sinefit,False,False,False
12207,"SINE-FIT routines are used extensively during the char-
",False,ieee_sinefit,False,False,False
12208,"acterization of analog-to-digital converters (ADC’s) and
",False,ieee_sinefit,False,False,False
12209,"digitaloscilloscopes[1]–[4].Thesesine-ﬁtalgorithmsestimate
",False,ieee_sinefit,False,False,False
12210,"the four parameters (amplitude, frequency, phase, and offset)
",False,ieee_sinefit,False,False,False
12211,"of the sinewave that best ﬁts a given ﬁnite length recordof discrete samples, which are assumed to be samples of asinewave, possibly corrupted by noise and distortion. These
",False,ieee_sinefit,False,False,False
12212,"approaches are basically a gradient search method which op-
",False,ieee_sinefit,False,False,False
12213,"erates iteratively. Two common problems which this iterativeapproach has are that the convergence is not guaranteed andthat the results from different runs may not be consistent due
",False,ieee_sinefit,False,False,False
12214,"to possible trapping at a local minimum. If one wishes to
",False,ieee_sinefit,False,False,False
12215,"guarantee the convergence by choosing a small step size inthe iteration, experience indicates that it usually takes a longtime to converge [6].
",False,ieee_sinefit,False,False,False
12216,"In order to reduce these difﬁculties, Jenq et al. proposed a
",False,ieee_sinefit,False,False,False
12217,"method which used the weighted least-square and windowingtechnique to estimate sinewave parameters in [5]–[6]. Toimplement their method in a real life situation is, how-
",False,ieee_sinefit,False,False,False
12218,"ever, problematic because it is difﬁcult how to maintain the
",False,ieee_sinefit,False,False,False
12219,"monotonicity of the
",False,ieee_sinefit,False,False,False
12220,"(where
",False,ieee_sinefit,False,False,False
12221,"are sampling data of a sinewave. More details
",False,ieee_sinefit,False,False,False
12222,"aboutthisequationcanbefoundin[5]).Inthesimulatedmode,
",False,ieee_sinefit,False,False,False
12223,"oneknowspreciselythefourparametersoftheinputsinewave;
",False,ieee_sinefit,False,False,False
12224,"while in practice one has only the “digitized sinewave record”
",False,ieee_sinefit,False,False,False
12225,"Manuscript received June 3, 1996. This work was supported by the Chinese
",False,ieee_sinefit,False,False,False
12226,"Natural Science Fund under Grant 59577019.
",False,ieee_sinefit,False,False,False
12227,"The authors are with the Department of Electrical Engineering, Harbin
",False,ieee_sinefit,False,False,False
12228,"Institute of Technology, Harbin, China.
",False,ieee_sinefit,False,False,False
12229,"Publisher Item Identiﬁer S 0018-9456(97)06510-8.and does not know at which quadrant of the sinewave these
",False,ieee_sinefit,False,False,False
12230,"data are located due to and
",False,ieee_sinefit,False,False,False
12231,". Especially, one
",False,ieee_sinefit,False,False,False
12232,"is unaware at which quadrant of the sinwave the originalsampling point
",False,ieee_sinefit,False,False,False
12233,"of the digitized sinewave record is
",False,ieee_sinefit,False,False,False
12234,"located.
",False,ieee_sinefit,False,False,False
12235,"Inviewofthesedifﬁcultiesandproblems,weproposeanew
",False,ieee_sinefit,False,False,False
12236,"method, which is based on the trigonometric identity of sam-pling data of a sinewave function and the total least-squares
",False,ieee_sinefit,False,False,False
12237,"method, to estimate the four parameters of a sinewave. First,
",False,ieee_sinefit,False,False,False
12238,"the algorithm presented in this paper attenuates the difﬁcultiesof traditional nonlinear least-squares sine-ﬁt algorithm aboutfrequency estimation. Next, the proposed algorithm is optimal
",False,ieee_sinefit,False,False,False
12239,"in the sense of
",False,ieee_sinefit,False,False,False
12240,"norm because using total least-squares (TLS)
",False,ieee_sinefit,False,False,False
12241,"are used to estimate four parameters of the sinewave.
",False,ieee_sinefit,False,False,False
12242,"This paper is organized as follows. In Section II, the princi-
",False,ieee_sinefit,False,False,False
12243,"pleoftheTLSisbrieﬂyreviewed.Thederivationofestimation
",False,ieee_sinefit,False,False,False
12244,"algorithmofafourparameterssinewaveisgiveninSection III.
",False,ieee_sinefit,False,False,False
12245,"Simulation results and comparison between proposed andtraditional algorithm are discussed in Section IV. Section Vconcludes the report.
",False,ieee_sinefit,False,False,False
12246,"II. T
",False,ieee_sinefit,False,False,False
12247,"HETOTALLEASTSQUARESMETHOD
",False,ieee_sinefit,False,False,False
12248,"In this section we will brieﬂy review the principle of TLS
",False,ieee_sinefit,False,False,False
12249,"method. Major results will be stated without proofs. Moredetails about this method can be found in [7]–[9].
",False,ieee_sinefit,False,False,False
12250,"Given an overdetermined set of
",False,ieee_sinefit,False,False,False
12251,"linear equation in
",False,ieee_sinefit,False,False,False
12252,"unknowns
",False,ieee_sinefit,False,False,False
12253,"(1)
",False,ieee_sinefit,False,False,False
12254,"A good way to motivate the TLS method is to formulate
",False,ieee_sinefit,False,False,False
12255,"the ordinary least-squares (LS) problem as follows. The LSproblem involves ﬁnding a solution vector
",False,ieee_sinefit,False,False,False
12256,"such that
",False,ieee_sinefit,False,False,False
12257,"minimum (2)
",False,ieee_sinefit,False,False,False
12258,"and
",False,ieee_sinefit,False,False,False
12259,"range
",False,ieee_sinefit,False,False,False
12260,"whereis avector which represents the observation noise
",False,ieee_sinefit,False,False,False
12261,"record; is the norm given by
",False,ieee_sinefit,False,False,False
12262,"(3)
",False,ieee_sinefit,False,False,False
12263,"0018–9456/97$10.00 1997 IEEEZHANG et al.: ALGORITHM BASED ON TOTAL LEAST-SQUARES METHOD 1027
",False,ieee_sinefit,False,False,False
12264,"Thus, the LS problem amounts to perturbing the observation
",False,ieee_sinefit,False,False,False
12265,"by a minimum amount so that can be estimated by
",False,ieee_sinefit,False,False,False
12266,"and under minimum norm.
",False,ieee_sinefit,False,False,False
12267,"Now, simply put, the idea behind TLS is to consider
",False,ieee_sinefit,False,False,False
12268,"perturbation of both and, i.e.,
",False,ieee_sinefit,False,False,False
12269,"(4)
",False,ieee_sinefit,False,False,False
12270,"or (4) can be put into the following form:
",False,ieee_sinefit,False,False,False
12271,"or
",False,ieee_sinefit,False,False,False
12272,"(5)
",False,ieee_sinefit,False,False,False
12273,"where
",False,ieee_sinefit,False,False,False
12274,"The TLS solution to the above homogeneous equation (5)
",False,ieee_sinefit,False,False,False
12275,"can be formulated as seeking a solution vector such that
",False,ieee_sinefit,False,False,False
12276,"minimum (6)
",False,ieee_sinefit,False,False,False
12277,"and
",False,ieee_sinefit,False,False,False
12278,"range
",False,ieee_sinefit,False,False,False
12279,"where denotes the Frobenius norm given by
",False,ieee_sinefit,False,False,False
12280,"(7)
",False,ieee_sinefit,False,False,False
12281,"The TLS method minimizes the noise perturbation of effect
",False,ieee_sinefit,False,False,False
12282,"from both and. Equation (5) shows that the TLS problem
",False,ieee_sinefit,False,False,False
12283,"involves ﬁnding a perturbation matrix having
",False,ieee_sinefit,False,False,False
12284,"the minimum norm so that is rank deﬁcient. The
",False,ieee_sinefit,False,False,False
12285,"singular value decomposition (SVD) can be used for thispurpose. The solution to (5) is obtained from a right singularvector
",False,ieee_sinefit,False,False,False
12286,"corresponding to the smallest singular value of the
",False,ieee_sinefit,False,False,False
12287,"concatenated matrix . The TLS estimate is [7]–[9]
",False,ieee_sinefit,False,False,False
12288,"...
",False,ieee_sinefit,False,False,False
12289,"provided (8)
",False,ieee_sinefit,False,False,False
12290,"where are elements of the th singular vec-
",False,ieee_sinefit,False,False,False
12291,"tor. For methods of picking a total least-squares solution when
",False,ieee_sinefit,False,False,False
12292,"the smallest singular value is repeated or when is
",False,ieee_sinefit,False,False,False
12293,"zero,thereaderisreferredto[8].Since isnoterror-free,
",False,ieee_sinefit,False,False,False
12294,"thetotalleast-squaressolutionispreferabletotheleast-squaressolution.
",False,ieee_sinefit,False,False,False
12295,"In Fig. 1, the LS and the TLS measures of goodness of
",False,ieee_sinefit,False,False,False
12296,"ﬁt are shown for a simple case when
",False,ieee_sinefit,False,False,False
12297,". In the LS
",False,ieee_sinefit,False,False,False
12298,"problem, it is the vertical distances that are important, whereasin the TLS problem, it is the perpendicular distances that are
",False,ieee_sinefit,False,False,False
12299,"critical. So from this geometric interpretation, it follows that
",False,ieee_sinefit,False,False,False
12300,"the TLS method is better than the LS method with respect tothe residual error in the curve ﬁtting.
",False,ieee_sinefit,False,False,False
12301,"Fig. 1. Least-squared versus total squares.
",False,ieee_sinefit,False,False,False
12302,"III. THEESTIMATION ALGORITHM
",False,ieee_sinefit,False,False,False
12303,"OF THESINEWAVE PARAMETERS
",False,ieee_sinefit,False,False,False
12304,"Suppose a sine function is
",False,ieee_sinefit,False,False,False
12305,"sampled at equal interval , then the sampled data can
",False,ieee_sinefit,False,False,False
12306,"be written as
",False,ieee_sinefit,False,False,False
12307,"(9)
",False,ieee_sinefit,False,False,False
12308,"where is the digital frequency corresponding to
",False,ieee_sinefit,False,False,False
12309,"is the additive noise due to the sampling process (which
",False,ieee_sinefit,False,False,False
12310,"includes the quantization noise of ADC’s).
",False,ieee_sinefit,False,False,False
12311,"A. The Sine-Wave Frequency Estimator
",False,ieee_sinefit,False,False,False
12312,"According to (9), we deﬁne
",False,ieee_sinefit,False,False,False
12313,"(10)
",False,ieee_sinefit,False,False,False
12314,"where denotes “deﬁnition.” By means of trigonometric
",False,ieee_sinefit,False,False,False
12315,"identity, we can deduce
",False,ieee_sinefit,False,False,False
12316,"(11)
",False,ieee_sinefit,False,False,False
12317,"When (10) is substituted into (11), we get
",False,ieee_sinefit,False,False,False
12318,"(12)
",False,ieee_sinefit,False,False,False
12319,"and
",False,ieee_sinefit,False,False,False
12320,"(13)
",False,ieee_sinefit,False,False,False
12321,"Equation (13) minus (12) is
",False,ieee_sinefit,False,False,False
12322,"(14)
",False,ieee_sinefit,False,False,False
12323,"In (14) let respectively, we can formulate
",False,ieee_sinefit,False,False,False
12324,"overdetermined equation as (1) where
",False,ieee_sinefit,False,False,False
12325,"(for transpose)1028 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 46, NO. 4, AUGUST 1997
",False,ieee_sinefit,False,False,False
12326,"Fig. 2. The percentage errors of offset, amplitude, phase, and frequency estimation, of the new algorithm.
",False,ieee_sinefit,False,False,False
12327,"Obviously, when are replaced by
",False,ieee_sinefit,False,False,False
12328,"sampling data
",False,ieee_sinefit,False,False,False
12329,"which from (10), both andare perturbed by noise .
",False,ieee_sinefit,False,False,False
12330,"Thus, the total least-squares method, which is discussed inSection II, can be utilized to obtain the optimal estimation
",False,ieee_sinefit,False,False,False
12331,"of vector in the sense of norm. Once has been
",False,ieee_sinefit,False,False,False
12332,"estimated from (8), the estimation of sinewave frequency
",False,ieee_sinefit,False,False,False
12333,"can be written as1
",False,ieee_sinefit,False,False,False
12334,"(15)
",False,ieee_sinefit,False,False,False
12335,"B. Amplitude, Phase, and Offset Estimator
",False,ieee_sinefit,False,False,False
12336,"When the frequency is replaced by the estimation ,
",False,ieee_sinefit,False,False,False
12337,"and let in (9), we can also formulate the
",False,ieee_sinefit,False,False,False
12338,"1When the sampling rate is higher than Nyquist rate, the estimation result
",False,ieee_sinefit,False,False,False
12339,"of frequency is unique.overdetermined equation as follows:
",False,ieee_sinefit,False,False,False
12340,"............(16)
",False,ieee_sinefit,False,False,False
12341,"Similar to the frequency estimator discussed above, both
",False,ieee_sinefit,False,False,False
12342,"andin (16) are perturbed by noise. Again, the TLS method
",False,ieee_sinefit,False,False,False
12343,"can be used to solve (16). Consequently, we can obtain the
",False,ieee_sinefit,False,False,False
12344,"estimation of vector . Finally, the amplitude, phase, and
",False,ieee_sinefit,False,False,False
12345,"offset estimation can be obtained, respectively, as follows:
",False,ieee_sinefit,False,False,False
12346,"(17)
",False,ieee_sinefit,False,False,False
12347,"whereZHANG et al.: ALGORITHM BASED ON TOTAL LEAST-SQUARES METHOD 1029
",False,ieee_sinefit,False,False,False
12348,"IV. COMPUTER SIMULATION
",False,ieee_sinefit,False,False,False
12349,"Consider an example whose digitizer output is
",False,ieee_sinefit,False,False,False
12350,"(18)
",False,ieee_sinefit,False,False,False
12351,"where
",False,ieee_sinefit,False,False,False
12352,"simulated digitizer input;
",False,ieee_sinefit,False,False,False
12353,"simulated harmonic distor-
",False,ieee_sinefit,False,False,False
12354,"tion;
",False,ieee_sinefit,False,False,False
12355,"simulated offset;
",False,ieee_sinefit,False,False,False
12356,"simulated normal random noise;
",False,ieee_sinefit,False,False,False
12357,"simulated quantization error.
",False,ieee_sinefit,False,False,False
12358,"A. Sinewave Fit
",False,ieee_sinefit,False,False,False
12359,"The digitizer has eight bits. The sinewave frequency is
",False,ieee_sinefit,False,False,False
12360,"nearly one-third of the sampling rate of the digitizer. The totaldata points used are 100. The operation of quantizing was to
",False,ieee_sinefit,False,False,False
12361,"roundthesumof
",False,ieee_sinefit,False,False,False
12362,"and tothenearestinteger.
",False,ieee_sinefit,False,False,False
12363,"In simulation, the four situations are considered as follows:
",False,ieee_sinefit,False,False,False
12364,"1) . Fifty esti-
",False,ieee_sinefit,False,False,False
12365,"mations are performed with of the input sinewave
",False,ieee_sinefit,False,False,False
12366,"uniformly distributed on, , then the curves of
",False,ieee_sinefit,False,False,False
12367,"percentage errors of four parameters estimation of
",False,ieee_sinefit,False,False,False
12368,"andare shown in Fig. 2. and the percentage error is
",False,ieee_sinefit,False,False,False
12369,"deﬁned as
",False,ieee_sinefit,False,False,False
12370,"percentage error % (19)
",False,ieee_sinefit,False,False,False
12371,"whereis the no error parameter, is the estimation of
",False,ieee_sinefit,False,False,False
12372,".inFig. 2denotescurvesofthepercentageerrors
",False,ieee_sinefit,False,False,False
12373,"of phase, offset, amplitude and frequency respectivelyin this situation.
",False,ieee_sinefit,False,False,False
12374,"2)
",False,ieee_sinefit,False,False,False
12375,",thatis,thereisharmonic
",False,ieee_sinefit,False,False,False
12376,"distortion in the digitizer data records. in Fig. 2
",False,ieee_sinefit,False,False,False
12377,"denotes the percentage errors curves of phase, offset,amplitude, and frequency, respectively, in this situation.
",False,ieee_sinefit,False,False,False
12378,"3)
",False,ieee_sinefit,False,False,False
12379,", that is, digitizer data
",False,ieee_sinefit,False,False,False
12380,"records are perturbed by random noise of the
",False,ieee_sinefit,False,False,False
12381,"Gaussian distribution with mean 0 and variance 0.01,which are produced by means of Monte Carlo method.
",False,ieee_sinefit,False,False,False
12382,"In this situation the percentage errors curves of phase,
",False,ieee_sinefit,False,False,False
12383,"offset, amplitude, and frequency, which are denoted as
",False,ieee_sinefit,False,False,False
12384,", respectively, are shown in Fig. 2.
",False,ieee_sinefit,False,False,False
12385,"4) In (18) all the interference has been considered. In
",False,ieee_sinefit,False,False,False
12386,"this situation, denotes the percentage errors of
",False,ieee_sinefit,False,False,False
12387,"phase, offset, amplitude, and frequency estimation, re-spectively, in Fig. 2.
",False,ieee_sinefit,False,False,False
12388,"B. Comparison Between TLS and Traditional Methods [1]
",False,ieee_sinefit,False,False,False
12389,"Making use of the same digitizing data record, we compare
",False,ieee_sinefit,False,False,False
12390,"the performance between algorithm proposed in this paperand the traditional one in [1]. The speciﬁed comparisonenvironment is
",False,ieee_sinefit,False,False,False
12391,"in (18); in addition, the traditional algorithm
",False,ieee_sinefit,False,False,False
12392,"hasarequirementforareasonablyaccurateinitialguessforthesoughtparameters.ThecomparisonresultshavebeenshowninTABLE I
",False,ieee_sinefit,False,False,False
12393,"COMPARATIVE RESULTSBETWEEN THE TLS M ETHOD AND TRADITIONAL METHOD
",False,ieee_sinefit,False,False,False
12394,"TABLE II
",False,ieee_sinefit,False,False,False
12395,"SIMULATED AND IDEALEFFECTIVE BITS
",False,ieee_sinefit,False,False,False
12396,"Table I. From Table I one can ﬁnd that the estimation errors of
",False,ieee_sinefit,False,False,False
12397,"the frequency, phase and amplitude of the algorithm describedin this paper are at least 10
",False,ieee_sinefit,False,False,False
12398,"less than those of the algorithm
",False,ieee_sinefit,False,False,False
12399,"in [1], whereas the estimation errors of the offset is a little
",False,ieee_sinefit,False,False,False
12400,"better than one of the algorithm in [1]. Furthermore, because
",False,ieee_sinefit,False,False,False
12401,"this new algorithm is noniterative, it gives consistent resultsfor various runs. It is also swift: for a 203-point data record, itonly takes a couple of seconds to estimate the four parameters
",False,ieee_sinefit,False,False,False
12402,"of a sinewave, whereas the traditional one [1] needs to take
",False,ieee_sinefit,False,False,False
12403,"1.5 h to obtain the results in Table I.
",False,ieee_sinefit,False,False,False
12404,"C. ADC Effective Bits Measurement
",False,ieee_sinefit,False,False,False
12405,"Acommonlyusedformulaforan
",False,ieee_sinefit,False,False,False
12406,"-bitADCisgivenby[2]
",False,ieee_sinefit,False,False,False
12407,"(20)
",False,ieee_sinefit,False,False,False
12408,"If one assumes that the quantization noise is uniformly dis-
",False,ieee_sinefit,False,False,False
12409,"tribution and that the quantization errors from sample tosample are statistically independent, then,
",False,ieee_sinefit,False,False,False
12410,",
",False,ieee_sinefit,False,False,False
12411,"where is the ideal code bin width. We use the sinewave
",False,ieee_sinefit,False,False,False
12412,"ﬁt algorithm described in this paper to estimate the four
",False,ieee_sinefit,False,False,False
12413,"parameters of the input sinewave, and then use the estimated
",False,ieee_sinefit,False,False,False
12414,"sinewave as if it were the actual sinewave to compute theeffective bits of the ADC by using (20). The critical questionis “How good is it?” A simulation to implement this proposal
",False,ieee_sinefit,False,False,False
12415,"has been done. The results are shown in Table II. It is seen
",False,ieee_sinefit,False,False,False
12416,"that this approach gives excellent results with resolution notless than three bits.1030 IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT, VOL. 46, NO. 4, AUGUST 1997
",False,ieee_sinefit,False,False,False
12417,"V. CONCLUSION
",False,ieee_sinefit,False,False,False
12418,"In this paper, we have presented a high-performance
",False,ieee_sinefit,False,False,False
12419,"sinewave parameters estimation algorithm, which gives highly
",False,ieee_sinefit,False,False,False
12420,"accurate and speedy estimations. The total least-squaresmethod is utilized to estimate the four parameters of asinewave in order to make the estimation errors be minimized
",False,ieee_sinefit,False,False,False
12421,"in the sense of
",False,ieee_sinefit,False,False,False
12422,"norm. Simulation results exhibit that the
",False,ieee_sinefit,False,False,False
12423,"proposed method gives superior performance over traditionalones and produces excellent estimation of the true resolutionof the simulated ideal ADC.
",False,ieee_sinefit,False,False,False
12424,"R
",False,ieee_sinefit,False,False,False
12425,"EFERENCES
",False,ieee_sinefit,False,False,False
12426,"[1] “IEEE trial-use standard for digitizing wave-form recorders,” IEEE Std.
",False,ieee_sinefit,False,False,False
12427,"1057, July 1989.
",False,ieee_sinefit,False,False,False
12428,"[2] B. E. Peetz, “Dynamic testing of wave-form recorder,” IEEE Trans.
",False,ieee_sinefit,False,False,False
12429,"Instrum. Meas. , vol. IM-32, pp. 12–17, Mar. 1983.
",False,ieee_sinefit,False,False,False
12430,"[3] J. Kuffel, T. R. McComb, and R. Malewski, “Comparative evaluation
",False,ieee_sinefit,False,False,False
12431,"of computer methods for calculating the best-ﬁt sinusoid to the digital
",False,ieee_sinefit,False,False,False
12432,"record of a high-purity sine wave,” IEEE Trans. Instrum. Meas. , vol.
",False,ieee_sinefit,False,False,False
12433,"IM-36, pp. 418–422, June 1987.
",False,ieee_sinefit,False,False,False
12434,"[4] T. R. Mccomb, J. Kuffel,and B. C. Le Roux, “Acomparative evaluation
",False,ieee_sinefit,False,False,False
12435,"of some practical algorithm used in the effective bits test of wave-form
",False,ieee_sinefit,False,False,False
12436,"record,”IEEE Trans. Instrum. Meas. , vol. 38, pp. 37–42, Feb. 1989.
",False,ieee_sinefit,False,False,False
12437,"[5] Y. C. Jenq, “High-precision sinusoidal frequency estimator based on
",False,ieee_sinefit,False,False,False
12438,"weightedleast-square method,” IEEETrans.Instrum.Meas. , vol.IM-36,
",False,ieee_sinefit,False,False,False
12439,"pp. 124–127, Mar. 1987.
",False,ieee_sinefit,False,False,False
12440,"[6] Y. C. Jenq and P. B. Crosby, “Sinewave parameter estimation algorithm
",False,ieee_sinefit,False,False,False
12441,"with application to wave-form digitizer effective bits measurement,”
",False,ieee_sinefit,False,False,False
12442,"IEEE Trans. Instrum. Meas. , vol. 37, pp. 529–532, Dec. 1988.
",False,ieee_sinefit,False,False,False
12443,"[7] G. H. Golub and C. F. Van Loan, “An analysis of the total least-squares
",False,ieee_sinefit,False,False,False
12444,"problem,” SIAM J. Numer. Anal. , vol. 17, pp. 883–893, Dec. 1980.
",False,ieee_sinefit,False,False,False
12445,"[8] S. Van Huffel and J. Vandewalle, “Analysis and solution of the non-
",False,ieee_sinefit,False,False,False
12446,"generic total least-squares problem,” SIAM J. Numer. Anal. , vol. 9, pp.
",False,ieee_sinefit,False,False,False
12447,"360–372, July 1988.
",False,ieee_sinefit,False,False,False
12448,"[9] G. H. Golub and C. F. Van Loan, Matrix Computation , Baltimore, MD:
",False,ieee_sinefit,False,False,False
12449,"Johns Hopins Univ. Press, 1989.Jian Qiu Zhang for a photograph and biography, see this issue, p. 940.
",False,ieee_sinefit,False,False,False
12450,"Zhao Xinmin for a photograph and biography, see this issue, p. 940.
",False,ieee_sinefit,False,False,False
12451,"Hu Xiao was born in Sichuan, China, in 1972. She received the B.S.
",False,ieee_sinefit,False,False,False
12452,"degree in electrical engineering from the Huazhong University of Science
",False,ieee_sinefit,False,False,False
12453,"and Technology, Wuhan, China, in 1994 and the M.S. degree in electrical
",False,ieee_sinefit,False,False,False
12454,"engineering from the Harbin Institute of Technology, Harbin, China, in 1996.
",False,ieee_sinefit,False,False,False
12455,"She is now an Engineer at the Shenzhen Huawei Tech. Co., Ltd. Her
",False,ieee_sinefit,False,False,False
12456,"research interests are communication testing systems and digital signal pro-
",False,ieee_sinefit,False,False,False
12457,"cessing and its application in measurement.
",False,ieee_sinefit,False,False,False
12458,"Sun Jinwei was born was born in Harbin, China,
",False,ieee_sinefit,False,False,False
12459,"in 1964. He received the B.S. degree and the M.S.
",False,ieee_sinefit,False,False,False
12460,"degrees from the Harbin Institute of Technology(HIT) in electrical engineering, in 1987 and 1990,
",False,ieee_sinefit,False,False,False
12461,"respectively.
",False,ieee_sinefit,False,False,False
12462,"Currently, he is a Lecturer in the Department
",False,ieee_sinefit,False,False,False
12463,"of Electrical Engineering, HIT. He is interested in
",False,ieee_sinefit,False,False,False
12464,"sensor information fusion, ADC testing, DSP, and
",False,ieee_sinefit,False,False,False
12465,"artiﬁcial intelligence in electrical instrumentation.
",False,ieee_sinefit,False,False,False
12466,View publication stats,False,ieee_sinefit,False,False,False
12467,"Abstract: For an organization to be customer centric and service oriented requires that it use each
",True,ijerph-18-05686-v2,False,False,True
12468,"1. Introduction
",True,ijerph-18-05686-v2,False,False,True
12469,"2.1. Self-Health Management Services
",True,ijerph-18-05686-v2,False,False,True
12470,"2.2. Civic Engagement Services
",True,ijerph-18-05686-v2,False,False,True
12471,"2.3. Caregiver Engagement Services
",True,ijerph-18-05686-v2,False,False,True
12472,"2.4. Community Engagement Services
",True,ijerph-18-05686-v2,False,False,True
12473,"3.1. Increase Value Cycle Frequency
",True,ijerph-18-05686-v2,False,False,True
12474,"3.4. Building an Agile Digital Platform
",True,ijerph-18-05686-v2,False,False,True
12475,"3.4. Building an Agile Digital Platform 
",True,ijerph-18-05686-v2,False,False,True
12476,"4.1. Clients
",True,ijerph-18-05686-v2,False,False,True
12477,"4.2. Technologies
",True,ijerph-18-05686-v2,False,False,True
12478,"4.3. Value Creating Services
",True,ijerph-18-05686-v2,False,False,True
12479,"4.4. Value Fulﬁllment Service Modules
",True,ijerph-18-05686-v2,False,False,True
12480,"5. Conclusions  
",True,ijerph-18-05686-v2,False,False,True
12481,"5. Conclusions
",True,ijerph-18-05686-v2,False,False,True
12482,"6. Directions for Future Research
",True,ijerph-18-05686-v2,False,False,True
12483,"International  Journal  of 
",False,ijerph-18-05686-v2,False,False,True
12484,"Environmental Research
",False,ijerph-18-05686-v2,False,False,True
12485,"and Public Health
",False,ijerph-18-05686-v2,False,False,True
12486,"Article
",False,ijerph-18-05686-v2,False,False,True
12487,"An Agile Digital Platform to Support Population
",False,ijerph-18-05686-v2,False,False,True
12488,"Health—A Case Study of a Digital Platform to Support Patients
",False,ijerph-18-05686-v2,False,False,True
12489,"with Delirium Using IoT, NLP , and AI
",False,ijerph-18-05686-v2,False,False,True
12490,"Mohan R. Tanniru1,*, Nimit Agarwal2, Amanda Sokan1and Salim Hariri3
",False,ijerph-18-05686-v2,False,False,True
12491,"/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001
",False,ijerph-18-05686-v2,False,False,True
12492,"/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046
",False,ijerph-18-05686-v2,False,False,True
12493,"Citation: Tanniru, M.R.; Agarwal, N.;
",False,ijerph-18-05686-v2,False,False,True
12494,"Sokan, A.; Hariri, S. An Agile Digital
",False,ijerph-18-05686-v2,False,False,True
12495,"Platform to Support Population
",False,ijerph-18-05686-v2,False,False,True
12496,"Health—A Case Study of a Digital
",False,ijerph-18-05686-v2,False,False,True
12497,"Platform to Support Patients with
",False,ijerph-18-05686-v2,False,False,True
12498,"Delirium Using IoT, NLP , and AI. Int.
",False,ijerph-18-05686-v2,False,False,True
12499,"J. Environ. Res. Public Health 2021 ,18,
",False,ijerph-18-05686-v2,False,False,True
12500,"5686. https://doi.org/10.3390/
",False,ijerph-18-05686-v2,False,False,True
12501,"ijerph18115686
",False,ijerph-18-05686-v2,False,False,True
12502,"Academic Editor: Paul B. Tchounwou
",False,ijerph-18-05686-v2,False,False,True
12503,"Received: 7 April 2021
",False,ijerph-18-05686-v2,False,False,True
12504,"Accepted: 21 May 2021
",False,ijerph-18-05686-v2,False,False,True
12505,"Published: 26 May 2021
",False,ijerph-18-05686-v2,False,False,True
12506,"Publisher’s Note: MDPI stays neutral
",False,ijerph-18-05686-v2,False,False,True
12507,"with regard to jurisdictional claims in
",False,ijerph-18-05686-v2,False,False,True
12508,"published maps and institutional afﬁl-
",False,ijerph-18-05686-v2,False,False,True
12509,"iations.
",False,ijerph-18-05686-v2,False,False,True
12510,"Copyright: © 2021 by the authors.
",False,ijerph-18-05686-v2,False,False,True
12511,"Licensee MDPI, Basel, Switzerland.
",False,ijerph-18-05686-v2,False,False,True
12512,"This article is an open access article
",False,ijerph-18-05686-v2,False,False,True
12513,"distributed under the terms and
",False,ijerph-18-05686-v2,False,False,True
12514,"conditions of the Creative Commons
",False,ijerph-18-05686-v2,False,False,True
12515,"Attribution (CC BY) license (https://
",False,ijerph-18-05686-v2,False,False,True
12516,"creativecommons.org/licenses/by/
",False,ijerph-18-05686-v2,False,False,True
12517,"4.0/).1Mel and Enid Zuckerman College of Public Health, University of Arizona, Phoenix, AZ 85004, USA;
",False,ijerph-18-05686-v2,False,False,True
12518,"aesokan@arizona.edu
",False,ijerph-18-05686-v2,False,False,True
12519,"2Department of Internal Medicine, Banner University Medical Center, University of Arizona,
",False,ijerph-18-05686-v2,False,False,True
12520,"Phoenix, AZ 85004, USA; NimitAgarwal@arizona.edu or Nimit.Agarwal@bannerhealth.com
",False,ijerph-18-05686-v2,False,False,True
12521,"3College of Electrical and Computer Engineering, University of Arizona, Tucson, AZ 85721, USA;
",False,ijerph-18-05686-v2,False,False,True
12522,"hariri@arizona.edu
",False,ijerph-18-05686-v2,False,False,True
12523,"*Correspondence: mtanniru@arizona.edu
",False,ijerph-18-05686-v2,False,False,True
12524,"encounter with a customer to create value, leverage advanced technologies to design digital services
",False,ijerph-18-05686-v2,False,False,True
12525,"to fulﬁll the value, and assess perceived value-in-use to continue to revise the value as customer
",False,ijerph-18-05686-v2,False,False,True
12526,"expectations evolve. The adaptation of value cycles to address the rapid changes in customer
",False,ijerph-18-05686-v2,False,False,True
12527,"expectations requires agile digital platforms with dynamic software ecosystems interacting with
",False,ijerph-18-05686-v2,False,False,True
12528,"multiple actors. For public health agencies focused on population health, these agile digital platforms
",False,ijerph-18-05686-v2,False,False,True
12529,"should provide tailored care to address the distinct needs of select population groups. Using prior
",False,ijerph-18-05686-v2,False,False,True
12530,"research on aging and dynamic software ecosystems, this paper develops a template for the design
",False,ijerph-18-05686-v2,False,False,True
12531,"of an agile digital platform to support value cycle activities among clinical and non-clinical actors,
",False,ijerph-18-05686-v2,False,False,True
12532,"including population groups. It illustrates the design of an agile digital platform to support clients
",False,ijerph-18-05686-v2,False,False,True
12533,"that suffer from delirium, using digital services that leverage Internet of Things, natural language
",False,ijerph-18-05686-v2,False,False,True
12534,"processing, and AI that uses real-time data for learning and care adaption. We conclude the paper
",False,ijerph-18-05686-v2,False,False,True
12535,"with directions for future research.
",False,ijerph-18-05686-v2,False,False,True
12536,"Keywords: population health; value cycles; digital health; dynamic software ecosystems; delirium
",False,ijerph-18-05686-v2,False,False,True
12537,"patient care; agile digital platform
",False,ijerph-18-05686-v2,False,False,True
12538,"1. Introduction
",False,ijerph-18-05686-v2,False,False,True
12539,"Businesses have recognized the importance of understanding the customer journey
",False,ijerph-18-05686-v2,False,False,True
12540,"and reacting quickly to address customers’ changing expectations in an evolving technology
",False,ijerph-18-05686-v2,False,False,True
12541,"landscape [ 1]. Such a customer-centric and service orientation means businesses have to
",False,ijerph-18-05686-v2,False,False,True
12542,"use each encounter with a customer or service exchange to create value propositions and
",False,ijerph-18-05686-v2,False,False,True
12543,"assess their perceived value-in-use or value-in-context to revise these value propositions
",False,ijerph-18-05686-v2,False,False,True
12544,"as the customer expectations change [ 2]. By using such a value lens to create, fulﬁll, and
",False,ijerph-18-05686-v2,False,False,True
12545,"assess value-in-use, often using digital services (i.e., services that create value supported
",False,ijerph-18-05686-v2,False,False,True
12546,"by advanced technologies), businesses can create differentiated value in a competitive
",False,ijerph-18-05686-v2,False,False,True
12547,"marketplace. More importantly, such a market dynamic calls on businesses to use agile
",False,ijerph-18-05686-v2,False,False,True
12548,"organizational models to innovate and explore new value propositions [ 3]. Depending on
",False,ijerph-18-05686-v2,False,False,True
12549,"the customer value cycle (time between two purchases of the same product/service) and
",False,ijerph-18-05686-v2,False,False,True
12550,"product life cycle (time to create a new product/service to fulﬁll value), this also means
",False,ijerph-18-05686-v2,False,False,True
12551,"that businesses may need to use multiple value cycles to keep customers engaged and
",False,ijerph-18-05686-v2,False,False,True
12552,"empowered [4].
",False,ijerph-18-05686-v2,False,False,True
12553,"This market dynamic is blurring the difference between business and IT strategies. In
",False,ijerph-18-05686-v2,False,False,True
12554,"fact, it is interweaving or fusing these two strategies under the framework of the digital
",False,ijerph-18-05686-v2,False,False,True
12555,"business strategy [ 5], depending on where value is created (e.g., productivity within the
",False,ijerph-18-05686-v2,False,False,True
12556,"Int. J. Environ. Res. Public Health 2021 ,18, 5686. https://doi.org/10.3390/ijerph18115686 https://www.mdpi.com/journal/ijerphInt. J. Environ. Res. Public Health 2021 ,18, 5686 2 of 22
",False,ijerph-18-05686-v2,False,False,True
12557,"business or value creation to support customer needs). When value creation is driven by
",False,ijerph-18-05686-v2,False,False,True
12558,"the changes in customers’ needs, the digital business strategy has to develop innovative
",False,ijerph-18-05686-v2,False,False,True
12559,"digital services that create, fulﬁll, and assess value-in-use. Analyzing feedback to improve
",False,ijerph-18-05686-v2,False,False,True
12560,"clinical diagnosis (i.e., clinical informatics), business performance (i.e., business analytics),
",False,ijerph-18-05686-v2,False,False,True
12561,"or system performance (i.e., analytics in general) is necessary when there are changes in the
",False,ijerph-18-05686-v2,False,False,True
12562,"inputs, or models used to process these inputs. When such changes are driven by evolving
",False,ijerph-18-05686-v2,False,False,True
12563,"needs of the external consumer, feedback to improve value created by an organization
",False,ijerph-18-05686-v2,False,False,True
12564,"comes from a consumer’s perception of such value (value-in-use).
",False,ijerph-18-05686-v2,False,False,True
12565,"Independent of where the feedback comes from, a software ecosystem designed to
",False,ijerph-18-05686-v2,False,False,True
12566,"improve performance has to be agile to support the faster design of these new digital
",False,ijerph-18-05686-v2,False,False,True
12567,"services, or the reconﬁguration and/or redesign of the current digital services to meet
",False,ijerph-18-05686-v2,False,False,True
12568,"evolving customer expectations. Just as businesses are leveraging multiple partners to
",False,ijerph-18-05686-v2,False,False,True
12569,"fulﬁll value, today’s software ecosystems also have to leverage the expertise and resources
",False,ijerph-18-05686-v2,False,False,True
12570,"of multiple software developers and technology partners as they design digital services to
",False,ijerph-18-05686-v2,False,False,True
12571,"create and fulﬁll value [ 6]. In fact, the digital platforms used to support the communication
",False,ijerph-18-05686-v2,False,False,True
12572,"of information and the coordination of value cycle activities across business and customer
",False,ijerph-18-05686-v2,False,False,True
12573,"ecosystems need an agile software ecosystem that leverages a mix of technologies and
",False,ijerph-18-05686-v2,False,False,True
12574,"application environments (e.g., mobile apps, web browsers, IoT sensors, cloud base services,
",False,ijerph-18-05686-v2,False,False,True
12575,"etc.) to create and sustain value [7].
",False,ijerph-18-05686-v2,False,False,True
12576,"Designing an agile digital platform to address customer needs is an even bigger
",False,ijerph-18-05686-v2,False,False,True
12577,"challenge for healthcare organizations such as hospitals. In a business ecosystem, value
",False,ijerph-18-05686-v2,False,False,True
12578,"cycles that leverage external partners are often under the control and coordination of the
",False,ijerph-18-05686-v2,False,False,True
12579,"business. The business can therefore direct value creation and value fulﬁllment, and it has
",False,ijerph-18-05686-v2,False,False,True
12580,"some inﬂuence on gathering feedback on value-in-use. However, health systems, even
",False,ijerph-18-05686-v2,False,False,True
12581,"if they are inherently service driven, must operate two different value cycles. The ﬁrst
",False,ijerph-18-05686-v2,False,False,True
12582,"value cycle is within a provider ecosystem (e.g., hospital) where the initial value is created
",False,ijerph-18-05686-v2,False,False,True
12583,"(e.g., a health condition diagnosis) and some value is fulﬁlled (e.g., testing of patients and
",False,ijerph-18-05686-v2,False,False,True
12584,"treatment plan design when they are in-patients). The second and often much longer value
",False,ijerph-18-05686-v2,False,False,True
12585,"cycle is within the patient ecosystem (e.g., a patient’s home or at a temporary facility such as
",False,ijerph-18-05686-v2,False,False,True
12586,"a skilled nursing or rehab facility). The value fulﬁllment, value-in-use tracking for feedback,
",False,ijerph-18-05686-v2,False,False,True
12587,"and even new value creation (e.g., changes to treatment plans) are supported by multiple
",False,ijerph-18-05686-v2,False,False,True
12588,"external clinical and non-clinical partners, including patients and health care providers.
",False,ijerph-18-05686-v2,False,False,True
12589,"While digital platforms used to support the ﬁrst value cycle can be designed for agility with
",False,ijerph-18-05686-v2,False,False,True
12590,"healthcare provider oversight, even when they use several external software partners [ 8],
",False,ijerph-18-05686-v2,False,False,True
12591,"building an agile digital platform for the second value cycle is much more complex.
",False,ijerph-18-05686-v2,False,False,True
12592,"With increased access to technologies such as wearables and mobile apps to monitor
",False,ijerph-18-05686-v2,False,False,True
12593,"health conditions and track physical activities, and with the growing use of telehealth,
",False,ijerph-18-05686-v2,False,False,True
12594,"portals and social media tools to seek and share information, the patient demand for digital
",False,ijerph-18-05686-v2,False,False,True
12595,"services to create and fulﬁll value in healthcare is growing. At the same time, healthcare
",False,ijerph-18-05686-v2,False,False,True
12596,"providers are looking to use patient interest in self-managing their care at home to shift
",False,ijerph-18-05686-v2,False,False,True
12597,"a signiﬁcant part of the value fulﬁllment and successive value cycles into the patient
",False,ijerph-18-05686-v2,False,False,True
12598,"ecosystem to reduce readmission costs and improve patient satisfaction. For public health
",False,ijerph-18-05686-v2,False,False,True
12599,"agencies with a focus on prevention, most of the value cycle activities to tailor practices to
",False,ijerph-18-05686-v2,False,False,True
12600,"create value, fulﬁll value, and assess value-in-use have to be done within the population
",False,ijerph-18-05686-v2,False,False,True
12601,"or patient ecosystem using a number of external actors, both clinical and non-clinical.
",False,ijerph-18-05686-v2,False,False,True
12602,"Hence, the following research question: How can we build an agile digital platform to
",False,ijerph-18-05686-v2,False,False,True
12603,"support health care delivery in a client ecosystem? The client here refers to both the patients
",False,ijerph-18-05686-v2,False,False,True
12604,"seeking care transition services and the population groups looking to adhere to preventive
",False,ijerph-18-05686-v2,False,False,True
12605,"healthcare practices.
",False,ijerph-18-05686-v2,False,False,True
12606,"We will try to answer this question in the paper using the following three steps:
",False,ijerph-18-05686-v2,False,False,True
12607,"(1) design innovative care delivery models that create value for a speciﬁc client population;
",False,ijerph-18-05686-v2,False,False,True
12608,"(2) develop an agile digital platform that uses conﬁgurable digital services to communicate
",False,ijerph-18-05686-v2,False,False,True
12609,"information and coordinate activities in support of value cycle activities; and (3) illustrate
",False,ijerph-18-05686-v2,False,False,True
12610,"how such an agile digital platform using artiﬁcial intelligence (AI) and Internet of ThingsInt. J. Environ. Res. Public Health 2021 ,18, 5686 3 of 22
",False,ijerph-18-05686-v2,False,False,True
12611,"(IoT) technologies can be developed to support a particular client group, being patients
",False,ijerph-18-05686-v2,False,False,True
12612,"who suffer with delirium conditions in this case. The paper is organized as follows to
",False,ijerph-18-05686-v2,False,False,True
12613,"discuss each of these three steps:
",False,ijerph-18-05686-v2,False,False,True
12614,"Section 2 focuses on the ﬁrst step. With a growing proportion of an aging population
",False,ijerph-18-05686-v2,False,False,True
12615,"needing to address multiple chronic care conditions, and with healthcare systems looking to
",False,ijerph-18-05686-v2,False,False,True
12616,"design care delivery models to reduce costs by shifting chronic care management to clients’
",False,ijerph-18-05686-v2,False,False,True
12617,"homes using technologies, the challenge becomes one of tailoring care to such a population
",False,ijerph-18-05686-v2,False,False,True
12618,"effectively. The individuals in this age bracket have varying social, technological, and
",False,ijerph-18-05686-v2,False,False,True
12619,"educational capabilities to self-manage their care at home, and they need a mix of clinical
",False,ijerph-18-05686-v2,False,False,True
12620,"and non-clinical care providers to support their care. Using research on aging, this section
",False,ijerph-18-05686-v2,False,False,True
12621,"develops a set of services that can be used to create value that is tailored to the speciﬁc
",False,ijerph-18-05686-v2,False,False,True
12622,"client’s health condition and ecosystem needs;
",False,ijerph-18-05686-v2,False,False,True
12623,"Section 3 focuses on the second step. For the services identiﬁed in Section 2, we will
",False,ijerph-18-05686-v2,False,False,True
12624,"discuss the strategies used to design a digital platform to support value cycle activities.
",False,ijerph-18-05686-v2,False,False,True
12625,"With technologies helping clients monitor their health conditions and communicate with
",False,ijerph-18-05686-v2,False,False,True
12626,"healthcare providers, connecting the clients with various clinical actors (labs, pharmacies,
",False,ijerph-18-05686-v2,False,False,True
12627,"etc.) for treatment adherence, and non-clinical actors for social and emotional support,
",False,ijerph-18-05686-v2,False,False,True
12628,"the digital platform must work with conﬁgurable digital services that evolve as the client
",False,ijerph-18-05686-v2,False,False,True
12629,"needs and actors supporting these needs evolve. Using research on agile digital platforms,
",False,ijerph-18-05686-v2,False,False,True
12630,"this section develops a template for the design of an agile digital platform to support value
",False,ijerph-18-05686-v2,False,False,True
12631,"cycle activities;
",False,ijerph-18-05686-v2,False,False,True
12632,"Section 4 focuses on the third step. It illustrates the design of an agile digital platform
",False,ijerph-18-05686-v2,False,False,True
12633,"to support clients that suffer from delirium. With the growing use of the IoT to monitor a
",False,ijerph-18-05686-v2,False,False,True
12634,"client’s health condition in real time, and with the availability of AI technologies to analyze
",False,ijerph-18-05686-v2,False,False,True
12635,"a large volume of real-time data for learning and adaption [ 9], the digital platform will
",False,ijerph-18-05686-v2,False,False,True
12636,"address the client needs in a hospital. While its use is discussed within a provider context,
",False,ijerph-18-05686-v2,False,False,True
12637,"it can be used in a home care environment as well with adaptations;
",False,ijerph-18-05686-v2,False,False,True
12638,"Section 5 provides some concluding comments and Section 6 provides some directions
",False,ijerph-18-05686-v2,False,False,True
12639,"for future research.
",False,ijerph-18-05686-v2,False,False,True
12640,"2. Research on Aging and Services to Create Value
",False,ijerph-18-05686-v2,False,False,True
12641,"The term “aging” generally refers to our understanding of how we as human beings
",False,ijerph-18-05686-v2,False,False,True
12642,"progressively deteriorate in our health during our adult period of life [ 10]. When measured
",False,ijerph-18-05686-v2,False,False,True
12643,"on the life calendar, it often becomes a discrete number, and societies worldwide use such a
",False,ijerph-18-05686-v2,False,False,True
12644,"number to make assumptions about people’s ability to contribute to economic productivity
",False,ijerph-18-05686-v2,False,False,True
12645,"(e.g., setting an age for retirement), determine eligibility for government assistance (e.g.,
",False,ijerph-18-05686-v2,False,False,True
12646,"health insurance), and use statistical models to classify disease conditions such as chronic
",False,ijerph-18-05686-v2,False,False,True
12647,"care conditions (e.g., how a certain percentage of the older population contributes to
",False,ijerph-18-05686-v2,False,False,True
12648,"healthcare costs). Even health systems design care delivery models to provide care at home,
",False,ijerph-18-05686-v2,False,False,True
12649,"with the assumption that the digital services needed to deliver care have to support a
",False,ijerph-18-05686-v2,False,False,True
12650,"population with lower levels of visual and mobility capabilities, technological literacy, and
",False,ijerph-18-05686-v2,False,False,True
12651,"healthcare knowledge. This often induces bias in the way the systems are designed and
",False,ijerph-18-05686-v2,False,False,True
12652,"services are identiﬁed to create value. Therefore, stereotyping all adults over a certain age
",False,ijerph-18-05686-v2,False,False,True
12653,"as a single monolithic group for designing health services to deliver care at home leads to
",False,ijerph-18-05686-v2,False,False,True
12654,"missed opportunities [11].
",False,ijerph-18-05686-v2,False,False,True
12655,"Aging is a process, and innovations in medical science for clinical diagnosis and drug-
",False,ijerph-18-05686-v2,False,False,True
12656,"based therapies have been extending human life, thus transforming the older population
",False,ijerph-18-05686-v2,False,False,True
12657,"into a mix of sub-population groups with varying capabilities to manage their health con-
",False,ijerph-18-05686-v2,False,False,True
12658,"ditions. The concept of active aging, deﬁned by the World Health Organization [ 12], uses
",False,ijerph-18-05686-v2,False,False,True
12659,"eight classes of determinants to gather information for the possible clustering of services
",False,ijerph-18-05686-v2,False,False,True
12660,"for such populations. These include the physical environment, access to health and social
",False,ijerph-18-05686-v2,False,False,True
12661,"services, individual behaviors (healthy behaviors, addictions, medications), personal char-
",False,ijerph-18-05686-v2,False,False,True
12662,"acteristics (biology, genetics, psychology), support ecosystem (social support, violence and
",False,ijerph-18-05686-v2,False,False,True
12663,"abuse, education and literacy), economic factors (income, social protection, work), gender,Int. J. Environ. Res. Public Health 2021 ,18, 5686 4 of 22
",False,ijerph-18-05686-v2,False,False,True
12664,"and culture. Overlying these determinants are other factors often considered in social
",False,ijerph-18-05686-v2,False,False,True
12665,"diagnosis [ 13], such as the roles people play (within their ecosystem), the relationships they
",False,ijerph-18-05686-v2,False,False,True
12666,"have (with others in their network), their reactions to ecosystem conditions (fear, stigma,
",False,ijerph-18-05686-v2,False,False,True
12667,"stress, etc.), and the resources they have (physical, economic, emotional). All of these can
",False,ijerph-18-05686-v2,False,False,True
12668,"inﬂuence the clients’ abilities to manage their health conditions as some of these factors
",False,ijerph-18-05686-v2,False,False,True
12669,"change. This means that healthcare value delivery is a dynamic process, and the services
",False,ijerph-18-05686-v2,False,False,True
12670,"used to create value must continue to evolve based on the feedback from value-in-use.
",False,ijerph-18-05686-v2,False,False,True
12671,"For the purposes of this discussion, we will use prior research to classify services
",False,ijerph-18-05686-v2,False,False,True
12672,"that create value into the following four types: self-health management, civic engagement,
",False,ijerph-18-05686-v2,False,False,True
12673,"caregiver engagement, and community engagement. While certain older population groups
",False,ijerph-18-05686-v2,False,False,True
12674,"today have limited literacy on health and technical knowledge, more baby boomers are
",False,ijerph-18-05686-v2,False,False,True
12675,"entering this age bracket and are increasingly conversant with and interested in using
",False,ijerph-18-05686-v2,False,False,True
12676,"technology to self-manage their health conditions. With a greater capacity to contribute,
",False,ijerph-18-05686-v2,False,False,True
12677,"these older populations can engage in many civic activities that beneﬁt both the community
",False,ijerph-18-05686-v2,False,False,True
12678,"and the health and wellbeing of the older adults. As health conditions deteriorate, some
",False,ijerph-18-05686-v2,False,False,True
12679,"begin to rely on services provided by professional or informal caregivers, such as family
",False,ijerph-18-05686-v2,False,False,True
12680,"members. Lastly, some older populations continue to face health inequities by virtue of
",False,ijerph-18-05686-v2,False,False,True
12681,"economic, social, and geographical disparities, and need the support of social services to
",False,ijerph-18-05686-v2,False,False,True
12682,"overcome barriers for access to quality care. We will look at each in detail below.
",False,ijerph-18-05686-v2,False,False,True
12683,"2.1. Self-Health Management Services
",False,ijerph-18-05686-v2,False,False,True
12684,"Several digital services to support population engagement at home are designed
",False,ijerph-18-05686-v2,False,False,True
12685,"to allow people to self-manage their health conditions. This includes sharing health
",False,ijerph-18-05686-v2,False,False,True
12686,"information, asking speciﬁc questions, seeking consultation, scheduling appointments, etc.
",False,ijerph-18-05686-v2,False,False,True
12687,"The technologies used to support these services can include patient portals, websites, and
",False,ijerph-18-05686-v2,False,False,True
12688,"digital health infomediaries [ 14]. The recent health emergency (COVID-19), in fact, has
",False,ijerph-18-05686-v2,False,False,True
12689,"made many of these digital services important for care delivery at home (e.g., telehealth
",False,ijerph-18-05686-v2,False,False,True
12690,"technologies). The effectiveness of such digital services depends on how engaged both
",False,ijerph-18-05686-v2,False,False,True
12691,"the patient and health care providers are in frequent communication and follow-ups in
",False,ijerph-18-05686-v2,False,False,True
12692,"the area of mental health [ 15]. Remote monitoring services are used, from tracking blood
",False,ijerph-18-05686-v2,False,False,True
12693,"sugar levels to other complex chronic care [ 16]. Social media technologies, such as digital
",False,ijerph-18-05686-v2,False,False,True
12694,"infomediaries, are used for consultation and seeking answers to medical questions [17].
",False,ijerph-18-05686-v2,False,False,True
12695,"In summary, there are important opportunities to create value for older clients who
",False,ijerph-18-05686-v2,False,False,True
12696,"want to self-manage their health conditions.
",False,ijerph-18-05686-v2,False,False,True
12697,"2.2. Civic Engagement Services
",False,ijerph-18-05686-v2,False,False,True
12698,"Even with medical advances and economic prosperity increasing longevity and well-
",False,ijerph-18-05686-v2,False,False,True
12699,"being, and moving more people into the broad category of the older population, ageism
",False,ijerph-18-05686-v2,False,False,True
12700,"continues to stigmatize this population in the labor market [ 18] and marginalize their role
",False,ijerph-18-05686-v2,False,False,True
12701,"in productive social engagement [ 19]. Active aging strategies include promoting physical
",False,ijerph-18-05686-v2,False,False,True
12702,"activity through volunteering and socializing [ 20]. These strategies can allow cities to
",False,ijerph-18-05686-v2,False,False,True
12703,"use multiple agencies to make public places health friendly [ 21,22] and engage the older
",False,ijerph-18-05686-v2,False,False,True
12704,"population in community-based voluntary activities [ 23]. Such an engagement in social,
",False,ijerph-18-05686-v2,False,False,True
12705,"economic, cultural, spiritual, and civic affairs can help reduce isolation and keep the older
",False,ijerph-18-05686-v2,False,False,True
12706,"population both physically and mentally active [ 24]. It can also lead to lower risks of
",False,ijerph-18-05686-v2,False,False,True
12707,"morbidities, disability, and cognitive decline, and their civic engagement can lead to better
",False,ijerph-18-05686-v2,False,False,True
12708,"physical and mental health, higher cognitive function, decreased loneliness, etc. [ 25]. In
",False,ijerph-18-05686-v2,False,False,True
12709,"summary, the engagement of the older population in voluntary services (e.g., social care,
",False,ijerph-18-05686-v2,False,False,True
12710,"and recreational and local community work) not only beneﬁts society, but also improves
",False,ijerph-18-05686-v2,False,False,True
12711,"the social and emotional health of the older population [26–28].
",False,ijerph-18-05686-v2,False,False,True
12712,"In summary, such a holistic and life-course approach means that the services used to
",False,ijerph-18-05686-v2,False,False,True
12713,"create value need to use a temporal lens, as the shorter and longer term needs of the older
",False,ijerph-18-05686-v2,False,False,True
12714,"population change over time. While health systems cannot be responsible for providing a
",False,ijerph-18-05686-v2,False,False,True
12715,"broader range of services and varying them over time, they need to leverage a network ofInt. J. Environ. Res. Public Health 2021 ,18, 5686 5 of 22
",False,ijerph-18-05686-v2,False,False,True
12716,"actors, including private and government sectors and social entrepreneurs that focus on
",False,ijerph-18-05686-v2,False,False,True
12717,"providing social beneﬁts [29–31].
",False,ijerph-18-05686-v2,False,False,True
12718,"2.3. Caregiver Engagement Services
",False,ijerph-18-05686-v2,False,False,True
12719,"Even as the demand for home-based care for older adults is rising across many
",False,ijerph-18-05686-v2,False,False,True
12720,"high-income countries [ 32], the nature of this care is becoming multifaceted. The care is
",False,ijerph-18-05686-v2,False,False,True
12721,"sometimes designed to support both older adults and their informal caregivers, especially
",False,ijerph-18-05686-v2,False,False,True
12722,"when adults living at home start to lose mental capacities and the ability to care for
",False,ijerph-18-05686-v2,False,False,True
12723,"themselves. In other words, digital services have to meet the needs of both the patient
",False,ijerph-18-05686-v2,False,False,True
12724,"client and the caregiver client. The client services have to be time sensitive as the health
",False,ijerph-18-05686-v2,False,False,True
12725,"conditions continue to deteriorate by using services that test their physical and cognitive
",False,ijerph-18-05686-v2,False,False,True
12726,"capabilities to create value, such as reducing client falls or engaging in activities that
",False,ijerph-18-05686-v2,False,False,True
12727,"improve cognition [ 33,34]. The role of caregivers, informal as well as specialized, is
",False,ijerph-18-05686-v2,False,False,True
12728,"growing, and the challenge is to create cohesion between the services provided by the
",False,ijerph-18-05686-v2,False,False,True
12729,"healthcare provider and other actors while addressing risks [ 35–37]. Providing reliable
",False,ijerph-18-05686-v2,False,False,True
12730,"information and supporting trust building opportunities are key to engender the trust of
",False,ijerph-18-05686-v2,False,False,True
12731,"clients in seeking such services.
",False,ijerph-18-05686-v2,False,False,True
12732,"In summary, caregivers are unpaid family members, and they provide an important
",False,ijerph-18-05686-v2,False,False,True
12733,"means to reach and support this patient client population [ 38,39]. The services provided
",False,ijerph-18-05686-v2,False,False,True
12734,"should include services that will help caregivers manage their own health while caring
",False,ijerph-18-05686-v2,False,False,True
12735,"for others. Such caregiver services are even more important when the caregiver lives with
",False,ijerph-18-05686-v2,False,False,True
12736,"the person they are caring for [ 33]. With the availability of informal caregivers expected
",False,ijerph-18-05686-v2,False,False,True
12737,"to decrease as many families are not engaging in co-residency, the communication and
",False,ijerph-18-05686-v2,False,False,True
12738,"coordination of the caregiver and patient services requires a mix of value fulﬁllment
",False,ijerph-18-05686-v2,False,False,True
12739,"activities from multiple actors [40–42].
",False,ijerph-18-05686-v2,False,False,True
12740,"2.4. Community Engagement Services
",False,ijerph-18-05686-v2,False,False,True
12741,"While medical advances are increasing the life span, they are not necessarily inﬂuenc-
",False,ijerph-18-05686-v2,False,False,True
12742,"ing the economic status of many older adult citizens living under or near poverty [ 43,44].
",False,ijerph-18-05686-v2,False,False,True
12743,"Many hospitals are using community providers to address the social determinants that con-
",False,ijerph-18-05686-v2,False,False,True
12744,"tribute to health inequities (e.g., supporting transportation, nutrition assistance, childcare,
",False,ijerph-18-05686-v2,False,False,True
12745,"or education). Community health workers, peer navigators, etc., are often used to assist
",False,ijerph-18-05686-v2,False,False,True
12746,"clients with mental health problems, sexually transmitted diseases, and diabetes [ 45,46].
",False,ijerph-18-05686-v2,False,False,True
12747,"Some social agencies leverage the support of other social agencies, particularly when deliv-
",False,ijerph-18-05686-v2,False,False,True
12748,"ering care-related services is complex, as in supporting homeless populations [ 47]. With
",False,ijerph-18-05686-v2,False,False,True
12749,"many social factors determining a client’s ability to manage their health condition, the chal-
",False,ijerph-18-05686-v2,False,False,True
12750,"lenge is to look upstream for prevention and downstream for care transition post-hospital
",False,ijerph-18-05686-v2,False,False,True
12751,"discharge in order to identify the services needed and actors to engage with [48].
",False,ijerph-18-05686-v2,False,False,True
12752,"In summary, older adults with health inequities have limited resources and technolo-
",False,ijerph-18-05686-v2,False,False,True
12753,"gies, such as limited wireless access and/or smart phones. As seen during the COVID-19
",False,ijerph-18-05686-v2,False,False,True
12754,"health emergency, these clients are at a greater risk for infection and mortality [ 49], and
",False,ijerph-18-05686-v2,False,False,True
12755,"their self-isolation for safety led to psychological and social challenges [ 50,51]. There-
",False,ijerph-18-05686-v2,False,False,True
12756,"fore, innovative ways to deliver care to such populations has become critical during the
",False,ijerph-18-05686-v2,False,False,True
12757,"COVID-19 lockdowns [ 52], including the use of a mix of digital and non-digital services
",False,ijerph-18-05686-v2,False,False,True
12758,"(e.g., telephone, local community workers, etc.) to maintain social connections considered
",False,ijerph-18-05686-v2,False,False,True
12759,"essential [53] despite the technology barriers [54,55].
",False,ijerph-18-05686-v2,False,False,True
12760,"While this classiﬁcation is not exhaustive, it does provide a range of services to older
",False,ijerph-18-05686-v2,False,False,True
12761,"clients that span from self-health management to continual health monitoring in both
",False,ijerph-18-05686-v2,False,False,True
12762,"supportive and less than supportive ecosystems [ 46]. Using these four different service
",False,ijerph-18-05686-v2,False,False,True
12763,"types, Figure 1 below identiﬁes a list of services needed to support these clients. As
",False,ijerph-18-05686-v2,False,False,True
12764,"discussed earlier, the goal here is not to build a point-to-point solution that supports one
",False,ijerph-18-05686-v2,False,False,True
12765,"group by one provider and its partners, but to develop a platform that is agile to address
",False,ijerph-18-05686-v2,False,False,True
12766,"client needs as they evolve or change. The value fulﬁllment services that will become a
",False,ijerph-18-05686-v2,False,False,True
12767,"part of a digital platform are discussed in the next section.Int. J. Environ. Res. Public Health 2021 ,18, 5686 6 of 22
",False,ijerph-18-05686-v2,False,False,True
12768,"Int. J. Environ. Res. Public Health 2021 , 18, x  6 of 22 
",False,ijerph-18-05686-v2,False,False,True
12769," 
",False,ijerph-18-05686-v2,False,False,True
12770," While this classification is not exhaustive, it  does provide a range of services to older 
",False,ijerph-18-05686-v2,False,False,True
12771,"clients that span from self-health management to continual health monitoring in both sup-
",False,ijerph-18-05686-v2,False,False,True
12772,"portive and less than supportive ecosystems [46] . Using these four different service types, 
",False,ijerph-18-05686-v2,False,False,True
12773,"Figure 1 below identifies a list of services needed to support these clients. As discussed 
",False,ijerph-18-05686-v2,False,False,True
12774,"earlier, the goal here is not to build a poin t-to-point solution that supports one group by 
",False,ijerph-18-05686-v2,False,False,True
12775,"one provider and its partners, but to develop a platform that is agile to address client 
",False,ijerph-18-05686-v2,False,False,True
12776,"needs as they evolve or change. The value fulf illment services that will become a part of a 
",False,ijerph-18-05686-v2,False,False,True
12777,"digital platform are discu ssed in the next section. 
",False,ijerph-18-05686-v2,False,False,True
12778," 
",False,ijerph-18-05686-v2,False,False,True
12779,"Figure 1. A template of services to suppo rt select popu lation groups. 
",False,ijerph-18-05686-v2,False,False,True
12780,"3. Designing a Digital Platform to Support Agility 
",False,ijerph-18-05686-v2,False,False,True
12781,"A client-centric and value-driven service model to address care delivery outside a 
",False,ijerph-18-05686-v2,False,False,True
12782,"health system is a different paradigm for health care systems [56]. It represents a shift from 
",False,ijerph-18-05686-v2,False,False,True
12783,"the provider-centric view of “the doctor kn ows best” to a “patient-centric” view where 
",False,ijerph-18-05686-v2,False,False,True
12784,"care delivered at home is best for the pati ent and the provider. Healthcare organizations 
",False,ijerph-18-05686-v2,False,False,True
12785,"that have longer value cycles and multiple ac tors outside their ecos ystem fulfilling many 
",False,ijerph-18-05686-v2,False,False,True
12786,"value cycle activities, must tr ansform their care delivery models using advances in tech-
",False,ijerph-18-05686-v2,False,False,True
12787,"nologies. This is especially critical as chronic care conditions continue to predominate in older populations. 
",False,ijerph-18-05686-v2,False,False,True
12788,"Figure 1. A template of services to support select population groups.
",False,ijerph-18-05686-v2,False,False,True
12789,"3. Designing a Digital Platform to Support Agility
",False,ijerph-18-05686-v2,False,False,True
12790,"A client-centric and value-driven service model to address care delivery outside a
",False,ijerph-18-05686-v2,False,False,True
12791,"health system is a different paradigm for healthcare systems [ 56]. It represents a shift from
",False,ijerph-18-05686-v2,False,False,True
12792,"the provider-centric view of “the doctor knows best” to a “patient-centric” view where
",False,ijerph-18-05686-v2,False,False,True
12793,"care delivered at home is best for the patient and the provider. Healthcare organizations
",False,ijerph-18-05686-v2,False,False,True
12794,"that have longer value cycles and multiple actors outside their ecosystem fulﬁlling many
",False,ijerph-18-05686-v2,False,False,True
12795,"value cycle activities, must transform their care delivery models using advances in tech-
",False,ijerph-18-05686-v2,False,False,True
12796,"nologies. This is especially critical as chronic care conditions continue to predominate in
",False,ijerph-18-05686-v2,False,False,True
12797,"older populations.
",False,ijerph-18-05686-v2,False,False,True
12798,"In the drive to use technologies, one cannot ignore the human element when it comes
",False,ijerph-18-05686-v2,False,False,True
12799,"to digital service use. For example, digital services that leverage sensors in wearables to
",False,ijerph-18-05686-v2,False,False,True
12800,"drive behavioral change such as reducing obesity, will not be effective if the clients do not
",False,ijerph-18-05686-v2,False,False,True
12801,"know how to use these digital services. Web-based interventions or remote monitoring
",False,ijerph-18-05686-v2,False,False,True
12802,"services designed to measure hemoglobin A1c or blood pressure have limited impact on
",False,ijerph-18-05686-v2,False,False,True
12803,"client behavior without adequate training [ 57–59]. When digital services are supported
",False,ijerph-18-05686-v2,False,False,True
12804,"by social and community health workers that provide education and answer questions,
",False,ijerph-18-05686-v2,False,False,True
12805,"it improves their use [ 46,60]. When patient portals used to disseminate information are
",False,ijerph-18-05686-v2,False,False,True
12806,"contextualized to lead clients to ask questions and seek clariﬁcation, they can prove to be
",False,ijerph-18-05686-v2,False,False,True
12807,"of greater value [ 61]. Even with intuitive interfaces, such as videos to support physicalInt. J. Environ. Res. Public Health 2021 ,18, 5686 7 of 22
",False,ijerph-18-05686-v2,False,False,True
12808,"or mental exercises and calming music to reduce pain, digital services will have a limited
",False,ijerph-18-05686-v2,False,False,True
12809,"impact when the patients have mobility challenges. Alternative technologies, such as
",False,ijerph-18-05686-v2,False,False,True
12810,"virtual reality (VR) goggles with animated games or travel to distant places, are shown to
",False,ijerph-18-05686-v2,False,False,True
12811,"be effective in reducing clients’ pain [62,63].
",False,ijerph-18-05686-v2,False,False,True
12812,"The examples discussed above highlight the importance of understanding the client
",False,ijerph-18-05686-v2,False,False,True
12813,"context or ecosystem where the value is perceived, so that care delivery models can be
",False,ijerph-18-05686-v2,False,False,True
12814,"effective. However, context is not static and client capabilities and expectations continue
",False,ijerph-18-05686-v2,False,False,True
12815,"to change. As clients become more experienced in using digital services and build their
",False,ijerph-18-05686-v2,False,False,True
12816,"capacity to self-manage their health condition by asking questions, seeking clariﬁcation
",False,ijerph-18-05686-v2,False,False,True
12817,"and second opinions, etc., the digital services have to adapt to address the changes in
",False,ijerph-18-05686-v2,False,False,True
12818,"client capabilities and expectations. In addition, the social ecosystem around a client may
",False,ijerph-18-05686-v2,False,False,True
12819,"change as well, such as their roles, relationships, reactions to their health condition, and
",False,ijerph-18-05686-v2,False,False,True
12820,"resources [ 13]. This makes client context dynamic, and continual value-in-use feedback is
",False,ijerph-18-05686-v2,False,False,True
12821,"needed to adapt the digital services to this evolving context. Implicitly, the mental model
",False,ijerph-18-05686-v2,False,False,True
12822,"or the “lens”, within which a client interprets the value created, evolves over time [ 64].
",False,ijerph-18-05686-v2,False,False,True
12823,"This is especially the case when a client condition deteriorates and the role of others (e.g.,
",False,ijerph-18-05686-v2,False,False,True
12824,"caregivers) increases, or when a client moves from a supportive ecosystem to a less than
",False,ijerph-18-05686-v2,False,False,True
12825,"supportive ecosystem. This has been seen during the COVID-19 pandemic, with many
",False,ijerph-18-05686-v2,False,False,True
12826,"clients moving from preventive care services to address food and economic insecurities.
",False,ijerph-18-05686-v2,False,False,True
12827,"Without claiming exhaustiveness, we will discuss the following three ways to make
",False,ijerph-18-05686-v2,False,False,True
12828,"digital services adapt to a changing client context: increase value cycle frequency, broaden
",False,ijerph-18-05686-v2,False,False,True
12829,"the scope of value-in-use assessment, and broaden the scale of value-in-use assessment.
",False,ijerph-18-05686-v2,False,False,True
12830,"3.1. Increase Value Cycle Frequency
",False,ijerph-18-05686-v2,False,False,True
12831,"The goal here is to make digital services adaptive by gathering feedback frequently
",False,ijerph-18-05686-v2,False,False,True
12832,"to track changing client expectations. The increased frequency of information gathering
",False,ijerph-18-05686-v2,False,False,True
12833,"has been shown to improve behavioral changes among clients. For example, the real-
",False,ijerph-18-05686-v2,False,False,True
12834,"time monitoring of glucose levels with education has been shown to improve individual
",False,ijerph-18-05686-v2,False,False,True
12835,"adherence [ 46], frequent feedback on sleep quality has shown improvement for clients
",False,ijerph-18-05686-v2,False,False,True
12836,"with apnea [ 65], and research is ongoing on how adaptive feedback control systems with
",False,ijerph-18-05686-v2,False,False,True
12837,"real-time feedback can improve smoking cessation behavior [66].
",False,ijerph-18-05686-v2,False,False,True
12838,"Remote and continual monitoring of clients generates large volumes of data, and tools
",False,ijerph-18-05686-v2,False,False,True
12839,"such as AI have become useful in identifying changing customer needs and redesigning
",False,ijerph-18-05686-v2,False,False,True
12840,"digital services accordingly. For example, an AI-assisted alert system was shown to be
",False,ijerph-18-05686-v2,False,False,True
12841,"effective in monitoring patients in an intensive care unit or emergency rooms [ 67]. Data
",False,ijerph-18-05686-v2,False,False,True
12842,"from a patient’s vital signs are analyzed to modify early warning scores that can lead
",False,ijerph-18-05686-v2,False,False,True
12843,"to predicting a cardiac arrest [ 9]. Even routinely available data analyzed by providers
",False,ijerph-18-05686-v2,False,False,True
12844,"can lead to decisions on how to alter external actor engagement in supporting clients
",False,ijerph-18-05686-v2,False,False,True
12845,"(e.g., emergency management technicians visiting clients at home, specialists consulting
",False,ijerph-18-05686-v2,False,False,True
12846,"nursing home patients, and staff changes in patient rooms) [ 46,68,69]. In summary, frequent
",False,ijerph-18-05686-v2,False,False,True
12847,"feedback from clients during value-in-use may lead to not only a reconﬁguration of digital
",False,ijerph-18-05686-v2,False,False,True
12848,"services, but also broaden the scope of actors used in value-in-use assessment. This is
",False,ijerph-18-05686-v2,False,False,True
12849,"discussed next.
",False,ijerph-18-05686-v2,False,False,True
12850,"3.2. Broaden the Scope of Value-in-Use Assessment
",False,ijerph-18-05686-v2,False,False,True
12851,"All innovative value propositions used to create value need the engagement of all the
",False,ijerph-18-05686-v2,False,False,True
12852,"actors, including clients, in value fulﬁllment. When these value propositions change to meet
",False,ijerph-18-05686-v2,False,False,True
12853,"customer expectations, it can lead to changes in the actors involved. The strategies used to
",False,ijerph-18-05686-v2,False,False,True
12854,"support the adoption and diffusion of an innovation (i.e., value-in-use of a digital service)
",False,ijerph-18-05686-v2,False,False,True
12855,"focus on client and digital service complexity [ 70]. In other words, value cycles have a
",False,ijerph-18-05686-v2,False,False,True
12856,"predictable frequency (i.e., how quickly feedback is used to redesign digital services to
",False,ijerph-18-05686-v2,False,False,True
12857,"create new value). However, changing the value cycle frequency to quickly monitor client
",False,ijerph-18-05686-v2,False,False,True
12858,"behavior means altering the value fulﬁllment frequency, and this may alter the behavior
",False,ijerph-18-05686-v2,False,False,True
12859,"of other actors. For example, consider a nurse calling system designed to use multipleInt. J. Environ. Res. Public Health 2021 ,18, 5686 8 of 22
",False,ijerph-18-05686-v2,False,False,True
12860,"buttons for the following different types of services: bathroom visits, pain management,
",False,ijerph-18-05686-v2,False,False,True
12861,"and general information. When the speed to respond to a nurse call is altered to improve
",False,ijerph-18-05686-v2,False,False,True
12862,"patient satisfaction, using text alerts and escalation protocols that transfer calls to others
",False,ijerph-18-05686-v2,False,False,True
12863,"when response time is slow, this alters the value fulﬁllment activities of the nursing staff. If
",False,ijerph-18-05686-v2,False,False,True
12864,"there is a difference in the perceived value-in-use of the patients (e.g., immediate response
",False,ijerph-18-05686-v2,False,False,True
12865,"to a call made to nursing staff) and the perceived value-in-use of the nursing staff (e.g.,
",False,ijerph-18-05686-v2,False,False,True
12866,"staff’s perception of the “urgency of the call” depending of the call source, for example
",False,ijerph-18-05686-v2,False,False,True
12867,"surgical or cardiac patients vs. oncology or neurology patients) when the frequency of
",False,ijerph-18-05686-v2,False,False,True
12868,"the value cycle is changed, it can lead to not meeting patient expectations [ 69,71]. In
",False,ijerph-18-05686-v2,False,False,True
12869,"other words, increasing the value cycle frequency to address the context changes in the
",False,ijerph-18-05686-v2,False,False,True
12870,"clients may have a negative impact, or be misaligned with the actors who are fulﬁlling the
",False,ijerph-18-05686-v2,False,False,True
12871,"value created.
",False,ijerph-18-05686-v2,False,False,True
12872,"Such misalignment can be signiﬁcant when some value fulﬁllment activities use
",False,ijerph-18-05686-v2,False,False,True
12873,"machine actors (e.g., systems that send alerts when calls are made, escalate to others,
",False,ijerph-18-05686-v2,False,False,True
12874,"etc.) and others use human actors such as nursing staff, whose activities cannot change
",False,ijerph-18-05686-v2,False,False,True
12875,"quickly (e.g., process changes, policy implications, etc., take time to change). This is often
",False,ijerph-18-05686-v2,False,False,True
12876,"reﬂected in model or algorithmic bias or fairness when these biases get embedded in digital
",False,ijerph-18-05686-v2,False,False,True
12877,"services [ 72]. For example, the algorithms used to make predictions on heart conditions may
",False,ijerph-18-05686-v2,False,False,True
12878,"be biased if they undercount women and especially women of color, impacting diagnoses
",False,ijerph-18-05686-v2,False,False,True
12879,"and treatment plans [ 73]. When machine learning algorithms are used to make predictions
",False,ijerph-18-05686-v2,False,False,True
12880,"based on large volumes of data, they can minimize the inﬂuence of a few outliers, leading to
",False,ijerph-18-05686-v2,False,False,True
12881,"the misapplication of forecasted outcomes [ 74,75]. Research on the design for diffusion [ 76],
",False,ijerph-18-05686-v2,False,False,True
12882,"and being aware of the situation as a whole [ 77], calls for viewing the client context more
",False,ijerph-18-05686-v2,False,False,True
12883,"broadly as value fulﬁllment and value-in-use is occurring within a complex ecosystem with
",False,ijerph-18-05686-v2,False,False,True
12884,"many actors besides the clients. Therefore, when the value cycle frequency is increased
",False,ijerph-18-05686-v2,False,False,True
12885,"using machine actors to address the changes in context from client feedback, feedback is
",False,ijerph-18-05686-v2,False,False,True
12886,"also needed from other actors inﬂuencing value fulﬁllment. Such broadening of the scope
",False,ijerph-18-05686-v2,False,False,True
12887,"of value assessment (actors providing the feedback) means potentially broadening the scale
",False,ijerph-18-05686-v2,False,False,True
12888,"of value assessment as well, which is discussed next.
",False,ijerph-18-05686-v2,False,False,True
12889,"3.3. Broaden the Scale of Value-in-Use Assessment
",False,ijerph-18-05686-v2,False,False,True
12890,"Technology-induced change has been studied in decision support systems [ 78], and
",False,ijerph-18-05686-v2,False,False,True
12891,"clients using digital services to self-manage their care condition may seek different types
",False,ijerph-18-05686-v2,False,False,True
12892,"of information not supported by any of the digital service modules or components. For
",False,ijerph-18-05686-v2,False,False,True
12893,"example, as older peoples’ cognitive functions change over time, digital services used to
",False,ijerph-18-05686-v2,False,False,True
12894,"support caregivers must evolve as the patients deteriorate in their health conditions. Even
",False,ijerph-18-05686-v2,False,False,True
12895,"with limited access to mobile technology to care for their health conditions [ 79], vulnerable
",False,ijerph-18-05686-v2,False,False,True
12896,"populations saw their service needs dramatically change during COVID-19 (from preven-
",False,ijerph-18-05686-v2,False,False,True
12897,"tion to address food and economic insecurities). Those who used apps to manage their
",False,ijerph-18-05686-v2,False,False,True
12898,"glucose levels have started to want to use the apps to apply for health insurance [ 46]. Given
",False,ijerph-18-05686-v2,False,False,True
12899,"that many social determinants contribute to health inequities, addressing some of them
",False,ijerph-18-05686-v2,False,False,True
12900,"using digital services may lead to clients seeking other services, thus implicitly calling for
",False,ijerph-18-05686-v2,False,False,True
12901,"broadening the sources used to provide feedback, i.e., the scale of value assessment. As
",False,ijerph-18-05686-v2,False,False,True
12902,"one uses a digital service (e.g., mobile health unit (MHU) calling or texting each client to
",False,ijerph-18-05686-v2,False,False,True
12903,"provide information on COVID-19), it may lead to other services for which assistance is
",False,ijerph-18-05686-v2,False,False,True
12904,"needed (food, transportation, etc.). This can lead to gathering feedback on these to create
",False,ijerph-18-05686-v2,False,False,True
12905,"new services and altering current services given their interdependency.
",False,ijerph-18-05686-v2,False,False,True
12906,"In summary, digital services need to increase the frequency of value cycles to show less
",False,ijerph-18-05686-v2,False,False,True
12907,"bias in fulﬁlling value and greater agility to meet evolving client expectations. They also
",False,ijerph-18-05686-v2,False,False,True
12908,"need to assess any change in the scope (or depth) of all those involved in fulﬁlling the value,
",False,ijerph-18-05686-v2,False,False,True
12909,"and change in the scale (or breadth) of the services needed to address the clients’ value
",False,ijerph-18-05686-v2,False,False,True
12910,"expectations. The design of a digital platform to support such agility is discussed next.Int. J. Environ. Res. Public Health 2021 ,18, 5686 9 of 22
",False,ijerph-18-05686-v2,False,False,True
12911,"3.4. Building an Agile Digital Platform
",False,ijerph-18-05686-v2,False,False,True
12912,"As organizations use agile models of structure and governance to support the explo-
",False,ijerph-18-05686-v2,False,False,True
12913,"ration and evaluation of new services to create and fulﬁll value and leverage technologies
",False,ijerph-18-05686-v2,False,False,True
12914,"to design digital services, they similarly need an agile software ecosystem that uses a
",False,ijerph-18-05686-v2,False,False,True
12915,"number of digital service modules or components that can be easily conﬁgured to adapt to
",False,ijerph-18-05686-v2,False,False,True
12916,"the evolving client needs. Figure 2 shows some of the service modules designed to support
",False,ijerph-18-05686-v2,False,False,True
12917,"the select senior population segments discussed in the previous section. The software
",False,ijerph-18-05686-v2,False,False,True
12918,"modules interact with each other, potentially across partner and client ecosystems, using a
",False,ijerph-18-05686-v2,False,False,True
12919,"distributed digital platform. This platform supports the communication of information and
",False,ijerph-18-05686-v2,False,False,True
12920,"the coordination of value cycle activities among all the actors involved. Each service mod-
",False,ijerph-18-05686-v2,False,False,True
12921,"ule supports a distinct value proposition (e.g., searching for an external caregiver, tracking
",False,ijerph-18-05686-v2,False,False,True
12922,"health conditions using a glucose monitor, ﬁnding a physician to get a second opinion).
",False,ijerph-18-05686-v2,False,False,True
12923,"They can be conﬁgured to work together to create value for a client, such as a caregiver
",False,ijerph-18-05686-v2,False,False,True
12924,"servicing a dementia patient or an older client looking to monitor their blood pressure.
",False,ijerph-18-05686-v2,False,False,True
12925,"Int. J. Environ. Res. Public Health 2021 , 18, x  9 of 22 
",False,ijerph-18-05686-v2,False,False,True
12926," 
",False,ijerph-18-05686-v2,False,False,True
12927," ing for broadening the sources used to prov ide feedback, i.e., the scale of value assess-
",False,ijerph-18-05686-v2,False,False,True
12928,"ment. As one uses a digital service (e.g., mobile health unit (MHU) calling or texting each 
",False,ijerph-18-05686-v2,False,False,True
12929,"client to provide information on COVID-19), it may lead to other services for which assis-
",False,ijerph-18-05686-v2,False,False,True
12930,"tance is needed (food, transportation, etc.). Th is can lead to gathering feedback on these 
",False,ijerph-18-05686-v2,False,False,True
12931,"to create new services and altering current services given their interdependency. 
",False,ijerph-18-05686-v2,False,False,True
12932,"In summary, digital services need to increase the frequency of value cycles to show 
",False,ijerph-18-05686-v2,False,False,True
12933,"less bias in fulfilling value and greater agility to meet evolving client expectations. They 
",False,ijerph-18-05686-v2,False,False,True
12934,"also need to assess any change in the scope (or depth) of all those involved in fulfilling the 
",False,ijerph-18-05686-v2,False,False,True
12935,"value, and change in the scale (or breadth) of the services needed to address the clients’ value expectations. The design of a digital platform to support such agility is discussed next. 
",False,ijerph-18-05686-v2,False,False,True
12936,"3.4. Building an Agile Digital Platform 
",False,ijerph-18-05686-v2,False,False,True
12937,"As organizations use agile models of structure and governance to support the explo-
",False,ijerph-18-05686-v2,False,False,True
12938,"ration and evaluation of new services to crea te and fulfill value and leverage technologies 
",False,ijerph-18-05686-v2,False,False,True
12939,"to design digital services, they similarly need an agile software ecos ystem that uses a num-
",False,ijerph-18-05686-v2,False,False,True
12940,"ber of digital service modules or components that can be easily configured to adapt to the 
",False,ijerph-18-05686-v2,False,False,True
12941,"evolving client needs. Figure 2 shows some of the service modules designed to support 
",False,ijerph-18-05686-v2,False,False,True
12942,"the select senior population segments discu ssed in the previous section. The software 
",False,ijerph-18-05686-v2,False,False,True
12943,"modules interact with each other, potentially across partner and client ecosystems, using 
",False,ijerph-18-05686-v2,False,False,True
12944,"a distributed digital platform. This platform  supports the communication of information 
",False,ijerph-18-05686-v2,False,False,True
12945,"and the coordination of value cycle activities  among all the actors involved. Each service 
",False,ijerph-18-05686-v2,False,False,True
12946,"module supports a distinct value proposition (e.g., searching for an external caregiver, 
",False,ijerph-18-05686-v2,False,False,True
12947,"tracking health conditions using a glucose monitor, finding a physician to get a second opinion). They can be configured to work together to create value for a client, such as a 
",False,ijerph-18-05686-v2,False,False,True
12948,"caregiver servicing a dementia patient or an older client looking to monitor their blood 
",False,ijerph-18-05686-v2,False,False,True
12949,"pressure. 
",False,ijerph-18-05686-v2,False,False,True
12950," 
",False,ijerph-18-05686-v2,False,False,True
12951,"Figure 2. Digital platform to support digital service configuration. 
",False,ijerph-18-05686-v2,False,False,True
12952,"A recent institute of electrical and electr onics engineers (IEEE) special Issue on man-
",False,ijerph-18-05686-v2,False,False,True
12953,"aging software ecosystems has recognized th e need for agile ecosystems to support com-
",False,ijerph-18-05686-v2,False,False,True
12954,"plex and dynamic environments such as the one discussed here [80]. There can be a mix 
",False,ijerph-18-05686-v2,False,False,True
12955,"Figure 2. Digital platform to support digital service conﬁguration.
",False,ijerph-18-05686-v2,False,False,True
12956,"A recent institute of electrical and electronics engineers (IEEE) special Issue on manag-
",False,ijerph-18-05686-v2,False,False,True
12957,"ing software ecosystems has recognized the need for agile ecosystems to support complex
",False,ijerph-18-05686-v2,False,False,True
12958,"and dynamic environments such as the one discussed here [ 80]. There can be a mix of
",False,ijerph-18-05686-v2,False,False,True
12959,"autonomous and automated systems tracking rapid changes in the external environment,
",False,ijerph-18-05686-v2,False,False,True
12960,"and a mix of machine actors (software systems). Human actors (in the clinical and non-
",False,ijerph-18-05686-v2,False,False,True
12961,"clinical ecosystem), including technology developers, need a digital platform to support
",False,ijerph-18-05686-v2,False,False,True
12962,"their collaboration [ 81] and interactions to share data and collaborate their activities in
",False,ijerph-18-05686-v2,False,False,True
12963,"real time [ 82]. Such a dynamic orchestration of digital services has shown success in
",False,ijerph-18-05686-v2,False,False,True
12964,"multi-tenant business networks during run-time as actors join and leave [ 83], and among
",False,ijerph-18-05686-v2,False,False,True
12965,"innovation ecosystems where ideas are generated and evaluated [84].
",False,ijerph-18-05686-v2,False,False,True
12966,"A dynamic software ecosystem for rapidly evolving client expectations, as shown in
",False,ijerph-18-05686-v2,False,False,True
12967,"Figure 2, needs not only increased frequency of value assessment for reconﬁguration, but
",False,ijerph-18-05686-v2,False,False,True
12968,"an ability to address potential changes in the scope and scale of such an assessment. For
",False,ijerph-18-05686-v2,False,False,True
12969,"example, if the run-time feedback on deﬁned parameters (e.g., pain medication requests
",False,ijerph-18-05686-v2,False,False,True
12970,"from a nurse call system or smart bed alerts for bathroom support) is lower than expected,
",False,ijerph-18-05686-v2,False,False,True
12971,"what conclusions follow? Did it address a value proposition (i.e., patient satisfaction
",False,ijerph-18-05686-v2,False,False,True
12972,"attributed to staff response)? Should it lead to broadening of the scale by seeking feedback
",False,ijerph-18-05686-v2,False,False,True
12973,"on other services (e.g., food catering services, discharge services, etc.)? Should it lead to aInt. J. Environ. Res. Public Health 2021 ,18, 5686 10 of 22
",False,ijerph-18-05686-v2,False,False,True
12974,"deepening of the scope by seeking feedback from other actors in the network (e.g., nursing
",False,ijerph-18-05686-v2,False,False,True
12975,"staff on changing patient conditions, improved care processes such as hourly rounding,
",False,ijerph-18-05686-v2,False,False,True
12976,"etc.)? Such an insight on the scope and scale needed in value assessment requires an
",False,ijerph-18-05686-v2,False,False,True
12977,"understanding of the client or actor intent and should be explicitly considered in the design
",False,ijerph-18-05686-v2,False,False,True
12978,"of the digital services.
",False,ijerph-18-05686-v2,False,False,True
12979,"In summary, gathering client intent if other value-in-use feedback is not sufﬁcient to
",False,ijerph-18-05686-v2,False,False,True
12980,"autonomously reconﬁgure digital services is needed if agile digital platforms and software
",False,ijerph-18-05686-v2,False,False,True
12981,"ecosystems are to support a health system that is seeking to support client needs quickly
",False,ijerph-18-05686-v2,False,False,True
12982,"using value cycle activities. One pilot application that combines client intent and a dynamic
",False,ijerph-18-05686-v2,False,False,True
12983,"software ecosystem is discussed in the next section.
",False,ijerph-18-05686-v2,False,False,True
12984,"4. Digital Platform to Support Patients with Delirium
",False,ijerph-18-05686-v2,False,False,True
12985,"A fear of falling is often a contributor to fall injuries inside and outside hospitals [ 85].
",False,ijerph-18-05686-v2,False,False,True
12986,"Such a fear is a pervasive psychological problem in older people and not only contributes
",False,ijerph-18-05686-v2,False,False,True
12987,"to falls [ 86], but also makes them avoid activities that can keep them healthy, i.e., less
",False,ijerph-18-05686-v2,False,False,True
12988,"dependent on society and medications [ 87–89]. We will focus on one particular symptom
",False,ijerph-18-05686-v2,False,False,True
12989,"of this, delirium, which contributes to the fear of falling. The digital platform that uses
",False,ijerph-18-05686-v2,False,False,True
12990,"advanced technologies and digital services to create, fulﬁll, and assess value-in-use to
",False,ijerph-18-05686-v2,False,False,True
12991,"support delirium patients is summarized in Figure 3, and is elaborated on in this section.
",False,ijerph-18-05686-v2,False,False,True
12992,"Int. J. Environ. Res. Public Health 2021 , 18, x  10 of 22 
",False,ijerph-18-05686-v2,False,False,True
12993," 
",False,ijerph-18-05686-v2,False,False,True
12994," of autonomous and automated systems tracking rapid changes in the external environ-
",False,ijerph-18-05686-v2,False,False,True
12995,"ment, and a mix of machine actors (software systems). Human actors (in the clinical and 
",False,ijerph-18-05686-v2,False,False,True
12996,"non-clinical ecosystem), including technology  developers, need a digital platform to sup-
",False,ijerph-18-05686-v2,False,False,True
12997,"port their collaboration [81] and interactions to share data and collaborate their activities in real time [82]. Such a dynamic orchestration of digital services has shown success in 
",False,ijerph-18-05686-v2,False,False,True
12998,"multi-tenant business networks during run-time as actors join and leave [83], and among 
",False,ijerph-18-05686-v2,False,False,True
12999,"innovation ecosystems where ideas are generated and evaluated [84]. 
",False,ijerph-18-05686-v2,False,False,True
13000,"A dynamic software ecosystem for rapidly ev olving client expectations, as shown in 
",False,ijerph-18-05686-v2,False,False,True
13001,"Figure 2, needs not only increased frequency of value assessment for reconfiguration, but an ability to address potential changes in the scope and scale of such an assessment. For 
",False,ijerph-18-05686-v2,False,False,True
13002,"example, if the run-time feedback on defined pa rameters (e.g., pain medication requests 
",False,ijerph-18-05686-v2,False,False,True
13003,"from a nurse call system or smart bed alerts for bathroom support) is lower than expected, 
",False,ijerph-18-05686-v2,False,False,True
13004,"what conclusions follow? Did it address a valu e proposition (i.e., patient satisfaction at-
",False,ijerph-18-05686-v2,False,False,True
13005,"tributed to staff response)? Should it lead to  broadening of the scale by seeking feedback 
",False,ijerph-18-05686-v2,False,False,True
13006,"on other services (e.g., food catering services, discharge services, etc.)? Should it lead to a 
",False,ijerph-18-05686-v2,False,False,True
13007,"deepening of the scope by seeking feedback from other actors in the network (e.g., nursing 
",False,ijerph-18-05686-v2,False,False,True
13008,"staff on changing patient conditions, improv ed care processes such as hourly rounding, 
",False,ijerph-18-05686-v2,False,False,True
13009,"etc.)? Such an insight on the scope and scale needed in value assessment requires an un-
",False,ijerph-18-05686-v2,False,False,True
13010,"derstanding of the client or actor intent and should be explicitly considered in the design 
",False,ijerph-18-05686-v2,False,False,True
13011,"of the digital services.  
",False,ijerph-18-05686-v2,False,False,True
13012,"In summary, gathering client intent if other value-in-use feedback is not sufficient to 
",False,ijerph-18-05686-v2,False,False,True
13013,"autonomously reconfigure digital services is  needed if agile digital platforms and soft-
",False,ijerph-18-05686-v2,False,False,True
13014,"ware ecosystems are to support a health syst em that is seeking to support client needs 
",False,ijerph-18-05686-v2,False,False,True
13015,"quickly using value cycle activities. One pilot application that combines client intent and 
",False,ijerph-18-05686-v2,False,False,True
13016,"a dynamic software ecosystem is discussed in the next section. 
",False,ijerph-18-05686-v2,False,False,True
13017,"4.
",False,ijerph-18-05686-v2,False,False,True
13018," Digital Platform to Support  Patients with Delirium 
",False,ijerph-18-05686-v2,False,False,True
13019,"A fear of falling is often a contributor to fall injuries inside and outside hospitals [85]. 
",False,ijerph-18-05686-v2,False,False,True
13020,"Such a fear is a pervasive psychological problem in older people and not only contributes 
",False,ijerph-18-05686-v2,False,False,True
13021,"to falls [86], but also makes them avoid activi ties that can keep them healthy, i.e., less 
",False,ijerph-18-05686-v2,False,False,True
13022,"dependent on society and medications [87–89]. We will focus on one particular symptom 
",False,ijerph-18-05686-v2,False,False,True
13023,"of this, delirium, which contribu tes to the fear of falling. The digital platform that uses 
",False,ijerph-18-05686-v2,False,False,True
13024,"advanced technologies and digital services to create, fulfill, and assess value-in-use to 
",False,ijerph-18-05686-v2,False,False,True
13025,"support delirium patients is summarized in Figure  3, and is elaborated on in this section. 
",False,ijerph-18-05686-v2,False,False,True
13026," 
",False,ijerph-18-05686-v2,False,False,True
13027,"Figure 3. Digital platform to support delirium patients. 
",False,ijerph-18-05686-v2,False,False,True
13028,"Figure 3. Digital platform to support delirium patients.
",False,ijerph-18-05686-v2,False,False,True
13029,"4.1. Clients
",False,ijerph-18-05686-v2,False,False,True
13030,"Delirium is considered a syndrome due to its multicomplex nature and is dangerous-
",False,ijerph-18-05686-v2,False,False,True
13031,"ness. It is common in older hospitalized adults, with a prevalence of about 30–50%, and
",False,ijerph-18-05686-v2,False,False,True
13032,"affects around seven million patients annually. A big challenge for healthcare systems is
",False,ijerph-18-05686-v2,False,False,True
13033,"that this syndrome can go undiagnosed by healthcare team members, including nurses
",False,ijerph-18-05686-v2,False,False,True
13034,"and physicians. This leads to a longer length of hospital stay, loss of physical function,
",False,ijerph-18-05686-v2,False,False,True
13035,"which can lead to institutionalization at long-term care facilities, progression to dementia,
",False,ijerph-18-05686-v2,False,False,True
13036,"and even death. The major challenges of the effective management of delirium include
",False,ijerph-18-05686-v2,False,False,True
13037,"the need for a multidisciplinary approach, difﬁculties with risk modiﬁcation, and a lack
",False,ijerph-18-05686-v2,False,False,True
13038,"of effective low-risk pharmacological treatments. Thus, a system that can assist in early
",False,ijerph-18-05686-v2,False,False,True
13039,"identiﬁcation, by means of timely screening and assessment, is needed so that precipitating
",False,ijerph-18-05686-v2,False,False,True
13040,"factors can be identiﬁed and removed for improvement in patient care. Clients with cogni-
",False,ijerph-18-05686-v2,False,False,True
13041,"tive impairment can fall during “unexpected care windows”, and no amount of nursing
",False,ijerph-18-05686-v2,False,False,True
13042,"resources can prevent these falls. However, such falls contribute to hospital admissions,Int. J. Environ. Res. Public Health 2021 ,18, 5686 11 of 22
",False,ijerph-18-05686-v2,False,False,True
13043,"medical treatments, and high insurance costs, not to mention the personal physical and
",False,ijerph-18-05686-v2,False,False,True
13044,"psychological pain to the patients.
",False,ijerph-18-05686-v2,False,False,True
13045,"4.2. Technologies
",False,ijerph-18-05686-v2,False,False,True
13046,"Remote monitoring of such patients can be done using wearable-based, non-wearable-
",False,ijerph-18-05686-v2,False,False,True
13047,"based, and fusion-based systems [ 90]. The wearable-based system uses sensors tied to
",False,ijerph-18-05686-v2,False,False,True
13048,"various body parts, but are relatively inﬂexible and uncomfortable. The non-wearable-
",False,ijerph-18-05686-v2,False,False,True
13049,"based systems, such as smartphone-based solutions, could be a very competitive alternative
",False,ijerph-18-05686-v2,False,False,True
13050,"to the conventional wearable systems for fall detection [ 91]. However, as discussed earlier,
",False,ijerph-18-05686-v2,False,False,True
13051,"when the value cycle frequency increases due to remote monitoring, multiple sources of
",False,ijerph-18-05686-v2,False,False,True
13052,"information are needed to be analyzed, and fusion-based systems can help leverage diverse
",False,ijerph-18-05686-v2,False,False,True
13053,"information sources. In the proposed system, information from visual clues are combined
",False,ijerph-18-05686-v2,False,False,True
13054,"with audio responses to questions posed by the system to understand patient intent in
",False,ijerph-18-05686-v2,False,False,True
13055,"creating and assessing value-in-use for patients.
",False,ijerph-18-05686-v2,False,False,True
13056,"In the digital platform proposed, called SeVa, the frontend used to create value is a
",False,ijerph-18-05686-v2,False,False,True
13057,"mobile application written in the Apple iOS native program language Swift on Xcode 11,
",False,ijerph-18-05686-v2,False,False,True
13058,"which runs on the iOS device with iOS 13. The chatbot engine is Dialogﬂow, a UI-based
",False,ijerph-18-05686-v2,False,False,True
13059,"platform for creating smart and proactive chatbots. SeVA is a fusion-based system, and
",False,ijerph-18-05686-v2,False,False,True
13060,"supports remote diagnosis and medical consultation. The SeVA backend server to fulﬁll
",False,ijerph-18-05686-v2,False,False,True
13061,"value has four dedicated advanced reduced instruction set computer machines (ARMs)
",False,ijerph-18-05686-v2,False,False,True
13062,"with 2GB memory and the Ubuntu Xenial system.
",False,ijerph-18-05686-v2,False,False,True
13063,"The remote monitoring service here includes a fusion of clues from the audio and
",False,ijerph-18-05686-v2,False,False,True
13064,"visual content. It connects the following two different environments: SeVA Patient Room
",False,ijerph-18-05686-v2,False,False,True
13065,"app (SPR), which is an application that resides in a patient room or ecosystem, and SeVA
",False,ijerph-18-05686-v2,False,False,True
13066,"Master Control app (SMC), which is a master control application that resides in the provider
",False,ijerph-18-05686-v2,False,False,True
13067,"ecosystem, connected to both the autonomous part of the digital service as well as the
",False,ijerph-18-05686-v2,False,False,True
13068,"human actor engaged component, i.e., nursing station to react to speciﬁc patient context.
",False,ijerph-18-05686-v2,False,False,True
13069,"4.3. Value Creating Services
",False,ijerph-18-05686-v2,False,False,True
13070,"The value creating services are supported using a remote diagnosis that does not use
",False,ijerph-18-05686-v2,False,False,True
13071,"text but rather visual gestures, such as the waving of hands or visual eye movement, as
",False,ijerph-18-05686-v2,False,False,True
13072,"well as voice conversation. Both of these are important for elders or patients suffering
",False,ijerph-18-05686-v2,False,False,True
13073,"with delirium. It uses personalized conversation to answer questions and provide advice.
",False,ijerph-18-05686-v2,False,False,True
13074,"Speciﬁcally, the system creates value to clients by responding to patient gestures with
",False,ijerph-18-05686-v2,False,False,True
13075,"consultation sessions. In addition, it does regular checks to ensure that the patient condi-
",False,ijerph-18-05686-v2,False,False,True
13076,"tion is monitored periodically, including a morning check and hourly rounding. It uses
",False,ijerph-18-05686-v2,False,False,True
13077,"delirium checks to assess a patient’s cognitive capabilities and provides music to create a
",False,ijerph-18-05686-v2,False,False,True
13078,"relaxed environment to reduce stress and/or help clients manage pain. Table 1 lists these
",False,ijerph-18-05686-v2,False,False,True
13079,"patient services.
",False,ijerph-18-05686-v2,False,False,True
13080,"Table 1. Seva skills in the Dialogﬂow.
",False,ijerph-18-05686-v2,False,False,True
13081,"Type Skill Story Description
",False,ijerph-18-05686-v2,False,False,True
13082,"Movement Response Sensor ResponseFall detectionRespond to patient and notify
",False,ijerph-18-05686-v2,False,False,True
13083,"nurse by recognizing
",False,ijerph-18-05686-v2,False,False,True
13084,"patient movement. Wave
",False,ijerph-18-05686-v2,False,False,True
13085,"Regular CheckHourly RoundingFeeling check
",False,ijerph-18-05686-v2,False,False,True
13086,"Perform regular hourly check
",False,ijerph-18-05686-v2,False,False,True
13087,"to fulﬁll patient needs actively.Restroom check
",False,ijerph-18-05686-v2,False,False,True
13088,"Brace check
",False,ijerph-18-05686-v2,False,False,True
13089,"Heat pack check
",False,ijerph-18-05686-v2,False,False,True
13090,"Delirium CheckWhat day is today?Perform regular delirium
",False,ijerph-18-05686-v2,False,False,True
13091,"check to evaluate
",False,ijerph-18-05686-v2,False,False,True
13092,"patient cognition. List weekdays in reversed orderInt. J. Environ. Res. Public Health 2021 ,18, 5686 12 of 22
",False,ijerph-18-05686-v2,False,False,True
13093,"Table 1. Cont.
",False,ijerph-18-05686-v2,False,False,True
13094,"Type Skill Story Description
",False,ijerph-18-05686-v2,False,False,True
13095,"RelaxationsSoothing Music Play music
",False,ijerph-18-05686-v2,False,False,True
13096,"Use music, jokes, and small
",False,ijerph-18-05686-v2,False,False,True
13097,"talk to improve patient’s
",False,ijerph-18-05686-v2,False,False,True
13098,"mental state.Small Talk Random talk
",False,ijerph-18-05686-v2,False,False,True
13099,"JokeTell me a joke
",False,ijerph-18-05686-v2,False,False,True
13100,"More jokes
",False,ijerph-18-05686-v2,False,False,True
13101,"4.4. Value Fulﬁllment Service Modules
",False,ijerph-18-05686-v2,False,False,True
13102,"The service begins when the chatbot engine receives a trigger upon patient touch or
",False,ijerph-18-05686-v2,False,False,True
13103,"sound. It sends the ﬁrst sentence to the SPR and then waits for the patient’s response (i.e.,
",False,ijerph-18-05686-v2,False,False,True
13104,"returning sentence). The patient’s response will then be classiﬁed as the intent “Yes” or
",False,ijerph-18-05686-v2,False,False,True
13105,"“No”, each leading to a different set of conversations. For periodic monitoring, such as
",False,ijerph-18-05686-v2,False,False,True
13106,"hourly and morning checks, the SPR will ask a question every hour (except for the rest
",False,ijerph-18-05686-v2,False,False,True
13107,"hour) to make sure the client’s needs are satisﬁed.
",False,ijerph-18-05686-v2,False,False,True
13108,"For delirium checks, the SPR starts by checking the patient’s delirium status by asking
",False,ijerph-18-05686-v2,False,False,True
13109,"questions, or launches the delirium check game and uses cognitive assessment tests [ 92].
",False,ijerph-18-05686-v2,False,False,True
13110,"Two delirium check games are used at present. The ﬁrst game, “Connect Node”, is used
",False,ijerph-18-05686-v2,False,False,True
13111,"for testing visuospatial and executive ability. The patient is required to connect the node in
",False,ijerph-18-05686-v2,False,False,True
13112,"a given order. If a patient fails the test, a message will be immediately sent to the SMC. The
",False,ijerph-18-05686-v2,False,False,True
13113,"second game, “Click Animal”, tests patient attention. Pictures are displayed and disappear.
",False,ijerph-18-05686-v2,False,False,True
13114,"The patient has to click the animal picture. The checking result will also be sent to the SeVA
",False,ijerph-18-05686-v2,False,False,True
13115,"Master Control application. For supporting relaxation services, the SPR has embedded
",False,ijerph-18-05686-v2,False,False,True
13116,"music created by the therapist in order to create a calm atmosphere and relieve panic.
",False,ijerph-18-05686-v2,False,False,True
13117,"Figure 4 shows the coordination of the value fulﬁlling activities using a particular
",False,ijerph-18-05686-v2,False,False,True
13118,"scenario, beginning with an action from the patient. The fulﬁllment of the activities
",False,ijerph-18-05686-v2,False,False,True
13119,"supported by the chatbot engine scenario are deﬁned by medical staff.
",False,ijerph-18-05686-v2,False,False,True
13120,"Int. J. Environ. Res. Public Health 2021 , 18, x  13 of 22 
",False,ijerph-18-05686-v2,False,False,True
13121," 
",False,ijerph-18-05686-v2,False,False,True
13122,"  
",False,ijerph-18-05686-v2,False,False,True
13123,"Figure 4. Coordination of activities among multip le actors within SeVa architecture. 
",False,ijerph-18-05686-v2,False,False,True
13124,"4.5. Agile Digital Platform to Suppor t Activity Coordination among Actors 
",False,ijerph-18-05686-v2,False,False,True
13125,"Since many of the components of the system are autonomous agents (or machine 
",False,ijerph-18-05686-v2,False,False,True
13126,"actors), the activity of each actor is discu ssed below. Some of these are running continu-
",False,ijerph-18-05686-v2,False,False,True
13127,"ally, and others are trigged based on new ev ents. Within the actor-network theory, this 
",False,ijerph-18-05686-v2,False,False,True
13128,"human–machine–other human (e.g.,  nurse) interaction has to occur throughout the entire 
",False,ijerph-18-05686-v2,False,False,True
13129,"value cycle and let the value-in-use feedback be used to support agility in the software 
",False,ijerph-18-05686-v2,False,False,True
13130,"ecosystem. To coordinate the activities, the system here functions as discussed below. 
",False,ijerph-18-05686-v2,False,False,True
13131,"Upon user registration and authentication, the user conversation can be initiated by 
",False,ijerph-18-05686-v2,False,False,True
13132,"hand waving, using the wake-up word, or simp ly touching the screen. Currently, a smart 
",False,ijerph-18-05686-v2,False,False,True
13133,"wristband detects the user movement and another sensor detects a patient’s wave gesture. 
",False,ijerph-18-05686-v2,False,False,True
13134,"Once an event is triggered, it will be routed to  the chatbot engine to initiate the conversa-
",False,ijerph-18-05686-v2,False,False,True
13135,"tion using the SeVA mobile application. 
",False,ijerph-18-05686-v2,False,False,True
13136,"The chatbot engine transforms the user’s voice into plain text, which is processed by 
",False,ijerph-18-05686-v2,False,False,True
13137,"the natural language processing (NLP ) module. Using discrete word pre-processing and 
",False,ijerph-18-05686-v2,False,False,True
13138,"tokenizing, it is sent to a recurrent neural  network (RNN) module. The output of the RNN 
",False,ijerph-18-05686-v2,False,False,True
13139,"is matching the word candidates if the patient poses a directly interpretable request or a 
",False,ijerph-18-05686-v2,False,False,True
13140,"suggested intent score of the patient’s question. If it is an intent score that needs further 
",False,ijerph-18-05686-v2,False,False,True
13141,"interpretation, it is then sent to the dialogue  engine of the expert system module to decide 
",False,ijerph-18-05686-v2,False,False,True
13142,"on the final interpretation of the patient request. See Figure 5. 
",False,ijerph-18-05686-v2,False,False,True
13143,"The medical expert defines skills, such as interpreting the intent and trigger, and re-
",False,ijerph-18-05686-v2,False,False,True
13144,"plies. Once the intent matches with the skil ls, the system can begin the conversation au-
",False,ijerph-18-05686-v2,False,False,True
13145,"tonomously. The skill used to begin the next  conversation is derived from a knowledge 
",False,ijerph-18-05686-v2,False,False,True
13146,"base of skills, and this can be triggered either  by a directly interpreted patient request or 
",False,ijerph-18-05686-v2,False,False,True
13147,"extracted from the interpretation using both the RNN and expert knowledge. 
",False,ijerph-18-05686-v2,False,False,True
13148," 
",False,ijerph-18-05686-v2,False,False,True
13149,"Figure 5. Chatbot engine skill and story design. 
",False,ijerph-18-05686-v2,False,False,True
13150,"Figure 4. Coordination of activities among multiple actors within SeVa architecture.
",False,ijerph-18-05686-v2,False,False,True
13151," A patient begins the scenario with an event, for example “waving hand.” When this
",False,ijerph-18-05686-v2,False,False,True
13152,"patient request is received, the peripheral sensor recognizes the action and sends an
",False,ijerph-18-05686-v2,False,False,True
13153,"hypertext transfer protocol secure (HTTPS) request to the SeVA backend server;
",False,ijerph-18-05686-v2,False,False,True
13154," The request is parsed to fetch the necessary information, such as the room number,
",False,ijerph-18-05686-v2,False,False,True
13155,"from the database in order to trigger the chatbot engine to begin the conversation. The
",False,ijerph-18-05686-v2,False,False,True
13156,"SeVA patient room application has a WebSocket connection with the chatbot engine
",False,ijerph-18-05686-v2,False,False,True
13157,"so that the conversation can be executed;
",False,ijerph-18-05686-v2,False,False,True
13158," The conversation result is sent back to the SeVA backend server in the form of a SeVA
",False,ijerph-18-05686-v2,False,False,True
13159,"command, and it is sent to the SeVA Master Control application in the nurse room;Int. J. Environ. Res. Public Health 2021 ,18, 5686 13 of 22
",False,ijerph-18-05686-v2,False,False,True
13160," This leads to the nurse taking timely intervention if this is a critical event;
",False,ijerph-18-05686-v2,False,False,True
13161," The running status of the system is monitored continually by the monitoring server to
",False,ijerph-18-05686-v2,False,False,True
13162,"ensure system reliability.
",False,ijerph-18-05686-v2,False,False,True
13163,"4.5. Agile Digital Platform to Support Activity Coordination among Actors
",False,ijerph-18-05686-v2,False,False,True
13164,"Since many of the components of the system are autonomous agents (or machine
",False,ijerph-18-05686-v2,False,False,True
13165,"actors), the activity of each actor is discussed below. Some of these are running continually,
",False,ijerph-18-05686-v2,False,False,True
13166,"and others are trigged based on new events. Within the actor-network theory, this human–
",False,ijerph-18-05686-v2,False,False,True
13167,"machine–other human (e.g., nurse) interaction has to occur throughout the entire value
",False,ijerph-18-05686-v2,False,False,True
13168,"cycle and let the value-in-use feedback be used to support agility in the software ecosystem.
",False,ijerph-18-05686-v2,False,False,True
13169,"To coordinate the activities, the system here functions as discussed below.
",False,ijerph-18-05686-v2,False,False,True
13170,"Upon user registration and authentication, the user conversation can be initiated by
",False,ijerph-18-05686-v2,False,False,True
13171,"hand waving, using the wake-up word, or simply touching the screen. Currently, a smart
",False,ijerph-18-05686-v2,False,False,True
13172,"wristband detects the user movement and another sensor detects a patient’s wave gesture.
",False,ijerph-18-05686-v2,False,False,True
13173,"Once an event is triggered, it will be routed to the chatbot engine to initiate the conversation
",False,ijerph-18-05686-v2,False,False,True
13174,"using the SeVA mobile application.
",False,ijerph-18-05686-v2,False,False,True
13175,"The chatbot engine transforms the user’s voice into plain text, which is processed by
",False,ijerph-18-05686-v2,False,False,True
13176,"the natural language processing (NLP) module. Using discrete word pre-processing and
",False,ijerph-18-05686-v2,False,False,True
13177,"tokenizing, it is sent to a recurrent neural network (RNN) module. The output of the RNN
",False,ijerph-18-05686-v2,False,False,True
13178,"is matching the word candidates if the patient poses a directly interpretable request or a
",False,ijerph-18-05686-v2,False,False,True
13179,"suggested intent score of the patient’s question. If it is an intent score that needs further
",False,ijerph-18-05686-v2,False,False,True
13180,"interpretation, it is then sent to the dialogue engine of the expert system module to decide
",False,ijerph-18-05686-v2,False,False,True
13181,"on the ﬁnal interpretation of the patient request. See Figure 5.
",False,ijerph-18-05686-v2,False,False,True
13182,"Int. J. Environ. Res. Public Health 2021 , 18, x  13 of 22 
",False,ijerph-18-05686-v2,False,False,True
13183," 
",False,ijerph-18-05686-v2,False,False,True
13184,"  
",False,ijerph-18-05686-v2,False,False,True
13185,"Figure 4. Coordination of activities among multip le actors within SeVa architecture. 
",False,ijerph-18-05686-v2,False,False,True
13186,"4.5. Agile Digital Platform to Suppor t Activity Coordination among Actors 
",False,ijerph-18-05686-v2,False,False,True
13187,"Since many of the components of the system are autonomous agents (or machine 
",False,ijerph-18-05686-v2,False,False,True
13188,"actors), the activity of each actor is discu ssed below. Some of these are running continu-
",False,ijerph-18-05686-v2,False,False,True
13189,"ally, and others are trigged based on new ev ents. Within the actor-network theory, this 
",False,ijerph-18-05686-v2,False,False,True
13190,"human–machine–other human (e.g.,  nurse) interaction has to occur throughout the entire 
",False,ijerph-18-05686-v2,False,False,True
13191,"value cycle and let the value-in-use feedback be used to support agility in the software 
",False,ijerph-18-05686-v2,False,False,True
13192,"ecosystem. To coordinate the activities, the system here functions as discussed below. 
",False,ijerph-18-05686-v2,False,False,True
13193,"Upon user registration and authentication, the user conversation can be initiated by 
",False,ijerph-18-05686-v2,False,False,True
13194,"hand waving, using the wake-up word, or simp ly touching the screen. Currently, a smart 
",False,ijerph-18-05686-v2,False,False,True
13195,"wristband detects the user movement and another sensor detects a patient’s wave gesture. 
",False,ijerph-18-05686-v2,False,False,True
13196,"Once an event is triggered, it will be routed to  the chatbot engine to initiate the conversa-
",False,ijerph-18-05686-v2,False,False,True
13197,"tion using the SeVA mobile application. 
",False,ijerph-18-05686-v2,False,False,True
13198,"The chatbot engine transforms the user’s voice into plain text, which is processed by 
",False,ijerph-18-05686-v2,False,False,True
13199,"the natural language processing (NLP ) module. Using discrete word pre-processing and 
",False,ijerph-18-05686-v2,False,False,True
13200,"tokenizing, it is sent to a recurrent neural  network (RNN) module. The output of the RNN 
",False,ijerph-18-05686-v2,False,False,True
13201,"is matching the word candidates if the patient poses a directly interpretable request or a 
",False,ijerph-18-05686-v2,False,False,True
13202,"suggested intent score of the patient’s question. If it is an intent score that needs further 
",False,ijerph-18-05686-v2,False,False,True
13203,"interpretation, it is then sent to the dialogue  engine of the expert system module to decide 
",False,ijerph-18-05686-v2,False,False,True
13204,"on the final interpretation of the patient request. See Figure 5. 
",False,ijerph-18-05686-v2,False,False,True
13205,"The medical expert defines skills, such as interpreting the intent and trigger, and re-
",False,ijerph-18-05686-v2,False,False,True
13206,"plies. Once the intent matches with the skil ls, the system can begin the conversation au-
",False,ijerph-18-05686-v2,False,False,True
13207,"tonomously. The skill used to begin the next  conversation is derived from a knowledge 
",False,ijerph-18-05686-v2,False,False,True
13208,"base of skills, and this can be triggered either  by a directly interpreted patient request or 
",False,ijerph-18-05686-v2,False,False,True
13209,"extracted from the interpretation using both the RNN and expert knowledge. 
",False,ijerph-18-05686-v2,False,False,True
13210," 
",False,ijerph-18-05686-v2,False,False,True
13211,"Figure 5. Chatbot engine skill and story design. 
",False,ijerph-18-05686-v2,False,False,True
13212,"Figure 5. Chatbot engine skill and story design.
",False,ijerph-18-05686-v2,False,False,True
13213,"The medical expert deﬁnes skills, such as interpreting the intent and trigger, and
",False,ijerph-18-05686-v2,False,False,True
13214,"replies. Once the intent matches with the skills, the system can begin the conversation
",False,ijerph-18-05686-v2,False,False,True
13215,"autonomously. The skill used to begin the next conversation is derived from a knowledge
",False,ijerph-18-05686-v2,False,False,True
13216,"base of skills, and this can be triggered either by a directly interpreted patient request or
",False,ijerph-18-05686-v2,False,False,True
13217,"extracted from the interpretation using both the RNN and expert knowledge.
",False,ijerph-18-05686-v2,False,False,True
13218,"4.6. Agile Digital Platform to Support Information Sharing among Actors
",False,ijerph-18-05686-v2,False,False,True
13219,"The following ﬁve-layer framework is used to support the communication of data
",False,ijerph-18-05686-v2,False,False,True
13220,"among the actors in the value cycle: infrastructure layer, data layer, scenario layer, applica-
",False,ijerph-18-05686-v2,False,False,True
13221,"tion layer, and security layer. For more information on the infrastructure, see Appendix A.
",False,ijerph-18-05686-v2,False,False,True
13222,"The infrastructure layer provides the basic hardware requirement of the data collec-
",False,ijerph-18-05686-v2,False,False,True
13223,"tion unit. It includes the minimum requirement for deploying our system to a different
",False,ijerph-18-05686-v2,False,False,True
13224,"environment, as most of our system service is on the cloud.
",False,ijerph-18-05686-v2,False,False,True
13225,"The data layer processes the incoming raw data from the infrastructure layer. The
",False,ijerph-18-05686-v2,False,False,True
13226,"main tasks here are the text-to-speech conversion, speech-to-text conversion, and action
",False,ijerph-18-05686-v2,False,False,True
13227,"recognition, using a neural network as the backbone. The patient action is recognizedInt. J. Environ. Res. Public Health 2021 ,18, 5686 14 of 22
",False,ijerph-18-05686-v2,False,False,True
13228,"based on the temporal movement data, and a long-short-term memory neural network
",False,ijerph-18-05686-v2,False,False,True
13229,"processes this input. The output of this layer will be the conversation plain text and the
",False,ijerph-18-05686-v2,False,False,True
13230,"user action classiﬁed as a result.
",False,ijerph-18-05686-v2,False,False,True
13231,"The scenario layer contains the scenario provided to the professional medical expert
",False,ijerph-18-05686-v2,False,False,True
13232,"and its return as a conversation presented to the user. More speciﬁcally, the medical expert
",False,ijerph-18-05686-v2,False,False,True
13233,"predeﬁnes the scenario representation with the related incoming conversation plain text or
",False,ijerph-18-05686-v2,False,False,True
13234,"action class. This will lead to a conversation and initiates a trigger to instantiate the next
",False,ijerph-18-05686-v2,False,False,True
13235,"program or module in the application layer.
",False,ijerph-18-05686-v2,False,False,True
13236,"The application layer consists of the following two mobile applications: the SPR
",False,ijerph-18-05686-v2,False,False,True
13237,"application and the SMC application. The SPR application will work as the main interface
",False,ijerph-18-05686-v2,False,False,True
13238,"for the patient. The SMC app is designed for the medical staff to receive notiﬁcations from
",False,ijerph-18-05686-v2,False,False,True
13239,"the patient room and return quick feedback to the patient.
",False,ijerph-18-05686-v2,False,False,True
13240,"The security layer serves as the auxiliary component of the system as it is critical when-
",False,ijerph-18-05686-v2,False,False,True
13241,"ever the patient data are moved across networks. In this application, all the data collected
",False,ijerph-18-05686-v2,False,False,True
13242,"conform to the privacy policy with the consent of the user, and the data transmission and
",False,ijerph-18-05686-v2,False,False,True
13243,"storage is executed in an encrypted form. For system robustness, dual systems are used
",False,ijerph-18-05686-v2,False,False,True
13244,"in case of disruption, such as accidental break down of a server, and crash reports in the
",False,ijerph-18-05686-v2,False,False,True
13245,"application layer and system monitoring server are used to respond to any anomalies in
",False,ijerph-18-05686-v2,False,False,True
13246,"system behavior. The chat messages on the Internet are analyzed for author identiﬁcation
",False,ijerph-18-05686-v2,False,False,True
13247,"using machine learning algorithms [ 93], an intermediate security broker is used to improve
",False,ijerph-18-05686-v2,False,False,True
13248,"data privacy and security when communication occurs between the cloud and the user [ 94],
",False,ijerph-18-05686-v2,False,False,True
13249,"biometric-based authentication schemes are used to help protect patient privacy [ 95], an
",False,ijerph-18-05686-v2,False,False,True
13250,"IoT security framework is used to protect smart infrastructures from cyber-attacks [ 96],
",False,ijerph-18-05686-v2,False,False,True
13251,"and trusted data servers are used to protect user information using differentiated privacy
",False,ijerph-18-05686-v2,False,False,True
13252,"protocols [97].
",False,ijerph-18-05686-v2,False,False,True
13253,"In summary, all the client interactions are supported by the SeVA Patient Room
",False,ijerph-18-05686-v2,False,False,True
13254,"app (SPR) as shown in Figure 6, using an Apple built-in speech framework for both the
",False,ijerph-18-05686-v2,False,False,True
13255,"speech-to-text and text-to-speech conversion. It uses authorization and user conﬁguration
",False,ijerph-18-05686-v2,False,False,True
13256,"pages for tailoring value creating services. The value fulﬁlling activity coordination is
",False,ijerph-18-05686-v2,False,False,True
13257,"supported by the SeVA Master Control app (SMC) situated in a staff room where the nurses
",False,ijerph-18-05686-v2,False,False,True
13258,"are located. It monitors the client status from the messages received regularly from the
",False,ijerph-18-05686-v2,False,False,True
13259,"SPR, and provides the feedback from the nurses, if warranted. The SMC lets the nurses
",False,ijerph-18-05686-v2,False,False,True
13260,"monitor the status in multiple patient rooms and process incoming requests. The nurse can
",False,ijerph-18-05686-v2,False,False,True
13261,"send back acknowledgement, such as communicating critical movement information like
",False,ijerph-18-05686-v2,False,False,True
13262,"“patient is sitting up” on the textbox to direct timely nurse intervention.
",False,ijerph-18-05686-v2,False,False,True
13263,"Int. J. Environ. Res. Public Health 2021 , 18, x  15 of 22 
",False,ijerph-18-05686-v2,False,False,True
13264," 
",False,ijerph-18-05686-v2,False,False,True
13265,"  
",False,ijerph-18-05686-v2,False,False,True
13266,"Figure 6. SPR application user interface. 
",False,ijerph-18-05686-v2,False,False,True
13267,"5. Conclusions  
",False,ijerph-18-05686-v2,False,False,True
13268,"Returning to the research question posed in Section one, in building an agile digital 
",False,ijerph-18-05686-v2,False,False,True
13269,"platform to address population health within a client ecosystem, it is necessary to consider 
",False,ijerph-18-05686-v2,False,False,True
13270,"the following two areas: (1) understanding the services needed to create value and design-
",False,ijerph-18-05686-v2,False,False,True
13271,"ing digital services that leverage technologies accessible to clients to fulfill the value; and 
",False,ijerph-18-05686-v2,False,False,True
13272,"(2) designing an agile digital platform that can evolve with changing client needs by 
",False,ijerph-18-05686-v2,False,False,True
13273,"quickly reconfiguring the digital services needed  to fulfill the value. With the client con-
",False,ijerph-18-05686-v2,False,False,True
13274,"text changing continuously, th e real-time tracking of value-in-use is needed to support 
",False,ijerph-18-05686-v2,False,False,True
13275,"the reconfiguration of digital services using bo th advanced technologies, such as Internet 
",False,ijerph-18-05686-v2,False,False,True
13276,"of Things (IoT), NLP, and artificial intelligence tools such as neural networks, as well as 
",False,ijerph-18-05686-v2,False,False,True
13277,"human interaction and engagement (e.g., client  and nursing staff). The value of classifying 
",False,ijerph-18-05686-v2,False,False,True
13278,"service types to segment population group need s in creating value, modularization of ser-
",False,ijerph-18-05686-v2,False,False,True
13279,"vice modules for faster reconfiguration to fulfill value, and value-in-use feedback to assess 
",False,ijerph-18-05686-v2,False,False,True
13280,"both the depth and breadth of changes in a client context to create value for the next value 
",False,ijerph-18-05686-v2,False,False,True
13281,"cycle is proposed. While no digital platform can be agile enough to  support the evolving 
",False,ijerph-18-05686-v2,False,False,True
13282,"client intent, the case study uses questions as  a potential strategy to surface the evolving 
",False,ijerph-18-05686-v2,False,False,True
13283,"needs of the client. 
",False,ijerph-18-05686-v2,False,False,True
13284,"The digital platform developed to support delirium patients is currently being eval-
",False,ijerph-18-05686-v2,False,False,True
13285,"uated for its viability within a hospital setting , using a set of services considered critical 
",False,ijerph-18-05686-v2,False,False,True
13286,"to reduce patient falls. One of the goals of buil ding an agile digital platform is to be able 
",False,ijerph-18-05686-v2,False,False,True
13287,"to add new services where there is change in the scale (e.g., new services are needed when 
",False,ijerph-18-05686-v2,False,False,True
13288,"the patient moves from the hospital to home) or scope (e.g., the condition of a patient 
",False,ijerph-18-05686-v2,False,False,True
13289,"changes and needs the engagement of caregivers to respond to needs as opposed to pro-
",False,ijerph-18-05686-v2,False,False,True
13290,"fessional nurses in a hospital). While we can’t claim that the platform will support all fu-
",False,ijerph-18-05686-v2,False,False,True
13291,"ture changes, both the service modularization and flexible architecture used to create and 
",False,ijerph-18-05686-v2,False,False,True
13292,"fulfill value should help its adaptation when new information sharing and activity coor-dination is called for. Work is on-going in supporting such agility. 
",False,ijerph-18-05686-v2,False,False,True
13293,"  
",False,ijerph-18-05686-v2,False,False,True
13294,"Figure 6. SPR application user interface.Int. J. Environ. Res. Public Health 2021 ,18, 5686 15 of 22
",False,ijerph-18-05686-v2,False,False,True
13295,"5. Conclusions
",False,ijerph-18-05686-v2,False,False,True
13296,"Returning to the research question posed in Section one, in building an agile digital
",False,ijerph-18-05686-v2,False,False,True
13297,"platform to address population health within a client ecosystem, it is necessary to consider
",False,ijerph-18-05686-v2,False,False,True
13298,"the following two areas: (1) understanding the services needed to create value and de-
",False,ijerph-18-05686-v2,False,False,True
13299,"signing digital services that leverage technologies accessible to clients to fulﬁll the value;
",False,ijerph-18-05686-v2,False,False,True
13300,"and (2) designing an agile digital platform that can evolve with changing client needs
",False,ijerph-18-05686-v2,False,False,True
13301,"by quickly reconﬁguring the digital services needed to fulﬁll the value. With the client
",False,ijerph-18-05686-v2,False,False,True
13302,"context changing continuously, the real-time tracking of value-in-use is needed to support
",False,ijerph-18-05686-v2,False,False,True
13303,"the reconﬁguration of digital services using both advanced technologies, such as Internet
",False,ijerph-18-05686-v2,False,False,True
13304,"of Things (IoT), NLP , and artiﬁcial intelligence tools such as neural networks, as well as
",False,ijerph-18-05686-v2,False,False,True
13305,"human interaction and engagement (e.g., client and nursing staff). The value of classifying
",False,ijerph-18-05686-v2,False,False,True
13306,"service types to segment population group needs in creating value, modularization of
",False,ijerph-18-05686-v2,False,False,True
13307,"service modules for faster reconﬁguration to fulﬁll value, and value-in-use feedback to
",False,ijerph-18-05686-v2,False,False,True
13308,"assess both the depth and breadth of changes in a client context to create value for the
",False,ijerph-18-05686-v2,False,False,True
13309,"next value cycle is proposed. While no digital platform can be agile enough to support the
",False,ijerph-18-05686-v2,False,False,True
13310,"evolving client intent, the case study uses questions as a potential strategy to surface the
",False,ijerph-18-05686-v2,False,False,True
13311,"evolving needs of the client.
",False,ijerph-18-05686-v2,False,False,True
13312,"The digital platform developed to support delirium patients is currently being evalu-
",False,ijerph-18-05686-v2,False,False,True
13313,"ated for its viability within a hospital setting, using a set of services considered critical to
",False,ijerph-18-05686-v2,False,False,True
13314,"reduce patient falls. One of the goals of building an agile digital platform is to be able to
",False,ijerph-18-05686-v2,False,False,True
13315,"add new services where there is change in the scale (e.g., new services are needed when the
",False,ijerph-18-05686-v2,False,False,True
13316,"patient moves from the hospital to home) or scope (e.g., the condition of a patient changes
",False,ijerph-18-05686-v2,False,False,True
13317,"and needs the engagement of caregivers to respond to needs as opposed to professional
",False,ijerph-18-05686-v2,False,False,True
13318,"nurses in a hospital). While we can’t claim that the platform will support all future changes,
",False,ijerph-18-05686-v2,False,False,True
13319,"both the service modularization and ﬂexible architecture used to create and fulﬁll value
",False,ijerph-18-05686-v2,False,False,True
13320,"should help its adaptation when new information sharing and activity coordination is
",False,ijerph-18-05686-v2,False,False,True
13321,"called for. Work is on-going in supporting such agility.
",False,ijerph-18-05686-v2,False,False,True
13322,"6. Directions for Future Research
",False,ijerph-18-05686-v2,False,False,True
13323,"The unprecedented content from clients and partners, and the urgency to act quickly
",False,ijerph-18-05686-v2,False,False,True
13324,"to empower and engage clients in self-managing their care within their ecosystem, is
",False,ijerph-18-05686-v2,False,False,True
13325,"leading to the rapid digital transformation of healthcare organizations [ 98]. Such digital
",False,ijerph-18-05686-v2,False,False,True
13326,"transformation, as it is linked to performance on quality, convenience, and lower costs in
",False,ijerph-18-05686-v2,False,False,True
13327,"population health, may call for disruptive delivery models [ 99]. This means that adminis-
",False,ijerph-18-05686-v2,False,False,True
13328,"trative leadership models that have permeated healthcare organizations with a culture that
",False,ijerph-18-05686-v2,False,False,True
13329,"values long service, organized groups of teams and professionals motivated by personal
",False,ijerph-18-05686-v2,False,False,True
13330,"and professional standards, and external accountabilities [ 100], are no longer adequate.
",False,ijerph-18-05686-v2,False,False,True
13331,"Visionary leadership and organizational change have to complement administrative leader-
",False,ijerph-18-05686-v2,False,False,True
13332,"ship with enabling and adaptive leadership processes [ 101] to let innovative client-centric
",False,ijerph-18-05686-v2,False,False,True
13333,"and value-focused digital services support change incrementally and a culture of learning
",False,ijerph-18-05686-v2,False,False,True
13334,"cumulatively. A future research question is, “What agile digital services and the popula-
",False,ijerph-18-05686-v2,False,False,True
13335,"tion groups they service can help lead to sustained digital transformation that improves
",False,ijerph-18-05686-v2,False,False,True
13336,"performance and mitigates risks of all actors involved, including clients?”
",False,ijerph-18-05686-v2,False,False,True
13337,"The discussion and the case study argue for an agile digital platform that not only
",False,ijerph-18-05686-v2,False,False,True
13338,"brings together clients and clinical and non-clinical care providers, but also a mix of human
",False,ijerph-18-05686-v2,False,False,True
13339,"and machine actors using a set of automated and autonomous digital services dynamically
",False,ijerph-18-05686-v2,False,False,True
13340,"to create and fulﬁll value. Research on the actor-network theory [ 102] suggests that we
",False,ijerph-18-05686-v2,False,False,True
13341,"often fail to distinguish between the efﬁcacy of the network and the efﬁcacy of the processes
",False,ijerph-18-05686-v2,False,False,True
13342,"used in the formation of the network itself. We discussed the design of digital platforms to
",False,ijerph-18-05686-v2,False,False,True
13343,"make a network agile, where clinical and non-clinical actors used value cycle activities and
",False,ijerph-18-05686-v2,False,False,True
13344,"digital services to address the needs of the population groups. A future research question
",False,ijerph-18-05686-v2,False,False,True
13345,"is, “How can the digital platform expand its agility to redeﬁne the network itself?” For
",False,ijerph-18-05686-v2,False,False,True
13346,"example, can the digital platform used to support delirium patients learn from the client
",False,ijerph-18-05686-v2,False,False,True
13347,"questions their evolving intent, such as the need to reduce stress or help avoid isolationInt. J. Environ. Res. Public Health 2021 ,18, 5686 16 of 22
",False,ijerph-18-05686-v2,False,False,True
13348,"by adding new digital actors (e.g., a new digital service that autonomously delivers audio
",False,ijerph-18-05686-v2,False,False,True
13349,"or video content to suit the client’s tastes in music) or new human actors (e.g., add an
",False,ijerph-18-05686-v2,False,False,True
13350,"automated digital service that uses teleconsultation with a mental health counselor or a
",False,ijerph-18-05686-v2,False,False,True
13351,"social media interaction with others to exchange stories or play games)? Both of these call
",False,ijerph-18-05686-v2,False,False,True
13352,"for altering the process used to create the network, which in turn is used to create value.
",False,ijerph-18-05686-v2,False,False,True
13353,"Author Contributions: M.R.T., conceptualization, investigation, methodology, project administra-
",False,ijerph-18-05686-v2,False,False,True
13354,"tion, writing—original draft and review and editing; N.A., investigation, project administration,
",False,ijerph-18-05686-v2,False,False,True
13355,"resources, software, validation and visualization, review and editing; A.S., conceptualization, method-
",False,ijerph-18-05686-v2,False,False,True
13356,"ology, review and editing; S.H., investigation, methodology, resources and software. All authors
",False,ijerph-18-05686-v2,False,False,True
13357,"have read and agreed to the published version of the manuscript.
",False,ijerph-18-05686-v2,False,False,True
13358,"Funding: This research received funding from the University of Arizona Foundation.
",False,ijerph-18-05686-v2,False,False,True
13359,"Institutional Review Board Statement: Not applicable as the study discussed is a system designed
",False,ijerph-18-05686-v2,False,False,True
13360,"to support patients and does not involve human subject use and evaluation at this time.
",False,ijerph-18-05686-v2,False,False,True
13361,"Informed Consent Statement: Not Applicable.
",False,ijerph-18-05686-v2,False,False,True
13362,"Data Availability Statement: The authors conﬁrm that the data supporting the digital platform
",False,ijerph-18-05686-v2,False,False,True
13363,"development are available within the article and referenced material.
",False,ijerph-18-05686-v2,False,False,True
13364,"Acknowledgments: We want to thank the U of Arizona PHP 305 class students for generating
",False,ijerph-18-05686-v2,False,False,True
13365,"ideas on potential services that will address the needs of seniors living at home with various
",False,ijerph-18-05686-v2,False,False,True
13366,"health conditions.
",False,ijerph-18-05686-v2,False,False,True
13367,"Conﬂicts of Interest: The authors declare no conﬂict of interest.
",False,ijerph-18-05686-v2,False,False,True
13368,"Appendix A
",False,ijerph-18-05686-v2,False,False,True
13369,"Technologies
",False,ijerph-18-05686-v2,False,False,True
13370,"Mobile applications have used AI to support interaction with patients and providers.
",False,ijerph-18-05686-v2,False,False,True
13371,"Some are used to assistant a patient in improving their healthy lifestyle, providing medical
",False,ijerph-18-05686-v2,False,False,True
13372,"advice, or remotely monitoring a patient for diagnosis. A healthy lifestyle assistant applica-
",False,ijerph-18-05686-v2,False,False,True
13373,"tion is used for disease prevention by providing suggestions for healthy eating, physical
",False,ijerph-18-05686-v2,False,False,True
13374,"exercise, etc., (e.g., Pact Care provides a patient-centric healthcare data solution, Florence is
",False,ijerph-18-05686-v2,False,False,True
13375,"a chatbot based personal health assistant that provides medication reminders and tracks
",False,ijerph-18-05686-v2,False,False,True
13376,"health conditions, and AI chatbots are used for remote monitoring to provide nutrition
",False,ijerph-18-05686-v2,False,False,True
13377,"education and behavior change interventions) [103].
",False,ijerph-18-05686-v2,False,False,True
13378,"Remote monitoring services to fulﬁll value have been used to support client–provider
",False,ijerph-18-05686-v2,False,False,True
13379,"interactions for diagnosis, especially when there are limited medical resources within
",False,ijerph-18-05686-v2,False,False,True
13380,"a geographical region. These applications are using AI to assist patients in ﬁnding re-
",False,ijerph-18-05686-v2,False,False,True
13381,"mote doctors for diagnosis [ 104], engage in consultation based on a picture or text input
",False,ijerph-18-05686-v2,False,False,True
13382,"from mobile devices, assist patients to self-diagnose and triage for appropriate care using
",False,ijerph-18-05686-v2,False,False,True
13383,"chatbot consultations [ 105,106], as well as provide suggestions on generic medicine for
",False,ijerph-18-05686-v2,False,False,True
13384,"children [107].
",False,ijerph-18-05686-v2,False,False,True
13385,"With the advancement of natural language processing, value cycle interactions can
",False,ijerph-18-05686-v2,False,False,True
13386,"be made more effective as it allows for conversational knowledge to support the patient–
",False,ijerph-18-05686-v2,False,False,True
13387,"provider interaction in real time. For example, Microsoft’s healthcare bot is used to build
",False,ijerph-18-05686-v2,False,False,True
13388,"and deploy conversational medical experience at scale [ 108]. Combining this with natural
",False,ijerph-18-05686-v2,False,False,True
13389,"language capabilities, IBM’s question-answering Watson has been used as a “diagnosis
",False,ijerph-18-05686-v2,False,False,True
13390,"and treatment advisor” to assist physicians [109].
",False,ijerph-18-05686-v2,False,False,True
13391,"Architecture
",False,ijerph-18-05686-v2,False,False,True
13392,"This architecture provides a general methodology for building a chatbot-based health-
",False,ijerph-18-05686-v2,False,False,True
13393,"care system that uses real-time patient data as an input for diagnosis, and expert medical
",False,ijerph-18-05686-v2,False,False,True
13394,"knowledge as a resource to provide advice. The dark background blocks in Figure A1 repre-
",False,ijerph-18-05686-v2,False,False,True
13395,"sent the foundation or necessary utility of the technologies associated with the SeVa system.Int. J. Environ. Res. Public Health 2021 ,18, 5686 17 of 22
",False,ijerph-18-05686-v2,False,False,True
13396,"The infrastructure layer provides the basic hardware requirement of the data collec-
",False,ijerph-18-05686-v2,False,False,True
13397,"tion unit. It includes the minimum requirement for deploying our system to a different
",False,ijerph-18-05686-v2,False,False,True
13398,"environment, as most of our system service is on the cloud.
",False,ijerph-18-05686-v2,False,False,True
13399," The wireless network block works as a communication module for real-time data transmission;
",False,ijerph-18-05686-v2,False,False,True
13400," The mobile device, such as the tablet or cell phone, shows the user interface and
",False,ijerph-18-05686-v2,False,False,True
13401,"conducts the conversation. The peripheral sensor collects patient movement data to
",False,ijerph-18-05686-v2,False,False,True
13402,"infer the patient position status without infringing user privacy;
",False,ijerph-18-05686-v2,False,False,True
13403," The voice data and movement data will then be transferred to the upper layer.
",False,ijerph-18-05686-v2,False,False,True
13404,"Thedata layer processes the incoming raw data from the infrastructure layer. The main
",False,ijerph-18-05686-v2,False,False,True
13405,"task here is text-to-speech conversion, speech-to-text conversion, and action recognition,
",False,ijerph-18-05686-v2,False,False,True
13406,"using a neural network as the backbone. We use a neural network as the backbone to solve
",False,ijerph-18-05686-v2,False,False,True
13407,"the task, and RNN is considered appropriate for this step-by-step approach used to solve
",False,ijerph-18-05686-v2,False,False,True
13408,"the problem.
",False,ijerph-18-05686-v2,False,False,True
13409,"The scenario layer contains the scenario provided to the professional medical expert
",False,ijerph-18-05686-v2,False,False,True
13410,"and its return as a conversation presented to the user. A chatbot engine is used to support
",False,ijerph-18-05686-v2,False,False,True
13411,"the scenario representation.
",False,ijerph-18-05686-v2,False,False,True
13412,"The application layer consists of the following two mobile applications: the SPR
",False,ijerph-18-05686-v2,False,False,True
13413,"application and the SMC application. The SPR application will work as the main interface
",False,ijerph-18-05686-v2,False,False,True
13414,"for the patient. The SMC app is designed for the medical staff to receive notiﬁcations from
",False,ijerph-18-05686-v2,False,False,True
13415,"the patient room and return quick feedback to the patient.
",False,ijerph-18-05686-v2,False,False,True
13416,"The security layer serves as the auxiliary component of the system as it is critical
",False,ijerph-18-05686-v2,False,False,True
13417,"whenever patient data are moved across networks.
",False,ijerph-18-05686-v2,False,False,True
13418,"Int. J. Environ. Res. Public Health 2021 , 18, x  18 of 22 
",False,ijerph-18-05686-v2,False,False,True
13419," 
",False,ijerph-18-05686-v2,False,False,True
13420,"  
",False,ijerph-18-05686-v2,False,False,True
13421,"Figure A1. Communication of information supported  by the SeVa system framework. 
",False,ijerph-18-05686-v2,False,False,True
13422,"Additional Architectural Details to Support Patient Interaction 
",False,ijerph-18-05686-v2,False,False,True
13423,"Upon user registration, a lo gin request from the user is sent to the SeVA backend 
",False,ijerph-18-05686-v2,False,False,True
13424,"server for authentication. Upon authentication , the user conversation can be initiated by 
",False,ijerph-18-05686-v2,False,False,True
13425,"hand waving, using the wake-u p word, or simply touching the screen. The peripheral 
",False,ijerph-18-05686-v2,False,False,True
13426,"sensors monitor the patient’s status continuously and look for an event to occur before a response is triggered, such as user movement  and a patient’s wave gesture. Once an event 
",False,ijerph-18-05686-v2,False,False,True
13427,"is triggered, it will be routed to the chatbot engine to initiate the conversation using the 
",False,ijerph-18-05686-v2,False,False,True
13428,"SeVA mobile application. 
",False,ijerph-18-05686-v2,False,False,True
13429,"The chatbot engine transforms the user’s qu estion into plain text, which is processed 
",False,ijerph-18-05686-v2,False,False,True
13430,"by the NLP module and sent to an RNN modu le. The output of the RNN is a suggested 
",False,ijerph-18-05686-v2,False,False,True
13431,"intent score of the patient’s question. If it is  an intent score that needs further interpreta-
",False,ijerph-18-05686-v2,False,False,True
13432,"tion, it is then sent to the dialogue engine of the expert system module to decide on the 
",False,ijerph-18-05686-v2,False,False,True
13433,"final interpretation of the patient request. Th e medical expert defines skills, such as inter-
",False,ijerph-18-05686-v2,False,False,True
13434,"preting the intent and trigger, and replies. A skill is composed of inputs, slots, replies, 
",False,ijerph-18-05686-v2,False,False,True
13435,"actions, and stories, as discussed in Table 1 . The inputs define events that a bot can react 
",False,ijerph-18-05686-v2,False,False,True
13436,"to. The slots are the memory of the bot fo r remembering some information during the 
",False,ijerph-18-05686-v2,False,False,True
13437,"conversation. The replies are all the possible sentences that a bot can reply to a user. The 
",False,ijerph-18-05686-v2,False,False,True
13438,"stories define the logic behind a skill. In su mmary, the input is differentiated by the RNN, 
",False,ijerph-18-05686-v2,False,False,True
13439,"which has trained with the sample inputs. Once the intent matches with the skills, the 
",False,ijerph-18-05686-v2,False,False,True
13440,"system can begin the conversation autonomously . The skill used to begin the next conver-
",False,ijerph-18-05686-v2,False,False,True
13441,"sation is derived from a knowledge base of sk ills, and this can be triggered either by a 
",False,ijerph-18-05686-v2,False,False,True
13442,"directly interpreted patient request or extrac ted from the interpretation using both the 
",False,ijerph-18-05686-v2,False,False,True
13443,"RNN and expert knowledge.  
",False,ijerph-18-05686-v2,False,False,True
13444,"The SeVA backend server processes all the incoming requests and manages the user 
",False,ijerph-18-05686-v2,False,False,True
13445,"data. It centrally coordinates activities, such  as user account management, and uses APIs 
",False,ijerph-18-05686-v2,False,False,True
13446,"to connect the mobile application, database, and monitoring server. It uses a monitoring 
",False,ijerph-18-05686-v2,False,False,True
13447,"server along with the backend server to trac k API availability, the number of actors en-
",False,ijerph-18-05686-v2,False,False,True
13448,"gaged in interaction, and the current system status. It tracks abnormal behaviors, such as backend server API unavailability or nurse unavailability, for online interaction and re-
",False,ijerph-18-05686-v2,False,False,True
13449,"ports them to the appropriate st akeholders through an email. 
",False,ijerph-18-05686-v2,False,False,True
13450,"References 
",False,ijerph-18-05686-v2,False,False,True
13451,"1. McKinsey Digital. Rethinking Customer Journeys with the Next-Generation Operating Model ; McKinsey & Company: New York, 
",False,ijerph-18-05686-v2,False,False,True
13452,"NY, USA, 2018. 
",False,ijerph-18-05686-v2,False,False,True
13453,"2. Vargo, S.L.; Lusch, R.F. Why “service”? J. Acad. Mark. Sci. 2008 , 36, 25–38. 
",False,ijerph-18-05686-v2,False,False,True
13454,"Figure A1. Communication of information supported by the SeVa system framework.
",False,ijerph-18-05686-v2,False,False,True
13455,"Additional Architectural Details to Support Patient Interaction
",False,ijerph-18-05686-v2,False,False,True
13456,"Upon user registration, a login request from the user is sent to the SeVA backend
",False,ijerph-18-05686-v2,False,False,True
13457,"server for authentication. Upon authentication, the user conversation can be initiated by
",False,ijerph-18-05686-v2,False,False,True
13458,"hand waving, using the wake-up word, or simply touching the screen. The peripheral
",False,ijerph-18-05686-v2,False,False,True
13459,"sensors monitor the patient’s status continuously and look for an event to occur before a
",False,ijerph-18-05686-v2,False,False,True
13460,"response is triggered, such as user movement and a patient’s wave gesture. Once an event
",False,ijerph-18-05686-v2,False,False,True
13461,"is triggered, it will be routed to the chatbot engine to initiate the conversation using the
",False,ijerph-18-05686-v2,False,False,True
13462,"SeVA mobile application.
",False,ijerph-18-05686-v2,False,False,True
13463,"The chatbot engine transforms the user’s question into plain text, which is processed
",False,ijerph-18-05686-v2,False,False,True
13464,"by the NLP module and sent to an RNN module. The output of the RNN is a suggestedInt. J. Environ. Res. Public Health 2021 ,18, 5686 18 of 22
",False,ijerph-18-05686-v2,False,False,True
13465,"intent score of the patient’s question. If it is an intent score that needs further interpretation,
",False,ijerph-18-05686-v2,False,False,True
13466,"it is then sent to the dialogue engine of the expert system module to decide on the ﬁnal
",False,ijerph-18-05686-v2,False,False,True
13467,"interpretation of the patient request. The medical expert deﬁnes skills, such as interpreting
",False,ijerph-18-05686-v2,False,False,True
13468,"the intent and trigger, and replies. A skill is composed of inputs, slots, replies, actions, and
",False,ijerph-18-05686-v2,False,False,True
13469,"stories, as discussed in Table 1. The inputs deﬁne events that a bot can react to. The slots
",False,ijerph-18-05686-v2,False,False,True
13470,"are the memory of the bot for remembering some information during the conversation. The
",False,ijerph-18-05686-v2,False,False,True
13471,"replies are all the possible sentences that a bot can reply to a user. The stories deﬁne the
",False,ijerph-18-05686-v2,False,False,True
13472,"logic behind a skill. In summary, the input is differentiated by the RNN, which has trained
",False,ijerph-18-05686-v2,False,False,True
13473,"with the sample inputs. Once the intent matches with the skills, the system can begin the
",False,ijerph-18-05686-v2,False,False,True
13474,"conversation autonomously. The skill used to begin the next conversation is derived from
",False,ijerph-18-05686-v2,False,False,True
13475,"a knowledge base of skills, and this can be triggered either by a directly interpreted patient
",False,ijerph-18-05686-v2,False,False,True
13476,"request or extracted from the interpretation using both the RNN and expert knowledge.
",False,ijerph-18-05686-v2,False,False,True
13477,"The SeVA backend server processes all the incoming requests and manages the user
",False,ijerph-18-05686-v2,False,False,True
13478,"data. It centrally coordinates activities, such as user account management, and uses APIs
",False,ijerph-18-05686-v2,False,False,True
13479,"to connect the mobile application, database, and monitoring server. It uses a monitoring
",False,ijerph-18-05686-v2,False,False,True
13480,"server along with the backend server to track API availability, the number of actors engaged
",False,ijerph-18-05686-v2,False,False,True
13481,"in interaction, and the current system status. It tracks abnormal behaviors, such as backend
",False,ijerph-18-05686-v2,False,False,True
13482,"server API unavailability or nurse unavailability, for online interaction and reports them to
",False,ijerph-18-05686-v2,False,False,True
13483,"the appropriate stakeholders through an email.
",False,ijerph-18-05686-v2,False,False,True
13484,"References
",False,ijerph-18-05686-v2,False,False,True
13485,"1. McKinsey Digital. Rethinking Customer Journeys with the Next-Generation Operating Model ; McKinsey & Company: New York, NY,
",False,ijerph-18-05686-v2,False,False,True
13486,"USA, 2018.
",False,ijerph-18-05686-v2,False,False,True
13487,"2. Vargo, S.L.; Lusch, R.F. Why “service”? J. Acad. Mark. Sci. 2008 ,36, 25–38. [CrossRef]
",False,ijerph-18-05686-v2,False,False,True
13488,"3. Aghena, W.; De Smet, A.; Weerda, K. Agility: It Rhymes with Stability. McKinsey Quarterly. 2015. Available online: https:
",False,ijerph-18-05686-v2,False,False,True
13489,"//www.mckinsey.com/business-functions/organization/our-insights/agility-it-rhymes-with-stability (accessed on 1 December
",False,ijerph-18-05686-v2,False,False,True
13490,"2015).
",False,ijerph-18-05686-v2,False,False,True
13491,"4. Saini, V .; Tanniru, M.; Liang, T.P . E-relationship strategies in digital age using value lens. In Proceedings of the 2020 International
",False,ijerph-18-05686-v2,False,False,True
13492,"Conference on Service Science and Innovation, Hsinchu, Taiwan, 15–17 October 2020; p. 33.
",False,ijerph-18-05686-v2,False,False,True
13493,"5. Bharadwaj, A.; El Sawy, O.A.; Pavlou, P .A.; Venkatraman, N. Digital business strategy: Toward a next generation of insights. MIS
",False,ijerph-18-05686-v2,False,False,True
13494,"Q.2013 ,37, 471–482. [CrossRef]
",False,ijerph-18-05686-v2,False,False,True
13495,"6. Trattner, A.; Hvam, L.; Forza, C.; Herbert-Hansen, Z.N.L. Product complexity and operational performance: A systematic
",False,ijerph-18-05686-v2,False,False,True
13496,"literature review. CIRP J. Manuf. Sci. Technol. 2019 ,25, 69–83. [CrossRef]
",False,ijerph-18-05686-v2,False,False,True
13497,"7. Rodr /acute.ts1Iguez, P .; Haghighatkhah, A.; Lwakatare, L.E.; Teppola, S.; Suomalainen, T.; Eskeli, J.; Karvonen, T.; Kuvaja, P .; Verner, J.M.;
",False,ijerph-18-05686-v2,False,False,True
13498,"Oivo, M. Continuous deployment of software intensive products and services: A systematic mapping study. J. Syst. Softw. 2017 ,
",False,ijerph-18-05686-v2,False,False,True
13499,"123, 263–291. [CrossRef]
",False,ijerph-18-05686-v2,False,False,True
13500,"123, 263–291. [CrossRef]
",False,ijerph-18-05686-v2,False,False,True
13501,"Abstract
",True,iNNvestigate Neural Networks!,False,False,True
13502,"1. Introduction
",True,iNNvestigate Neural Networks!,False,False,True
13503,"2iNNvestigate Neural Networks!
",True,iNNvestigate Neural Networks!,False,False,True
13504,"2. Library
",True,iNNvestigate Neural Networks!,False,False,True
13505,"1 import i n n v e s t i g a t e
",True,iNNvestigate Neural Networks!,False,False,True
13506,"2 model = c r e a t e ak e r a s m o d e l ( )
",True,iNNvestigate Neural Networks!,False,False,True
13507,"4 analyzer . f i t ( X train ) # i f needed
",True,iNNvestigate Neural Networks!,False,False,True
13508,"3Alber et al.
",True,iNNvestigate Neural Networks!,False,False,True
13509,"2.1. Details
",True,iNNvestigate Neural Networks!,False,False,True
13510,"3. Conclusion
",True,iNNvestigate Neural Networks!,False,False,True
13511,"4iNNvestigate Neural Networks!
",True,iNNvestigate Neural Networks!,False,False,True
13512,"5Alber et al.
",True,iNNvestigate Neural Networks!,False,False,True
13513,"Journal of Machine Learning Research 20 (2019) 1-8 Submitted 8/18; Revised 5/19; Published 5/19
",False,iNNvestigate Neural Networks!,False,False,True
13514,"iNNvestigate Neural Networks!
",False,iNNvestigate Neural Networks!,False,False,True
13515,"Maximilian Alber maximilian.alber@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13516,"Technische Universit at Berlin, Machine Learning Group
",False,iNNvestigate Neural Networks!,False,False,True
13517,"10623 Berlin, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13518,"Sebastian Lapuschkin sebastian.lapuschkin@hhi.fraunhofer.de
",False,iNNvestigate Neural Networks!,False,False,True
13519,"Fraunhofer Heinrich Hertz Institute, Video Coding and Analytics
",False,iNNvestigate Neural Networks!,False,False,True
13520,"10587 Berlin, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13521,"Philipp Seegerer philipp.seegerer@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13522,"Miriam H agele haegele@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13523,"Kristof T. Sch utt kristof.schuett@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13524,"Gr egoire Montavon gregoire.montavon@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13525,"Technische Universit at Berlin, Machine Learning Group
",False,iNNvestigate Neural Networks!,False,False,True
13526,"10623 Berlin, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13527,"Wojciech Samek wojciech.samek@hhi.fraunhofer.de
",False,iNNvestigate Neural Networks!,False,False,True
13528,"Fraunhofer Heinrich Hertz Institute, Video Coding and Analytics
",False,iNNvestigate Neural Networks!,False,False,True
13529,"10587 Berlin, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13530,"Klaus-Robert M uller klaus-robert.mueller@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13531,"Technische Universit at Berlin, Machine Learning Group
",False,iNNvestigate Neural Networks!,False,False,True
13532,"10623 Berlin, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13533,"Korea University, Department of Brain and Cognitive Engineering
",False,iNNvestigate Neural Networks!,False,False,True
13534,"Seoul 02841, Korea
",False,iNNvestigate Neural Networks!,False,False,True
13535,"Max Planck Institute for Informatics
",False,iNNvestigate Neural Networks!,False,False,True
13536,"66123 Saarbr ucken, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13537,"Sven D ahne sven.daehne@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13538,"Pieter-Jan Kindermans p.kindermans@tu-berlin.de
",False,iNNvestigate Neural Networks!,False,False,True
13539,"Technische Universit at Berlin, Machine Learning Group
",False,iNNvestigate Neural Networks!,False,False,True
13540,"10623 Berlin, Germany
",False,iNNvestigate Neural Networks!,False,False,True
13541,"Editor: Alexandre Gramfort
",False,iNNvestigate Neural Networks!,False,False,True
13542,"In recent years, deep neural networks have revolutionized many application domains of
",False,iNNvestigate Neural Networks!,False,False,True
13543,"machine learning and are key components of many critical decision or predictive processes.
",False,iNNvestigate Neural Networks!,False,False,True
13544,"Therefore, it is crucial that domain specialists can understand and analyze actions and pre-
",False,iNNvestigate Neural Networks!,False,False,True
13545,"dictions, even of the most complex neural network architectures. Despite these arguments
",False,iNNvestigate Neural Networks!,False,False,True
13546,"neural networks are often treated as black boxes. In the attempt to alleviate this short-
",False,iNNvestigate Neural Networks!,False,False,True
13547,"coming many analysis methods were proposed, yet the lack of reference implementations
",False,iNNvestigate Neural Networks!,False,False,True
13548,"often makes a systematic comparison between the methods a major eort. The presented
",False,iNNvestigate Neural Networks!,False,False,True
13549,"library iNNvestigate addresses this by providing a common interface and out-of-the-
",False,iNNvestigate Neural Networks!,False,False,True
13550,"box implementation for many analysis methods, including the reference implementation
",False,iNNvestigate Neural Networks!,False,False,True
13551,"for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the
",False,iNNvestigate Neural Networks!,False,False,True
13552,"versatility of iNNvestigate , we provide an analysis of image classications for variety of
",False,iNNvestigate Neural Networks!,False,False,True
13553,"state-of-the-art neural network architectures.
",False,iNNvestigate Neural Networks!,False,False,True
13554,"c
",False,iNNvestigate Neural Networks!,False,False,True
13555,"2019 Maximilian Alber, Sebastian Lapuschkin, Philipp Seegerer, Miriam H agele, Kristof T. Sch utt, Gr egoire
",False,iNNvestigate Neural Networks!,False,False,True
13556,"Montavon, Wojciech Samek, Klaus-Robert M uller, Sven D ahne, Pieter-Jan Kindermans.
",False,iNNvestigate Neural Networks!,False,False,True
13557,"License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/ . Attribution requirements are provided
",False,iNNvestigate Neural Networks!,False,False,True
13558,"athttp://jmlr.org/papers/v20/18-540.html .Alber et al.
",False,iNNvestigate Neural Networks!,False,False,True
13559,"Keywords: Articial neural networks, deep learning, analyzing classiers, explaining
",False,iNNvestigate Neural Networks!,False,False,True
13560,"classiers, computer vision
",False,iNNvestigate Neural Networks!,False,False,True
13561,"1. Introduction
",False,iNNvestigate Neural Networks!,False,False,True
13562,"In recent years deep neural networks have revolutionized many domains, e.g., image recog-
",False,iNNvestigate Neural Networks!,False,False,True
13563,"nition, speech recognition, speech synthesis, and knowledge discovery (Krizhevsky et al.,
",False,iNNvestigate Neural Networks!,False,False,True
13564,"2012; LeCun et al., 2012; Schmidhuber, 2015; LeCun et al., 2015; Van Den Oord et al.,
",False,iNNvestigate Neural Networks!,False,False,True
13565,"2016). Due to their ability to naturally learn from structured data and exhibit superior
",False,iNNvestigate Neural Networks!,False,False,True
13566,"performance, they are increasingly used in practical applications and critical decision pro-
",False,iNNvestigate Neural Networks!,False,False,True
13567,"cesses, such as novel knowledge discovery techniques, autonomous driving or medical image
",False,iNNvestigate Neural Networks!,False,False,True
13568,"analysis. To fully leverage their potential it is essential that users can comprehend and ana-
",False,iNNvestigate Neural Networks!,False,False,True
13569,"lyzethese processes. E.g., in neural architecture (Zoph et al., 2018) or chemical compound
",False,iNNvestigate Neural Networks!,False,False,True
13570,"searches (Montavon et al., 2013; Sch utt et al., 2017) it would be extremely useful to know
",False,iNNvestigate Neural Networks!,False,False,True
13571,"which properties help a neural network to choose appropriate candidates. Furthermore for
",False,iNNvestigate Neural Networks!,False,False,True
13572,"some applications understanding the decision process might be a legal requirement.
",False,iNNvestigate Neural Networks!,False,False,True
13573,"Despite these arguments neural networks are often treated as black boxes, because their
",False,iNNvestigate Neural Networks!,False,False,True
13574,"complex internal workings and the basis for their predictions are not fully understood. In
",False,iNNvestigate Neural Networks!,False,False,True
13575,"the attempt to alleviate this shortcoming several methods were proposed, e.g., Saliency
",False,iNNvestigate Neural Networks!,False,False,True
13576,"Map (Baehrens et al., 2010; Simonyan et al., 2013), SmoothGrad (Smilkov et al., 2017), In-
",False,iNNvestigate Neural Networks!,False,False,True
13577,"tegratedGradients (Sundararajan et al., 2017), Deconvnet (Zeiler and Fergus, 2014), Guid-
",False,iNNvestigate Neural Networks!,False,False,True
13578,"edBackprop (Springenberg et al., 2015), PatternNet and PatternAttribution (Kindermans
",False,iNNvestigate Neural Networks!,False,False,True
13579,"et al., 2018), LRP (Bach et al., 2015; Lapuschkin et al., 2016a,b, 2019; Montavon et al.,
",False,iNNvestigate Neural Networks!,False,False,True
13580,"2018), and DeepTaylor (Montavon et al., 2017). Theoretically it is not clear which method
",False,iNNvestigate Neural Networks!,False,False,True
13581,"solves the stated problems best, therefore an empirical comparison is required (Samek et al.,
",False,iNNvestigate Neural Networks!,False,False,True
13582,"2017; Kindermans et al., 2017). In order to evaluate these methods, we present iNNvesti-
",False,iNNvestigate Neural Networks!,False,False,True
13583,"gate which provides a common interface to a variety of analysis methods.
",False,iNNvestigate Neural Networks!,False,False,True
13584,"In particular, iNNvestigate contributes:
",False,iNNvestigate Neural Networks!,False,False,True
13585,"A common interface for a growing number of analysis methods that is applicable to a
",False,iNNvestigate Neural Networks!,False,False,True
13586,"broad class of neural networks. With this instantiating a method is as uncomplicated
",False,iNNvestigate Neural Networks!,False,False,True
13587,"as passing a trained neural network to it and allows for easy qualitative comparisons of
",False,iNNvestigate Neural Networks!,False,False,True
13588,"methods. For quantitative evaluations of (image) classication task we further provide
",False,iNNvestigate Neural Networks!,False,False,True
13589,"an implementation of the method \perturbation analysis"" (Samek et al., 2017).
",False,iNNvestigate Neural Networks!,False,False,True
13590,"Support of all methods listed above|this includes the rst reference implementation
",False,iNNvestigate Neural Networks!,False,False,True
13591,"for PatternNet and PatternAttribution and an extended implementation for LRP|
",False,iNNvestigate Neural Networks!,False,False,True
13592,"and an open source repository for further contributions.
",False,iNNvestigate Neural Networks!,False,False,True
13593,"A clean and modular implementation, casting each analysis in terms of layer-wise
",False,iNNvestigate Neural Networks!,False,False,True
13594,"forward and backward computations. This limits code redundancy, takes advantage
",False,iNNvestigate Neural Networks!,False,False,True
13595,"of automatic dierentiation, and eases future integration of new methods.
",False,iNNvestigate Neural Networks!,False,False,True
13596,"iNNvestigate is available at repository: https://github.com/albermax/innvestigate .
",False,iNNvestigate Neural Networks!,False,False,True
13597,"It can be simply installed as Python package and contains documentation for code and ap-
",False,iNNvestigate Neural Networks!,False,False,True
13598,"plications. To demonstrate the versatility of iNNvestigate we provide examples for the
",False,iNNvestigate Neural Networks!,False,False,True
13599,"analysis of image classications for a variety of state-of-the-art neural networks.
",False,iNNvestigate Neural Networks!,False,False,True
13600,"2iNNvestigate Neural Networks!
",False,iNNvestigate Neural Networks!,False,False,True
13601,"Terminology: The dierent methods pose dierent assumption to tasks and are designed
",False,iNNvestigate Neural Networks!,False,False,True
13602,"for dierent objectives, yet they are related to \explaining"" or \interpreting"" neural net-
",False,iNNvestigate Neural Networks!,False,False,True
13603,"works, see Montavon et al. (2018). We actively refrain from using this terminology in order
",False,iNNvestigate Neural Networks!,False,False,True
13604,"to prevent misunderstandings between the design choices of the algorithms and the implicit
",False,iNNvestigate Neural Networks!,False,False,True
13605,"assumption these terms bring along. Therefore we will solely use the neutral term analyzing
",False,iNNvestigate Neural Networks!,False,False,True
13606,"and leave any interpretation to the user.
",False,iNNvestigate Neural Networks!,False,False,True
13607,"2. Library
",False,iNNvestigate Neural Networks!,False,False,True
13608,"Interface: The main feature is a common interface to several analysis methods. The
",False,iNNvestigate Neural Networks!,False,False,True
13609,"work
",False,iNNvestigate Neural Networks!,False,False,True
13610,"ow is as simple as passing a Keras neural network model to instantiate an analyzer
",False,iNNvestigate Neural Networks!,False,False,True
13611,"object for a desired algorithm. Then, if needed, the analyzer will be tted to the data and
",False,iNNvestigate Neural Networks!,False,False,True
13612,"eventually be used to analyze the model's predictions. The corresponding Python code is:
",False,iNNvestigate Neural Networks!,False,False,True
13613,"1 import i n n v e s t i g a t e
",False,iNNvestigate Neural Networks!,False,False,True
13614,"2 model = c r e a t e ak e r a s m o d e l ( )
",False,iNNvestigate Neural Networks!,False,False,True
13615,"3 analyzer = i n n v e s t i g a t e . c r e a t e a n a l y z e r ( ` ` analyzer name ' ' , model )
",False,iNNvestigate Neural Networks!,False,False,True
13616,"4 analyzer . f i t ( X train ) # i f needed
",False,iNNvestigate Neural Networks!,False,False,True
13617,"5 a n a l y s i s = analyzer . analyze ( X test )
",False,iNNvestigate Neural Networks!,False,False,True
13618,"Implemented methods: At publication time the following algorithms are supported: Gra-
",False,iNNvestigate Neural Networks!,False,False,True
13619,"dient Saliency Map, SmoothGrad, IntegratedGradients, Deconvnet, GuidedBackprop, Pat-
",False,iNNvestigate Neural Networks!,False,False,True
13620,"ternNet and PatternAttribution, DeepTaylor, and LRP including LRP-Z, -Epsilon, -AlphaBeta.
",False,iNNvestigate Neural Networks!,False,False,True
13621,"In contrast, current related work is limited to gradient- and perturbation-based methods
",False,iNNvestigate Neural Networks!,False,False,True
13622,"(Kotikalapudi and contributors, 2017; Ancona et al., 2018) or focuses on a single algorithm
",False,iNNvestigate Neural Networks!,False,False,True
13623,"(E.g., Lundberg and Lee, 2017; Ribeiro et al., 2016). For an overview see Alber (2019). We
",False,iNNvestigate Neural Networks!,False,False,True
13624,"intend to extend this selection and invite the community to contribute implementations as
",False,iNNvestigate Neural Networks!,False,False,True
13625,"new methods emerge.
",False,iNNvestigate Neural Networks!,False,False,True
13626,"Documentation: The library's documentation contains several introductory scripts and ex-
",False,iNNvestigate Neural Networks!,False,False,True
13627,"ample applications. We demonstrate how the analyses can be applied to the following state-
",False,iNNvestigate Neural Networks!,False,False,True
13628,"of-the-art models: VGG16 (Simonyan and Zisserman, 2014), InceptionV3 (Szegedy et al.,
",False,iNNvestigate Neural Networks!,False,False,True
13629,"2016), ResNet50 (He et al., 2016), InceptionResNetV2 (Szegedy et al., 2017), DenseNet
",False,iNNvestigate Neural Networks!,False,False,True
13630,"(Huang et al., 2017), NASNet (Zoph et al., 2018). Figure 1 shows the result for a subset.
",False,iNNvestigate Neural Networks!,False,False,True
13631,"network: vgg16
",False,iNNvestigate Neural Networks!,False,False,True
13632,"pred: baseball
",False,iNNvestigate Neural Networks!,False,False,True
13633,"Input
",False,iNNvestigate Neural Networks!,False,False,True
13634," Gradient
",False,iNNvestigate Neural Networks!,False,False,True
13635," SmoothGrad
",False,iNNvestigate Neural Networks!,False,False,True
13636," Guided Backprop
",False,iNNvestigate Neural Networks!,False,False,True
13637," PatternNet
",False,iNNvestigate Neural Networks!,False,False,True
13638," Input * Gradient
",False,iNNvestigate Neural Networks!,False,False,True
13639," Integrated Gradients
",False,iNNvestigate Neural Networks!,False,False,True
13640," LRP-Epsilon
",False,iNNvestigate Neural Networks!,False,False,True
13641," LRP-PresetA
",False,iNNvestigate Neural Networks!,False,False,True
13642," DeepTaylor
",False,iNNvestigate Neural Networks!,False,False,True
13643," PatternAttribution
",False,iNNvestigate Neural Networks!,False,False,True
13644,"logit: 17.28
",False,iNNvestigate Neural Networks!,False,False,True
13645,"prob: 0.54
",False,iNNvestigate Neural Networks!,False,False,True
13646,"network: inception_v3
",False,iNNvestigate Neural Networks!,False,False,True
13647,"pred: baseball
",False,iNNvestigate Neural Networks!,False,False,True
13648,"logit: 8.61
",False,iNNvestigate Neural Networks!,False,False,True
13649,"prob: 0.59
",False,iNNvestigate Neural Networks!,False,False,True
13650,"network: resnet50
",False,iNNvestigate Neural Networks!,False,False,True
13651,"pred: baseball
",False,iNNvestigate Neural Networks!,False,False,True
13652,"logit: 10.05
",False,iNNvestigate Neural Networks!,False,False,True
13653,"prob: 0.44
",False,iNNvestigate Neural Networks!,False,False,True
13654,"network: nasnet_large
",False,iNNvestigate Neural Networks!,False,False,True
13655,"pred: baseball
",False,iNNvestigate Neural Networks!,False,False,True
13656,"logit: 9.86
",False,iNNvestigate Neural Networks!,False,False,True
13657,"prob: 0.94
",False,iNNvestigate Neural Networks!,False,False,True
13658,"Figure 1: Result of methods applied to various neural networks (blank, if a method does
",False,iNNvestigate Neural Networks!,False,False,True
13659,"not support a network's architecture).
",False,iNNvestigate Neural Networks!,False,False,True
13660,"3Alber et al.
",False,iNNvestigate Neural Networks!,False,False,True
13661,"2.1. Details
",False,iNNvestigate Neural Networks!,False,False,True
13662,"Modular implementation: All of the methods have in common that they perform a back-
",False,iNNvestigate Neural Networks!,False,False,True
13663,"propagation from the model outputs to the inputs. The core of iNNvestigate is a set of
",False,iNNvestigate Neural Networks!,False,False,True
13664,"base classes and functions that is designed to allow for rapid and easy development of such
",False,iNNvestigate Neural Networks!,False,False,True
13665,"algorithms. The developer only needs to implement specic changes to the base algorithm
",False,iNNvestigate Neural Networks!,False,False,True
13666,"and the library will take care of the complex and error-prone handling of the propagation
",False,iNNvestigate Neural Networks!,False,False,True
13667,"along the graph structure. Further details can be found in the repositories documentation.
",False,iNNvestigate Neural Networks!,False,False,True
13668,"Another advantage of the modular design is that one can extend any analyzer with
",False,iNNvestigate Neural Networks!,False,False,True
13669,"a given set of wrappers. One application of this is the smoothing of the analysis results
",False,iNNvestigate Neural Networks!,False,False,True
13670,"by adding Gaussian noise to the copies of the input and averaging the outcome. E.g.,
",False,iNNvestigate Neural Networks!,False,False,True
13671,"SmoothGrad is realized in this way by combining a smoothing wrapper with a gradient
",False,iNNvestigate Neural Networks!,False,False,True
13672,"analyzer.
",False,iNNvestigate Neural Networks!,False,False,True
13673,"Training: PatternNet and PatternAttribution (Kindermans et al., 2018) are two novel
",False,iNNvestigate Neural Networks!,False,False,True
13674,"approaches that condition their analysis on the data distribution. This is done by identifying
",False,iNNvestigate Neural Networks!,False,False,True
13675,"the signal and noise direction for each neuron of a neural network. Our software scales
",False,iNNvestigate Neural Networks!,False,False,True
13676,"favorably, e.g., one can train required patterns for the methods on large data sets like
",False,iNNvestigate Neural Networks!,False,False,True
13677,"Imagenet (Deng et al., 2009) in less than an hour using one GPU. We present the rst
",False,iNNvestigate Neural Networks!,False,False,True
13678,"reference implementation of these methods.
",False,iNNvestigate Neural Networks!,False,False,True
13679,"Quantitative evaluation: Often analysis methods for neural networks are compared by
",False,iNNvestigate Neural Networks!,False,False,True
13680,"qualitative (visual) inspection of the result. This is can lead to subjective evaluations and
",False,iNNvestigate Neural Networks!,False,False,True
13681,"one approach to create a more objective and quantitative comparison of analysis algorithms
",False,iNNvestigate Neural Networks!,False,False,True
13682,"is the method \perturbation analysis"" (Samek et al., 2017, also known as \PixelFlipping"").
",False,iNNvestigate Neural Networks!,False,False,True
13683,"The intuition behind this method is that perturbing regions which are recognized as im-
",False,iNNvestigate Neural Networks!,False,False,True
13684,"portant for the classication task by the analyzing method, will impact the classication
",False,iNNvestigate Neural Networks!,False,False,True
13685,"most. This allows to assess which analysis method best identies regions that matter for a
",False,iNNvestigate Neural Networks!,False,False,True
13686,"specic task and neural network. iNNvestigate contains an implementation of this method.
",False,iNNvestigate Neural Networks!,False,False,True
13687,"Installation & license: iNNvestigate is published as open-source software under the BSD-
",False,iNNvestigate Neural Networks!,False,False,True
13688,"2-license and can be downloaded from: https://github.com/albermax/innvestigate . It
",False,iNNvestigate Neural Networks!,False,False,True
13689,"is built as a Python 2 or 3 application on top of the popular and established Keras (Chollet
",False,iNNvestigate Neural Networks!,False,False,True
13690,"et al., 2015) framework. This allows to use the library on various platforms and devices
",False,iNNvestigate Neural Networks!,False,False,True
13691,"like CPUs and GPUs. At the time of publication only the TensorFlow (Abadi et al., 2016)
",False,iNNvestigate Neural Networks!,False,False,True
13692,"Keras-backend is supported. The library can be simply installed as Python package.
",False,iNNvestigate Neural Networks!,False,False,True
13693,"3. Conclusion
",False,iNNvestigate Neural Networks!,False,False,True
13694,"We have presented iNNvestigate , a library that makes it easier to analyze neural networks'
",False,iNNvestigate Neural Networks!,False,False,True
13695,"predictions and to compare dierent analysis methods. This is done by providing a common
",False,iNNvestigate Neural Networks!,False,False,True
13696,"interface and implementations for many analysis methods as well as making tools for training
",False,iNNvestigate Neural Networks!,False,False,True
13697,"and comparing methods available. In particular it contains reference implementations for
",False,iNNvestigate Neural Networks!,False,False,True
13698,"many methods (PatternNet, PatternAttribution, LRP) and example application for a large
",False,iNNvestigate Neural Networks!,False,False,True
13699,"number of state-of-the-art applications. We expect that this library will support the eld of
",False,iNNvestigate Neural Networks!,False,False,True
13700,"analyzing machine learning and facilitate research using neural networks in domains such
",False,iNNvestigate Neural Networks!,False,False,True
13701,"as drug design or medical image analysis.
",False,iNNvestigate Neural Networks!,False,False,True
13702,"4iNNvestigate Neural Networks!
",False,iNNvestigate Neural Networks!,False,False,True
13703,"Acknowledgments
",False,iNNvestigate Neural Networks!,False,False,True
13704,"Correspondence to MA, SL, KRM, WS and PJK. This work was supported by the Fed-
",False,iNNvestigate Neural Networks!,False,False,True
13705,"eral Ministry of Education and Research (BMBF) for the Berlin Big Data Center BBDC
",False,iNNvestigate Neural Networks!,False,False,True
13706,"(01IS14013A). Additional support was provided by the BK21 program funded by Korean
",False,iNNvestigate Neural Networks!,False,False,True
13707,"National Research Foundation grant (No. 2012-005741) and the Institute for Information
",False,iNNvestigate Neural Networks!,False,False,True
13708,"& Communications Technology Promotion (IITP) grant funded by the Korea government
",False,iNNvestigate Neural Networks!,False,False,True
13709,"(no. 2017-0-00451, No. 2017-0-01779).
",False,iNNvestigate Neural Networks!,False,False,True
13710,"5Alber et al.
",False,iNNvestigate Neural Networks!,False,False,True
13711,"References
",False,iNNvestigate Neural Networks!,False,False,True
13712,"Mart n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jerey Dean,
",False,iNNvestigate Neural Networks!,False,False,True
13713,"Matthieu Devin, Sanjay Ghemawat, Georey Irving, Michael Isard, et al. Tensor
",False,iNNvestigate Neural Networks!,False,False,True
13714,"ow: a
",False,iNNvestigate Neural Networks!,False,False,True
13715,"system for large-scale machine learning. Proceedings of the 11th USENIX Symposium on
",False,iNNvestigate Neural Networks!,False,False,True
13716,"Operating Systems Design and Implementation , 16:265{283, 2016.
",False,iNNvestigate Neural Networks!,False,False,True
13717,"Maximilian Alber. Software and application patterns for explanation methods. In Inter-
",False,iNNvestigate Neural Networks!,False,False,True
13718,"pretable AI: Interpreting, Explaining and Visualizing Deep Learning . Springer, 2019.
",False,iNNvestigate Neural Networks!,False,False,True
13719,"Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better under-
",False,iNNvestigate Neural Networks!,False,False,True
13720,"standing of gradient-based attribution methods for deep neural networks. In International
",False,iNNvestigate Neural Networks!,False,False,True
13721,"Conference on Learning Representations , 2018.
",False,iNNvestigate Neural Networks!,False,False,True
13722,"Sebastian Bach, Alexander Binder, Gr egoire Montavon, Frederick Klauschen, Klaus-Robert
",False,iNNvestigate Neural Networks!,False,False,True
13723,"M uller, and Wojciech Samek. On pixel-wise explanations for non-linear classier decisions
",False,iNNvestigate Neural Networks!,False,False,True
13724,"by layer-wise relevance propagation. PLOS ONE , 10(7):1{46, 2015.
",False,iNNvestigate Neural Networks!,False,False,True
13725,"David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen,
",False,iNNvestigate Neural Networks!,False,False,True
13726,"and Klaus-Robert M uller. How to explain individual classication decisions. Journal of
",False,iNNvestigate Neural Networks!,False,False,True
13727,"Machine Learning Research , 11:1803{1831, 2010.
",False,iNNvestigate Neural Networks!,False,False,True
13728,"Fran cois Chollet et al. Keras. https://github.com/fchollet/keras , 2015.
",False,iNNvestigate Neural Networks!,False,False,True
13729,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
",False,iNNvestigate Neural Networks!,False,False,True
13730,"scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and
",False,iNNvestigate Neural Networks!,False,False,True
13731,"Pattern Recognition , pages 248{255, 2009.
",False,iNNvestigate Neural Networks!,False,False,True
13732,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
",False,iNNvestigate Neural Networks!,False,False,True
13733,"recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition ,
",False,iNNvestigate Neural Networks!,False,False,True
13734,"pages 770{778, 2016.
",False,iNNvestigate Neural Networks!,False,False,True
13735,"Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely
",False,iNNvestigate Neural Networks!,False,False,True
13736,"connected convolutional networks. In 2017 IEEE Conference on Computer Vision and
",False,iNNvestigate Neural Networks!,False,False,True
13737,"Pattern Recognition , pages 2261{2269, 2017.
",False,iNNvestigate Neural Networks!,False,False,True
13738,"Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch utt,
",False,iNNvestigate Neural Networks!,False,False,True
13739,"Sven D ahne, Dumitru Erhan, and Been Kim. The (un)reliability of saliency methods.
",False,iNNvestigate Neural Networks!,False,False,True
13740,"Neural Information Processing Systems 2017 - Interpreting, Explaining and Visualizing
",False,iNNvestigate Neural Networks!,False,False,True
13741,"Deep Learning - Now what? workshop , 2017.
",False,iNNvestigate Neural Networks!,False,False,True
13742,"Pieter-Jan Kindermans, Kristof T Sch utt, Maximilian Alber, Klaus-Robert M uller, Dumitru
",False,iNNvestigate Neural Networks!,False,False,True
13743,"Erhan, Been Kim, and Sven D ahne. Learning how to explain neural networks: PatternNet
",False,iNNvestigate Neural Networks!,False,False,True
13744,"and PatternAttribution. In International Conference on Learning Representations , 2018.
",False,iNNvestigate Neural Networks!,False,False,True
13745,"Raghavendra Kotikalapudi and contributors. keras-vis. https://github.com/raghakot/
",False,iNNvestigate Neural Networks!,False,False,True
13746,"keras-vis , 2017.
",False,iNNvestigate Neural Networks!,False,False,True
13747,"Alex Krizhevsky, Ilya Sutskever, and Georey E Hinton. Imagenet classication with deep
",False,iNNvestigate Neural Networks!,False,False,True
13748,"convolutional neural networks. In Advances in Neural Information Processing Systems
",False,iNNvestigate Neural Networks!,False,False,True
13749,"25, pages 1097{1105, 2012.
",False,iNNvestigate Neural Networks!,False,False,True
13750,"25, pages 1097{1105, 2012.
",False,iNNvestigate Neural Networks!,False,False,True
13751,"25, pages 1097{1105, 2012.
",False,iNNvestigate Neural Networks!,True,False,True
13752,"25, pages 1097{1105, 2012.
",False,iNNvestigate Neural Networks!,True,False,True
13753,"25, pages 1097{1105, 2012.
",False,iNNvestigate Neural Networks!,True,False,True
13754,"Abstract
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13755,"1 Introduction
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13756,"2 Dynamic Bayesian networks
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13757,"3 Models
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13758,"3.1 Conversation type
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13759,"3.2 Sentence length
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13760,"3.3 Part of speech tags
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13761,"4 Experiments
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13762,"5 Conclusion
",True,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13763,"See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/254762355
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13764,"Language Modeling With Dynamic Ba yesian Networks Using Conversation T ypes
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13765,"and Part of Speech Information
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13766,"Article  · Januar y 2010
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13767,"CITATIONS
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13768,"9READS
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13769,"125
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13770,"3 author s:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13771,"Some o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13772,"CoreSAEP: Comput ational R easoning f or Socially Adaptiv e Electr onic P artner s View pr oject
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13773,"Interactiv e Collabor ativ e Inf ormation Syst ems View pr oject
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13774,"Yang yang Shi
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13775,"Micr osoft , Sunnyv ale, Unit es St ates
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13776,"66 PUBLICA TIONS    1,268  CITATIONS    
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13777,"SEE PROFILE
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13778,"Pascal Wigg ers
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13779,"Amst erdam Univ ersity of Applied Scienc es/Centr e for Applied R esearch on Educ ation
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13780,"17 PUBLICA TIONS    194 CITATIONS    
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13781,"SEE PROFILE
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13782,"Catholijn M. Jonk er
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13783,"Delft Univ ersity of T echnolog y and L eiden Univ ersity
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13784,"620 PUBLICA TIONS    9,239  CITATIONS    
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13785,"SEE PROFILE
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13786,"All c ontent f ollo wing this p age was uplo aded b y Catholijn M. Jonk er on 22 Dec ember 2014.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13787,"The user has r equest ed enhanc ement of the do wnlo aded file.Language Modeling With Dynamic Bayesian
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13788,"Networks Using Conversation Types and Part of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13789,"Speech Information
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13790,"Yangyang Shi Pascal Wiggers Catholijn M. Jonker
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13791,"Delft University of Technology, 2628 CD Delft
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13792,"In this paper we investigate whether more accurate modeling of differences in language in different types
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13793,"of conversations, e.g. formal presentations vs. spontaneous conversations can improve the quality of a
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13794,"language model. We also investigate whether the modeling of sentence lengths can improve a language
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13795,"model. A language model is an important component of statistical natural language processing systems,
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13796,"such as automatic speech recognizers and spelling checkers, that judges the plausibility of sentence hy-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13797,"potheses. Standard language modeling approaches rely on statistics over word sequences. Our experi-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13798,"ments show that modeling the conversation type and part-of-speech tags sequences improves the language
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13799,"models, while modeling sentence length does not.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13800,"1 Introduction
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13801,"A recurring task in natural language processing is to judge whether a sequence of words constitutes a well-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13802,"formed sentence in a given language or similarly, which of a number of sentence hypotheses is syntactically
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13803,"and semantically most plausible.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13804,"An automatic speech recognizer for example, has to choose among sentence hypotheses based on acous-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13805,"tic evidence, while optical character recognition and handwriting recognition hypothesize sentences based
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13806,"on visual information [5, 10]. In spelling correction one wants to identify misspelled words [7] using the
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13807,"context provided by the sentence and in statistical machine translation the ﬂuency of sentences in the target
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13808,"language is rated [1].
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13809,"Statistical language models fulﬁll this task by assigning a probability to every word sequence in a lan-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13810,"guage. The idea is that some word sequences are much more likely than others because of syntactic, semantic
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13811,"and pragmatic constraints. There are multiple ways to deﬁne the probability of a sentence, but commonly
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13812,"the chain-rule of probability theory is applied to rephrase the task as assigning a conditional probability to
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13813,"every word in a sentence given the words preceding it:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13814,"P(w1,t) =P(w1)P(w2|w1)P(w3|w1w2). . . P (wn|w1,t−1), (1)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13815,"where wiis the i-th word in the sentence and w1,t=w1w2. . . w t. From this angle, language modeling can
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13816,"also be seen as predicting the next word in a sentence.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13817,"The most common language models, n-grams, are based directly on this idea and predict the next word
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13818,"using a limited history of npreceding words, where nis usually two or three. The advantage of n-gram
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13819,"models is that their parameters can be estimated reliably from a sample corpus. Despite their simplicity
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13820,"these models are surprisingly powerful because their locality makes them robust while at the same time they
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13821,"capture many of the local syntactic and semantic constraints in language.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13822,"However, much information that is potentially useful for language modeling gets lost in n-gram models.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13823,"For example, the assumption that the relative frequency of a word combination is the same for all conver-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13824,"sations is clearly incorrect. A word may be more likely in a particular conversation than on average (in a
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13825,"corpus) and far more likely in that conversation than in other conversations.(a) spontaneous
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13826," (b) news
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13827," (c) read
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13828,"Figure 1: Sentence length distributions for components of the CGN (Northern Dutch)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13829,"A second potential problem with the n-gram approach is that, by deﬁnition, it prefers short sentences
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13830,"over longer sentences. For spontaneous spoken language this assumption is generally correct but for more
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13831,"formal, written language it does not hold. To illustrate this point, Figure 1 shows the distributions over
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13832,"sentence lengths in different components of the Corpus Spoken Dutch ( CGN) [11].
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13833,"In the research presented in this paper, we investigated whether more accurate modeling of differences
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13834,"in word use among conversation types and of sentence length distributions results can improve language
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13835,"models. We designed six new language models, which we will explain in the section 3. We formulated
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13836,"these models in terms of dynamic Bayesian networks as those provide a natural generalization of statistical
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13837,"language models of n-grams.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13838,"Previous research [1, 4, 12] showed that language models can greatly beneﬁt from the inclusion of part-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13839,"of-speech ( POS) information, i.e. information on word categories such as verbs, nouns and adjectives and
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13840,"their relative positions, e.g. the fact that a determiner is often followed by an adjective or a noun. The
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13841,"distributions over POS-tags differ for different conversation types [13], therefore we also include POStags in
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13842,"some of our models.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13843,"We will present our new models in section 3 of this paper. Before that we will provide a brief background
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13844,"on Bayesian networks in the next section. Section 4 discusses the experiments we did to test the performance
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13845,"of the models.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13846,"2 Dynamic Bayesian networks
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13847,"Bayesian networks originate in artiﬁcial intelligence as a method for reasoning with uncertainty based on
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13848,"the formal rules of probability theory [9]. A Bayesian network represents the joint probability distribution
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13849,"over a set of random variables X=X1, X2. . . XN. It consists of two parts:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13850,"1. A directed acyclic graph ( DAG)G, i.e. a directed graph without any directed cycles. There exists a
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13851,"one to one mapping between the variables Xiin the domain and the nodes viofG. The directed arcs
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13852,"in the network represent the direct dependencies between variables. The absence of an arc between
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13853,"two nodes means that the variables corresponding to the nodes do not directly depend on each other.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13854,"2. A set of conditional probability distributions ( CPDs). With each variable Xia conditional probability
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13855,"distribution P(Xi|Pa(Xi))is associated, that quantiﬁes how Xidepends on Pa(Xi), the set of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13856,"variables represented by the parents of node viinGrepresenting Xi.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13857,"The probabilities are obtained from domain experts, learned from data or a combination of both. Applying
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13858,"the chain rule of probability theory and the independence assumptions made by the network, we can write the
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13859,"joint probability distribution represented by the network in factored form as a product of the local probability
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13860,"distributions:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13861,"P(X1, X2, . . . , XN) =N/productdisplay
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13862,"i=1P(Xi|Pa(Xi)). (2)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13863,"Inference in Bayesian networks is the process of calculating the probability of one or more random variables
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13864,"given some evidence, i.e. computing P(XQ|XE=xE)whereXQis a set of query variables and XEis
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13865,"a set of evidence variables. A number of efﬁcient inference algorithms that exploit the independence of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13866,"variables in a network exists.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13867,"Dynamic Bayesian networks ( DBNs) [3, 8] offer a concise way to model processes that evolve over time
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13868,"for which the number of time steps is not known beforehand. A DBN can be deﬁned by two BayesianFigure 2: DBN representation of an interpolated trigram (i-trigram)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13869,"networks: an a prior model P(X1)and a transition model that deﬁnes how the variables at a particular time
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13870,"depend on the nodes at the previous time steps:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13871,"P(Xt|Xt−1) =N/productdisplay
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13872,"i=1P(Xi
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13873,"t|Pa(Xi
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13874,"t)) (3)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13875,"wereXtis the set of variables at time tandXi
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13876,"tis the ith variable in time step t. The parents of a node can
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13877,"either be in the current or in a previous time slice. Typically, ﬁrst order Markov assumptions are made, i.e.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13878,"the nodes in a time slice only depend on the nodes in the previous time slice. The advantage of DBNs for
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13879,"language modeling is that one can deﬁne rich models without having to worry about the details of special
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13880,"purpose inference algorithms [12].
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13881,"3 Models
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13882,"As our baseline in this research we use an interpolated trigram language model, which is a common choice
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13883,"in language modeling, particularly for speech recognition. This model is based on a trigram, i.e. an n-gram
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13884,"withn= 3.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13885,"Even though the assumption that a word only depends on the previous two words is a crude approxima-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13886,"tion of the dependencies in language, it is already difﬁcult to obtain reliable estimates for the parameters
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13887,"of the model. This is especially true since language has a rather skewed distribution. To overcome this
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13888,"problem, language models are typically smoothed. One way to achieve this is by interpolation with lower
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13889,"order n-grams such as bigrams ( n= 2) and unigrams ( n= 1):
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13890,"Pint(wi|wi−1wi−2) =λ1P(wi|wi−1wi−2) +λ2P(wi|wi−1) +λ3P(wi), (4)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13891,"where λ1+λ2+λ3= 1.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13892,"Figure 2 shows the DBN representation of the standard interpolated trigram. In every time slice, it
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13893,"consists of a word variable Wthat depends on the previous two words and on the interpolation variable Λ
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13894,"that takes the three interpolation weights as its values. Its CPDis thus given by equation 3.3. Exceptions are
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13895,"made for the ﬁrst two words of a sentence. The Nvariable is a counter that indicates the number of words
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13896,"preceding a word in a sentence. Wis conditioned on Nsuch that the current word does not depend on any
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13897,"preceding words if N= 0 and if N= 1 it only depends on the previous word. The binary Evariables
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13898,"model the end of a sentence, while EOS models the end of a complete text. This last node is necessary to
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13899,"ensure that the model is a proper probability model in the sense that the sum of the probabilities over all
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13900,"possible texts is 1.Figure 3: Interpolated trigram with conversation type (i-trigram-c)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13901,"Figure 4: Interpolated trigram with sentence length (i-trigram-l)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13902,"3.1 Conversation type
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13903,"The sentence structure and vocabulary varies greatly with different conversation types. For example, spon-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13904,"taneous speech consist of short sentence with many interjections, adverbs, pronouns and incomplete, while
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13905,"in formal and read speech, longer, grammatically more complicated sentences are used which contain more
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13906,"nouns and determiners [13].
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13907,"To account for these effects, we added the conversation type as a variable in the model. In Figure 3 , the
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13908,"variable Cindicates the conversation type. By conditioning the word variable Won the conversation type,
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13909,"the model can dynamically adapt to the type of conversation. There are many ways in which the inﬂuence of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13910,"the conversation type on the words can be modeled. After some experimenting, we chose to include a term
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13911,"P(wi|ci)in the interpolated word distribution:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13912,"Pint(wi|wi−1wi−2ci) =λ1P(wi|wi−1wi−2) +λ2P(wi|wi−1) +λ3P(wi) +λ4P(wi|ci). (5)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13913,"This formulation has the advantage that the probability of a conversation type is not reduced to zero as
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13914,"soon as the model encounters a word-conversation type combination that is not in the training data.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13915,"∞/summationdisplay
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13916,"n=1exp−6∗6n
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13917,"n!
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13918,"3.2 Sentence length
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13919,"Asn-gram language models assign probabilities to sentences by multiplying word probabilities, they typi-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13920,"cally prefer short sentences over long sentences. As was shown in Figure 1 this ﬁts well with the nature ofFigure 5: Interpolated trigram with conversation type and sentence length (i-trigram-l-c)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13921,"Figure 6: Interpolated trigram with part of speech (i-trigram-p)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13922,"spontaneous speech but not with more formal speech such a broadcast news or read speech.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13923,"Figure 4 shows an interpolated trigram in which sentence length is explicitly modeled. In this model L
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13924,"is a counter variable that counts the word positions within a sentence. The end of sentence variable Eis
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13925,"conditioned on this variable to ensure that the model will learn the distribution over the sentence lengths.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13926,"As mentioned above the distribution of sentence lengths varies greatly with different types of conversa-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13927,"tions. Therefore we created a model in which the sentence length distribution depends on the conversation
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13928,"type (Figure 5).
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13929,"3.3 Part of speech tags
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13930,"Including part of speech tags in a language model usually makes the model better [2] as it requires less data
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13931,"to ﬁnd reliable statistics on the combinations of POStags that can occur than on the combinations of words.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13932,"In addition, we can also relate the POStags to other variables in the model such as the end of the sentence
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13933,"and the conversation type, to make the prediction of the values of those variables more accurate.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13934,"Figure 6 shows a model that includes part-of-speech tags. Every part-of-speech depends on the previous
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13935,"two POS-tags, this allows the model to encode simple grammatical constructions. Like the word distribution,
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13936,"thePOStag distribution should be an interpolated distribution to ensure that the model will assign a non-zero
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13937,"probability to every sentence. Every word Wis restricted by its own POStagP. Since the part-of-speechFigure 7: Interpolated trigram with part of speech and conversation type(i-trigram-p-c)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13938,"is a property of a word, it is added a conditioning variable in each of the three components of the word
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13939,"distribution (Equation ).
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13940,"We created two other models. One is shown in Figure 7. It depicts a model with conversation types and
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13941,"part of speech. Each of these two factors affect the word respectively. The other model, which combines all
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13942,"three types information: conversation type, part of speech and sentence length. The inﬂuence between the
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13943,"three factors themselves and the inﬂuence of the three factors to other nodes in the model are the same in
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13944,"the models we discussed before.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13945,"4 Experiments
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13946,"To ﬁnd out whether our models can improve over standard interpolated trigrams, we trained and tested the
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13947,"models on the Corpus Spoken Dutch ( CGN). A corpus of standard Dutch as spoken in the Netherlands
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13948,"and Flanders. The corpus is based on collected audio recordings. For those experiments we used the tran-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13949,"scriptions of those audio fragments that are distributed with the corpus. The corpus is subdivided in 15
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13950,"components that contain different types of speech, ranging from spontaneous conversations to more formal
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13951,"speech. We used the following components:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13952,"•comp-d spontaneous telephone dialogues,
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13953,"•comp-f broadcasted interviews, discussions and debates
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13954,"•comp-h lessons recorded in a classroom
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13955,"•comp-k broadcasted news
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13956,"•comp-l broadcasted commentaries, columns and reviews
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13957,"•comp-m ceremonial speeches and sermons
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13958,"•comp-n lectures and seminars
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13959,"The set contains a total of 2843655 words 80% of which we use for training, 10% for development
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13960,"testing and tuning and the remaining 10% for evaluation. We created a vocabulary of 21865 words and 293
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13961,"different parts-of-speech. The vocabulary contains all unique words that occur more than once in the training
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13962,"data. All words in the data that are not in the vocabulary were replaced by a out-of-vocabulary token. Each
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13963,"of the seven sets listed above represents a conversation type. This implies that a complete document always
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13964,"has a single conversation type.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13965,"For training, the values of all variables were provided therefore we calculated all distributions using
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13966,"maximum likelihood estimation. The interpolation weights were trained on the development test set using
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13967,"the expectation-maximization algorithm.Table 1: Perplexity results on CGN components d,f,h,k,l,m,n
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13968,"models perplexity
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13969,"i-trigram 340.47
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13970,"i-trigram-l 344.32
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13971,"i-trigram-c 312.13
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13972,"i-trigram-l-c 317.34
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13973,"i-trigram-p 309.79
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13974,"i-trigram-p-c 304.37
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13975,"i-trigram-p-c-l 310.53
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13976,"We evaluated the models in terms of perplexity, a standard measure in language modeling which is based
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13977,"on the cross-entropy of the language model and a test data set (see for example [6]). The better the model
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13978,"ﬁts the data set, the lower the perplexity will be. Perplexity is calculated as:
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13979,"PP(w1,t) = 2−1
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13980,"tlogP(w1,t). (6)
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13981,"where P(w1,t)is the probability assigned by the model to the test data set. We assume independence of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13982,"individual documents, therefore the probabilities of the documents are multiplied to obtain this result.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13983,"We tested the performance of the all models discussed in the previous section on the held out evalua-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13984,"tion set. During this test only the words, the word positions and the punctuation (i.e. sentence ends) were
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13985,"provided to the model, all other variables were treated as hidden variables. Table 1 shows the resulting
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13986,"perplexities.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13987,"From these results we can see that explicitly modeling the conversation type does lower perplexity. As
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13988,"should be expected from similar research including POS-tags also improves the model. Combination of those
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13989,"factors results in the lowest perplexity we achieved, a reduction of slightly over 10%. Modeling sentence
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13990,"length on the other hand does not help. In all cases it actually slightly hurt performance.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13991,"5 Conclusion
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13992,"The statistical language models play a important role in natural language processing systems by making
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13993,"a judgement of the probability of sentences. In this paper we presented six new language models which
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13994,"not only focused on the statistics on the word sequences, but also considered the conversation type, part
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13995,"of speech tags, and sentence length which are not used in standard language models. The part of speech
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13996,"provides syntactic information of the speech and the 7 conversation types are forms such as spontaneous
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13997,"conversation and formal presentation. We implemented these new models as Dynamic Bayesian Networks
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13998,"and compared them with the standard trigram language model. The perplexity results of the experiments
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
13999,"show that the new language models with conversation type, with part of speech and with both of them
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14000,"improve upon the standard trigram by almost 8 %, 9%and 10 %, respectively. But the sentence length
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14001,"information did not contribute to improve the trigram language model. In fact, the results are not as good
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14002,"as those of the trigram language models. In the future we plan to investigate whether different interpolation
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14003,"schemes can further improve these models and introduce additional context information such as the topic of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14004,"a conversation.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14005,"References
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14006,"[1] Peter F. Brown, John Cocke, Stephen Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14007,"Lafferty, Robert L. Mercer, and Paul S. Roossin. A statistical approach to machine translation. Com-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14008,"putational Linguistics , 16(2):79–85, 1990.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14009,"[2] Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14010,"Class-based n-gram models of natural language. Computational Linguistics , 18(4):467–479, 1992.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14011,"[3] Thomas Dean and Keiji Kanazawa. A model for reasoning about persistence and causation. Compu-
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14012,"tational Intelligence , 5(3):142–150, 1989.[4] J. Goodman. A bit of progress in language modeling. Technical report, Microsoft Research, 56 Fuchun
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14013,"Peng, 2000.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14014,"[5] Jianying Hu, Michael K. Brown, and William Turin. HMM based on-line handwriting recognition.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14015,"IEEE Transactions on Pattern Analysis and Machine Intelligence , 18(10):1039–1045, October 1996.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14016,"[6] Daniel Jurafsky and James H. Martin. Speech and Language Processing - Second Edition . Prentice
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14017,"Hall, 2009.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14018,"[7] Eric Mays, Fred J. Damerau, and Robert L. Mercer. Context based spelling correction. Information
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14019,"Processing and Management , 27(5):517–522, 1991.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14020,"[8] Kevin Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning . PhD thesis,
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14021,"University of California, Berkeley, 2002.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14022,"[9] Judea Pearl. Probabilistic Reasoning in Intelligent Systems - Networks of Plausible Inference . Morgan
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14023,"Kaufmann Publishers, Inc., 1988.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14024,"[10] R Plamondon and S.N. Srihari. Online and off-line handwriting recognition: a comprehensive survey.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14025,"IEEE Transactions on Pattern Analysis and Machine Intelligence , 22:63–84, 2000.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14026,"[11] I. Schuurman, M. Schouppe, H. Hoekstra, and T. van der Wouden. CGN, an annotated corpus of
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14027,"spoken Dutch. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14028,"(LINC-03) , Budapest, Hungary, 14 April 2003.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14029,"[12] Pascal Wiggers and Leon J. M. Rothkrantz. Topic-based language modeling with dynamic Bayesian
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14030,"networks. In the Ninth International Conference on Spoken Language Processing (Interspeech 2006 -
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14031,"ICSLP) , pages 1866–1869, Pittsburgh, Pennsylvania, September 2006.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14032,"[13] Pascal Wiggers and Leon J. M. Rothkrantz. Exploratory analysis of word use and sentence length in
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14033,"the Spoken Dutch Corpus. In V ´aclav Matousek and Pavel Mautner, editors, Lecture notes in Artiﬁcial
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14034,"Intelligence 4629: Text, Speech and Dialogue 2007 , 2007.
",False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14035,View publication stats,False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,False,False,True
14036,View publication stats,False,Language_Modeling_With_Dynamic_Bayesian_Networks_U,True,False,True
14037,Abstract ,True,lfw_paper,False,False,False
14038,"I. I NTRODUCTION
",True,lfw_paper,False,False,False
14039,"II. R ELATED DATABASES
",True,lfw_paper,False,False,False
14040,"III. I NTENDED USES
",True,lfw_paper,False,False,False
14041,"IV. T RANSITIVITY AND THE IMAGE -RESTRICTED AND
",True,lfw_paper,False,False,False
14042,"V. T HEDETECTION -ALIGNMENT -RECOGNITION PIPELINE
",True,lfw_paper,False,False,False
14043,"VI. C ONSTRUCTION AND COMPOSITION DETAILS
",True,lfw_paper,False,False,False
14044,"VII. S UMMARY
",True,lfw_paper,False,False,False
14045,"1
",False,lfw_paper,False,False,False
14046,"Labeled Faces in the Wild: A Database for Studying
",False,lfw_paper,False,False,False
14047,"Face Recognition in Unconstrained Environments
",False,lfw_paper,False,False,False
14048,"Gary B. Huang,Manu Ramesh, Tamara Berg, and Erik Learned-Mi ller
",False,lfw_paper,False,False,False
14049,"many databases that have been produced to study it. Most of
",False,lfw_paper,False,False,False
14050,"these databases have been created under controlled conditi ons to
",False,lfw_paper,False,False,False
14051,"facilitate the study of speciﬁc parameters on the face recog nition
",False,lfw_paper,False,False,False
14052,"problem. These parameters include such variables as positi on,
",False,lfw_paper,False,False,False
14053,"pose, lighting, expression, background, camera quality, o cclusion,
",False,lfw_paper,False,False,False
14054,"age, and gender.
",False,lfw_paper,False,False,False
14055,"While there are many applications for face recognition tech nol-
",False,lfw_paper,False,False,False
14056,"ogy in which one can control the parameters of image acquisit ion,
",False,lfw_paper,False,False,False
14057,"there are also many applications in which the practitioner h as
",False,lfw_paper,False,False,False
14058,"little or no control over such parameters. This database is
",False,lfw_paper,False,False,False
14059,"provided as an aid in studying the latter, unconstrained, fa ce
",False,lfw_paper,False,False,False
14060,"recognition problem. The database represents an initial at tempt
",False,lfw_paper,False,False,False
14061,"to provide a set of labeled face photographs spanning the ran ge
",False,lfw_paper,False,False,False
14062,"of conditions typically encountered by people in their ever yday
",False,lfw_paper,False,False,False
14063,"lives. The database exhibits “natural” variability in pose , lighting,
",False,lfw_paper,False,False,False
14064,"focus, resolution, facial expression, age, gender, race, a ccessories,
",False,lfw_paper,False,False,False
14065,"make-up, occlusions, background, and photographic qualit y.
",False,lfw_paper,False,False,False
14066,"Despite this variability, the images in the database are pre sented
",False,lfw_paper,False,False,False
14067,"in a simple and consistent format for maximum ease of use.
",False,lfw_paper,False,False,False
14068,"In addition to describing the details of the database and
",False,lfw_paper,False,False,False
14069,"its acquisition, we provide speciﬁc experimental paradigm s for
",False,lfw_paper,False,False,False
14070,"which the database is suitable. This is done in an effort to
",False,lfw_paper,False,False,False
14071,"make research performed with the database as consistent and
",False,lfw_paper,False,False,False
14072,"comparable as possible.
",False,lfw_paper,False,False,False
14073,"I. I NTRODUCTION
",False,lfw_paper,False,False,False
14074,"This report describes a database of human face im-
",False,lfw_paper,False,False,False
14075,"ages designed as an aid in studying the problem of
",False,lfw_paper,False,False,False
14076,"unconstrained face recognition .1The database can be
",False,lfw_paper,False,False,False
14077,"viewed and downloaded at the following web address:
",False,lfw_paper,False,False,False
14078,"http://vis-www.cs.umass.edu/lfw/ .
",False,lfw_paper,False,False,False
14079,"Face recognition is the problem of identifying a speciﬁc
",False,lfw_paper,False,False,False
14080,"individual, rather than merely detecting the presence of a
",False,lfw_paper,False,False,False
14081,"human face, which is often called face detection . The general
",False,lfw_paper,False,False,False
14082,"term “face recognition” can refer to a number of different
",False,lfw_paper,False,False,False
14083,"problems including, but not limited to, the following.
",False,lfw_paper,False,False,False
14084,"1) Given two pictures, each of which contains a face,
",False,lfw_paper,False,False,False
14085,"decide whether the two people pictured represent the
",False,lfw_paper,False,False,False
14086,"same individual.
",False,lfw_paper,False,False,False
14087,"1We note that for more general classes of objects such as cars o r dogs, the
",False,lfw_paper,False,False,False
14088,"term “recognition” often refers to the problem of recognizi ng a member of the
",False,lfw_paper,False,False,False
14089,"larger class , rather than a speciﬁc instance. That is, when one “recogniz es”
",False,lfw_paper,False,False,False
14090,"a cat (in the context of computer vision research), it is mean t that one has
",False,lfw_paper,False,False,False
14091,"identiﬁed a particular object as a cat, rather than that one h as identiﬁed a
",False,lfw_paper,False,False,False
14092,"particular cat. In the context of more general objects, we pr efer the term
",False,lfw_paper,False,False,False
14093,"identiﬁcation to refer to the problem of recognizing a speciﬁc instance of a
",False,lfw_paper,False,False,False
14094,"class (such as Bob’s Toyota). For example, see the work by Fer encz et al. to
",False,lfw_paper,False,False,False
14095,"see examples of this usage [7], [8], [15]. However, in the lit erature on human
",False,lfw_paper,False,False,False
14096,"faces, the term recognition is typically used to refer to the identiﬁcation of a
",False,lfw_paper,False,False,False
14097,"particular individual, not just a human being. Since this re port is about faces,
",False,lfw_paper,False,False,False
14098,"we adopt this latter terminology here.2) Given a picture of a person’s face, decide whether it is
",False,lfw_paper,False,False,False
14099,"an example of a particular individual. This may be done
",False,lfw_paper,False,False,False
14100,"by comparing the face to a model for that individual or
",False,lfw_paper,False,False,False
14101,"to other pictures of the individual.
",False,lfw_paper,False,False,False
14102,"3) Given a picture of a face, decide which person from
",False,lfw_paper,False,False,False
14103,"among a set of people the picture represents, if any. (This
",False,lfw_paper,False,False,False
14104,"is often referred to as the face veriﬁcation paradigm.)
",False,lfw_paper,False,False,False
14105,"Our database, which we call Labeled Faces in the Wild (LFW),
",False,lfw_paper,False,False,False
14106,"is designed to address the ﬁrst of these problems, although i t
",False,lfw_paper,False,False,False
14107,"can be used to address the others if desired. We shall refer to
",False,lfw_paper,False,False,False
14108,"this problem as the pair matching problem.
",False,lfw_paper,False,False,False
14109,"The main motivation for the database, which is discussed
",False,lfw_paper,False,False,False
14110,"in more detail below, is to provide a large set of relatively
",False,lfw_paper,False,False,False
14111,"unconstrained face images. By unconstrained, we mean faces
",False,lfw_paper,False,False,False
14112,"that show a large range of the variation seen in everyday
",False,lfw_paper,False,False,False
14113,"life. This includes variation in pose, lighting, expressio n,
",False,lfw_paper,False,False,False
14114,"background, race, ethnicity, age, gender, clothing, hairs tyles,
",False,lfw_paper,False,False,False
14115,"camera quality, color saturation, focus, and other paramet ers.
",False,lfw_paper,False,False,False
14116,"Figures 1 and 2 show some examples of the database images.
",False,lfw_paper,False,False,False
14117,"The reason we are interested in natural variation is that we
",False,lfw_paper,False,False,False
14118,"are interested in the problem of pair matching given a pair of
",False,lfw_paper,False,False,False
14119,"pre-existing face images , i.e., images whose composition we
",False,lfw_paper,False,False,False
14120,"had no control over. We view this problem of unconstrained
",False,lfw_paper,False,False,False
14121,"pair matching as one of the most general and fundamental
",False,lfw_paper,False,False,False
14122,"face recognition problems.
",False,lfw_paper,False,False,False
14123,"Before proceeding with the details of the database, we
",False,lfw_paper,False,False,False
14124,"present some summary statistics and properties of the datab ase.
",False,lfw_paper,False,False,False
14125,"•The database contains 13,233 target face images. Some
",False,lfw_paper,False,False,False
14126,"images contain more than one face, but it is the face
",False,lfw_paper,False,False,False
14127,"that contains the central pixel of the image which is
",False,lfw_paper,False,False,False
14128,"considered the deﬁning face for the image. Faces other
",False,lfw_paper,False,False,False
14129,"than the target face should be ignored as “background”.
",False,lfw_paper,False,False,False
14130,"•The name of the person pictured in the center of the
",False,lfw_paper,False,False,False
14131,"image is given. Each person is given a unique name
",False,lfw_paper,False,False,False
14132,"(“George WBush” is the current U.S. president while
",False,lfw_paper,False,False,False
14133,"“George HW Bush” is the previous U.S. president), so
",False,lfw_paper,False,False,False
14134,"no name should correspond to more than one person, and
",False,lfw_paper,False,False,False
14135,"each individual should appear under no more than one
",False,lfw_paper,False,False,False
14136,"name (unless there are unknown errors in the database).
",False,lfw_paper,False,False,False
14137,"•The database contains images of 5749 different individ-
",False,lfw_paper,False,False,False
14138,"uals. Of these, 1680 people have two or more images
",False,lfw_paper,False,False,False
14139,"in the database. The remaining 4069 people have just a
",False,lfw_paper,False,False,False
14140,"single image in the database.
",False,lfw_paper,False,False,False
14141,"•The images are available as 250 by 250 pixel JPEG
",False,lfw_paper,False,False,False
14142,"images. Most images are in color, although a few are
",False,lfw_paper,False,False,False
14143,"grayscale only.
",False,lfw_paper,False,False,False
14144,"•All of the images are the result of detections by the2
",False,lfw_paper,False,False,False
14145,"Fig. 1. Matched pairs . These are the ﬁrst six matching pairs in the database
",False,lfw_paper,False,False,False
14146,"under View 1, as speciﬁed in the ﬁle pairsDevTrain.txt . These pairs
",False,lfw_paper,False,False,False
14147,"show a number of properties of the database. A person may appe ar in more
",False,lfw_paper,False,False,False
14148,"than one training pair (ﬁrst two rows). An image may have been cropped to
",False,lfw_paper,False,False,False
14149,"center the face (3rd row, right image) according to the Viola -Jones detector,
",False,lfw_paper,False,False,False
14150,"but the image has been padded with zeros to make it the same siz e as other
",False,lfw_paper,False,False,False
14151,"images.
",False,lfw_paper,False,False,False
14152,"Viola-Jones face detector [35], but have been rescaled
",False,lfw_paper,False,False,False
14153,"and cropped to a ﬁxed size (see Section VI for details).
",False,lfw_paper,False,False,False
14154,"After running the Viola-Jones detector on a large database
",False,lfw_paper,False,False,False
14155,"of images, false positive face detections were manually
",False,lfw_paper,False,False,False
14156,"eliminated, along with images for whom the name of the
",False,lfw_paper,False,False,False
14157,"individual could not be identiﬁed.
",False,lfw_paper,False,False,False
14158,"•We deﬁne two “Views” of the database, one for algorithmdevelopment, and one for performance reporting. By us-
",False,lfw_paper,False,False,False
14159,"ing View 1 for algorithm development, the experimenter
",False,lfw_paper,False,False,False
14160,"may avoid inappropriately ﬁtting a classiﬁer to the ﬁnal
",False,lfw_paper,False,False,False
14161,"test data. See Section III for details.
",False,lfw_paper,False,False,False
14162,"Additional details are given in the remainder of the report,
",False,lfw_paper,False,False,False
14163,"which is organized as follows. In Section II, we discuss othe r
",False,lfw_paper,False,False,False
14164,"databases, and the origins of Labeled Faces in the Wild. In
",False,lfw_paper,False,False,False
14165,"Section III, we describe the structure of the database and it s
",False,lfw_paper,False,False,False
14166,"intended use for the unconstrained pair matching problem. W e
",False,lfw_paper,False,False,False
14167,"focus particular attention on the proper use of the two datab ase
",False,lfw_paper,False,False,False
14168,"Views, which is critical for accurate measurement of classi ﬁer
",False,lfw_paper,False,False,False
14169,"generalization. In Section IV, we discuss two paradigms for
",False,lfw_paper,False,False,False
14170,"using training data, the image-restricted paradigm, and th e
",False,lfw_paper,False,False,False
14171,"unrestricted paradigm. Experimenters should be careful to
",False,lfw_paper,False,False,False
14172,"report which method is used when results are published. In
",False,lfw_paper,False,False,False
14173,"Section V, we discuss the role of LFW in the Detection-
",False,lfw_paper,False,False,False
14174,"Alignment-Recognition pipeline. In Section VI, we describ e
",False,lfw_paper,False,False,False
14175,"the construction of the database and details about resoluti on,
",False,lfw_paper,False,False,False
14176,"cropping, removal of duplicate images, and other propertie s.
",False,lfw_paper,False,False,False
14177,"II. R ELATED DATABASES
",False,lfw_paper,False,False,False
14178,"There are a large number of face databases available to
",False,lfw_paper,False,False,False
14179,"researchers in face recognition. A non-exhaustive list can
",False,lfw_paper,False,False,False
14180,"be found in Figure 3. These databases range in size, scope
",False,lfw_paper,False,False,False
14181,"and purpose. The photographs in many of these databases
",False,lfw_paper,False,False,False
14182,"were acquired by small teams of researchers speciﬁcally for
",False,lfw_paper,False,False,False
14183,"the purpose of studying face recognition. Acquisition of a
",False,lfw_paper,False,False,False
14184,"face database over a short time and in a particular location
",False,lfw_paper,False,False,False
14185,"has signiﬁcant advantages for certain types of research. Su ch
",False,lfw_paper,False,False,False
14186,"an acquisition gives the experimenter direct control over t he
",False,lfw_paper,False,False,False
14187,"parameters of variability in the database.
",False,lfw_paper,False,False,False
14188,"On the other hand, in order to study more general face
",False,lfw_paper,False,False,False
14189,"recognition problems, in which faces are drawn from a very
",False,lfw_paper,False,False,False
14190,"broad distribution, one may wish to train and test face recog -
",False,lfw_paper,False,False,False
14191,"nition algorithms on highly diverse sets of faces. While it
",False,lfw_paper,False,False,False
14192,"is possible to manipulate a large number of variables in
",False,lfw_paper,False,False,False
14193,"the laboratory in an attempt to make such a database, there
",False,lfw_paper,False,False,False
14194,"are two drawbacks to this approach. The ﬁrst is that it is
",False,lfw_paper,False,False,False
14195,"extremely labor intensive. The second is that it is difﬁcult
",False,lfw_paper,False,False,False
14196,"to gauge exactly which distributions of various parameters
",False,lfw_paper,False,False,False
14197,"one should use in order to make the most useful database.
",False,lfw_paper,False,False,False
14198,"What percentage of subjects should wear sunglasses? What
",False,lfw_paper,False,False,False
14199,"percentage should have beards? How many should be smiling?
",False,lfw_paper,False,False,False
14200,"How many backgrounds should contain cars, boats, grass,
",False,lfw_paper,False,False,False
14201,"deserts, or basketball courts?
",False,lfw_paper,False,False,False
14202,"One possible solution to this problem is simply to measure a
",False,lfw_paper,False,False,False
14203,"“natural” distribution of faces. Of course, no single canon ical
",False,lfw_paper,False,False,False
14204,"distribution of faces can capture a natural distribution of faces
",False,lfw_paper,False,False,False
14205,"that is valid across all possible application domains. Our
",False,lfw_paper,False,False,False
14206,"database uses a set of images that was originally gathered fr om
",False,lfw_paper,False,False,False
14207,"news articles on the web. This set clearly has its own biases.
",False,lfw_paper,False,False,False
14208,"For example, there are not many images which occur under
",False,lfw_paper,False,False,False
14209,"extreme lighting conditions, or very low lighting conditio ns.
",False,lfw_paper,False,False,False
14210,"Also, because we use the Viola-Jones detector as a ﬁlter for t he
",False,lfw_paper,False,False,False
14211,"database, there are a limited number of side views of faces, a nd
",False,lfw_paper,False,False,False
14212,"few views from above or below. But the range and diversity
",False,lfw_paper,False,False,False
14213,"of pictures present is very large. We believe such a database3
",False,lfw_paper,False,False,False
14214,"Fig. 2. Mismatched pairs . These are the ﬁrst six mismatched pairs in the
",False,lfw_paper,False,False,False
14215,"database under View 1, as speciﬁed in the ﬁle pairsDevTrain.txt .
",False,lfw_paper,False,False,False
14216,"will be an important tool in studying the unconstrained pair
",False,lfw_paper,False,False,False
14217,"matching problem.
",False,lfw_paper,False,False,False
14218,"While some other databases (such as the Caltech 10000
",False,lfw_paper,False,False,False
14219,"Web Faces [1]) also present highly diverse image sets, these
",False,lfw_paper,False,False,False
14220,"databases are not designed for face recognition, but rather for
",False,lfw_paper,False,False,False
14221,"face detection. We now discuss the origin for Labeled Faces
",False,lfw_paper,False,False,False
14222,"in the Wild and a number of related databases.
",False,lfw_paper,False,False,False
14223,"Faces in the Wild . The impetus for the Labeled Faces in
",False,lfw_paper,False,False,False
14224,"the Wild database grew out of work at Berkeley by Tamara
",False,lfw_paper,False,False,False
14225,"Berg, David Forsyth, and the computer vision group at UC
",False,lfw_paper,False,False,False
14226,"Berkeley [3], [4]. In this work, it was shown that a large,partially labeled, database of face images could be built by
",False,lfw_paper,False,False,False
14227,"using imperfect data gathered from the web. In particular, t he
",False,lfw_paper,False,False,False
14228,"Berg database of faces was built by jointly analyzing pictur es
",False,lfw_paper,False,False,False
14229,"and their associated captions to cluster images by identity . The
",False,lfw_paper,False,False,False
14230,"resulting data set, which achieved a labelling accuracy of 7 7%
",False,lfw_paper,False,False,False
14231,"[3], was informally referred to as the “Faces in the Wild” dat a
",False,lfw_paper,False,False,False
14232,"set.
",False,lfw_paper,False,False,False
14233,"However, since the database was not originally intended to
",False,lfw_paper,False,False,False
14234,"act as training and test data for new experiments, it contain ed
",False,lfw_paper,False,False,False
14235,"a high percentage of label errors and a high percentage of
",False,lfw_paper,False,False,False
14236,"duplicated images. As a result, various researchers derive d
",False,lfw_paper,False,False,False
14237,"ad hoc subsets of the database for new research projects
",False,lfw_paper,False,False,False
14238,"[14], [15], [25], [27]. It seemed that there would be sufﬁcie nt
",False,lfw_paper,False,False,False
14239,"interest in a clean version of the data set to warrant doing th e
",False,lfw_paper,False,False,False
14240,"job thoroughly and publishing a new database.
",False,lfw_paper,False,False,False
14241,"Before addressing the details of LFW, we discuss some of
",False,lfw_paper,False,False,False
14242,"the databases most closely related to it. While these databa ses
",False,lfw_paper,False,False,False
14243,"share some features with LFW, we believe that LFW represents
",False,lfw_paper,False,False,False
14244,"an important contribution to existing databases, especial ly for
",False,lfw_paper,False,False,False
14245,"studying the problem of unconstrained face recognition.
",False,lfw_paper,False,False,False
14246,"The Face Recognition Grand Challenge Databases [28].
",False,lfw_paper,False,False,False
14247,"The Face Recognition Grand Challenge (FRGC) was not
",False,lfw_paper,False,False,False
14248,"just a set of databases, but a carefully planned scientiﬁc
",False,lfw_paper,False,False,False
14249,"program designed to promote rigorous scientiﬁc analysis of
",False,lfw_paper,False,False,False
14250,"face recognition, fair comparison of face recognition tech -
",False,lfw_paper,False,False,False
14251,"nologies, and advances in face recognition research [28]. I t
",False,lfw_paper,False,False,False
14252,"represents the most comprehensive and scientiﬁcally rigor ous
",False,lfw_paper,False,False,False
14253,"study of face recognition to date. We applaud the organizers
",False,lfw_paper,False,False,False
14254,"and implementers of the FRGC, and believe that the FRGC,
",False,lfw_paper,False,False,False
14255,"along with earlier vendor tests, have been important motiva tors
",False,lfw_paper,False,False,False
14256,"and reality checks for the face recognition community. The
",False,lfw_paper,False,False,False
14257,"FRGC was successful in stimulating researchers (in both the
",False,lfw_paper,False,False,False
14258,"private sector and academia) to achieve certain milestones in
",False,lfw_paper,False,False,False
14259,"face recognition.
",False,lfw_paper,False,False,False
14260,"The goals of our research, and hence of our database, are
",False,lfw_paper,False,False,False
14261,"somewhat different from the goals of the FRGC. One of the
",False,lfw_paper,False,False,False
14262,"key differences is that the organizers of the FRGC wished to
",False,lfw_paper,False,False,False
14263,"study the effect of new, richer data types on the face recogni -
",False,lfw_paper,False,False,False
14264,"tion problem. The databases for the FRGC thus include high
",False,lfw_paper,False,False,False
14265,"resolution data, three-dimensional scans, and image seque nces
",False,lfw_paper,False,False,False
14266,"of each individual. (The databases contain more than 50,000
",False,lfw_paper,False,False,False
14267,"total recordings, including 3D scans and images.) Each of
",False,lfw_paper,False,False,False
14268,"these data types is potentially more informative than the si mple
",False,lfw_paper,False,False,False
14269,"and moderate resolution images of our database. While one of
",False,lfw_paper,False,False,False
14270,"the major goals of the FRGC was to study how higher ﬁdelity
",False,lfw_paper,False,False,False
14271,"data can help make face recognition more accurate, the goal o f
",False,lfw_paper,False,False,False
14272,"Labeled Faces in the Wild is to help study the problem of face
",False,lfw_paper,False,False,False
14273,"recognition using previously existing images , that is, images
",False,lfw_paper,False,False,False
14274,"that were not taken for the special purpose of face recogniti on
",False,lfw_paper,False,False,False
14275,"by machine. Thus, from the beginning we decided to build
",False,lfw_paper,False,False,False
14276,"our database from previously existing photographs that wer e
",False,lfw_paper,False,False,False
14277,"taken for other purposes.
",False,lfw_paper,False,False,False
14278,"Another important difference between the data sets associ-
",False,lfw_paper,False,False,False
14279,"ated with the FRGC and our data set is the general variety
",False,lfw_paper,False,False,False
14280,"of images. For example, while there are large numbers of
",False,lfw_paper,False,False,False
14281,"images with uncontrolled lighting in the FRGC data sets, the se
",False,lfw_paper,False,False,False
14282,"images contain a great deal less natural variation than the
",False,lfw_paper,False,False,False
14283,"LFW images. For example, the FRGC outdoor uncontrolled4
",False,lfw_paper,False,False,False
14284,"Database # of people Total images Highlights References
",False,lfw_paper,False,False,False
14285,"AR Face Database, Purdue
",False,lfw_paper,False,False,False
14286,"University, USA126 4000 frontal pose, expression, illumination, occlusions,
",False,lfw_paper,False,False,False
14287,"eye glasses, scarves[21]
",False,lfw_paper,False,False,False
14288,"AT&T Database (formerly
",False,lfw_paper,False,False,False
14289,"ORL Database)40 400 variation of time, lighting, facial expression, eye
",False,lfw_paper,False,False,False
14290,"glasses[29]
",False,lfw_paper,False,False,False
14291,"BioID Face Database 23 1521 real world conditions, gray scale, background,
",False,lfw_paper,False,False,False
14292,"lighting, expression, eye positions given[17]
",False,lfw_paper,False,False,False
14293,"Caltech Faces 27 450 lighting, expression, background
",False,lfw_paper,False,False,False
14294,"Caltech 10000 Web Faces ≈10000 10000 wide variability, facial features annotated [1]
",False,lfw_paper,False,False,False
14295,"CAS-PEAL Face Database 1040 99,594 very large, expression, accessories, lighting, si-
",False,lfw_paper,False,False,False
14296,"multaneous capture of multiple poses, Chinese[10]
",False,lfw_paper,False,False,False
14297,"Cohn-Kanade AU-Coded
",False,lfw_paper,False,False,False
14298,"Facial Expression Database100 500 sequences dynamic sequences of facial expressions [6]
",False,lfw_paper,False,False,False
14299,"EQUINOX HID Face
",False,lfw_paper,False,False,False
14300,"Database? ? non-visible light modalities
",False,lfw_paper,False,False,False
14301,"Face Video Database of the
",False,lfw_paper,False,False,False
14302,"Max Planck Institute for Bi-
",False,lfw_paper,False,False,False
14303,"ological Cybernetics? 246 video
",False,lfw_paper,False,False,False
14304,"sequences6 simulataneous viewpoints, carefully synchro-
",False,lfw_paper,False,False,False
14305,"nized, video data[18]
",False,lfw_paper,False,False,False
14306,"Facial Actions and Expres-
",False,lfw_paper,False,False,False
14307,"sions24 ≈7000 expression, color, grayscale
",False,lfw_paper,False,False,False
14308,"Face Recognition Grand
",False,lfw_paper,False,False,False
14309,"Challenge Databases>466 >50,000 images
",False,lfw_paper,False,False,False
14310,"and 3D scansvery large, lighting, expression, background, 3D,
",False,lfw_paper,False,False,False
14311,"sequences[28]
",False,lfw_paper,False,False,False
14312,"FERET Database, Color 1199 14126 color images, changes in appearance through
",False,lfw_paper,False,False,False
14313,"time, controlled pose variation, facial expression[23]
",False,lfw_paper,False,False,False
14314,"Georgia Tech Face Database 50 750 expression, illumination, scale, orientation [26]
",False,lfw_paper,False,False,False
14315,"Indian Face Database 40 >440 frontal, Indian subjects [16]
",False,lfw_paper,False,False,False
14316,"Japanese Female Facial Ex-
",False,lfw_paper,False,False,False
14317,"pression (JAFFE) Database10 213 rated for emotional content, female, Japanese [19]
",False,lfw_paper,False,False,False
14318,"MIT-CBCL Face Recogni-
",False,lfw_paper,False,False,False
14319,"tion Database10 >2000 synthetic images from 3D models, illumination,
",False,lfw_paper,False,False,False
14320,"pose, background[37]
",False,lfw_paper,False,False,False
14321,"M2VTS Multimodel Face
",False,lfw_paper,False,False,False
14322,"Database (Release 1.00)37 185 large pose changes, speaking subjects, eye
",False,lfw_paper,False,False,False
14323,"glasses, time change[30]
",False,lfw_paper,False,False,False
14324,"M2VTS, Extended, Univ. of
",False,lfw_paper,False,False,False
14325,"Surrey, UK295 1180 videos rotating head, speaking subjects, 3D models, high
",False,lfw_paper,False,False,False
14326,"quality images[22]
",False,lfw_paper,False,False,False
14327,"NIST Mugshot ID 1573 3248 front and side views [36]
",False,lfw_paper,False,False,False
14328,"NLPR Face Database ≈22 450 lighting, expression, backgrounds [24]
",False,lfw_paper,False,False,False
14329,"PIE Database, CMU 68 41368 very large database, pose, illumination, expres-
",False,lfw_paper,False,False,False
14330,"sion[33]
",False,lfw_paper,False,False,False
14331,"Psychological Image Col-
",False,lfw_paper,False,False,False
14332,"lection at Stirling (PICS)? ? targeted at psychology experiments [13]
",False,lfw_paper,False,False,False
14333,"UCD Colour Face Image
",False,lfw_paper,False,False,False
14334,"Database for Face Detection≈299 299 targeted at detection applications, highly varied,
",False,lfw_paper,False,False,False
14335,"color[32]
",False,lfw_paper,False,False,False
14336,"UMIST Face Database 20 564 pose, gender, race, grayscale [12]
",False,lfw_paper,False,False,False
14337,"University of Essex, UK 395 7900 racial diversity, eye glasses, beards, college age [34]
",False,lfw_paper,False,False,False
14338,"University of Oulu Physics-
",False,lfw_paper,False,False,False
14339,"Based Face Database125 >2000 highly varied illumination, eye glasses [20]
",False,lfw_paper,False,False,False
14340,"V ALID Database 106 530 highly variable ofﬁce conditions [9]
",False,lfw_paper,False,False,False
14341,"VidTIMIT Database 43 multiple videos
",False,lfw_paper,False,False,False
14342,"per personvideo, audio, reading, head rotation [31]
",False,lfw_paper,False,False,False
14343,"Yale Face Database 15 165 expressions, eye glasses, lighting [2]
",False,lfw_paper,False,False,False
14344,"Yale Face Database B 10 5760 pose, illumination [11]
",False,lfw_paper,False,False,False
14345,"Fig. 3. Face databases. This table shows some of the face data bases available at the time of writing. This list is not meant to be exhaustive, nor to describe
",False,lfw_paper,False,False,False
14346,"the databases in detail, but merely to provide a sampling of t he types of databases that are available. Where possible, a p eer-reviewed paper or technical report
",False,lfw_paper,False,False,False
14347,"was cited, and otherwise a citation referring to the web page for the database is given when available. Much of the informa tion on this page was gathered
",False,lfw_paper,False,False,False
14348,"with the help of the excellent “Face Recognition Homepage,” maintained by Mislav Grgic and Kresimir Delac ( http://www.face-rec.org/ ).5
",False,lfw_paper,False,False,False
14349,"lighting images contain two images of each subject, one
",False,lfw_paper,False,False,False
14350,"smiling and one with a neutral expression. The LFW images,
",False,lfw_paper,False,False,False
14351,"in contrast contain arbitrary expressions. Variation in cl othing,
",False,lfw_paper,False,False,False
14352,"pose, background, and other variables is much greater in LFW
",False,lfw_paper,False,False,False
14353,"than in the FRGC databases. One may sum up the differences
",False,lfw_paper,False,False,False
14354,"ascontrolled variation (FRGC) versus natural orrandom
",False,lfw_paper,False,False,False
14355,"variation (LFW).
",False,lfw_paper,False,False,False
14356,"We believe that the FRGC served a very important role in
",False,lfw_paper,False,False,False
14357,"advancing the state of the art in face recognition, especial ly
",False,lfw_paper,False,False,False
14358,"the speciﬁc problem of face recognition under the assumptio n
",False,lfw_paper,False,False,False
14359,"that certain types of data can be acquired. We believe that ou r
",False,lfw_paper,False,False,False
14360,"database ﬁlls a complementary need for a large data set of
",False,lfw_paper,False,False,False
14361,"labeled images in studying the unconstrained face recognit ion
",False,lfw_paper,False,False,False
14362,"problem.
",False,lfw_paper,False,False,False
14363,"The BioID Face Database [17]. Another database which
",False,lfw_paper,False,False,False
14364,"shares important properties with LFW is the BioID Face
",False,lfw_paper,False,False,False
14365,"Database. This database consists of 1521 gray level images
",False,lfw_paper,False,False,False
14366,"with a resolution of 384 by 286 pixels. Each image shows a
",False,lfw_paper,False,False,False
14367,"frontal view of the face of one out of 23 different test per-
",False,lfw_paper,False,False,False
14368,"sons. The most important property shared by the BioID Face
",False,lfw_paper,False,False,False
14369,"Database and Labeled Faces in the Wild is that both databases
",False,lfw_paper,False,False,False
14370,"strive to capture realistic settings, with signiﬁcant vari ability
",False,lfw_paper,False,False,False
14371,"in pose, lighting, and expression. BioID backgrounds inclu de
",False,lfw_paper,False,False,False
14372,"what appear to be realistic ofﬁce or home settings for their
",False,lfw_paper,False,False,False
14373,"pictures, and these backgrounds vary simultaneously with
",False,lfw_paper,False,False,False
14374,"subject pose, expression, and other parameters. Since one o f
",False,lfw_paper,False,False,False
14375,"the main goals of LFW is to provide realistic images, this is
",False,lfw_paper,False,False,False
14376,"a signiﬁcant similarity.
",False,lfw_paper,False,False,False
14377,"Despite this important similarity, BioID is quite differen t
",False,lfw_paper,False,False,False
14378,"from LFW. Important differences include the following.
",False,lfw_paper,False,False,False
14379,"•While BioID and LFW both strive to capture a set of
",False,lfw_paper,False,False,False
14380,"realistic images, the distributions they capture are signi f-
",False,lfw_paper,False,False,False
14381,"icantly different. The distribution of images in BioID is
",False,lfw_paper,False,False,False
14382,"focussed on a small number of ofﬁce and home envi-
",False,lfw_paper,False,False,False
14383,"ronments. For each individual, most pictures are taken in
",False,lfw_paper,False,False,False
14384,"the same setting, but from a slightly different point of
",False,lfw_paper,False,False,False
14385,"view. LFW pictures of the same individual, in contrast,
",False,lfw_paper,False,False,False
14386,"are often taken in completely different settings, and at
",False,lfw_paper,False,False,False
14387,"different times. For example, the same athlete may be
",False,lfw_paper,False,False,False
14388,"photographed during a sporting event and at a news
",False,lfw_paper,False,False,False
14389,"conference.
",False,lfw_paper,False,False,False
14390,"•According to the database web site, it appears that BioID
",False,lfw_paper,False,False,False
14391,"is targeted more at the face detection problem. LFW is
",False,lfw_paper,False,False,False
14392,"targeted at face recognition, or identiﬁcation.
",False,lfw_paper,False,False,False
14393,"•BioID has relatively low variability with respect to race,
",False,lfw_paper,False,False,False
14394,"with the large majority of images being of caucasians.
",False,lfw_paper,False,False,False
14395,"LFW has a broad distribution of people from different
",False,lfw_paper,False,False,False
14396,"parts of the world, different races, and different ethnici-
",False,lfw_paper,False,False,False
14397,"ties.
",False,lfw_paper,False,False,False
14398,"•BioID has manually marked eye positions in each image.
",False,lfw_paper,False,False,False
14399,"LFW has no such markings. The only positional infor-
",False,lfw_paper,False,False,False
14400,"mation given for LFW is that the image is the immediate
",False,lfw_paper,False,False,False
14401,"output (up to a ﬁxed rescaling and recropping, described
",False,lfw_paper,False,False,False
14402,"in Section VI) of the Viola-Jones face detector. Thus, the
",False,lfw_paper,False,False,False
14403,"face is usually (but not always) centered and usually (but
",False,lfw_paper,False,False,False
14404,"not always) at a similar scale.
",False,lfw_paper,False,False,False
14405,"•LFW includes color images. BioID does not.
",False,lfw_paper,False,False,False
14406,"•BioID has a relatively large number of images per person(66.13), with a relatively small number of people (23).
",False,lfw_paper,False,False,False
14407,"LFW has a much smaller average number of images
",False,lfw_paper,False,False,False
14408,"per person ( 2.30), with a much larger number of people
",False,lfw_paper,False,False,False
14409,"(5749).
",False,lfw_paper,False,False,False
14410,"Overall, BioID is an interesting database of face images
",False,lfw_paper,False,False,False
14411,"which may be useful for a number of purposes such as face
",False,lfw_paper,False,False,False
14412,"detection in indoor enviroments. We believe that LFW, on the
",False,lfw_paper,False,False,False
14413,"other hand, will be useful for solving more general and difﬁc ult
",False,lfw_paper,False,False,False
14414,"face recognition problems with large populations in highly
",False,lfw_paper,False,False,False
14415,"variable environments.
",False,lfw_paper,False,False,False
14416,"Caltech 10000 Web Faces [1]. The Caltech 10000 Web
",False,lfw_paper,False,False,False
14417,"Faces database is interesting in that it also provides a very
",False,lfw_paper,False,False,False
14418,"broad distribution of faces. The distribution of faces incl uded
",False,lfw_paper,False,False,False
14419,"in the Caltech collection is similar to the distribution of f aces
",False,lfw_paper,False,False,False
14420,"in LFW. In particular, the faces in each database show a broad
",False,lfw_paper,False,False,False
14421,"mixture of ages, expression, hairstyles, lighting effects , race,
",False,lfw_paper,False,False,False
14422,"and gender. The backgrounds are highly varied in both data
",False,lfw_paper,False,False,False
14423,"sets, although the Caltech data set includes signiﬁcantly m ore
",False,lfw_paper,False,False,False
14424,"background area.
",False,lfw_paper,False,False,False
14425,"However, the Caltech database is again geared more toward
",False,lfw_paper,False,False,False
14426,"face detection and alignment rather than face recognition. It
",False,lfw_paper,False,False,False
14427,"provides the position of four facial features, but does not g ive
",False,lfw_paper,False,False,False
14428,"the identity of individuals. Thus, it is not particularly su itable
",False,lfw_paper,False,False,False
14429,"for face recognition experiments.
",False,lfw_paper,False,False,False
14430,"In summary, there are a great number of face databases
",False,lfw_paper,False,False,False
14431,"available, and while each has a role in the problems of face
",False,lfw_paper,False,False,False
14432,"recognition or face detection, we believe LFW ﬁlls an impor-
",False,lfw_paper,False,False,False
14433,"tant gap for the problem of unconstrained face recognition.
",False,lfw_paper,False,False,False
14434,"III. I NTENDED USES
",False,lfw_paper,False,False,False
14435,"As mentioned in the introduction, this database is aimed at
",False,lfw_paper,False,False,False
14436,"studying the problem of pair matching. That is, given a pair
",False,lfw_paper,False,False,False
14437,"of face images, we wish to decide whether the images are
",False,lfw_paper,False,False,False
14438,"of the same person. By outputting a probability of match or
",False,lfw_paper,False,False,False
14439,"mismatch rather than a hard decision, one can easily create a
",False,lfw_paper,False,False,False
14440,"Receiver Operating Characteristic, or ROC curve, that give s
",False,lfw_paper,False,False,False
14441,"the minimium cost decisions for given relative error costs
",False,lfw_paper,False,False,False
14442,"(false match or false mismatch).
",False,lfw_paper,False,False,False
14443,"Even within what we call the pair matching paradigm, there
",False,lfw_paper,False,False,False
14444,"are a number of subtly, but importantly different recogniti on
",False,lfw_paper,False,False,False
14445,"problems. Some of these differences concern the speciﬁc
",False,lfw_paper,False,False,False
14446,"organization of training and testing subsets of the databas e.
",False,lfw_paper,False,False,False
14447,"A critical aspect of our database is that for any given
",False,lfw_paper,False,False,False
14448,"training-testing split, the people in each subset are mutu-
",False,lfw_paper,False,False,False
14449,"ally exclusive. In other words, for any pair of images in the
",False,lfw_paper,False,False,False
14450,"training set, neither of the people pictured in those images is
",False,lfw_paper,False,False,False
14451,"in any of the test set pairs. Similarly, no test image appears in
",False,lfw_paper,False,False,False
14452,"a corresponding training set.
",False,lfw_paper,False,False,False
14453,"Thus, at training time, it is essentially impossible to buil d a
",False,lfw_paper,False,False,False
14454,"model for any person in the test set. This differs substantia lly
",False,lfw_paper,False,False,False
14455,"from paradigms in which there is a ﬁxed gallery of test subjec ts
",False,lfw_paper,False,False,False
14456,"for whom training images are available, and the goal is to ﬁnd
",False,lfw_paper,False,False,False
14457,"matches of so-called probe images to members of the gallery.
",False,lfw_paper,False,False,False
14458,"(Such ﬁxed gallery paradigms are often referred to as face
",False,lfw_paper,False,False,False
14459,"veriﬁcation .) In particular, for LFW, since the people in test
",False,lfw_paper,False,False,False
14460,"images have never been seen before, there is no opportunity
",False,lfw_paper,False,False,False
14461,"to build models for such individuals, except to do this at tes t
",False,lfw_paper,False,False,False
14462,"time from a single image.6
",False,lfw_paper,False,False,False
14463,"Instead, this paradigm is meant to focus on the generic prob-
",False,lfw_paper,False,False,False
14464,"lem of differentiating any two individuals that have never been
",False,lfw_paper,False,False,False
14465,"seen before. Thus, a different type of learning is suggested –
",False,lfw_paper,False,False,False
14466,"learning to discriminate among any pair of faces, rather tha n
",False,lfw_paper,False,False,False
14467,"learning to ﬁnd exemplars of a small (or even large) gallery o f
",False,lfw_paper,False,False,False
14468,"people as in face veriﬁcation. There are numerous examples
",False,lfw_paper,False,False,False
14469,"of this kind of face recognition research [7], [15], [25].
",False,lfw_paper,False,False,False
14470,"A. Pair Matching and Learning from One Example
",False,lfw_paper,False,False,False
14471,"We shall refer to the speciﬁc pair matching problem, in
",False,lfw_paper,False,False,False
14472,"which neither of the individuals pictured in a test pair has b een
",False,lfw_paper,False,False,False
14473,"seen during training, as the unseen pair match problem. This
",False,lfw_paper,False,False,False
14474,"is closely related to the problem of learning from one example ,
",False,lfw_paper,False,False,False
14475,"in which a single training image of a person is provided, and
",False,lfw_paper,False,False,False
14476,"the goal is to determine whether a new image represents the
",False,lfw_paper,False,False,False
14477,"individual for whom one training image was provided.
",False,lfw_paper,False,False,False
14478,"In particular, the unseen pair match problem can be viewed
",False,lfw_paper,False,False,False
14479,"as a speciﬁc instance of the problem of learning from one
",False,lfw_paper,False,False,False
14480,"example. Speciﬁcally, given a pair of images and the questio n
",False,lfw_paper,False,False,False
14481,"of whether they are the same, one of the images can be consid-
",False,lfw_paper,False,False,False
14482,"ered to deﬁne the “model”, and the other can be considered to
",False,lfw_paper,False,False,False
14483,"be an instance of the person deﬁned by the model or not. But
",False,lfw_paper,False,False,False
14484,"there are important differences between the classical prob lem
",False,lfw_paper,False,False,False
14485,"of learning from one example, as discussed for example in the
",False,lfw_paper,False,False,False
14486,"paper of Beymer et al. [5], and the unseen pair match problem
",False,lfw_paper,False,False,False
14487,"(see for example [7]). The main differences are as follows.
",False,lfw_paper,False,False,False
14488,"•In learning from one example (per person), training
",False,lfw_paper,False,False,False
14489,"examples are given at training time. Whereas in the
",False,lfw_paper,False,False,False
14490,"unseen pair match problem, the single model image is
",False,lfw_paper,False,False,False
14491,"not available until test time. If processing speed is an
",False,lfw_paper,False,False,False
14492,"important constraint, then it may be advantageous to have
",False,lfw_paper,False,False,False
14493,"a training example ahead of time, as in the learning from
",False,lfw_paper,False,False,False
14494,"one example paradigm.
",False,lfw_paper,False,False,False
14495,"•Another important difference is that in learning from
",False,lfw_paper,False,False,False
14496,"one example, at test time, the objective is usually to
",False,lfw_paper,False,False,False
14497,"determine which, if any, of the models the test image
",False,lfw_paper,False,False,False
14498,"corresponds to. One would not normally identify the test
",False,lfw_paper,False,False,False
14499,"image with more than one model, and so a winner-take-
",False,lfw_paper,False,False,False
14500,"all or maximum likelihood approach for selecting a match
",False,lfw_paper,False,False,False
14501,"would be reasonable. On the other hand, in the unseen
",False,lfw_paper,False,False,False
14502,"pair match problem, the objective is to make a binary
",False,lfw_paper,False,False,False
14503,"decision about whether a given single image matches
",False,lfw_paper,False,False,False
14504,"another image. If a test set contains multiple pairings
",False,lfw_paper,False,False,False
14505,"of a single image B, i.e., a group of pairs of images of
",False,lfw_paper,False,False,False
14506,"the form (Ai, B),1≤i≤n, there is no mechanism
",False,lfw_paper,False,False,False
14507,"for deciding that the image Bshould match only one of
",False,lfw_paper,False,False,False
14508,"the images Ai. In other words, each pairwise decision is
",False,lfw_paper,False,False,False
14509,"made independently. This rules out the winner-take-all or
",False,lfw_paper,False,False,False
14510,"maximum likelihood style approaches.
",False,lfw_paper,False,False,False
14511,"In summary then, LFW is intended for the unseen pair
",False,lfw_paper,False,False,False
14512,"matching paradigm, which is characterized by the condition s
",False,lfw_paper,False,False,False
14513,"that
",False,lfw_paper,False,False,False
14514,"•no images of test subjects are available at training time,
",False,lfw_paper,False,False,False
14515,"and
",False,lfw_paper,False,False,False
14516,"•the decisions for all test pairs are made independently.
",False,lfw_paper,False,False,False
14517,"Conformance to this second condition disallows techniques
",False,lfw_paper,False,False,False
14518,"such as semi-supervised learning in which examples are usedfrom across an entire test set. For each test pair, any algori thm
",False,lfw_paper,False,False,False
14519,"should behave as if these are the only two test images. Put
",False,lfw_paper,False,False,False
14520,"another way, an algorithm should not use more than a single
",False,lfw_paper,False,False,False
14521,"pair of test images at a time.
",False,lfw_paper,False,False,False
14522,"B. Training, Validation, and Testing
",False,lfw_paper,False,False,False
14523,"Proper use of training, validation, and testing sets is cruc ial
",False,lfw_paper,False,False,False
14524,"for the accurate comparison of face recognition algorithms .
",False,lfw_paper,False,False,False
14525,"In describing the Face Recognition Grand Challenge [28], th e
",False,lfw_paper,False,False,False
14526,"authors note that using sequestered test sets, i.e. test set s not
",False,lfw_paper,False,False,False
14527,"publicly available to researchers, is the best way to ensure
",False,lfw_paper,False,False,False
14528,"that algorithm developers do not unfairly ﬁt the parameters of
",False,lfw_paper,False,False,False
14529,"their algorithms to the test data. Allowing the experimente r to
",False,lfw_paper,False,False,False
14530,"choose the parameters of an algorithm that work best on a test
",False,lfw_paper,False,False,False
14531,"set, or equivalently, allowing the experimenter to choose t he
",False,lfw_paper,False,False,False
14532,"best algorithm for a given test set, biases upward the estimate
",False,lfw_paper,False,False,False
14533,"of accuracy such an algorithm would produce on a sequestered
",False,lfw_paper,False,False,False
14534,"test set. While we fully support this point of view, we have
",False,lfw_paper,False,False,False
14535,"decided for practical reasons not to use a sequestered test s et,
",False,lfw_paper,False,False,False
14536,"but to include the test data in the public database. We hope
",False,lfw_paper,False,False,False
14537,"that by providing clear guidelines for the use of this data, t hat
",False,lfw_paper,False,False,False
14538,"“ﬁtting to the test data” will be minimized. Also, the size an d
",False,lfw_paper,False,False,False
14539,"difﬁculty of the data set may mitigate the degree to which
",False,lfw_paper,False,False,False
14540,"unintended overﬁtting problems may occur.
",False,lfw_paper,False,False,False
14541,"We organize our data into two “Views”, or groups of
",False,lfw_paper,False,False,False
14542,"indices. View 1 is for algorithm development and general
",False,lfw_paper,False,False,False
14543,"experimentation, prior to formal evaluation. This might al so
",False,lfw_paper,False,False,False
14544,"be called a model selection or validation view. View 2, for
",False,lfw_paper,False,False,False
14545,"performance reporting, should be used only for the ﬁnal
",False,lfw_paper,False,False,False
14546,"evaluation of a method. The goal of this methodology is to
",False,lfw_paper,False,False,False
14547,"use the ﬁnal test sets as seldom as possible before reporting .
",False,lfw_paper,False,False,False
14548,"Ideally, of course, each test set should only be used once. We
",False,lfw_paper,False,False,False
14549,"now describe the two views in more detail.
",False,lfw_paper,False,False,False
14550,"View 1: Model selection and algorithm development.
",False,lfw_paper,False,False,False
14551,"This view of the data consists of two subsets of the database,
",False,lfw_paper,False,False,False
14552,"one for training ( pairsDevTrain.txt ), and one for testing
",False,lfw_paper,False,False,False
14553,"(pairsDevTest.txt ). The training set consists of 1100
",False,lfw_paper,False,False,False
14554,"pairs of matched images and 1100 pairs of mismatched im-
",False,lfw_paper,False,False,False
14555,"ages. The test set consists of 500 pairs of matched and 500
",False,lfw_paper,False,False,False
14556,"pairs of mismatched images. In order to support the unseen
",False,lfw_paper,False,False,False
14557,"pair match paradigm, the people who appear in the training
",False,lfw_paper,False,False,False
14558,"and testing sets are mutually exclusive.
",False,lfw_paper,False,False,False
14559,"The main purpose of this view of the data is so that
",False,lfw_paper,False,False,False
14560,"researchers can freely experiment with algorithms and para m-
",False,lfw_paper,False,False,False
14561,"eter settings without worrying about overusing test data. F or
",False,lfw_paper,False,False,False
14562,"example, if one is using support vector machines and trying
",False,lfw_paper,False,False,False
14563,"to decide upon which kernel to use, it would be appropriate to
",False,lfw_paper,False,False,False
14564,"test various kernels (linear, polynomial, radial basis fun ction,
",False,lfw_paper,False,False,False
14565,"etc.) on View 1 of the database.
",False,lfw_paper,False,False,False
14566,"To use this view, simply train an algorithm on the training
",False,lfw_paper,False,False,False
14567,"set and test on the test set. This may be repeated as often as
",False,lfw_paper,False,False,False
14568,"desired without signiﬁcantly biasing ﬁnal results. (See ca veats
",False,lfw_paper,False,False,False
14569,"below.)
",False,lfw_paper,False,False,False
14570,"View 2: Performance reporting. The second view of the
",False,lfw_paper,False,False,False
14571,"data should be used sparingly, and only for performance
",False,lfw_paper,False,False,False
14572,"reporting. Ideally, it should only be used once, as choosing the
",False,lfw_paper,False,False,False
14573,"best performer from multiple algorithms, or multiple param eter
",False,lfw_paper,False,False,False
14574,"settings, will bias results toward artiﬁcially high accura cy.7
",False,lfw_paper,False,False,False
14575,"The second view of the data consists of ten subsets of the
",False,lfw_paper,False,False,False
14576,"database. Once a model or algorithm has been selected (using
",False,lfw_paper,False,False,False
14577,"View 1 of the database if desired), the performance of that
",False,lfw_paper,False,False,False
14578,"algorithm can be measured using View 2. To report accuracy
",False,lfw_paper,False,False,False
14579,"results on View 2, the experimenter should report the aggreg ate
",False,lfw_paper,False,False,False
14580,"performance of a classiﬁer on 10 separate experiments in a
",False,lfw_paper,False,False,False
14581,"leave-one-out cross validation scheme. In each experiment ,
",False,lfw_paper,False,False,False
14582,"nine of the subsets should be combined to form a training
",False,lfw_paper,False,False,False
14583,"set, with the tenth subset used for testing. For example, the
",False,lfw_paper,False,False,False
14584,"ﬁrst experiment would use subsets (2,3,4,5,6,7,8,9,10)for
",False,lfw_paper,False,False,False
14585,"training and subset 1for testing. The fourth experiment would
",False,lfw_paper,False,False,False
14586,"use subsets (1,2,3,5,6,7,8,9,10)for training and subset 4
",False,lfw_paper,False,False,False
14587,"for testing.
",False,lfw_paper,False,False,False
14588,"It is critical for accuracy performance reporting that the
",False,lfw_paper,False,False,False
14589,"ﬁnal parameters of the classiﬁer under each experiment be se t
",False,lfw_paper,False,False,False
14590,"using only the training data for that experiment. In other
",False,lfw_paper,False,False,False
14591,"words, an algorithm may not, during performance reporting,
",False,lfw_paper,False,False,False
14592,"set its parameters to maximize the combined accuracy across
",False,lfw_paper,False,False,False
14593,"all 10 training sets. The reason for this is that training and
",False,lfw_paper,False,False,False
14594,"testing sets overlap across experiments, and optimizing a
",False,lfw_paper,False,False,False
14595,"classiﬁer simultaneously using all training sets is essent ially
",False,lfw_paper,False,False,False
14596,"ﬁtting to the test data, since the training set for one experi ment
",False,lfw_paper,False,False,False
14597,"is the testing data for another. In other words, for performa nce
",False,lfw_paper,False,False,False
14598,"reporting, each of the 10 experiments (both the training and
",False,lfw_paper,False,False,False
14599,"testing phases) should be run completely independently of t he
",False,lfw_paper,False,False,False
14600,"others, resulting in 10 separate classiﬁers (one for each te st
",False,lfw_paper,False,False,False
14601,"set).
",False,lfw_paper,False,False,False
14602,"While there are many methods for reporting the ﬁnal per-
",False,lfw_paper,False,False,False
14603,"formance of a classiﬁer, including ROC curves and Precision -
",False,lfw_paper,False,False,False
14604,"Recall curves, we ask that each experimenter, at a minimum,
",False,lfw_paper,False,False,False
14605,"report the estimated mean accuracy and the standard error
",False,lfw_paper,False,False,False
14606,"of the mean for View 2 of the database.
",False,lfw_paper,False,False,False
14607,"In particular, the estimated mean accuracy ˆµis given by
",False,lfw_paper,False,False,False
14608,"ˆµ=/summationtext10
",False,lfw_paper,False,False,False
14609,"i=1pi
",False,lfw_paper,False,False,False
14610,"10,
",False,lfw_paper,False,False,False
14611,"where piis the percentage of correct classiﬁcations on View 2,
",False,lfw_paper,False,False,False
14612,"using subset ifor testing. It is important to note that accuracy
",False,lfw_paper,False,False,False
14613,"should be computed with parameters and thresholds chosen
",False,lfw_paper,False,False,False
14614,"independently of the test data, ruling out, for instance, si mply
",False,lfw_paper,False,False,False
14615,"choosing the point on a Precision-Recall curve giving the
",False,lfw_paper,False,False,False
14616,"highest accuracy.
",False,lfw_paper,False,False,False
14617,"The standard error of the mean is given as
",False,lfw_paper,False,False,False
14618,"SE=ˆσ√
",False,lfw_paper,False,False,False
14619,"10,
",False,lfw_paper,False,False,False
14620,"where ˆσis the estimate of the standard deviation, given by
",False,lfw_paper,False,False,False
14621,"ˆσ=/radicalBigg/summationtext10
",False,lfw_paper,False,False,False
14622,"i=1(pi−ˆµ)2
",False,lfw_paper,False,False,False
14623,"9.
",False,lfw_paper,False,False,False
14624,"Because the training sets in View 2 overlap, the standard
",False,lfw_paper,False,False,False
14625,"error may be biased downward somewhat relative to what
",False,lfw_paper,False,False,False
14626,"would be obtained with fully independent training sets and t est
",False,lfw_paper,False,False,False
14627,"sets. However, because the test sets of View 2 are independen t,
",False,lfw_paper,False,False,False
14628,"we believe this quantity will be valuable in assessing thesigniﬁcance of the difference among algorithms.2
",False,lfw_paper,False,False,False
14629,"Discussion of data splits. The multiple-view approach
",False,lfw_paper,False,False,False
14630,"described above has been used, rather than a traditional
",False,lfw_paper,False,False,False
14631,"training-validation-testing split of the database, in ord er to
",False,lfw_paper,False,False,False
14632,"maximize the amount of data available for training and testi ng.
",False,lfw_paper,False,False,False
14633,"Ideally, one would have enough images in a database so that
",False,lfw_paper,False,False,False
14634,"training, validation, and testing sets could be non-overla pping.
",False,lfw_paper,False,False,False
14635,"However, in order to maximize the size of our training and
",False,lfw_paper,False,False,False
14636,"testing sets, we have allowed reuse of the data between View
",False,lfw_paper,False,False,False
14637,"1 of the database and View 2 of the database. While this
",False,lfw_paper,False,False,False
14638,"introduces some bias into the results, we believe the bias wi ll
",False,lfw_paper,False,False,False
14639,"be very small in most cases, and is outweighed by the beneﬁt
",False,lfw_paper,False,False,False
14640,"of the resulting larger training and test set sizes.
",False,lfw_paper,False,False,False
14641,"Given our multiple-view organization of the database, it is
",False,lfw_paper,False,False,False
14642,"possible to “cheat” and produce a classiﬁer which shows arti -
",False,lfw_paper,False,False,False
14643,"ﬁcially good results on the ﬁnal test set. In particular, dur ing
",False,lfw_paper,False,False,False
14644,"the model selection phase, using View 1 of the database, one
",False,lfw_paper,False,False,False
14645,"could build a classiﬁer which simply stores all of the traini ng
",False,lfw_paper,False,False,False
14646,"data in a ﬁle, and declare that this ﬁle is now part of the
",False,lfw_paper,False,False,False
14647,"classiﬁer. During performance reporting, using View 2 of th e
",False,lfw_paper,False,False,False
14648,"database, examples in each test set could be compared agains t
",False,lfw_paper,False,False,False
14649,"the stored examples from View 1, and since many of them are
",False,lfw_paper,False,False,False
14650,"the same, performance would be artiﬁcially high.
",False,lfw_paper,False,False,False
14651,"While we trust that no researcher would use such a scheme
",False,lfw_paper,False,False,False
14652,"intentionally, it is possible that similar schemes might be
",False,lfw_paper,False,False,False
14653,"implemented unintentionally by giving the classiﬁer a larg e
",False,lfw_paper,False,False,False
14654,"store of memory in which to memorize features of the View
",False,lfw_paper,False,False,False
14655,"1 training set, and then to reuse these memorized features
",False,lfw_paper,False,False,False
14656,"during performance reporting. The reason we believe that th is
",False,lfw_paper,False,False,False
14657,"scenario would not arise accidentally is that such a scheme
",False,lfw_paper,False,False,False
14658,"would do very poorly on the testing portion of View 1, since
",False,lfw_paper,False,False,False
14659,"the training and testing for View 1 do not overlap. That is,
",False,lfw_paper,False,False,False
14660,"there should be no performance beneﬁt during View 1 testing
",False,lfw_paper,False,False,False
14661,"from memorizing large sets of features or parts of images.
",False,lfw_paper,False,False,False
14662,"If the classiﬁer is built using View 1 in order to minimize
",False,lfw_paper,False,False,False
14663,"generalization error, then the memorization scheme descri bed
",False,lfw_paper,False,False,False
14664,"above would not be expected to work well. In other words, if
",False,lfw_paper,False,False,False
14665,"experimenters legitimately strive to maximize performanc e on
",False,lfw_paper,False,False,False
14666,"the testing data in View 1, and then run experiments on View
",False,lfw_paper,False,False,False
14667,"2 without modifying the inherent form of their classiﬁers, w e
",False,lfw_paper,False,False,False
14668,"believe our database organization will successfully measu re
",False,lfw_paper,False,False,False
14669,"the generalization ability of classiﬁers, which is our goal .
",False,lfw_paper,False,False,False
14670,"Summary of usage recommendations. In summary, for
",False,lfw_paper,False,False,False
14671,"proper use of the database, researchers should proceed roug hly
",False,lfw_paper,False,False,False
14672,"according to the following procedure.
",False,lfw_paper,False,False,False
14673,"1) Algorithm development or model selection.
",False,lfw_paper,False,False,False
14674,"a) Use View 1 of the database to train and test as
",False,lfw_paper,False,False,False
14675,"many models, with as many parameter settings, as
",False,lfw_paper,False,False,False
14676,"desired.
",False,lfw_paper,False,False,False
14677,"b) Retain model M∗which has best performance on
",False,lfw_paper,False,False,False
14678,"test set.
",False,lfw_paper,False,False,False
14679,"2) Performance reporting.
",False,lfw_paper,False,False,False
14680,"a) Use View 2 of the database.
",False,lfw_paper,False,False,False
14681,"2We remind the reader that for two algorithms whose standard e rrors
",False,lfw_paper,False,False,False
14682,"overlap, one may conclude that they their difference is not s tatistically
",False,lfw_paper,False,False,False
14683,"signiﬁcant at the 0.05level. However, one may not conclude, in general,
",False,lfw_paper,False,False,False
14684,"that algorithms whose standard errors do not overlap are sta tistically different
",False,lfw_paper,False,False,False
14685,"at the0.05level.8
",False,lfw_paper,False,False,False
14686,"b) For i= 1 to10
",False,lfw_paper,False,False,False
14687,"i) Form training set for experiment iby combin-
",False,lfw_paper,False,False,False
14688,"ing all subsets from View 2 except subset i.
",False,lfw_paper,False,False,False
14689,"ii) Set parameters of model M∗using training set,
",False,lfw_paper,False,False,False
14690,"producing classiﬁer i.
",False,lfw_paper,False,False,False
14691,"iii) Use subset iof View 2 as a test set.
",False,lfw_paper,False,False,False
14692,"iv) Record results of classiﬁer ion test set.
",False,lfw_paper,False,False,False
14693,"c) Use results from all 10 classiﬁers to compute the
",False,lfw_paper,False,False,False
14694,"estimated mean classiﬁcation accuracy ˆµand the
",False,lfw_paper,False,False,False
14695,"standard error of the mean SEas described above.
",False,lfw_paper,False,False,False
14696,"d) Finally, make sure to report which training method
",False,lfw_paper,False,False,False
14697,"(image-restricted or unrestricted) was used, as de-
",False,lfw_paper,False,False,False
14698,"scribed in Section IV.
",False,lfw_paper,False,False,False
14699,"IV. T RANSITIVITY AND THE IMAGE -RESTRICTED AND
",False,lfw_paper,False,False,False
14700,"UNRESTRICTED USE OF TRAINING DATA
",False,lfw_paper,False,False,False
14701,"Whenever one works with matched and mismatched data
",False,lfw_paper,False,False,False
14702,"pairs such as those described in pairsDevTrain.txt , the
",False,lfw_paper,False,False,False
14703,"issue of creating auxiliary training examples arises by usi ng
",False,lfw_paper,False,False,False
14704,"the transitivity of equality.
",False,lfw_paper,False,False,False
14705,"For example, in a training set, if one matched pair consists
",False,lfw_paper,False,False,False
14706,"of the 10th and 12th images of George WBush, and another
",False,lfw_paper,False,False,False
14707,"pair consists of the 42nd and 50th images of George WBush,
",False,lfw_paper,False,False,False
14708,"then it might seem reasonable to add other image pairs, such
",False,lfw_paper,False,False,False
14709,"as (10, 42), (10, 50), (12, 42) and (12, 50), to the training da ta
",False,lfw_paper,False,False,False
14710,"using an automatic procedure. One could argue that such pair s
",False,lfw_paper,False,False,False
14711,"areimplicitly present in the original training data, given that
",False,lfw_paper,False,False,False
14712,"the images have been labeled with the name George WBush.
",False,lfw_paper,False,False,False
14713,"Auxiliary examples could be added to the mismatched pairs
",False,lfw_paper,False,False,False
14714,"using a similar method.
",False,lfw_paper,False,False,False
14715,"Rather than disallowing such augmentation on the one hand,
",False,lfw_paper,False,False,False
14716,"or penalizing researchers who do not wish to add many
",False,lfw_paper,False,False,False
14717,"thousands of extra pairs of images to their training sets on
",False,lfw_paper,False,False,False
14718,"the other, we describe two separate methods for using traini ng
",False,lfw_paper,False,False,False
14719,"data. When reporting results, the experimenter should stat e
",False,lfw_paper,False,False,False
14720,"explicitly whether the image-restricted or the unrestricted
",False,lfw_paper,False,False,False
14721,"training method was used to generate results. These two
",False,lfw_paper,False,False,False
14722,"methods of training are described next.
",False,lfw_paper,False,False,False
14723,"A. Image-Restricted Training
",False,lfw_paper,False,False,False
14724,"The idea behind the image-restricted paradigm is that the
",False,lfw_paper,False,False,False
14725,"experimenter should notuse the name of a person to infer
",False,lfw_paper,False,False,False
14726,"the equivalence or non-equivalence of two face images that
",False,lfw_paper,False,False,False
14727,"are not explicitly given in the training set. Under the image -
",False,lfw_paper,False,False,False
14728,"restricted training paradigm, the experimenter should dis card
",False,lfw_paper,False,False,False
14729,"the actual names associated with a pair of training images, a nd
",False,lfw_paper,False,False,False
14730,"retain only the information about whether a pair of images is
",False,lfw_paper,False,False,False
14731,"matched or mismatched. Thus, if the pairs (10,12) and (42,50 )
",False,lfw_paper,False,False,False
14732,"of George WBush are both given explicitly in a training set,
",False,lfw_paper,False,False,False
14733,"then under the image-restricted training paradigm, there w ould
",False,lfw_paper,False,False,False
14734,"be no simple way of inferring that the 10th and 42nd images
",False,lfw_paper,False,False,False
14735,"of George WBush were the same person, and thus this image
",False,lfw_paper,False,False,False
14736,"pair should not be added to the training set.
",False,lfw_paper,False,False,False
14737,"Note that under this paradigm, it is still possible to augmen t
",False,lfw_paper,False,False,False
14738,"the training data set by comparing image similarity , as opposed
",False,lfw_paper,False,False,False
14739,"to name equivalence. For example, if the 1st and 2nd images
",False,lfw_paper,False,False,False
14740,"of a person form one matched training pair, while the 2nd and3rd images of the same person form another matched training
",False,lfw_paper,False,False,False
14741,"pair, one could infer from the equivalence of images in the
",False,lfw_paper,False,False,False
14742,"two pairs that the 1st and 3rd images came from the same
",False,lfw_paper,False,False,False
14743,"person, and add this pair to the training set as a matched pair .
",False,lfw_paper,False,False,False
14744,"Such image-based augmentation is allowed under the image-
",False,lfw_paper,False,False,False
14745,"restricted training paradigm.
",False,lfw_paper,False,False,False
14746,"Both Views of the database support the image-restricted
",False,lfw_paper,False,False,False
14747,"training paradigm. In View 1 of the database, the ﬁle
",False,lfw_paper,False,False,False
14748,"pairsDevTrain.txt is intended to support the image-
",False,lfw_paper,False,False,False
14749,"restricted use of training data, and pairsDevTest.txt
",False,lfw_paper,False,False,False
14750,"contains test pairs. In View 2 of the database, the ﬁle
",False,lfw_paper,False,False,False
14751,"pairs.txt supports the image-restricted use of training
",False,lfw_paper,False,False,False
14752,"data. Formats of all such ﬁles are given in Section VI-F.
",False,lfw_paper,False,False,False
14753,"B. Unrestricted Training
",False,lfw_paper,False,False,False
14754,"The idea behind the unrestricted training paradigm is that
",False,lfw_paper,False,False,False
14755,"one may form as many pairs of matched and mismatched
",False,lfw_paper,False,False,False
14756,"pairs as desired from a set of images labeled with individual s’
",False,lfw_paper,False,False,False
14757,"names. To support this use of the database, we deﬁned subsets
",False,lfw_paper,False,False,False
14758,"ofpeople , rather than image pairs, that can be used as a basis
",False,lfw_paper,False,False,False
14759,"for forming arbitrary pairs of matched and mismatched image s.
",False,lfw_paper,False,False,False
14760,"In View 1 of the database, the ﬁles
",False,lfw_paper,False,False,False
14761,"peopleDevTrain.txt andpeopleDevTest.txt
",False,lfw_paper,False,False,False
14762,"can be used to create arbitrary pairs of training and testing
",False,lfw_paper,False,False,False
14763,"images. For example, to create mismatched training pairs,
",False,lfw_paper,False,False,False
14764,"choose any two people from peopleDevTrain.txt ,
",False,lfw_paper,False,False,False
14765,"choose one image of each person, and add the pair to the
",False,lfw_paper,False,False,False
14766,"data set. Pairs should notbe constructed using mixtures of
",False,lfw_paper,False,False,False
14767,"images from training and testing sets.
",False,lfw_paper,False,False,False
14768,"In View 2 of the database, the ﬁle people.txt supports
",False,lfw_paper,False,False,False
14769,"the unrestricted training paradigm. Training pairs should be
",False,lfw_paper,False,False,False
14770,"formed only using pairs of images from the same subsets.
",False,lfw_paper,False,False,False
14771,"Thus, to form a training pair of mismatched images, choose
",False,lfw_paper,False,False,False
14772,"two people from the same subset of people, choose an image
",False,lfw_paper,False,False,False
14773,"of each person, and add the pair to the training set. Note
",False,lfw_paper,False,False,False
14774,"that in View 2 of the database, which is intended only for
",False,lfw_paper,False,False,False
14775,"performance reporting, the test data is fully speciﬁed by th e
",False,lfw_paper,False,False,False
14776,"ﬁlepairs.txt , and should not be constructed using the
",False,lfw_paper,False,False,False
14777,"unrestricted paradigm. The unrestricted paradigm is only for
",False,lfw_paper,False,False,False
14778,"use in creating training data.
",False,lfw_paper,False,False,False
14779,"Due to the added complexity of using the unre-
",False,lfw_paper,False,False,False
14780,"stricted paradigm, we suggest that users start with the
",False,lfw_paper,False,False,False
14781,"image-restricted paradigm by using the pairs described in
",False,lfw_paper,False,False,False
14782,"pairsDevTrain.txt ,pairsDevTest.txt , and, for
",False,lfw_paper,False,False,False
14783,"performance reporting, pairs.txt . Later, if the experi-
",False,lfw_paper,False,False,False
14784,"menters believes that that their algorithm may beneﬁt signi f-
",False,lfw_paper,False,False,False
14785,"icantly from larger amounts of training data, they may wish
",False,lfw_paper,False,False,False
14786,"to consider using the unrestricted paradigm. In either case ,
",False,lfw_paper,False,False,False
14787,"it should be made clear in any publications which training
",False,lfw_paper,False,False,False
14788,"paradigm was used to train classiﬁers for a given test result .
",False,lfw_paper,False,False,False
14789,"V. T HEDETECTION -ALIGNMENT -RECOGNITION PIPELINE
",False,lfw_paper,False,False,False
14790,"Many real world applications wish to automatically detect,
",False,lfw_paper,False,False,False
14791,"align, andrecognize faces in a larger still image, or in a video
",False,lfw_paper,False,False,False
14792,"of a larger scene. Thus, face recognition is often naturally de-
",False,lfw_paper,False,False,False
14793,"scribed as part of a Detection-Alignment-Recognition (DAR )
",False,lfw_paper,False,False,False
14794,"pipeline, as illustrated in Figure 4.9
",False,lfw_paper,False,False,False
14795,"Fig. 4. The Detection-Alignment-Recognition (DAR) pipeli ne. The images
",False,lfw_paper,False,False,False
14796,"of the Labeled Faces in the Wild database represent the outpu t of the Viola-
",False,lfw_paper,False,False,False
14797,"Jones detector. By working with such a database, the develop er of alignment
",False,lfw_paper,False,False,False
14798,"and recognition algorithms know that their methods will ﬁt e asily into the
",False,lfw_paper,False,False,False
14799,"DAR pipeline.
",False,lfw_paper,False,False,False
14800,"To complete this pipeline, we need automatic algorithms
",False,lfw_paper,False,False,False
14801,"for each stage of the pipeline. In addition, each stage of the
",False,lfw_paper,False,False,False
14802,"pipeline must either accept images from, or prepare images
",False,lfw_paper,False,False,False
14803,"for, the next stage of the pipeline. To facilitate this proce ss,
",False,lfw_paper,False,False,False
14804,"we have purposefully designed our database to represent the
",False,lfw_paper,False,False,False
14805,"output of the detection process.
",False,lfw_paper,False,False,False
14806,"In particular, every face image in our database is the
",False,lfw_paper,False,False,False
14807,"output of the Viola-Jones face detection algorithm [35]. Th e
",False,lfw_paper,False,False,False
14808,"motivation for this is as follows. If one can develop a face
",False,lfw_paper,False,False,False
14809,"alignment algorithm (and subsequent recognition algorith m)
",False,lfw_paper,False,False,False
14810,"that works directly on LFW, then it is likely to also work well
",False,lfw_paper,False,False,False
14811,"in an end-to-end system that uses the Viola-Jones detector a s
",False,lfw_paper,False,False,False
14812,"a ﬁrst step.
",False,lfw_paper,False,False,False
14813,"This alleviates the need for each researcher to worry about
",False,lfw_paper,False,False,False
14814,"the process of detection, on the one hand, and to worry
",False,lfw_paper,False,False,False
14815,"about the possibility that a manually aligned database does
",False,lfw_paper,False,False,False
14816,"not adequately represent the true variability seen in the wo rld.
",False,lfw_paper,False,False,False
14817,"In other words, it allows the experimenter to focus on the
",False,lfw_paper,False,False,False
14818,"problems of alignment and recognition rather than the probl em
",False,lfw_paper,False,False,False
14819,"of detection. The speciﬁc details of how the database was
",False,lfw_paper,False,False,False
14820,"constructed are given in the next section.
",False,lfw_paper,False,False,False
14821,"VI. C ONSTRUCTION AND COMPOSITION DETAILS
",False,lfw_paper,False,False,False
14822,"The process of building the database can be broken into the
",False,lfw_paper,False,False,False
14823,"following steps:
",False,lfw_paper,False,False,False
14824,"1) gathering raw images,
",False,lfw_paper,False,False,False
14825,"2) running a face detector and manually eliminating false
",False,lfw_paper,False,False,False
14826,"positives,
",False,lfw_paper,False,False,False
14827,"3) eliminating duplicate images,
",False,lfw_paper,False,False,False
14828,"4) labeling (naming) the detected people,
",False,lfw_paper,False,False,False
14829,"5) cropping and rescaling the detected faces, and
",False,lfw_paper,False,False,False
14830,"6) forming pairs of training and testing pairs for View 1
",False,lfw_paper,False,False,False
14831,"and View 2 of the database.
",False,lfw_paper,False,False,False
14832,"We describe each of these steps in the following subsections .
",False,lfw_paper,False,False,False
14833,"A. Gathering raw images
",False,lfw_paper,False,False,False
14834,"As a starting point, we used the raw images from the Faces
",False,lfw_paper,False,False,False
14835,"in the Wild database collected by Tamara Berg at Berkeley.
",False,lfw_paper,False,False,False
14836,"Details of this set of images can be found in the following
",False,lfw_paper,False,False,False
14837,"publication [4].
",False,lfw_paper,False,False,False
14838,"B. Detecting faces
",False,lfw_paper,False,False,False
14839,"A version of the Viola-Jones face detector [35]
",False,lfw_paper,False,False,False
14840,"was run on each image. Speciﬁcally, we used thecode in OpenCV , version 1.0.0, release 1. Faces were
",False,lfw_paper,False,False,False
14841,"detected using the function cvHaarDetectObjects ,
",False,lfw_paper,False,False,False
14842,"using the provided Haar classiﬁer cascade
",False,lfw_paper,False,False,False
14843,"haarcascade frontalface default.xml , with
",False,lfw_paper,False,False,False
14844,"scale factor set to 1.2, min neighbors set to 2, and the ﬂag
",False,lfw_paper,False,False,False
14845,"set toCVHAARDOCANNYPRUNING .
",False,lfw_paper,False,False,False
14846,"For each positive detection (if any), the following procedu re
",False,lfw_paper,False,False,False
14847,"was performed:
",False,lfw_paper,False,False,False
14848,"1) If the highlighted region was determined by the operator
",False,lfw_paper,False,False,False
14849,"to be a non-face, it was omitted from the database.
",False,lfw_paper,False,False,False
14850,"2) If the name of the person of a detected face from the
",False,lfw_paper,False,False,False
14851,"previous step could not be identiﬁed, either from general
",False,lfw_paper,False,False,False
14852,"knowledge or by inferring the name from the associated
",False,lfw_paper,False,False,False
14853,"caption, then the face was omitted from the database.
",False,lfw_paper,False,False,False
14854,"3) If the same picture of the same face was already in-
",False,lfw_paper,False,False,False
14855,"cluded in the database, the face was omitted from the
",False,lfw_paper,False,False,False
14856,"database. More details are given below about eliminating
",False,lfw_paper,False,False,False
14857,"duplicates from the database.
",False,lfw_paper,False,False,False
14858,"4) Finally, if all of these criteria were met, the face was
",False,lfw_paper,False,False,False
14859,"recropped and rescaled (as described below) and saved
",False,lfw_paper,False,False,False
14860,"as a separate JPEG ﬁle.
",False,lfw_paper,False,False,False
14861,"C. Eliminating duplicate face photos
",False,lfw_paper,False,False,False
14862,"A good deal of effort was expended in removing duplicates
",False,lfw_paper,False,False,False
14863,"from the database. While we considered including duplicate s,
",False,lfw_paper,False,False,False
14864,"since it could be argued that humans may often encounter
",False,lfw_paper,False,False,False
14865,"the exact same picture of a face in advertisements or in other
",False,lfw_paper,False,False,False
14866,"venues, ultimately it was decided that they would prove to be a
",False,lfw_paper,False,False,False
14867,"nuisance during training in which they might cause overﬁtti ng
",False,lfw_paper,False,False,False
14868,"of certain algorithms. In addition, any researcher who choo ses
",False,lfw_paper,False,False,False
14869,"may easily add duplicates for his or her own purposes, but
",False,lfw_paper,False,False,False
14870,"removing them is somewhat more tedious.
",False,lfw_paper,False,False,False
14871,"Deﬁnition of duplicate images. Before removing dupli-
",False,lfw_paper,False,False,False
14872,"cates, it is necessary to deﬁne exactly what they are. While
",False,lfw_paper,False,False,False
14873,"the simplest deﬁnition, that two pictures are duplicates if and
",False,lfw_paper,False,False,False
14874,"only if the images are numerically equivalent at each pixel, is
",False,lfw_paper,False,False,False
14875,"somewhat appealing, it fails to capture large numbers of im-
",False,lfw_paper,False,False,False
14876,"ages that are indistinguishable to the human eye. We found th at
",False,lfw_paper,False,False,False
14877,"the unﬁltered database contained large numbers of images th at
",False,lfw_paper,False,False,False
14878,"had been subtly recropped, rescaled, renormalized, or vari ably
",False,lfw_paper,False,False,False
14879,"compressed, producing pairs of images which were visually
",False,lfw_paper,False,False,False
14880,"nearly equivalent, but differed signiﬁcantly numerically .
",False,lfw_paper,False,False,False
14881,"We chose to deﬁne duplicates as images which were judged
",False,lfw_paper,False,False,False
14882,"to have a common original source photograph, irrespective o f
",False,lfw_paper,False,False,False
14883,"the processing they had undergone. While we attempted to
",False,lfw_paper,False,False,False
14884,"remove all duplicates as deﬁned above from the database, the re
",False,lfw_paper,False,False,False
14885,"may exist some remaining duplicates that were not found. We
",False,lfw_paper,False,False,False
14886,"believe the number of these is small enough so that they will
",False,lfw_paper,False,False,False
14887,"not signiﬁcantly impact research.
",False,lfw_paper,False,False,False
14888,"In addition, there remain a number of pairs of pictures which
",False,lfw_paper,False,False,False
14889,"are extremely similar, but clearly distinct. For example, t here
",False,lfw_paper,False,False,False
14890,"appeared to be pictures of celebrities taken nearly simulta ne-
",False,lfw_paper,False,False,False
14891,"ously by different photographers from only slightly differ ent
",False,lfw_paper,False,False,False
14892,"angles. Whenever there was evidence that a photograph was
",False,lfw_paper,False,False,False
14893,"distinct from another, and not merely a processed version of
",False,lfw_paper,False,False,False
14894,"another, it was maintained as an example in the database.10
",False,lfw_paper,False,False,False
14895,"D. Labeling the faces
",False,lfw_paper,False,False,False
14896,"Each person in the database was named using a manual
",False,lfw_paper,False,False,False
14897,"procedure that used the caption associated with a photograp h
",False,lfw_paper,False,False,False
14898,"as an aid in naming the person. It is possible that certain peo ple
",False,lfw_paper,False,False,False
14899,"have been given incorrect names, especially if the original
",False,lfw_paper,False,False,False
14900,"news caption was incorrect.
",False,lfw_paper,False,False,False
14901,"Signiﬁcant efforts were made to combine all photographs
",False,lfw_paper,False,False,False
14902,"of a single person into the same group under a single name.
",False,lfw_paper,False,False,False
14903,"This was at times challenging, since some people showed
",False,lfw_paper,False,False,False
14904,"up in the original captions under multiple names, such as
",False,lfw_paper,False,False,False
14905,"“Bob McNamara” and “Robert McNamara”. When there were
",False,lfw_paper,False,False,False
14906,"multiple possibilities for a person’s name, we strove to use the
",False,lfw_paper,False,False,False
14907,"most commonly seen name for that person. For Chinese and
",False,lfw_paper,False,False,False
14908,"some other Asian names, we maintained the common Chinese
",False,lfw_paper,False,False,False
14909,"ordering (family name followed by given name), as in “Hu
",False,lfw_paper,False,False,False
14910,"Jintao”. Note that there are some people in the database with
",False,lfw_paper,False,False,False
14911,"just a single name, such as “Abdullah” or “Madonna”.
",False,lfw_paper,False,False,False
14912,"E. Cropping and rescaling
",False,lfw_paper,False,False,False
14913,"For each labeled face, the ﬁnal image to place in the
",False,lfw_paper,False,False,False
14914,"database was created using the following procedure. The
",False,lfw_paper,False,False,False
14915,"region returned by the face detector for the given face was
",False,lfw_paper,False,False,False
14916,"expanded by 2.2 in each dimension. If this expanded region
",False,lfw_paper,False,False,False
14917,"would fall outside the original image area, then a new image
",False,lfw_paper,False,False,False
14918,"of size equal to the desired expanded region was created,
",False,lfw_paper,False,False,False
14919,"containing the corresponding portion of the original image
",False,lfw_paper,False,False,False
14920,"but padded with black pixels to ﬁll in the area outside the
",False,lfw_paper,False,False,False
14921,"original image. The expanded region was then resized to 250
",False,lfw_paper,False,False,False
14922,"by 250 pixels using the function cvResize , in conjuction
",False,lfw_paper,False,False,False
14923,"withcvSetImageROI as necessary. The images were then
",False,lfw_paper,False,False,False
14924,"saved in the JPEG 2.0 format.
",False,lfw_paper,False,False,False
14925,"F . Forming training and testing sets
",False,lfw_paper,False,False,False
14926,"Forming sets and pairs for View 1 and View 2 was done
",False,lfw_paper,False,False,False
14927,"using the following process. First, each speciﬁc person in t he
",False,lfw_paper,False,False,False
14928,"database was randomly assigned to a set. In the case of View
",False,lfw_paper,False,False,False
14929,"1, each person had a 0.7 probability of being placed into the
",False,lfw_paper,False,False,False
14930,"training set, and in the case of View 2, each person had a
",False,lfw_paper,False,False,False
14931,"uniform probability of being placed into each set.
",False,lfw_paper,False,False,False
14932,"The people in each set are given in
",False,lfw_paper,False,False,False
14933,"peopleDevTrain.txt andpeopleDevTest.txt
",False,lfw_paper,False,False,False
14934,"for View 1 and people.txt for View 2. The ﬁrst line
",False,lfw_paper,False,False,False
14935,"ofpeopleDevTrain.txt andpeopleDevTest.txt
",False,lfw_paper,False,False,False
14936,"gives the total number of people in the set, and each
",False,lfw_paper,False,False,False
14937,"subsequent line contains the name of a person followed
",False,lfw_paper,False,False,False
14938,"by the number of images of that person in the database.
",False,lfw_paper,False,False,False
14939,"people.txt is formatted similarly, except that the ﬁrst line
",False,lfw_paper,False,False,False
14940,"gives the number of sets. The next line gives the number of
",False,lfw_paper,False,False,False
14941,"people in the ﬁrst set, followed by the names and number of
",False,lfw_paper,False,False,False
14942,"images of people in the ﬁrst set, then the number of people
",False,lfw_paper,False,False,False
14943,"in the second set, and so on for all ten sets.
",False,lfw_paper,False,False,False
14944,"Matched pairs were formed as follows. First, from the set
",False,lfw_paper,False,False,False
14945,"ofpeople with at least two images, a person was chosen
",False,lfw_paper,False,False,False
14946,"uniformly at random (people with more images were given
",False,lfw_paper,False,False,False
14947,"the same probability of being chosen as people with fewer
",False,lfw_paper,False,False,False
14948,"images). Next, two images were drawn uniformly at random
",False,lfw_paper,False,False,False
14949,"from among the images of the given person. If the two imageswere identical or if the pair of images of the speciﬁc person
",False,lfw_paper,False,False,False
14950,"was already chosen previously as a matched pair, then the
",False,lfw_paper,False,False,False
14951,"whole process was repeated. Otherwise the pair was added to
",False,lfw_paper,False,False,False
14952,"the set of matched pairs.
",False,lfw_paper,False,False,False
14953,"Mismatched pairs were formed as follows. First, from the
",False,lfw_paper,False,False,False
14954,"set of people in the set, two people were chosen uniformly at
",False,lfw_paper,False,False,False
14955,"random (if the same person was chosen twice then the process
",False,lfw_paper,False,False,False
14956,"was repeated). One image was then chosen uniformly at ran-
",False,lfw_paper,False,False,False
14957,"dom from the set of images for each person. If this particular
",False,lfw_paper,False,False,False
14958,"image pair was already chosen previously as a mismatched
",False,lfw_paper,False,False,False
14959,"pair, then the whole process was repeated. Otherwise the pai r
",False,lfw_paper,False,False,False
14960,"was added to the set of mismatched pairs.
",False,lfw_paper,False,False,False
14961,"The pairs for each set are given in pairsDevTrain.txt
",False,lfw_paper,False,False,False
14962,"andpairsDevTest.txt for View 1 and pairs.txt
",False,lfw_paper,False,False,False
14963,"for View 2. The ﬁrst line of pairsDevTrain.txt and
",False,lfw_paper,False,False,False
14964,"pairsDevTest.txt gives the total number Nof matched
",False,lfw_paper,False,False,False
14965,"pairs (equal to the total number of mismatched pairs) in the
",False,lfw_paper,False,False,False
14966,"set. The next Nlines give the matched pairs in the format.
",False,lfw_paper,False,False,False
14967,"name n1 n2
",False,lfw_paper,False,False,False
14968,"which means the matched pair consists of the n1andn2
",False,lfw_paper,False,False,False
14969,"images for the person with the given name. For instance,
",False,lfw_paper,False,False,False
14970,"George_W_Bush 10 24
",False,lfw_paper,False,False,False
14971,"would mean that the pair consists of
",False,lfw_paper,False,False,False
14972,"images GeorgeWBush0010.jpg and
",False,lfw_paper,False,False,False
14973,"GeorgeWBush0024.jpg .
",False,lfw_paper,False,False,False
14974,"The following Nlines give the mismatched pairs in the
",False,lfw_paper,False,False,False
14975,"format
",False,lfw_paper,False,False,False
14976,"name1 n1 name2 n2
",False,lfw_paper,False,False,False
14977,"which means the mismatched pair consists of the n1image
",False,lfw_paper,False,False,False
14978,"of personname1 and then2image of person name2 . For
",False,lfw_paper,False,False,False
14979,"instance,
",False,lfw_paper,False,False,False
14980,"George_W_Bush 12 John_Kerry 8
",False,lfw_paper,False,False,False
14981,"would mean that the pair consists of images
",False,lfw_paper,False,False,False
14982,"GeorgeWBush0012.jpg andJohnKery0008.jpg .
",False,lfw_paper,False,False,False
14983,"The ﬁlepairs.txt is formatted similarly, except that the
",False,lfw_paper,False,False,False
14984,"ﬁrst line gives the number of sets followed by the number of
",False,lfw_paper,False,False,False
14985,"matched pairs N(equal to the number of mismatched pairs).
",False,lfw_paper,False,False,False
14986,"The next 2Nlines give the matched pairs and mismatched
",False,lfw_paper,False,False,False
14987,"pairs for set 1 in the same format as above. This is then
",False,lfw_paper,False,False,False
14988,"repeated nine more times to give the pairs for the other nine
",False,lfw_paper,False,False,False
14989,"sets.
",False,lfw_paper,False,False,False
14990,"VII. S UMMARY
",False,lfw_paper,False,False,False
14991,"We have introduced a new database, Labeled Faces in the
",False,lfw_paper,False,False,False
14992,"Wild, whose primary goals are to
",False,lfw_paper,False,False,False
14993,"1) provide a large database of real world face images for
",False,lfw_paper,False,False,False
14994,"the unseen pair matching problem of face recognition,
",False,lfw_paper,False,False,False
14995,"2) ﬁt neatly into the detection-alignment-recognition
",False,lfw_paper,False,False,False
14996,"pipeline, and
",False,lfw_paper,False,False,False
14997,"3) allow careful and easy comparison of face recognition
",False,lfw_paper,False,False,False
14998,"algorithms.
",False,lfw_paper,False,False,False
14999,"We hope this will provide another stimulus to the vibrant
",False,lfw_paper,False,False,False
15000,"research area of face recognition.11
",False,lfw_paper,False,False,False
15001,"ACKNOWLEDGMENTS
",False,lfw_paper,False,False,False
15002,"First, we would like to thank David Forsyth for the original
",False,lfw_paper,False,False,False
15003,"idea of building large face databases from web images. In
",False,lfw_paper,False,False,False
15004,"addition to the authors of this report, the following people
",False,lfw_paper,False,False,False
15005,"contributed to the construction of this database, in approx i-
",False,lfw_paper,False,False,False
15006,"mate order of their contribution: Vidit Jain, Marwan Mattar ,
",False,lfw_paper,False,False,False
15007,"Jerod Weinman, Andras Ferencz, Paul Dickson, David Walker
",False,lfw_paper,False,False,False
15008,"Duhon, Adam Williams, Piyanuch Silapachote, Chris Pal,
",False,lfw_paper,False,False,False
15009,"Allen Hanson, Dan Xie, Frank Stolle, and Lumin Zhang.
",False,lfw_paper,False,False,False
15010,"REFERENCES
",False,lfw_paper,False,False,False
15011,"[1] Anelia Angelova, Yaser Abu-Mostafa, and Pietro Perona. Pruning
",False,lfw_paper,False,False,False
15012,"training sets for learning of object categories. In CVPR , volume 1,
",False,lfw_paper,False,False,False
15013,"pages 495–501, 2005.
",False,lfw_paper,False,False,False
15014,"[2] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman. Eigen faces vs.
",False,lfw_paper,False,False,False
15015,"ﬁsherfaces: Recognition using class speciﬁc linear projec tions. IEEE
",False,lfw_paper,False,False,False
15016,"Pattern Analysis and Machine Intelligence , 19(7), 1997.
",False,lfw_paper,False,False,False
15017,"[3] Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and Dav id A.
",False,lfw_paper,False,False,False
15018,"Forsyth. Who’s in the picture. NIPS , 2004.
",False,lfw_paper,False,False,False
15019,"[4] Tamara L. Berg, Alexander C. Berg, Michael Maire, Ryan Wh ite,
",False,lfw_paper,False,False,False
15020,"Yee Whye Teh, Erik Learned-Miller, and David A. Forsyth. Nam es
",False,lfw_paper,False,False,False
15021,"and faces in the news. CVPR , 2004.
",False,lfw_paper,False,False,False
15022,"[5] David Beymer and Tomaso Poggio. Face recognition from on e example
",False,lfw_paper,False,False,False
15023,"view. Technical Report AIM-1536, MIT Artiﬁcial Intelligen ce Labora-
",False,lfw_paper,False,False,False
15024,"tory, 1995.
",False,lfw_paper,False,False,False
15025,"[6] Jeffrey F. Cohn, Adena J. Zlochower, James Lien, and Take o Kanade.
",False,lfw_paper,False,False,False
15026,"Automated face analysis by feature point tracking has high c oncurrent
",False,lfw_paper,False,False,False
15027,"validity with manual FACS coding. Psychophysiology , 36:35–43, 1999.
",False,lfw_paper,False,False,False
15028,"[7] Andras Ferencz, Erik Learned-Miller, and Jitendra Mali k. Building a
",False,lfw_paper,False,False,False
15029,"classiﬁcation cascade for visual identiﬁcation from one ex ample. In
",False,lfw_paper,False,False,False
15030,"ICCV , 2005.
",False,lfw_paper,False,False,False
15031,"[8] Andras Ferencz, Erik Learned-Miller, and Jitendra Mali k. Learning
",False,lfw_paper,False,False,False
15032,"hyper-features for visual identiﬁcation. In NIPS , volume 18, 2005.
",False,lfw_paper,False,False,False
15033,"[9] N. A. Fox, B. A. O’Mullane, and R. B. Reilly. The realistic multi-
",False,lfw_paper,False,False,False
15034,"modal V ALID database and visual speaker identiﬁcation comp arison
",False,lfw_paper,False,False,False
15035,"experiments. In Proceedings of the 5th International Conference on
",False,lfw_paper,False,False,False
15036,"Audio- and Video-Based Biometric Person Authentication , 2005.
",False,lfw_paper,False,False,False
15037,"[10] Wen Gao, Bo Cao, Shiguang Shan, Delong Zhou, Xiaohua Zha ng,
",False,lfw_paper,False,False,False
15038,"and Debin Zhao. The CAS-PEAL large-scale Chinese face datab ase
",False,lfw_paper,False,False,False
15039,"and baseline evaluations. Technical Report JDL-TR-04-FR- 001, Joint
",False,lfw_paper,False,False,False
15040,"Research and Development Laboratory (China), 2004.
",False,lfw_paper,False,False,False
15041,"[11] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. F rom few
",False,lfw_paper,False,False,False
15042,"to many: Illumination cone models for face recognition unde r variable
",False,lfw_paper,False,False,False
15043,"lighting and pose. IEEE Transactions on Pattern Analysis and Machine
",False,lfw_paper,False,False,False
15044,"Intelligence , 23(6):643–660, 2001.
",False,lfw_paper,False,False,False
15045,"[12] Daniel B. Graham and Nigel M. Allinson. Characterizing virtual
",False,lfw_paper,False,False,False
15046,"eigensignatures for general purpose face recognition. In H . Wechsler,
",False,lfw_paper,False,False,False
15047,"P. J. Phillips, V . Bruce, F. Fogelman-Soulie, and T. S. Huang , editors,
",False,lfw_paper,False,False,False
15048,"Face recognition: From theory to applications, NATO ASI Ser ies F ,
",False,lfw_paper,False,False,False
15049,"Computer and Systems Sciences , volume 163, pages 446–456. 1998.
",False,lfw_paper,False,False,False
15050,"[13] Peter Hancock. Psychological image collection at stir ling.
",False,lfw_paper,False,False,False
15051,"http://pics.psych.stir.ac.uk/.
",False,lfw_paper,False,False,False
15052,"[14] Gary B. Huang, Vidit Jain, and Erik Learned-Miller. Uns upervised joint
",False,lfw_paper,False,False,False
15053,"alignment of complex images. In ICCV , 2007.
",False,lfw_paper,False,False,False
15054,"[15] Vidit Jain, Andras Ferencz, and Erik Learned-Miller. D iscriminative
",False,lfw_paper,False,False,False
15055,"training of hyper-feature models for object identiﬁcation . In BMVC ,
",False,lfw_paper,False,False,False
15056,"2006.
",False,lfw_paper,False,False,False
15057,"[16] Vidit Jain and Amitabha Mukherjee. The Indian Face Data base.
",False,lfw_paper,False,False,False
15058,"http://vis-www.cs.umass.edu/ vidit/IndianFaceDatabas e/index.html,
",False,lfw_paper,False,False,False
15059,"2002.
",False,lfw_paper,False,False,False
15060,"[17] O. Jesorsky, K. Kirchberg, and R. Frischolz. Robust fac e detection using
",False,lfw_paper,False,False,False
15061,"the Hausdorff distance. In J. Bigun and F. Smeraldi, editors ,Audio and
",False,lfw_paper,False,False,False
15062,"Video Based Person Authentication , pages 90–95. Springer, 2001.
",False,lfw_paper,False,False,False
15063,"[18] M. Kleiner, C. Wallraven, and H. H. B¨ ulthoff. The MPI Vi deoLab -
",False,lfw_paper,False,False,False
15064,"A system for high quality synchronous recording of video and audio
",False,lfw_paper,False,False,False
15065,"from multiple viewpoints. Technical Report 123, Max Planck Institute
",False,lfw_paper,False,False,False
15066,"for Biological Cybernetics, 2004.
",False,lfw_paper,False,False,False
15067,"[19] Michael J. Lyons, Shigeru Akamatsu, Miyuki Kamachi, an d Jiro Gyoba.
",False,lfw_paper,False,False,False
15068,"Coding facial expressions with Gabor wavelets. In Proceedings of the
",False,lfw_paper,False,False,False
15069,"Third IEEE International Conference on Automatic Face and G esture
",False,lfw_paper,False,False,False
15070,"Recognition, Nara, Japan , pages 200–205, 1998.[20] E. Marszalec, B. Martinkauppi, M. Soriano, and M. Pieti k¨ ainen. A
",False,lfw_paper,False,False,False
15071,"physics-based face database for color research. Journal of Electronic
",False,lfw_paper,False,False,False
15072,"Imaging , 9(1):32–38, 2000.
",False,lfw_paper,False,False,False
15073,"[21] A. M. Martinez and R. Benavente. The ar face database. Te chnical
",False,lfw_paper,False,False,False
15074,"Report 24, Computer Vision Center, University of Barcelona , 1998.
",False,lfw_paper,False,False,False
15075,"[22] K Messer, J Matas, J Kittler, J Luettin, and G Maitre. Xm2 vtsdb: The
",False,lfw_paper,False,False,False
15076,"extended m2vts database. In Second International Conference on Audio
",False,lfw_paper,False,False,False
15077,"and Video-based Biometric Person Authentication , March 1999.
",False,lfw_paper,False,False,False
15078,"[23] National Institute of Standards and Technology. The Co lor FERET
",False,lfw_paper,False,False,False
15079,"Database. http://www.itl.nist.gov/iad/humanid/colorf eret/home.html,
",False,lfw_paper,False,False,False
15080,"2003.
",False,lfw_paper,False,False,False
15081,"[24] Chinese Academy of Sciences National Laboratory of Pat tern Recog-
",False,lfw_paper,False,False,False
15082,"nition, Institute of Automation. Nlpr face database. http: //nlpr-
",False,lfw_paper,False,False,False
15083,"web.ia.ac.cn/english/irds/facedatabase.htm.
",False,lfw_paper,False,False,False
15084,"[25] Eric Nowak and Fr´ ed´ eric Jurie. Learning visual simil arity measures for
",False,lfw_paper,False,False,False
15085,"comparing never seen objects. In CVPR , 2007.
",False,lfw_paper,False,False,False
15086,"[26] Georgia Institute of Technology. The Georgia Tech Face Database.
",False,lfw_paper,False,False,False
15087,"ftp://ftp.ee.gatech.edu/pub/users/hayes/facedb/.
",False,lfw_paper,False,False,False
15088,"[27] Derya Ozkan and Pinar Duygulu. A graph based approach fo r naming
",False,lfw_paper,False,False,False
15089,"faces in news photos. In CVPR , 2006.
",False,lfw_paper,False,False,False
15090,"[28] P. Jonathon Phillips, Patrick J. Flynn, Todd Scruggs, K evin Bowyer, Jin
",False,lfw_paper,False,False,False
15091,"Chang, Kevin Hoffman, Joe Marques, Jaesik Min, and William W orek.
",False,lfw_paper,False,False,False
15092,"Overview of the Face Recognition Grand Challenge. In CVPR , 2005.
",False,lfw_paper,False,False,False
15093,"[29] Ferdinando Samaria and Andy Harter. Parameterisation of a stochastic
",False,lfw_paper,False,False,False
15094,"model for human face identiﬁcation. In Proceedings of the Second
",False,lfw_paper,False,False,False
15095,"Workshop on Applications of Computer Vision, Sarasota, Flo rida, 1994.
",False,lfw_paper,False,False,False
15096,"[30] M. U. Ramos S` anchez, J. Matas, and J. Kittler. Statisti cal chromaticity
",False,lfw_paper,False,False,False
15097,"models for lip tracking with B-splines. In International Conference on
",False,lfw_paper,False,False,False
15098,"Audio- and Video-Based Biometric Person Authentication , 1997.
",False,lfw_paper,False,False,False
15099,"[31] C. Sanderson. Biometric Person Recognition: Face, Speech and Fusion .
",False,lfw_paper,False,False,False
15100,"VDM Verlag, 2008. ISBN 978-3-639-02769-3.
",False,lfw_paper,False,False,False
15101,"[32] Prag Sharma and Richard B. Reilly. A colour face image da tabase for
",False,lfw_paper,False,False,False
15102,"benchmarking of automatic face detection algorithms. In Proceedings
",False,lfw_paper,False,False,False
15103,"of EC-VIP-MC, the 4th EURASIP Conference focused on Video/I mage
",False,lfw_paper,False,False,False
15104,"Processing and Multimedia Communications , 2003.
",False,lfw_paper,False,False,False
15105,"[33] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination , and
",False,lfw_paper,False,False,False
15106,"expression database. PAMI , 25(12):1615–1618, 2003.
",False,lfw_paper,False,False,False
15107,"[34] Libor Spacek. University of Essex collection of facial images.
",False,lfw_paper,False,False,False
15108,"http://cswww.essex.ac.uk/mv/allfaces/index.html, 199 6.
",False,lfw_paper,False,False,False
15109,"[35] Paul Viola and Michael Jones. Robust real-time face det ection. IJCV ,
",False,lfw_paper,False,False,False
15110,"2004.
",False,lfw_paper,False,False,False
15111,"[36] Craig I. Watson. Nist mugshot identiﬁcation database.
",False,lfw_paper,False,False,False
15112,"http://www.nist.gov/srd/nistsd18.htm, 1994.
",False,lfw_paper,False,False,False
15113,"[37] B. Weyrauch, J. Huang, B. Heisele, and V . Blanz. Compone nt-based
",False,lfw_paper,False,False,False
15114,"face recognition with 3D morphable models. In First IEEE Workshop
",False,lfw_paper,False,False,False
15115,"on face processing in video , 2004.",False,lfw_paper,False,False,False
15116,Abstract,True,Low-Noise MEMS Vibration Sensor,False,False,False
15117,"I. INTRODUCTION
",True,Low-Noise MEMS Vibration Sensor,False,False,False
15118,"II. THEORY
",True,Low-Noise MEMS Vibration Sensor,False,False,False
15119,"III. ELECTRONICS DESIGN
",True,Low-Noise MEMS Vibration Sensor,False,False,False
15120,"IV. S
",True,Low-Noise MEMS Vibration Sensor,False,False,False
15121,"V. T
",True,Low-Noise MEMS Vibration Sensor,False,False,False
15122,"VI. D
",True,Low-Noise MEMS Vibration Sensor,False,False,False
15123,"See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/3329270
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15124,"Low-noise MEMS vibration sensor for geophysical applications
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15125,"Article    in  Journal of Micr oelectr omechanic al Syst ems  · Januar y 2000
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15126,"DOI: 10.1109/84.809058  · Sour ce: IEEE Xplor e
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15127,"CITATIONS
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15128,"132READS
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15129,"701
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15130,"4 author s, including:
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15131,"Some o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15132,"DARP A N- Zero View pr oject
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15133,"Epo xy curing char acterization  View pr oject
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15134,"Jonathan Bernst ein
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15135,"Draper Labor atory
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15136,"66 PUBLICA TIONS    1,973  CITATIONS    
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15137,"SEE PROFILE
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15138,"Raanan Miller
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15139,"Massachuse tts Instit ute of T echnolog y
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15140,"66 PUBLICA TIONS    3,326  CITATIONS    
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15141,"SEE PROFILE
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15142,"All c ontent f ollo wing this p age was uplo aded b y Jonathan Bernst ein on 10 F ebruar y 2015.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15143,"The user has r equest ed enhanc ement of the do wnlo aded file.JOURNAL OF MICROELECTROMECHANICAL SYSTEMS, VOL. 8, NO. 4, DECEMBER 1999 433
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15144,"Low-Noise MEMS Vibration Sensor
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15145,"for Geophysical Applications
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15146,"Jonathan Bernstein, Member, IEEE, Raanan Miller, William Kelley, and Paul Ward, Member, IEEE
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15147,"tion sensors for various applications, such as geophysical datacollection, tracking vehicles, intrusion detectors, and underwaterpressure gradient detection. In general, these sensors differ fromclassical accelerometers in that they require no direct currentresponse, but must have a very low noise ﬂoor over a requiredbandwidth. Theory indicates a capacitive micromachined sili-con vibration sensor can have a noise ﬂoor on the order of100 ng/
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15148,"/112
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15149,"/72/122over 1-kHz bandwidth, while reducing size and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15150,"weight tenfold compared to existing magnetic geophones. Withearly prototypes, we have demonstrated Brownian-limited noise
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15151,"ﬂoor at 1.0
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15152,"/22g//112
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15153,"/72/122 /59orders of magnitude more sensitive than
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15154,"surface micromachined devices such as the industry standardADXL05. [376]
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15155,"Index Terms— Acceleration measurement, capacitance trans-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15156,"ducers, geophysical measurements, microelectromechanical de-vices, micromachining.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15157,"I. INTRODUCTION
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15158,"AVIBRATION sensor can be thought of as a very high-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15159,"sensitivity accelerometer with no direct current (dc)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15160,"output requirement. With no drift or bias stability speci-ﬁcations, the design can be optimized to give the lowest
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15161,"noise ﬂoor. Applications for these devices include geophysical
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15162,"sensing, machinery vibration and failure prediction, trackingand identiﬁcation of vehicles or personnel, and underwaterpressure gradient sensing.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15163,"Traditional vibration sensors using permanent magnets and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15164,"ﬁne wire coils are called geophones,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15165,"1which measure veloc-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15166,"ityabove the fundamental resonance. This is in contrast to
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15167,"capacitive accelerometers that measure acceleration below
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15168,"their fundamental resonance. Piezoelectric and ferroelectric
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15169,"accelerometers are also used for these applications. Micro-machined sensors can offer size and weight advantages overtraditional sensors.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15170,"Previous efforts to make micromachined high-resolution
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15171,"vibration sensors or accelerometers have included capacitive[1]–[3], tunneling [4], [5], piezoresistive, optical, and piezo-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15172,"electric sensors. Capacitive sensors have the advantage of no
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15173,"exotic materials, low noise, and compatibility with CMOSreadout electronics. Tunneling sensors have a low noise ﬂoor,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15174,"Manuscript received August 8, 1998; revised June 14, 1999. This paper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15175,"was presented in part at the Transducers Research Foundation Hilton Head
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15176,"Workshop on Solid-State Sensor and Actuators, Hilton Head, SC, 1998.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15177,"Subject Editor, S. Tabata.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15178,"The authors are with the Charles Stark Draper Laboratory, Cambridge, MA
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15179,"02139-3563 USA
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15180,"Publisher Item Identiﬁer S 1057-7157(99)09609-2.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15181,"1Geospace, Inc., GS-14 geophone, Houston, TX 77040 USA.but due to the small allowable displacement at the tip require a
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15182,"very stiff feedback loop, which reduces the useful bandwidth
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15183,"and dynamic range.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15184,"II. THEORY
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15185,"Thesensor ismodeledas aspring-mass-dampersystemwith
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15186,"capacitive pickoff. Because no dc output is required, it was
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15187,"decided forinitial teststo build a single-capacitor sensor ratherthan a differential capacitor design. Noise sources modeledinclude Brownian mechanical noise from air damping and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15188,"electronic noise from the readout circuit [6].
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15189,"The Brownian force is
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15190,"Hz[6], which
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15191,"causes Brownian motion of the proof mass
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15192,"mHz (1)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15193,"where is the damping coefﬁcient of the proof mass
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15194,"supported by spring constant Solving for the acceleration
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15195,"which generates the same motion and substituting
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15196,"and m/sgives for
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15197,"Brownian equivalent acceleration noise in g/ Hz
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15198,"gHz(2)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15199,"From (2), we see that a large mass and high (low
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15200,"damping) are helpful to achieve a low noise ﬂoor. To achievea large mass in a micromachined sensor, one typically uses awafer-thick proof mass carved from the sensor chip.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15201,"In order to use a sensor with a high
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15202,", it must be force-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15203,"rebalanced to prevent ringing at the resonant frequency. Inthe work presented here, the sensors are overdamped, with a
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15204,"of about 0.3. A vacuum package is typically necessary to
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15205,"achieve very high 10000).
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15206,"The sensitivity of the device is calculated for the simple
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15207,"case of a dc bias voltage with a high-input impedance bufferampliﬁer. A bias voltage of
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15208,"the snap-down voltage
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15209,"is assumed. The sensitivity at low frequency is the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15210,"volts/meter in the sense gap times the meters/g of the proofmass well below resonance
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15211,"V/g (3)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15212,"The snap-down voltage is where
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15213,"is the capacitor sense gap and is the capacitor area.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15214,"1057–7157/99$10.00 1999 IEEE434 JOURNAL OF MICROELECTROMECHANICAL SYSTEMS, VOL. 8, NO. 4, DECEMBER 1999
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15215,"Fig. 1. Total noise in decibels referenced to 1 g//112
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15216,"Hz /59as a function of /81
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15217,"and front-end electrical noise.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15218,"Substituting for the mass andgives for sensitivity
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15219,"V/g (4)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15220,"whereis the proof mass thickness, and is the density
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15221,"of silicon. From this, we see that sensitivity is inverselyproportional to the fundamental resonant frequency.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15222,"We consider also an electronic readout noise component,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15223,"which has (in general) equivalent input current noise and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15224,"voltagenoise components.Thetotalequivalentfront-endnoiseis called
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15225,"This voltage noise can be converted to an equiv-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15226,"alent acceleration noise in ’s by dividing by the transducer
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15227,"sensitivity in volts per gram (V/g)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15228,"gHz (5)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15229,"The total noise is the rms combination of electrical and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15230,"Brownian contributions
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15231,"g/Hz (6)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15232,"Voltage noise for a low-noise CMOS front end is typically
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15233,"in the 5–10-nV/ Hz range above the corner. Lower
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15234,"noise can be obtained, but at the cost of increased currentconsumption.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15235,"To achieve a noise level of 10 ng, a vacuum package must
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15236,"be used to reduce Brownian motion noise.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15237,"Fig. 1 is a contour map of combined g-equivalent noise as
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15238,"a function of electrical noise (1 nV to 100 nV/
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15239,"Hzand(1
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15240,"to 10The decibel scale on the map is decibels referenced
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15241,"to 1 g/Hzhence g/Hz corresponds to 160 dB. To
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15242,"achieve a noise level of 10 ng/ Hz requires a of 3000
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15243,"and a front-end equivalent noise of under 2 nV/ HzFor this
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15244,"calculation, the silicon proof mass is assumed to be 4 mm
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15245,"4m m 0.38 mm with a resonant frequency of 1 kHz.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15246,"Table I lists the design goals for the three resonant frequen-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15247,"cies of the fabricated devices.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15248,"III. ELECTRONICS DESIGN
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15249,"To evaluate the vibration sensor prototypes, two types of
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15250,"electronics were used: an open-gate JFET source followerand a custom CMOS application-speciﬁed integrated circuitTABLE I
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15251,"DESIGNGOALS FOR MICROMACHINED VIBRATION SENSOR
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15252,"Fig. 2. Open-gate JFET buffer circuit.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15253,"(ASIC) fabricated at Orbit Semiconductor.2The JFET buffer
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15254,"allowed quick, reasonably low noise measurements to be
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15255,"made with a very compact circuit (inside the sensor package),
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15256,"although and current noise are high at low frequencies.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15257,"The ASIC uses a carrier to reduce noise, resulting in
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15258,"better low-frequency performance.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15259,"Fig. 2 shows the open-gate JFET buffer circuit used. This
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15260,"circuit has a dc input impedance of several teraohms, whichcombined with a typical sensor capacitance of 50 pF gives anRCtime constant of several minutes. To speed the approach
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15261,"to the stable bias point, it was sometimes necessary to shine
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15262,"light in the package. The light temporarily increases the JFETleakage current, thereby decreasing the impedance of the gatenode. This reset technique is sometimes used with nuclear
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15263,"particle counters to reset a critical node without adding stray
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15264,"capacitance and leakage current.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15265,"A custom CMOS mixed signal ASIC was designed in-house
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15266,"and fabricated at Orbit Semiconductor. The design implements
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15267,"synchronous modulation/demodulation using square waves
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15268,"and a novel radio frequency (RF) rebalance technique. Theblock diagram of the system is shown in Fig. 3. The circuitrebalances the proof mass below the frequency range of
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15269,"interest, while allowing higher frequency vibrations to move
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15270,"the proof mass open-loop.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15271,"Equal andopposite 100-kHz square wavesare appliedto the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15272,"sensor and a reference capacitor. Vibrations cause a mismatch
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15273,"between the ﬁxed capacitor and the time-varying sensor. The
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15274,"charge ampliﬁer converts the capacitance mismatch into anoutput voltage which is then ampliﬁed by the alternatingcurrent (ac) gain stage.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15275,"2Orbit Semiconductor, Inc., Sunnyvale, CA, USA.BERNSTEIN et al.: LOW-NOISE MEMS VIBRATION SENSOR 435
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15276,"Fig. 3. CMOS ASIC block diagram.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15277,"The resulting vibration signal is amplitude modulated on a
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15278,"100-kHz square wave carrier. The ampliﬁer stages process thissignal at 100 kHz to avoid
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15279,"noise in the CMOS transistors.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15280,"The vibration signal is demodulated after ac ampliﬁcation to
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15281,"recover the vibration signal. The output of the demodulator
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15282,"is an error signal representing an acceleration or capacitancemismatch between the sensor and the reference capacitor. Thehigh frequency part of this signal is passed to an external low-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15283,"noise ampliﬁer. The low frequency part is integrated, inverted,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15284,"and low-pass ﬁltered to create the low and high carrier rails.The difference between the low and high rails represents thelow-frequency feedback used to force the sensor capacitance
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15285,"back to the reference capacitor value. The modulator has three
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15286,"inputs, the low and high rails, and the clock signal. The outputof this modulator is switched between the high and low rails
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15287,"to create a variable amplitude square wave carrier, which is
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15288,"applied to the sense capacitor. The square wave is inverted todrive the reference capacitor.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15289,"Application of ac voltage to the sensor applies a force pro-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15290,"portional to the square of the applied voltage amplitude. The
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15291,"integral rebalance controller adjusts the carrier amplitude andtunes the time average sensor capacitance to match the ﬁxedreference capacitor. Under open loop operation, maximum g’s
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15292,"are determined when the proof mass moves about 10% of
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15293,"the sense gap. Closed-loop maximum g’s are determined bythe maximum available rebalance voltage, which is typicallylimited to some fraction of snap-down voltage.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15294,"The bandwidth of the rebalance loop is adjustable and was
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15295,"selected to be low
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15296,"1 Hz). The rebalance loop nulls dc and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15297,"sub 1-Hz accelerations maintaining signal null with changes intemperature and sensor orientation. This allows high gain for
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15298,"ac signals and avoids saturation of the 5-V CMOS electronics.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15299,"Vibrations above 1 Hz are not rebalanced and are sensedopen-loop from the demodulator output.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15300,"IV. S
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15301,"ENSORFABRICATION
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15302,"The sensors are fabricated on 0.38-mm-thick double-side
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15303,"polished wafers using the Bosch process in a surface tech-nology systems (STS) etcher. A recess 3-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15304,"m deep is etched
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15305,"into the wafer to deﬁne anchors and create the sense gap. 30-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15306,"m-deep damping-relief trenches are then etched to reduce
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15307,"squeeze-ﬁlm damping. A 10- m-thick boron diffusion is used
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15308,"to create an etch stop layer on both faces of the wafer. After
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15309,"electrostatic bonding to a glass wafer with readout electrodes,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15310,"the STS etcher is used to trench through the wafer. A briefanisotropic etch then undercuts the springs.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15311,"Fig. 4. Vibration sensor structure etched through the wafer (380 /22m), before
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15312,"anisotropic etch.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15313,"Fig. 5. Corner of device showing electrodes and damping-reduction
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15314,"trenches.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15315,"Fig. 6. Vibration sensor after anisotropic etching, leaving thin boron-dopedﬂexures supporting large proof mass.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15316,"Figs. 4 and 5 show a sensor chip after the deep inductively
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15317,"coupled plasma etch and before the anisotropic etch. A centralproof mass is supported by springs attached to four anchors on
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15318,"a glass substrate. Damping-relief trenches are visible in Fig. 5
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15319,"facing the glass substrate. Fig. 6 shows a completed prototypesensor.436 JOURNAL OF MICROELECTROMECHANICAL SYSTEMS, VOL. 8, NO. 4, DECEMBER 1999
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15320,"TABLE II
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15321,"TESTRESULTS FROM CMOS ASIC/V IBRATION SENSORINTEGRATION
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15322,"Fig. 7.CVcurve. Capacitance varies 17 pF (22%) at 4-V bias.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15323,"Fig. 8. Frequency response of 3 mm /23 mm device, design resonance at
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15324,"1 kHz, /81 /61/48 /58 /51 /58
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15325,"Devices were designed and fabricated with three proof
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15326,"mass sizes (3 mm 4m mand 5 mm ) and three resonant
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15327,"frequencies (500 Hz, 1 kHz, and 10 kHz) to cover various
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15328,"applications. Kovar ﬂat-packs were used to house the sensorswith internal (JFET) preamp or external (custom CMOSbuffer) readout circuit.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15329,"V. T
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15330,"ESTRESULTS
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15331,"Chipleveltestingincludes capacitance–voltage(CV) curves
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15332,"(Fig. 7) and current–voltage curves to measure leakage resis-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15333,"tance (typically greater than 1 T ). Frequency response with
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15334,"the open-gate JFET buffer is shown in Fig. 8. A sensitivity
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15335,"Fig. 9. Plot of acceleration noise in g//112
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15336,"Hz using source-follower buffer
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15337,"circuit. Device is same as Fig. 8.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15338,"Fig. 10. Baseband versus synchronous modulation/demodulation noise with
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15339,"Orbit CMOS ASIC readout chip.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15340,"of 42 mV/g was obtained at a bias voltage of 3.7 V. The
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15341,"noise ﬂoor (measured on a vibration isolated platform) was1.5
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15342,"g/Hz (Fig. 9), limited by Brownian noise for this over-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15343,"damped device. A battery powered low-noise pre-amp using
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15344,"OP 37 op-amps was necessary to lift the signal level above
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15345,"the noise ﬂoor of the HP 3563A dynamic signal analyzer fornoise measurements.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15346,"Fig. 10 shows the advantage of the RF carrier readout
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15347,"chip over a baseband CMOS readout. Orders of magnitude
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15348,"reduction in
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15349,"noise are achieved at low frequencies using
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15350,"this ASIC.BERNSTEIN et al.: LOW-NOISE MEMS VIBRATION SENSOR 437
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15351,"TABLE III
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15352,"VIBRATION SENSOR/ACCELEROMETER SURVEY
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15353,"Table II summarizes test results using the CMOS ASIC,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15354,"including electrical noise, ac gain, theoretical noise, and mea-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15355,"sured noise ﬂoor. A noise ﬂoor of 1.0 g/Hz was achieved
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15356,"with a 1-kHz nominal device, limited by Brownian noise.The predicted electrical noise ﬂoor was only 0.12
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15357,"g/Hz
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15358,"for this device. An electrical noise power spectral density of
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15359,"0.4g/Hz was measured with the bias voltage turned down
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15360,"to zero, which removes the Brownian motion component. Thehigher measured noise is due to higher than expected dampingand electrical voltage noise.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15361,"VI. D
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15362,"ISCUSSION AND CONCLUSIONS
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15363,"Table III is a summary of commercial off-the-shelf (COTS)
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15364,"and research vibration sensors about which information was
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15365,"available. For each device, the bandwidth (BW), noise ﬂoor(NF), and aﬁgure of merit (FOM) aregiven (FOM
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15366,"BW/NF).
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15367,"This FOM is independent of the BW for a given family
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15368,"of capacitive sensors, given constant electrical noise and the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15369,"relations between BW, sensitivity, and maximum bias voltage(which is a ﬁxed fraction of the snap-down voltage). One cansee from the table that the theoretical performance is 30 times
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15370,"better than that of the best experimental devices, which are in
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15371,"turn orders of magnitude better than current COTS devices.One can conclude that future devices will push down the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15372,"noise ﬂoor to well under 0.1
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15373,"g/Hz with a 1-kHz sensing
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15374,"bandwidth. At this sensitivity, they could replace geophonesin some applications.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15375,"In conclusion, a bulk micromachined vibration sensor has
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15376,"been fabricated and tested with a novel, custom CMOS ASIC,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15377,"which removes
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15378,"noise while requiring only a single sense
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15379,"capacitor and one dummy capacitor. Vibration noise as low as1.5
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15380,"g/Hz was achieved with a JFET buffered sensor and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15381,"1g/Hz using a custom CMOS ASIC. Theory indicates that
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15382,"orders of magnitude improvements can be made over industrystandard COTS sensor in both bandwidth and noise ﬂoor bythe use of wafer-thick bulk micromachining technology with
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15383,"3Centre Suisse d’Electronique et de Microtechnique SA (CSEM).optimized low-noise electronics. These sensors will be usefulfor geophysical work, machinery monitoring, and sensing ofvehicles and people.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15384,"A
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15385,"CKNOWLEDGMENT
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15386,"The authors would like to thank the Charles Stark Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15387,"Laboratory for supporting this work.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15388,"REFERENCES
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15389,"[1] W.Henrion,L.DiSanza,M.Ip,S.Terry,andH.Jerman,“Widedynamic
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15390,"rangedirectdigitalaccelerometer,”in Tech.Dig.1990Solid-StateSensor
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15391,"and Actuator Workshop , Hilton Head Island, SC, June 4–7, 1990, pp.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15392,"153–157.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15393,"[2] K. Bult, A. Burstein, and D. Chang, “Wireless integrated microsensors,”
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15394,"inProc. 1996 Hilton Head Solid State Sensor and Actuator Conf. , 1996,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15395,"pp. 205–210.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15396,"[3] M. Lemkin etal., “A 3-axis force balanced accelerometer using a single
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15397,"proof mass,” in Proc. 9th Int. Conf. Solid-State Sensors and Actuators-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15398,"Transducers ’97 , Chicago IL, June 1977, pp. 1185–1188.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15399,"[4] C. H. Liu, J. D. Grade, A. M. Barzilai, K. K. Reynolds, A. Partridge,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15400,"J. J. K. Rockstad, and T. W. Kenney, “Characterization of a highly
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15401,"sensitive tunneling accelerometer,” in Transducers’97Tech. Dig. , 1997,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15402,"pp. 471–472, paper 2B3.07.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15403,"[5] H. K. Rockalnd, T. W. Kenny, P. Kelley, and T. Gabrielson, “A micro-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15404,"fabricated electron-tunneling accelerometer as a directional underwater
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15405,"acoustic sensor,” in Proc. Acoustic Particle Velocity Sensors: Design,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15406,"Performance, and Applications , 1996, pp. 57–68.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15407,"[6] T. B. Gabrielson, “Mechanical-thermal noise in micromachined acoustic
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15408,"and vibration sensors,” IEEE Trans. Electron Devices , vol. 40, pp.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15409,"903–909.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15410,"Jonathan Bernstein (S’80–M’82) received the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15411,"B.S.E.E. degree with honors in engineering-physicsfrom Princeton University, Princeton, NJ, and the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15412,"M.S.E.E. and Ph.D. degrees from the University
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15413,"of California at Berkeley. In addition, he received
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15414,"National Science Foundation and Hertz Foundation
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15415,"graduate fellowships.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15416,"Currently, he is with the Charles Stark Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15417,"Laboratory, Cambridge, MA, as the Task Leaderformicromechanicalacousticsensors(hydrophones,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15418,"microphone, and vibration sensors), accelerometers,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15419,"and advanced micromachined tuning-fork gyroscopes, in which capacity he
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15420,"has designed, analyzed, and fabricated these transducers. He is responsible
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15421,"for the process development for silicon monolithic sensors, including
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15422,"single crystal silicon, polysilicon, PZT-on-Si, and electroformed metal
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15423,"microstructures. He has also carried out process development to combine
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15424,"on-chip JFET circuitry with these micromechanical sensors and PZT-on-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15425,"Silicon technology.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15426,"Dr. Bernstein has received Draper’s Distinguished Performance Award,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15427,"Best Invention Award (twice), and Best Publication Award.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15428,"Raanan Miller received the Ph.D. degree in elec-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15429,"trical engineering from the California Institute of
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15430,"Technology (Caltech), Pasadena, CA, in 1997
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15431,"Currently, he is with the Charles Stark Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15432,"Laboratory, Cambridge, MA, as a Senior Mem-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15433,"ber of the Technical Staff in the micromechanical
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15434,"group. He is involved in process development fornext-generation MEMS gyroscopes and accelerom-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15435,"eters. In addition, he has designed, fabricated, and
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15436,"tested a micromachined ﬁeld asymmetric ion mobil-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15437,"ity (FAIM) spectrometer for chemical warfare agent
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15438,"detection.438 JOURNAL OF MICROELECTROMECHANICAL SYSTEMS, VOL. 8, NO. 4, DECEMBER 1999
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15439,"William Kelley received the B.S.E.E. degree ( cum
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15440,"laude) from Northeastern University, Boston, MA,
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15441,"in 1991
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15442,"Currently, he is with the Charles Stark Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15443,"Laboratory, Cambridge, MA, as a Senior Engineer
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15444,"in the microelectronics group. He is the co-creatoron two patents: RF Balanced Capacitive Vibra-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15445,"tion Sensor System and Micromechanical Pressure
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15446,"Gauge having Extended Sensor Range.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15447,"Mr. Kelley has received two Draper awards as
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15448,"a member of the ASIC design team for First-Pass
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15449,"Success. In addition, he has received an Outstanding Performance Award for
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15450,"contributions to the development of the micromechanical gyro.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15451,"PaulWard (S’83–M’85)receivedtheB.S.andM.S.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15452,"degrees in electrical engineering from Northeastern
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15453,"University, Boston, MA.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15454,"In 1985, he joined the Charles Stark Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15455,"Laboratory, Cambridge, MA, where he is currently
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15456,"a Principal Engineer and Microelectronics GroupLeader. He is the principal engineer for Draper’s
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15457,"micromachined gyroscope and accelerometer elec-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15458,"tronics, including application-speciﬁc integrated cir-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15459,"cuits (ASIC’s). He has been the principal electrical
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15460,"engineer for many challenging projects, including
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15461,"ppm-level radiation test systems, electron-spin and nuclear-magnetic reso-
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15462,"nance precision signal references, resonator and interferometer ﬁber-opticgyroscopes, and most recently, micromechanical instrument electronics. He
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15463,"holds 11 U.S. patents, has many patents pending, and has co-authored
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15464,"numerous papers.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15465,"Mr. Ward is a member of Eta Kappa Nu and ARRL. He received the
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15466,"1994 Draper Distinguished Performance Award along with others for their
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15467,"work on the micromechanical gyroscope and electronics and the 1997 Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15468,"Distinguished Performance Award for his work on a commercially viable
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15469,"yaw rate sensor instrument. In addition, he has received numerous Draper
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15470,"Recognition Awards, as well as the 1996, 1997, and 1998 Best TechnicalPatent Awards.
",False,Low-Noise MEMS Vibration Sensor,False,False,False
15471,View publication stats,False,Low-Noise MEMS Vibration Sensor,False,False,False
15472,Abstract,True,lownoise-Aug_2002,False,False,False
15473,"I. INTRODUCTION
",True,lownoise-Aug_2002,False,False,False
15474,"II. F
",True,lownoise-Aug_2002,False,False,False
15475,"III. DESIGNCONSIDERATIONS FOR A TUNEDAMPLIFIER
",True,lownoise-Aug_2002,False,False,False
15476,"IV. EXPERIMENTAL RESULTS
",True,lownoise-Aug_2002,False,False,False
15477,"V. CONCLUSION
",True,lownoise-Aug_2002,False,False,False
15478,"994 IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 37, NO. 8, AUGUST 2002
",False,lownoise-Aug_2002,False,False,False
15479,"A Noise Optimization Technique for Integrated
",False,lownoise-Aug_2002,False,False,False
15480,"Low-Noise Amplifiers
",False,lownoise-Aug_2002,False,False,False
15481,"Jung-Suk Goo , Member, IEEE , Hee-Tae Ahn , Member,IEEE , Donald J. Ladwig, Zhiping Yu , Senior Member,IEEE ,
",False,lownoise-Aug_2002,False,False,False
15482,"Thomas H. Lee , Member, IEEE , and Robert W. Dutton , Fellow, IEEE
",False,lownoise-Aug_2002,False,False,False
15483,"two-port noise theory, considerations for noise optimization ofintegrated low-noise amplifier (LNA) designs are presented. Ifarbitrary values of source impedance are allowed, optimal noise
",False,lownoise-Aug_2002,False,False,False
15484,"performance of the LNA is obtained by adjusting the source
",False,lownoise-Aug_2002,False,False,False
15485,"degeneration inductance. Even for a fixed source impedance,the integrated LNA can achieve near
",False,lownoise-Aug_2002,False,False,False
15486,"/109/105/110by choosing an
",False,lownoise-Aug_2002,False,False,False
15487,"appropriatedevicegeometryalongwithanoptimalbiascondition.An 800-MHz LNA has been implemented in a standard 0.24-
",False,lownoise-Aug_2002,False,False,False
15488,"m
",False,lownoise-Aug_2002,False,False,False
15489,"CMOS technology. The amplifier possesses a 0.9-dB noise figurewith a 7.1-dBm third-order input intercept point, while drawing7.5 mW from a 2.0-V power supply, demonstrating that theproposed methodology can accurately predict noise performanceof integrated LNA designs.
",False,lownoise-Aug_2002,False,False,False
15490,"Index Terms— Amplifier noise, induced gate noise, low-noise
",False,lownoise-Aug_2002,False,False,False
15491,"amplifier, microwave amplifier, MOSFET amplifier, noise figure,random noise, semiconductor device noise.
",False,lownoise-Aug_2002,False,False,False
15492,"I. INTRODUCTION
",False,lownoise-Aug_2002,False,False,False
15493,"THE FIRST stage of a receiver is typically a low-noise
",False,lownoise-Aug_2002,False,False,False
15494,"amplifier (LNA). The LNA design involves tradeoffs
",False,lownoise-Aug_2002,False,False,False
15495,"between many figures of merit, such as gain, noise, power,
",False,lownoise-Aug_2002,False,False,False
15496,"impedance matching, stability, and linearity. Since the primary
",False,lownoise-Aug_2002,False,False,False
15497,"roleoftheLNAistolowertheoverallnoisefigureoftheentirereceiver, noise optimization is one of the most critical steps intheLNAdesignprocedure.Intraditionalmonolithicmicrowave
",False,lownoise-Aug_2002,False,False,False
15498,"integrated circuit (MMIC) design, active devices are given
",False,lownoise-Aug_2002,False,False,False
15499,"with fixed geometries and characteristics. For the given biasand frequency conditions, a source impedance
",False,lownoise-Aug_2002,False,False,False
15500,"is selected
",False,lownoise-Aug_2002,False,False,False
15501,"to minimize the noise figure [1]. Since the optimum source
",False,lownoise-Aug_2002,False,False,False
15502,"impedance for noise ( ) differs from the power-match
",False,lownoise-Aug_2002,False,False,False
15503,"conditionin general,thistechniqueoftenresults inlargepowerconsumption or input mismatching. Even in full custom ICs,despite an important option that the designer can choose the
",False,lownoise-Aug_2002,False,False,False
15504,"desireddevicegeometries,mostdesignersstillrelyonthesame
",False,lownoise-Aug_2002,False,False,False
15505,"optimization techniques [2]–[4] because no explicit guidanceis generally available on how to best exercise the IC designer’sfreedom in tailoring device geometries. They can achieve
",False,lownoise-Aug_2002,False,False,False
15506,"an optimum noise figure with acceptable input mismatching
",False,lownoise-Aug_2002,False,False,False
15507,"ManuscriptreceivedJanuary23,2001;revisedApril15,2002.Thisworkwas
",False,lownoise-Aug_2002,False,False,False
15508,"initiated by the Defense Advanced Research Projects Agency under Contract
",False,lownoise-Aug_2002,False,False,False
15509,"Army-DABT63-94-C-0055 and supported by Texas Instruments Incorporated
",False,lownoise-Aug_2002,False,False,False
15510,"through customized research under SRC Contract 99-NJ-695.
",False,lownoise-Aug_2002,False,False,False
15511,"J.-S.GooiswithAdvancedMicroDevices,Sunnyvale,CA94088-3453USA
",False,lownoise-Aug_2002,False,False,False
15512,"(e-mail: goojs@gloworm.stanford.edu).
",False,lownoise-Aug_2002,False,False,False
15513,"H.-T. Ahn is with Qualcomm, San Diego, CA 92121-1714 USA.
",False,lownoise-Aug_2002,False,False,False
15514,"D.J.LadwigiswithTexasInstrumentsIncorporated,Dallas,TX75243USA.Z.Yu,T.H.Lee,andR.W.DuttonarewiththeCenterforIntegratedSystems,
",False,lownoise-Aug_2002,False,False,False
15515,"Stanford University, Stanford, CA 94305-4075 USA.
",False,lownoise-Aug_2002,False,False,False
15516,"Publisher Item Identifier 10.1109/JSSC.2002.800956.(typically dB) but do not fully exploit the potential
",False,lownoise-Aug_2002,False,False,False
15517,"of integrated LNAs. Recently proposed noise optimization
",False,lownoise-Aug_2002,False,False,False
15518,"techniques for CMOS RF circuits permit greater flexibilityin the selection of device geometries as well as matchingelements and biasing conditions to minimize the noise figure
",False,lownoise-Aug_2002,False,False,False
15519,"for a specified gain or power dissipation [5], [6]. However,
",False,lownoise-Aug_2002,False,False,False
15520,"they use simplified small-signal models as well as constantnoise characteristics. These techniques also rely heavily onmathematical derivations that provide limited intuitive design
",False,lownoise-Aug_2002,False,False,False
15521,"guidance.
",False,lownoise-Aug_2002,False,False,False
15522,"This paper presents considerations for noise optimization
",False,lownoise-Aug_2002,False,False,False
15523,"of LNAs based directly on measured noise parameters andtwo-port noise theory; the approach requires neither sophisti-
",False,lownoise-Aug_2002,False,False,False
15524,"cated noise modeling nor circuit simulation to be used. All the
",False,lownoise-Aug_2002,False,False,False
15525,"analyses arebased onMOSFET designs,butthe same method-ologycanbeappliedtootherICtechnologies,suchasBiCMOSor heterojunction bipolar transistors (HBT). Section II reviews
",False,lownoise-Aug_2002,False,False,False
15526,"thebasicconceptofthenoisefigureandfour-noiseparameters.
",False,lownoise-Aug_2002,False,False,False
15527,"It also discusses the intrinsic noise model of the MOSFET andits relation to the measured noise performance of amplifiers.Section III explains how the noise performance of the LNA
",False,lownoise-Aug_2002,False,False,False
15528,"differs from that of the intrinsic device; design considerations
",False,lownoise-Aug_2002,False,False,False
15529,"for a CMOS-tuned LNA with power constraints are presented.SectionIVpresentsexperimentalresultsforanimplementationusing integrated CMOS technology to realize an LNA.
",False,lownoise-Aug_2002,False,False,False
15530,"II. F
",False,lownoise-Aug_2002,False,False,False
15531,"UNDAMENTAL NOISETHEORY FOR CMOS C IRCUITS
",False,lownoise-Aug_2002,False,False,False
15532,"A. Concept of the Four-Noise Parameters
",False,lownoise-Aug_2002,False,False,False
15533,"The noise performance of a circuit is usually characterized
",False,lownoise-Aug_2002,False,False,False
15534,"by a parameter called noise factor ()o rnoise figure (
",False,lownoise-Aug_2002,False,False,False
15535,") that represents how much the given system degrades
",False,lownoise-Aug_2002,False,False,False
15536,"the signal-to-noise ratio [1].
",False,lownoise-Aug_2002,False,False,False
15537,"(1)
",False,lownoise-Aug_2002,False,False,False
15538,"TotalOutput Noise Power
",False,lownoise-Aug_2002,False,False,False
15539,"Output Noise Power bySource Impedance(2)
",False,lownoise-Aug_2002,False,False,False
15540,"Atonefrequency,thenoisefactorofalinearcircuitshowsapar-
",False,lownoise-Aug_2002,False,False,False
15541,"abolic dependence on the source impedance driving the given
",False,lownoise-Aug_2002,False,False,False
15542,"circuit. This behavior results in constant noise circles on the
",False,lownoise-Aug_2002,False,False,False
15543,"Smith chart and can be characterized in terms of the four-noiseparameters [7] as follows:
",False,lownoise-Aug_2002,False,False,False
15544,"(3)
",False,lownoise-Aug_2002,False,False,False
15545,"0018-9200/02$17.00 © 2002 IEEEGOOet al.: NOISE OPTIMIZATION TECHNIQUE FOR INTEGRATED LOW NOISE AMPLIFIERS 995
",False,lownoise-Aug_2002,False,False,False
15546,"where istheminimumnoisefactor, andarerealand
",False,lownoise-Aug_2002,False,False,False
15547,"imaginary parts, respectively, of the source admittance (
",False,lownoise-Aug_2002,False,False,False
15548,");and arerealandimaginaryparts,respectively,
",False,lownoise-Aug_2002,False,False,False
15549,"oftheoptimumsourceadmittance( ,alsoknown
",False,lownoise-Aug_2002,False,False,False
15550,"asthenoise-matchingcondition),and istheequivalentnoise
",False,lownoise-Aug_2002,False,False,False
15551,"resistance. When is adjusted to , the circuit yields the
",False,lownoise-Aug_2002,False,False,False
15552,"bestachievablenoiseperformance .Ifdiffersfrom ,
",False,lownoise-Aug_2002,False,False,False
15553,"itsimpacton isamplifiedby .Evenif issufficiently
",False,lownoise-Aug_2002,False,False,False
15554,"low, large and a poor proximity between andresult
",False,lownoise-Aug_2002,False,False,False
15555,"in an unacceptably large noise figure in the actual circuit. Thisproblem becomes acute for MOSFET circuits because the re-
",False,lownoise-Aug_2002,False,False,False
15556,"flection coefficient
",False,lownoise-Aug_2002,False,False,False
15557,"1for optimum noise ( ) is nearly 1 and
",False,lownoise-Aug_2002,False,False,False
15558,"is3–10timeslargerthanforhighelectronmobilitytransistor
",False,lownoise-Aug_2002,False,False,False
15559,"(HEMT) devices [8].
",False,lownoise-Aug_2002,False,False,False
15560,"B. High-Frequency Noise in MOSFETs
",False,lownoise-Aug_2002,False,False,False
15561,"The thermal fluctuations of channel charge in the MOSFET
",False,lownoise-Aug_2002,False,False,False
15562,"produceeffectsthataremodeledbydrainandgatecurrentnoise
",False,lownoise-Aug_2002,False,False,False
15563,"generators[9].Thesecurrentsarepartiallycorrelatedwitheachotherbecausetheyshareacommonoriginandpossessaspectralpower given by
",False,lownoise-Aug_2002,False,False,False
15564,"(4)
",False,lownoise-Aug_2002,False,False,False
15565,"(5)
",False,lownoise-Aug_2002,False,False,False
15566,"(6)
",False,lownoise-Aug_2002,False,False,False
15567,"where is the drain output conductance under zero drain
",False,lownoise-Aug_2002,False,False,False
15568,"bias, is the real part of gate-to-source
",False,lownoise-Aug_2002,False,False,False
15569,"admittance,and ,,andarebias-dependentfactors.Forlong-
",False,lownoise-Aug_2002,False,False,False
15570,"channel MOSFETs, ,, andare, respectively, 2/3, 4/3, and
",False,lownoise-Aug_2002,False,False,False
15571,"0.395 in the saturation region, but short-channel MOSFETs
",False,lownoise-Aug_2002,False,False,False
15572,"exhibitlargervalues[5],[10].Theseexpressionsimplythatthespectral power density scales with the device width
",False,lownoise-Aug_2002,False,False,False
15573,".
",False,lownoise-Aug_2002,False,False,False
15574,"C. Scaling of the Noise Parameters
",False,lownoise-Aug_2002,False,False,False
15575,"In realizing a custom IC design of the LNA, one of the key
",False,lownoise-Aug_2002,False,False,False
15576,"issues is to understand the device scaling effects on the noise
",False,lownoise-Aug_2002,False,False,False
15577,"parameters. The four-noise parameters can be derived2from
",False,lownoise-Aug_2002,False,False,False
15578,"current noise spectral power, given in (4)–(6), as follows:
",False,lownoise-Aug_2002,False,False,False
15579,"(7)
",False,lownoise-Aug_2002,False,False,False
15580,"(8)
",False,lownoise-Aug_2002,False,False,False
15581,"(9)
",False,lownoise-Aug_2002,False,False,False
15582,"(10)
",False,lownoise-Aug_2002,False,False,False
15583,"Equation(7)suggeststhatdeviceswithshorterchannellength
",False,lownoise-Aug_2002,False,False,False
15584,"yield better noise figures because the angular cutoff frequency
",False,lownoise-Aug_2002,False,False,False
15585,"1Defined as /0 /61/40 /89 /0 /89 /41 /61 /40 /89 /43 /89 /41, where /89is the characteristic ad-
",False,lownoise-Aug_2002,False,False,False
15586,"mittance of the transmission line.
",False,lownoise-Aug_2002,False,False,False
15587,"2Exactexpressionscanbederivedbasedonthetwo-porttheory[11],[12].The
",False,lownoise-Aug_2002,False,False,False
15588,"approximated (7)–(10) neglect the distributed and Miller effects. Their deriva-
",False,lownoise-Aug_2002,False,False,False
15589,"tions are found in [10].is proportional to while becomes
",False,lownoise-Aug_2002,False,False,False
15590,"at most 6.5 times larger than the long-channel case, down to0.25
",False,lownoise-Aug_2002,False,False,False
15591,"m [6]. Likewise, (8) also suggests that shorter devices
",False,lownoise-Aug_2002,False,False,False
15592,"improve . Therefore, the selection of device geometries for
",False,lownoise-Aug_2002,False,False,False
15593,"theLNAdesignrequireswidthscalingofthedevice,consistentwiththeshortestchannellengththatcanberealized.In(7)–(10),
",False,lownoise-Aug_2002,False,False,False
15594,",,and scalelinearlywiththedevicewidth ,while
",False,lownoise-Aug_2002,False,False,False
15595,"noisefactors ,,,andarewidthindependent.Theseresults
",False,lownoise-Aug_2002,False,False,False
15596,"thus suggest the dependence of the four-noise parameters withrespect to the device width as follows:
",False,lownoise-Aug_2002,False,False,False
15597,"no width dependence (11)
",False,lownoise-Aug_2002,False,False,False
15598,"(12)
",False,lownoise-Aug_2002,False,False,False
15599,"(13)
",False,lownoise-Aug_2002,False,False,False
15600,"(14)
",False,lownoise-Aug_2002,False,False,False
15601,"Equations (3) and (12) imply that the larger device width
",False,lownoise-Aug_2002,False,False,False
15602,"offersthebestchanceof loweringnoisefigure.Theupperlimitofthewidthissetbytheconstrainedpowerbudgetinintegrated
",False,lownoise-Aug_2002,False,False,False
15603,"circuit implementations. When the supply voltage and power
",False,lownoise-Aug_2002,False,False,False
15604,"consumption aregiven, the device width of the input stage cor-respondingtoeachbiasconditioncanbeeasilycalculatedfromthe current density. In other words, for a fixed drain current,
",False,lownoise-Aug_2002,False,False,False
15605,"lower gate biasingallows a larger device width and higher gate
",False,lownoise-Aug_2002,False,False,False
15606,"biasing demands to decrease the device width.
",False,lownoise-Aug_2002,False,False,False
15607,"D. Noise Analysis of the Amplifier
",False,lownoise-Aug_2002,False,False,False
15608,"This paper utilizes the two-port theory [11], [12] instead
",False,lownoise-Aug_2002,False,False,False
15609,"of analytical equations. The four-noise parameters and
",False,lownoise-Aug_2002,False,False,False
15610,"-parameters were measured from a 0.24- m nMOSFET with
",False,lownoise-Aug_2002,False,False,False
15611,"m, using the ATN NP5B system. The frequency
",False,lownoise-Aug_2002,False,False,False
15612,"range was from 0.5 to 6.0 GHz with 0.5-GHz step and the gate
",False,lownoise-Aug_2002,False,False,False
15613,"and drain bias conditions were from 0.5 to 2.5 V with 0.2-Vstep, respectively. The resulting data were then smoothed forthe frequency as well as the bias dependences and used in the
",False,lownoise-Aug_2002,False,False,False
15614,"following analyses. The tuned amplifier illustrated in Fig. 1(a)
",False,lownoise-Aug_2002,False,False,False
15615,"is one of the most broadly used LNA architectures because itoffers the potential of achieving the best noise performance[5], [6], [10]. To evaluate its noise performance, the amplifier
",False,lownoise-Aug_2002,False,False,False
15616,"isdivided intothree cascadingstagesas illustrated in Fig.1(b).
",False,lownoise-Aug_2002,False,False,False
15617,"An admittance noise matrix for the first stage (
",False,lownoise-Aug_2002,False,False,False
15618,")i s
",False,lownoise-Aug_2002,False,False,False
15619,"found from the following network parameters:
",False,lownoise-Aug_2002,False,False,False
15620,"(15)
",False,lownoise-Aug_2002,False,False,False
15621,"This noise matrix is then transformed to the represen-
",False,lownoise-Aug_2002,False,False,False
15622,"tation ( ).
",False,lownoise-Aug_2002,False,False,False
15623,"In the second stage, an noise matrix of
",False,lownoise-Aug_2002,False,False,False
15624,"is obtained from the measured four-noise parameters and then
",False,lownoise-Aug_2002,False,False,False
15625,"transformed to the impedance noise matrix ( ). The total
",False,lownoise-Aug_2002,False,False,False
15626,"noiseofthesecondstage( )isthesumof andthe
",False,lownoise-Aug_2002,False,False,False
15627,"oneforthesourceinductorcomponent( ).Asdoneforthe
",False,lownoise-Aug_2002,False,False,False
15628,"first stage, an representation ( ) is obtained by a
",False,lownoise-Aug_2002,False,False,False
15629,"transformation.
",False,lownoise-Aug_2002,False,False,False
15630,"Finally, the noise matrix of the third stage ( )
",False,lownoise-Aug_2002,False,False,False
15631,"is acquired from the four-noise parameters of the996 IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 37, NO. 8, AUGUST 2002
",False,lownoise-Aug_2002,False,False,False
15632,"(a)
",False,lownoise-Aug_2002,False,False,False
15633,"(b)
",False,lownoise-Aug_2002,False,False,False
15634,"Fig.1. (a)TunedLNAarchitectureemployinginductivesourcedegeneration.
",False,lownoise-Aug_2002,False,False,False
15635,"(b) Noise performance evaluation sequence for an amplifier.
",False,lownoise-Aug_2002,False,False,False
15636,"common-gate-mode MOSFET. Alternatively, it can be ac-
",False,lownoise-Aug_2002,False,False,False
15637,"quired through a conversion process presented in [13]. Note
",False,lownoise-Aug_2002,False,False,False
15638,"that the noise contribution from needs to be subtracted
",False,lownoise-Aug_2002,False,False,False
15639,"from the second stage and added to the third stage.
",False,lownoise-Aug_2002,False,False,False
15640,"Thenoiseperformanceoftheentireamplifierisgivenbycas-
",False,lownoise-Aug_2002,False,False,False
15641,"cading the three stages, as follows:
",False,lownoise-Aug_2002,False,False,False
15642,"(16)
",False,lownoise-Aug_2002,False,False,False
15643,"(17)
",False,lownoise-Aug_2002,False,False,False
15644,"III. DESIGNCONSIDERATIONS FOR A TUNEDAMPLIFIER
",False,lownoise-Aug_2002,False,False,False
15645,"For the topology illustrated in Fig. 1(a), the desired input
",False,lownoise-Aug_2002,False,False,False
15646,"impedance of the amplifier is obtained for a narrow frequency
",False,lownoise-Aug_2002,False,False,False
15647,"bandbychoosing andindependently.When ,itis
",False,lownoise-Aug_2002,False,False,False
15648,"approximated as follows [5]:
",False,lownoise-Aug_2002,False,False,False
15649,"(18)
",False,lownoise-Aug_2002,False,False,False
15650,"Itisknownthatthesourcedegenerationinductance controls
",False,lownoise-Aug_2002,False,False,False
15651,"the noise performance of the given architecture [14], but the
",False,lownoise-Aug_2002,False,False,False
15652,"reasons are not well understood.
",False,lownoise-Aug_2002,False,False,False
15653,"Supposeapowersupplyhavingaconjugatelymatchedsource
",False,lownoise-Aug_2002,False,False,False
15654,"impedance isconnectedtotheLNAasillustratedinFig.1(a).
",False,lownoise-Aug_2002,False,False,False
15655,"This can mean that and ,o r
",False,lownoise-Aug_2002,False,False,False
15656,"thatandtransformagiven tobeconjugatelymatched
",False,lownoise-Aug_2002,False,False,False
15657,"to. Assuming the noise from passive components is negli-
",False,lownoise-Aug_2002,False,False,False
15658,"gible,basedonthedefinitionin(2),thenoisefactorofthegiventopology can be expressed as [6]
",False,lownoise-Aug_2002,False,False,False
15659,"(19)
",False,lownoise-Aug_2002,False,False,False
15660,"(20)
",False,lownoise-Aug_2002,False,False,False
15661,"Fig. 2. Dependence of output noise power components on /60 /91 /90 /93where /90
",False,lownoise-Aug_2002,False,False,False
15662,"istheinputimpedanceatthegateelectrodeof /77.Aconjugatepowermatchis
",False,lownoise-Aug_2002,False,False,False
15663,"assumed.
",False,lownoise-Aug_2002,False,False,False
15664,"(21)
",False,lownoise-Aug_2002,False,False,False
15665,"(22)
",False,lownoise-Aug_2002,False,False,False
15666,"(23)
",False,lownoise-Aug_2002,False,False,False
15667,"(24)
",False,lownoise-Aug_2002,False,False,False
15668,"(25)
",False,lownoise-Aug_2002,False,False,False
15669,"(26)
",False,lownoise-Aug_2002,False,False,False
15670,"(27)
",False,lownoise-Aug_2002,False,False,False
15671,"(28)
",False,lownoise-Aug_2002,False,False,False
15672,"(29)
",False,lownoise-Aug_2002,False,False,False
15673,"(30)
",False,lownoise-Aug_2002,False,False,False
15674,"(31)
",False,lownoise-Aug_2002,False,False,False
15675,"where denotes the real part of a complex number, and
",False,lownoise-Aug_2002,False,False,False
15676,",, and arecurrent noise power components at the
",False,lownoise-Aug_2002,False,False,False
15677,"output of the LNA contributed by ,, and , respec-
",False,lownoise-Aug_2002,False,False,False
15678,"tively. In Fig. 2, the device size and bias condition are fixed,
",False,lownoise-Aug_2002,False,False,False
15679,"then the impact of different are examined. The
",False,lownoise-Aug_2002,False,False,False
15680,"result shows that each component has a different dependenceon
",False,lownoise-Aug_2002,False,False,False
15681,". Since the feedback of reduces the current gain,
",False,lownoise-Aug_2002,False,False,False
15682,"asincreases, the output noise contributions from the source
",False,lownoise-Aug_2002,False,False,False
15683,"resistance and the induced gate noise of
",False,lownoise-Aug_2002,False,False,False
15684,"monotonically decrease, but their slopes are different due to
",False,lownoise-Aug_2002,False,False,False
15685,"different feedback gains. On the other hand, the contributionfromtheinducedgatenoiseof
",False,lownoise-Aug_2002,False,False,False
15686,"isnegligiblysmall.
",False,lownoise-Aug_2002,False,False,False
15687,"Thecontributionsfromdraincurrentnoise( and )
",False,lownoise-Aug_2002,False,False,False
15688,"have almost unity gain, and thus result in an -independent
",False,lownoise-Aug_2002,False,False,False
15689,"term. Hence, the LNA yields the best noise figure whenthe
",False,lownoise-Aug_2002,False,False,False
15690,"-dependent term ( ) and -independent term
",False,lownoise-Aug_2002,False,False,False
15691,"( ) give equal contributions, as illustrated in
",False,lownoise-Aug_2002,False,False,False
15692,"Fig. 2 by two dashed lines.
",False,lownoise-Aug_2002,False,False,False
15693,"Thefour-noiseparametersofferamoreintuitivemeansofex-
",False,lownoise-Aug_2002,False,False,False
15694,"planation for the phenomenon discussed above. The four-noiseGOOet al.: NOISE OPTIMIZATION TECHNIQUE FOR INTEGRATED LOW NOISE AMPLIFIERS 997
",False,lownoise-Aug_2002,False,False,False
15695,"(a)
",False,lownoise-Aug_2002,False,False,False
15696,"(b)
",False,lownoise-Aug_2002,False,False,False
15697,"Fig.3. NoiseperformanceoftheLNAforvarying /76.Thenoisecontributions
",False,lownoise-Aug_2002,False,False,False
15698,"of /77substrate and /77are excluded and /67 /61/48. (a) Optimum source
",False,lownoise-Aug_2002,False,False,False
15699,"impedance. (b) Noise figure.
",False,lownoise-Aug_2002,False,False,False
15700,"parametersoftheLNAarecalculatedfordifferentvaluesof
",False,lownoise-Aug_2002,False,False,False
15701,"using in Section II-D. Fig. 3(a) plots the power-
",False,lownoise-Aug_2002,False,False,False
15702,"matching condition ( ) and noise-matching condi-
",False,lownoise-Aug_2002,False,False,False
15703,"tion ( ) together on the Smith chart for varying
",False,lownoise-Aug_2002,False,False,False
15704,"from 0.1 to 10 nH. As increases, the real part of pro-
",False,lownoise-Aug_2002,False,False,False
15705,"portionally increases and the power-matching condition movesdownward counterclockwise. On the other hand,
",False,lownoise-Aug_2002,False,False,False
15706,"dramati-
",False,lownoise-Aug_2002,False,False,False
15707,"cally changes the noise-matching condition as well, but it ex-
",False,lownoise-Aug_2002,False,False,False
15708,"hibits atotally independenttrajectorytothe left.Aninterestingfactisthatthosetwoconditionscancomeintoagoodproximitybyanappropriateamountofthesourcedegeneration.Sincethe
",False,lownoise-Aug_2002,False,False,False
15709,"proximitymeans
",False,lownoise-Aug_2002,False,False,False
15710,"innoiseperformancecalculation,
",False,lownoise-Aug_2002,False,False,False
15711,"a better proximity essentially leads to a lower noise figure.
",False,lownoise-Aug_2002,False,False,False
15712,"Fig. 3(b) shows the noise figure when the source impedance
",False,lownoise-Aug_2002,False,False,False
15713,"() is chosen to providing a perfect power match. The
",False,lownoise-Aug_2002,False,False,False
15714,"best achievable noise figure is obtained when brings
",False,lownoise-Aug_2002,False,False,False
15715,"and to the point where they are in the closest proximity;
",False,lownoise-Aug_2002,False,False,False
15716,"those conditions are marked as and in
",False,lownoise-Aug_2002,False,False,False
15717,"Fig. 3(a). This fact implies that an accurate calculation of the
",False,lownoise-Aug_2002,False,False,False
15718,"input impedance is critical in the noise optimization process;
",False,lownoise-Aug_2002,False,False,False
15719,"approximate values are of limited use. Another beneficialimpact of using a source inductance is that it substantiallylowers
",False,lownoise-Aug_2002,False,False,False
15720,"and slightly improves as well. Thus, the LNA
",False,lownoise-Aug_2002,False,False,False
15721,"can potentially achieve a better noise figure than of the
",False,lownoise-Aug_2002,False,False,False
15722,"MOSFET alone if coincides with .
",False,lownoise-Aug_2002,False,False,False
15723,"Since isafunctionofthedevicesizeandbiascondition,
",False,lownoise-Aug_2002,False,False,False
15724,"so is the optimum . Fig. 4(a) demonstrates the optimum
",False,lownoise-Aug_2002,False,False,False
15725,"(a)
",False,lownoise-Aug_2002,False,False,False
15726,"(b)
",False,lownoise-Aug_2002,False,False,False
15727,"Fig.4. (a)Optimum /76yieldingthebestnoisefigureoftheLNAforthegiven
",False,lownoise-Aug_2002,False,False,False
15728,"biasconditionunderthepowerconstraint.(b)BestnoisefigureoftheLNAwith
",False,lownoise-Aug_2002,False,False,False
15729,"optimum /76.Thenoisecontributionsof /77substrateand /77areexcludedand
",False,lownoise-Aug_2002,False,False,False
15730,"/67 /61/48.
",False,lownoise-Aug_2002,False,False,False
15731,"isbias dependent and scales linearlywith the specifiedcurrent.
",False,lownoise-Aug_2002,False,False,False
15732,"However,thenoisefigureachievedbyoptimizing isindepen-
",False,lownoise-Aug_2002,False,False,False
15733,"dent of the current specification and very close to the intrinsic
",False,lownoise-Aug_2002,False,False,False
15734,", as shown in Fig. 4(b).
",False,lownoise-Aug_2002,False,False,False
15735,"A. Input Transistor Optimization
",False,lownoise-Aug_2002,False,False,False
15736,"To achieve for noise minimization, the designer
",False,lownoise-Aug_2002,False,False,False
15737,"has two options. The first approach is to adjust the sourceimpedance to a predetermined
",False,lownoise-Aug_2002,False,False,False
15738,", which is set by a given
",False,lownoise-Aug_2002,False,False,False
15739,"MOSFET. The other approach is to adjust to a prefixed
",False,lownoise-Aug_2002,False,False,False
15740,"by changing the geometries of the transistors, primarily
",False,lownoise-Aug_2002,False,False,False
15741,"the input device. The second option is very useful since thesource impedance has a fixed value of 50
",False,lownoise-Aug_2002,False,False,False
15742,"in many RF
",False,lownoise-Aug_2002,False,False,False
15743,"applications; also, the linearity specification often limits the
",False,lownoise-Aug_2002,False,False,False
15744,"choice of , which sets the real part of the input impedance.
",False,lownoise-Aug_2002,False,False,False
15745,"Thus,selection of theinput device is theprimary considerationin noise optimization of integrated circuits.
",False,lownoise-Aug_2002,False,False,False
15746,"This section assumes that the source impedance is fixed to
",False,lownoise-Aug_2002,False,False,False
15747,"50
",False,lownoise-Aug_2002,False,False,False
15748,"andseekstheoptimumsizeoftheinputtransistor.Fordif-
",False,lownoise-Aug_2002,False,False,False
15749,"ferentgatebiasrangingfrom0.6to1.5V,thewidthoftheinputdevice
",False,lownoise-Aug_2002,False,False,False
15750,"with the shortest channel length is first adjusted to
",False,lownoise-Aug_2002,False,False,False
15751,"satisfythegivenpowerconstraintateachgatebias.Tomakethe
",False,lownoise-Aug_2002,False,False,False
15752,"LNAbeinapower-matchcondition,theinductor ischosento
",False,lownoise-Aug_2002,False,False,False
15753,"providea50- realpartoftheinputimpedancefortheLNAand
",False,lownoise-Aug_2002,False,False,False
15754,"thenthevalueof isadjustedtocanceloftheimaginarypartof
",False,lownoise-Aug_2002,False,False,False
15755,"the input impedance. For better accuracy, the input impedance998 IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 37, NO. 8, AUGUST 2002
",False,lownoise-Aug_2002,False,False,False
15756,"(a) (b)
",False,lownoise-Aug_2002,False,False,False
15757,"(c) (d)
",False,lownoise-Aug_2002,False,False,False
15758,"Fig.5. Power-constrainednoiseperformanceof theLNAwhen /82 /61 /90 /61/53 /48 /10.Thenoise contributionsof /77substrateand /77areincludedand /67 /61/48.
",False,lownoise-Aug_2002,False,False,False
15759,"(a) Optimum source impedance at /102 /61/52GHz. (b) Optimum source impedance at /102 /61 /56/48/48MHz. (c) Equivalent noise resistance. (d)Noise figure.
",False,lownoise-Aug_2002,False,False,False
15760,"is calculated based on the methodology in [13]. In that case,
",False,lownoise-Aug_2002,False,False,False
15761,"while the power-match condition is fixed to 50 , the
",False,lownoise-Aug_2002,False,False,False
15762,"noise-match condition moves as shown in Fig. 5(a) and
",False,lownoise-Aug_2002,False,False,False
15763,"(b).Asdiscussedintheprevioussection,theproximityof
",False,lownoise-Aug_2002,False,False,False
15764,"to 50determines the noise figure. Even if somewhat
",False,lownoise-Aug_2002,False,False,False
15765,"deviates from 50 , however, this deviation does not substan-
",False,lownoise-Aug_2002,False,False,False
15766,"tially degrade the noise figure since the noise resistance of theLNAisreducedbyafactorofasmuchas5incomparisontotheMOSFET by itself, as shown in Fig. 5(c). In general, the cur-
",False,lownoise-Aug_2002,False,False,False
15767,"rentspecificationdirectlyscalestheallowabledevicewidthand
",False,lownoise-Aug_2002,False,False,False
15768,"loweringgatebiasgrantstousealargerdevicewidthforafixeddraincurrent.Asthenoiseresistanceisinverselyscaledwiththedevicewidth,itisevidentthathigherdraincurrentspecification
",False,lownoise-Aug_2002,False,False,False
15769,"and lower gate biasing make the noise figure less sensitive to a
",False,lownoise-Aug_2002,False,False,False
15770,"noise mismatch.
",False,lownoise-Aug_2002,False,False,False
15771,"Fig.5(d)clearlyshowstheoptimumgatebiasfornoise.Italso
",False,lownoise-Aug_2002,False,False,False
15772,"demonstrates that resulting noise figures are close to
",False,lownoise-Aug_2002,False,False,False
15773,".
",False,lownoise-Aug_2002,False,False,False
15774,"The valley-shaped noise figure profile can be described by an
",False,lownoise-Aug_2002,False,False,False
15775,"analytical expression as follows [6]:
",False,lownoise-Aug_2002,False,False,False
15776,"(32)
",False,lownoise-Aug_2002,False,False,False
15777,"The second and third terms include the drain conductance .
",False,lownoise-Aug_2002,False,False,False
15778,"Since it is linearly scaled with the width, it becomes smaller
",False,lownoise-Aug_2002,False,False,False
15779,"as the gate bias increases for a fixed current specification. Inthe second term,
",False,lownoise-Aug_2002,False,False,False
15780,"suggests that this term originates from the
",False,lownoise-Aug_2002,False,False,False
15781,"drain noise; in the numerator implies that this term is dom-inant when the gate bias is low due to width scaling. On the
",False,lownoise-Aug_2002,False,False,False
15782,"otherhand,inthethirdterm, suggeststhatthistermoriginates
",False,lownoise-Aug_2002,False,False,False
15783,"from the induced gate noise; in the denominator proposes
",False,lownoise-Aug_2002,False,False,False
15784,"that this term is dominant when the gate bias is high. In otherwords, the given formula has two independent noise compo-
",False,lownoise-Aug_2002,False,False,False
15785,"nentsthathavetheoppositegatebiasdependencetoeachother.
",False,lownoise-Aug_2002,False,False,False
15786,"Thenoisefigurethushasminimawheretheycontributeequallytothenoise figure.This facthighlights theimportanceof accu-rate gate noise modeling for circuit design.
",False,lownoise-Aug_2002,False,False,False
15787,"B. Cascode Stage Design
",False,lownoise-Aug_2002,False,False,False
15788,"The cascode stage has a relatively small impact on the
",False,lownoise-Aug_2002,False,False,False
15789,"overall noise figure if the input stage is not optimal. However,to squeeze out the best noise figure, it needs to be optimized as
",False,lownoise-Aug_2002,False,False,False
15790,"well. In fact, in Fig. 5(d), the difference between the minima
",False,lownoise-Aug_2002,False,False,False
15791,"of the noise figure valley and
",False,lownoise-Aug_2002,False,False,False
15792,"is primarily limited by
",False,lownoise-Aug_2002,False,False,False
15793,"the extra noise contribution from the cascode stage which isalso subject to the given power constraint. Thus, the second
",False,lownoise-Aug_2002,False,False,False
15794,"step of the noise optimization is choosing a proper size for
",False,lownoise-Aug_2002,False,False,False
15795,"the cascode stage. For the topology shown in Fig. 5(b),
",False,lownoise-Aug_2002,False,False,False
15796,"exhibits a larger deviation from the power-match condition.It is caused not by the operating frequency, but by the poorly
",False,lownoise-Aug_2002,False,False,False
15797,"optimized cascode stage. While it is known that increasing the
",False,lownoise-Aug_2002,False,False,False
15798,"width of the cascode device monotonically improves shieldingfromtheoutput,itsimpactonthenoiseperformanceisnotwellunderstood.GOOet al.: NOISE OPTIMIZATION TECHNIQUE FOR INTEGRATED LOW NOISE AMPLIFIERS 999
",False,lownoise-Aug_2002,False,False,False
15799,"Fig. 6. Impact of the cascode transistor on the overall noise figure under the
",False,lownoise-Aug_2002,False,False,False
15800,"power constraint. /82 /61 /90 /61/53 /48 /10.
",False,lownoise-Aug_2002,False,False,False
15801,"Thebiasofthecascodestageistightlylinkedtoitssize.The
",False,lownoise-Aug_2002,False,False,False
15802,"widthcanincreaseuntilthebiasofthecascodestageapproaches
",False,lownoise-Aug_2002,False,False,False
15803,"the threshold voltage, or it can decrease until the input device
",False,lownoise-Aug_2002,False,False,False
15804,"reaches the linear region. In this section, the gate bias and sizeof the input device is fixed to the optimum values found in
",False,lownoise-Aug_2002,False,False,False
15805,"Fig. 5(d) and the width of the cascode stage device is swept
",False,lownoise-Aug_2002,False,False,False
15806,"with the minimum channel length. The inductors
",False,lownoise-Aug_2002,False,False,False
15807,"and
",False,lownoise-Aug_2002,False,False,False
15808,"are readjusted to keep the input impedance to 50 . As the
",False,lownoise-Aug_2002,False,False,False
15809,"width of the cascode stage ( ) increases, the generated noise
",False,lownoise-Aug_2002,False,False,False
15810,"powerfromthecascodestagealsoincreases.Intuitively,thisfact
",False,lownoise-Aug_2002,False,False,False
15811,"suggests that smaller improves the noise figure monotoni-
",False,lownoise-Aug_2002,False,False,False
15812,"cally by reducing the noise contribution of as well as the
",False,lownoise-Aug_2002,False,False,False
15813,"capacitanceattheintermediatenodebetween and.Due
",False,lownoise-Aug_2002,False,False,False
15814,"totheMillereffect,however,therequired for
",False,lownoise-Aug_2002,False,False,False
15815,"increases as becomes smaller. Consequently, smaller
",False,lownoise-Aug_2002,False,False,False
15816,"yields a different noise-match condition as well as larger value
",False,lownoise-Aug_2002,False,False,False
15817,"of. Eventually,the noise figure becomes worse if istoo
",False,lownoise-Aug_2002,False,False,False
15818,"small.AnoptimalwidthexistsasshowninFig.6.Forthegiven
",False,lownoise-Aug_2002,False,False,False
15819,"topology, with , the cascode stage introduces 40%
",False,lownoise-Aug_2002,False,False,False
15820,"extra noise power to the input stage, which, in turn, increases
",False,lownoise-Aug_2002,False,False,False
15821,"by about 0.5 dB.
",False,lownoise-Aug_2002,False,False,False
15822,"C. Pad Capacitance
",False,lownoise-Aug_2002,False,False,False
15823,"In the process of practical LNA design, as illustrated in
",False,lownoise-Aug_2002,False,False,False
15824,"Fig. 1(a), the bonding pad introduces an extra ac current path
",False,lownoise-Aug_2002,False,False,False
15825,"to ground. In silicon technology, this can severely deteriorate
",False,lownoise-Aug_2002,False,False,False
15826,"the noise figure if the path contains a resistive component,such as the conductive substrate [15]. However, if the resistivecomponent is suppressed by replacing the bottom plate of the
",False,lownoise-Aug_2002,False,False,False
15827,"pad capacitor from the substrate to a metal layer [15], the
",False,lownoise-Aug_2002,False,False,False
15828,"bonding capacitance simply increases the required inductancevalue for the designated input impedance. This consequentlybrings the noise-match condition closer to
",False,lownoise-Aug_2002,False,False,False
15829,"and also
",False,lownoise-Aug_2002,False,False,False
15830,"diminishes the noise resistance further. In this section, the
",False,lownoise-Aug_2002,False,False,False
15831,"input device optimization process presented in Section III-A isperformed again with the presence of the pad capacitance. Anarbitrary value
",False,lownoise-Aug_2002,False,False,False
15832,"3ofis chosen first and then andare
",False,lownoise-Aug_2002,False,False,False
15833,"adjusted to make . Fig. 7(a) and (b) demonstrates
",False,lownoise-Aug_2002,False,False,False
15834,"that the bonding pad capacitance mitigates the strong gate
",False,lownoise-Aug_2002,False,False,False
15835,"3This is a part of the design rules in many cases.
",False,lownoise-Aug_2002,False,False,False
15836,"(a)
",False,lownoise-Aug_2002,False,False,False
15837,"(b)
",False,lownoise-Aug_2002,False,False,False
15838,"Fig.7. Power-constrainednoiseperformanceoftheLNAwhen /82 /61 /90 /61
",False,lownoise-Aug_2002,False,False,False
15839,"/53/48 /10fordifferentpadcapacitance.Thenoisecontributionsof /77substrateand
",False,lownoise-Aug_2002,False,False,False
15840,"/77are included. (a) Noise figure. (b) Gain.
",False,lownoise-Aug_2002,False,False,False
15841,"bias dependence of the noise figure. However, it is a tradeoff
",False,lownoise-Aug_2002,False,False,False
15842,"between the noise figure and gain.
",False,lownoise-Aug_2002,False,False,False
15843,"IV. EXPERIMENTAL RESULTS
",False,lownoise-Aug_2002,False,False,False
15844,"To evaluate the LNA performance, a single-ended LNA in-
",False,lownoise-Aug_2002,False,False,False
15845,"tended to achieve 1.0 dB of noise figure was designed using a
",False,lownoise-Aug_2002,False,False,False
15846,"0.24-m silicideCMOStechnology. The diephoto ofthe LNA
",False,lownoise-Aug_2002,False,False,False
15847,"isgiveninFig.8.First,theminimumsizepadwasimplementedusingmetal-5andmetal-1layerstosuppressextranoise,giving47fFofcapacitance.Thesupplyvoltagewaschosentobe2.0V
",False,lownoise-Aug_2002,False,False,False
15848,"to provide a voltage headroom for the cascode transistor.
",False,lownoise-Aug_2002,False,False,False
15849,"4The
",False,lownoise-Aug_2002,False,False,False
15850,"analysis in Fig. 5(d) suggests that at least 3.75 mA of bias cur-rent is required to achieve below 1.0 dB of noise figure. Thegatebiaswassetto0.7Vtoachievethebestnoisefigurebased
",False,lownoise-Aug_2002,False,False,False
15851,"onthecharacteristicinFig.7(a).Thecorrespondingsizeof
",False,lownoise-Aug_2002,False,False,False
15852,"was 90/0.24 for the given power budget. At the time of design,
",False,lownoise-Aug_2002,False,False,False
15853,"the cascode stage was not fully optimized and the size of
",False,lownoise-Aug_2002,False,False,False
15854,"waschosentobe45/0.24.Forthegiventopology,itisexpected
",False,lownoise-Aug_2002,False,False,False
15855,"to improve the noise figure by 0.1 dB with m. To
",False,lownoise-Aug_2002,False,False,False
15856,"minimize the distributed gate resistance, the MOSFETs weresegmented into 5-
",False,lownoise-Aug_2002,False,False,False
15857,"m-long gate fingers and each of the fingers
",False,lownoise-Aug_2002,False,False,False
15858,"wascontactedatbothends[16].Thespiralinductor wasim-
",False,lownoise-Aug_2002,False,False,False
15859,"plemented using the metal-5 layer and its value was chosen to
",False,lownoise-Aug_2002,False,False,False
15860,"4The threshold voltage is relatively high in the given technology. Further
",False,lownoise-Aug_2002,False,False,False
15861,"process adjustments can potentially reduce the supply voltage as well as the
",False,lownoise-Aug_2002,False,False,False
15862,"power consumption.1000 IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 37, NO. 8, AUGUST 2002
",False,lownoise-Aug_2002,False,False,False
15863,"Fig. 8. Die photo of the LNA.
",False,lownoise-Aug_2002,False,False,False
15864,"Fig. 9. Wire-bonding illustration of the LLP package.
",False,lownoise-Aug_2002,False,False,False
15865,"be 1.1 nH to provide 50 of real part of the input impedance,
",False,lownoise-Aug_2002,False,False,False
15866,"in combination with . The inductor was designed based on
",False,lownoise-Aug_2002,False,False,False
15867,"thecompactmodelpresentedby[17];apatternedgroundshieldwasemployedtoreducethesubstrateparasiticsofthespiralin-
",False,lownoise-Aug_2002,False,False,False
15868,"ductor[18].Sincetherequiredgateinductor
",False,lownoise-Aug_2002,False,False,False
15869,"tocanceloutthe
",False,lownoise-Aug_2002,False,False,False
15870,"imaginary partof theinput impedance was 36nH, whichistoolargetobeintegrated,anexternalinductorwasusedalongwithabondwireinductor.Finally,tocontroltheparasiticinductance
",False,lownoise-Aug_2002,False,False,False
15871,"from
",False,lownoise-Aug_2002,False,False,False
15872,"to ground, the die was mounted on a special leadless
",False,lownoise-Aug_2002,False,False,False
15873,"leadframe package (LLP) which allows direct downbonding tothe large ground plane, as shown in Fig. 9.
",False,lownoise-Aug_2002,False,False,False
15874,"The complete schematic of the device under test (DUT) is
",False,lownoise-Aug_2002,False,False,False
15875,"shown in Fig. 10. The real term of the input impedance of the
",False,lownoise-Aug_2002,False,False,False
15876,"fabricated LNA was 54
",False,lownoise-Aug_2002,False,False,False
15877,"and was adjusted to 50 using an
",False,lownoise-Aug_2002,False,False,False
15878,"off-chip tuner. To maximize accuracy in noise figure measure-ment,theoutput oftheLNA isalso impedancematched
",False,lownoise-Aug_2002,False,False,False
15879,"5using
",False,lownoise-Aug_2002,False,False,False
15880,"another off-chip tuner.
",False,lownoise-Aug_2002,False,False,False
15881,"Fig.11(a)and(b)showsmeasuredthird-orderinputintercept
",False,lownoise-Aug_2002,False,False,False
15882,"point (IIP3) and noise figure as well as the available gain. Themeasured performance of the LNA is summarized in Table I.
",False,lownoise-Aug_2002,False,False,False
15883,"With 3.75 mA of bias current, the LNA achieves about 0.9 dB
",False,lownoise-Aug_2002,False,False,False
15884,"ofnoisefigure,whichisthelowestreportednoisefigurewithaperfect power match for a CMOS LNA, and it adds just 0.3 dBto the
",False,lownoise-Aug_2002,False,False,False
15885,"of the intrinsic MOSFET device. The measured
",False,lownoise-Aug_2002,False,False,False
15886,"noisefigureisalsoquiteclosetotheexpectedvalueanddemon-
",False,lownoise-Aug_2002,False,False,False
15887,"strates that the proposed methodology accurately predicts thenoise performance of custom integrated LNA designs.
",False,lownoise-Aug_2002,False,False,False
15888,"5If the output is not matched, the measured noise figure needs a correction
",False,lownoise-Aug_2002,False,False,False
15889,"that may lead to errors [1].
",False,lownoise-Aug_2002,False,False,False
15890,"Fig. 10. Complete schematic of the LNA, including off-chipelements.
",False,lownoise-Aug_2002,False,False,False
15891,"(a)
",False,lownoise-Aug_2002,False,False,False
15892,"(b)
",False,lownoise-Aug_2002,False,False,False
15893,"Fig. 11. Measured performance of the LNA. (a) IIP3 result. (b) Noise figure
",False,lownoise-Aug_2002,False,False,False
15894,"and gain.
",False,lownoise-Aug_2002,False,False,False
15895,"V. CONCLUSION
",False,lownoise-Aug_2002,False,False,False
15896,"Based on the measured noise parameters of the 0.24- m
",False,lownoise-Aug_2002,False,False,False
15897,"MOSFETandontheresultsderivedfromtwo-portnoisetheory,
",False,lownoise-Aug_2002,False,False,False
15898,"considerations for a integrated LNA design are presented. The
",False,lownoise-Aug_2002,False,False,False
15899,"measured noise parameters can be scaled directly with the de-vice width; device sizing can be utilized for power-constraineddesign. The noise performance of the tuned LNA is primarilycontrolled by the source degeneration inductance, whichGOOet al.: NOISE OPTIMIZATION TECHNIQUE FOR INTEGRATED LOW NOISE AMPLIFIERS 1001
",False,lownoise-Aug_2002,False,False,False
15900,"TABLE I
",False,lownoise-Aug_2002,False,False,False
15901,"MEASURED PERFORMANCE OF AN 800-MH ZLNA
",False,lownoise-Aug_2002,False,False,False
15902,"determines both the power-matching and the noise-matching
",False,lownoise-Aug_2002,False,False,False
15903,"conditions. Therefore, if arbitrary values of source impedance
",False,lownoise-Aug_2002,False,False,False
15904,"are allowed, the optimal LNA design can be obtained by
",False,lownoise-Aug_2002,False,False,False
15905,"adjusting the source inductance. Even if the source impedanceis fixed, the integrated LNA can achieve noise performancenear
",False,lownoise-Aug_2002,False,False,False
15906,"by choosing an appropriate device geometry and
",False,lownoise-Aug_2002,False,False,False
15907,"optimizing the bias conditions. The cascode stage usually
",False,lownoise-Aug_2002,False,False,False
15908,"introduces at least 40% extra noise power to the input stage;thus, its width needs to be optimized.
",False,lownoise-Aug_2002,False,False,False
15909,"Although the demonstrated LNA uses a single-ended archi-
",False,lownoise-Aug_2002,False,False,False
15910,"tecture, future LNA designs will require differential operation
",False,lownoise-Aug_2002,False,False,False
15911,"since further scaling of the device sizes requires smaller values
",False,lownoise-Aug_2002,False,False,False
15912,"of source inductance. Fully integrated inductors with largevalues and high quality factors required for
",False,lownoise-Aug_2002,False,False,False
15913,"are an ongoing
",False,lownoise-Aug_2002,False,False,False
15914,"challenge. The results demonstrate that CMOS can be a goodcandidateforhigh-performanceLNAdesigns,competitivewith
",False,lownoise-Aug_2002,False,False,False
15915,"GaAs and bipolar LNAs.
",False,lownoise-Aug_2002,False,False,False
15916,"A
",False,lownoise-Aug_2002,False,False,False
15917,"CKNOWLEDGMENT
",False,lownoise-Aug_2002,False,False,False
15918,"The authors would like to thank Texas Instruments
",False,lownoise-Aug_2002,False,False,False
15919,"Incorporated for promoting and mentoring this project. Specialthanks go to National Semiconductor for fabricating the chipand accommodating test equipment.
",False,lownoise-Aug_2002,False,False,False
15920,"R
",False,lownoise-Aug_2002,False,False,False
15921,"EFERENCES
",False,lownoise-Aug_2002,False,False,False
15922,"[1] “Fundamentals of RF and microwave noise figure measurements,” Ag-
",False,lownoise-Aug_2002,False,False,False
15923,"ilent Technologies, Palo Alto, CA, Application note 57-1.
",False,lownoise-Aug_2002,False,False,False
15924,"[2] B. A. Floyd, J. Mehta, C. Gamero, and K. K. O, “A 900-MHz 0.8- /22m
",False,lownoise-Aug_2002,False,False,False
15925,"CMOS low-noise amplifier with 1.2-dB noise figure,” in Proc. IEEE
",False,lownoise-Aug_2002,False,False,False
15926,"Custom Integrated Circuits Conf. (CICC) , San Diego, CA, May 1999,
",False,lownoise-Aug_2002,False,False,False
15927,"pp. 661–664.
",False,lownoise-Aug_2002,False,False,False
15928,"[3] G. Gramegna, A. Magazzú, C. Sclafani, M. Paparo, and P. Erratico, “A
",False,lownoise-Aug_2002,False,False,False
15929,"9-mW900-MHzCMOSLNAwith1.05-dBnoisefigure,”in Proc.Eur.
",False,lownoise-Aug_2002,False,False,False
15930,"Solid-StateCircuitsConf.(ESSCIRC) ,Stockholm,Sweden,Sept.2000,
",False,lownoise-Aug_2002,False,False,False
15931,"pp. 112–115.
",False,lownoise-Aug_2002,False,False,False
15932,"[4] P. Leroux, J. Janssens, and M. Steyaert, “A 0.8-dB NF ESD-protected
",False,lownoise-Aug_2002,False,False,False
15933,"9-mW CMOS LNA,” in Proc. Int. Solid-State Circuits Conf. (ISSCC)
",False,lownoise-Aug_2002,False,False,False
15934,"Dig. Tech. Papers , San Francisco, CA, Feb. 2001, pp. 410–411.
",False,lownoise-Aug_2002,False,False,False
15935,"[5] D.K.Shaefferand T. H.Lee,“A1.5-V 1.5-GHzCMOSlow-noiseam-
",False,lownoise-Aug_2002,False,False,False
15936,"plifier,”IEEE J.Solid-State Circuits ,vol. 32,pp. 745–759,May 1997.[6] J.-S. Goo, K.-H. Oh, C.-H. Choi, Z. Yu, T. H. Lee, and R. W. Dutton,
",False,lownoise-Aug_2002,False,False,False
15937,"“Guidelinesforthepower-constraineddesignofaCMOStunedLNA,”
",False,lownoise-Aug_2002,False,False,False
15938,"inProc. Int. Conf. Simulation of Semiconductor Processes and Devices
",False,lownoise-Aug_2002,False,False,False
15939,"(SISPAD) , Seattle, WA, Sept. 2000, pp. 269–272.
",False,lownoise-Aug_2002,False,False,False
15940,"[7] H. Rothe and W. Dahlke, “Theory of noisy fourpoles,” in Proc. Inst.
",False,lownoise-Aug_2002,False,False,False
15941,"Radio Eng. , vol. 44, June 1956, pp. 811–815.
",False,lownoise-Aug_2002,False,False,False
15942,"[8] G. Dambrine, J.-P. Raskin, F. Danneville, D. Vanhoenacker-Janvier,
",False,lownoise-Aug_2002,False,False,False
15943,"J.-P. Colinge, and A. Cappy, “High-frequency four-noise parametersof silicon-on-insulator-based technology MOSFET for the design of
",False,lownoise-Aug_2002,False,False,False
15944,"low-noise RF integrated circuits,” IEEE Trans. Electron Devices , vol.
",False,lownoise-Aug_2002,False,False,False
15945,"46, pp. 1733–1741, Aug. 1999.
",False,lownoise-Aug_2002,False,False,False
15946,"[9] A. van der Ziel, Solid State Physical Electronics , 3rd ed. Englewood
",False,lownoise-Aug_2002,False,False,False
15947,"Cliffs, NJ: Prentice-Hall, 1976, ch. 18.
",False,lownoise-Aug_2002,False,False,False
15948,"[10] T. H. Lee, The Design of CMOS Radio-Frequency Integrated Circuits ,
",False,lownoise-Aug_2002,False,False,False
15949,"1st ed. Cambridge, U.K.: Cambridge Univ.Press, 1998, ch. 11.
",False,lownoise-Aug_2002,False,False,False
15950,"[11] H. A. Haus and R. B. Adler, Circuit Theory of Linear Noisy Net-
",False,lownoise-Aug_2002,False,False,False
15951,"works. New York: Wiley, 1959.
",False,lownoise-Aug_2002,False,False,False
15952,"[12] H.HillbrandandP.H.Russer,“Anefficientmethodforcomputer-aided
",False,lownoise-Aug_2002,False,False,False
15953,"noiseanalysisoflinearamplifiernetworks,” IEEETrans.CircuitsSyst. ,
",False,lownoise-Aug_2002,False,False,False
15954,"vol. 23, pp. 235–238, Apr. 1976.
",False,lownoise-Aug_2002,False,False,False
15955,"[13] J.-S.Goo,H.-T.Ahn,D.J.Ladwig,Z.Yu,T.H.Lee,andR.W.Dutton,
",False,lownoise-Aug_2002,False,False,False
15956,"“Design methodology for power-constrained low-noise RF circuits,” inProc.WorkshopSynthesisandSystemIntegrationofMixedTechnologies
",False,lownoise-Aug_2002,False,False,False
15957,"(SASIMI) , Nara, Japan, Oct. 2001, pp. 394–401.
",False,lownoise-Aug_2002,False,False,False
15958,"[14] Y. Imai, M. Tokumitsu, and A. Minakawa, “Design and performance
",False,lownoise-Aug_2002,False,False,False
15959,"of low-current GaAs MMICs for L-band front-end applications,” IEEE
",False,lownoise-Aug_2002,False,False,False
15960,"Trans. Microwave Theory Tech. , vol. 39, pp. 209–215, Feb. 1991.
",False,lownoise-Aug_2002,False,False,False
15961,"[15] C.E.Biber,M.L.Schmatz,T.Morf,U.Lott,E.Morifuji,andW.Bäch-
",False,lownoise-Aug_2002,False,False,False
15962,"told, “Technology independent degradation of minimum noise figureduetopadparasitics,”in Proc.IEEEMTT-SInt.MicrowaveSymp.Dig. ,
",False,lownoise-Aug_2002,False,False,False
15963,"Baltimore, MD, June 1998, pp. 145–148.
",False,lownoise-Aug_2002,False,False,False
15964,"[16] R. P. Jindal, “Noise associated with distributed resistance of MOSFET
",False,lownoise-Aug_2002,False,False,False
15965,"gatestructuresinintegratedcircuits,” IEEETrans.ElectronDevices ,vol.
",False,lownoise-Aug_2002,False,False,False
15966,"31, pp. 1505–1509, Oct. 1984.
",False,lownoise-Aug_2002,False,False,False
15967,"[17] M.delMarHershenson,S.S.Mohan,S.P.Boyd,andT.H.Lee,“Opti-
",False,lownoise-Aug_2002,False,False,False
15968,"mizationofinductorcircuitsviageometricprogramming,”in Proc.36th
",False,lownoise-Aug_2002,False,False,False
15969,"Design Automation Conf. , New Orleans,LA, June 1999,pp.994–998.
",False,lownoise-Aug_2002,False,False,False
15970,"[18] C. P. Yue and S. S. Wong, “On-chip spiral inductors with patterned
",False,lownoise-Aug_2002,False,False,False
15971,"ground shields for Si-based RF ICs,” IEEE J. Solid-State Circuits , vol.
",False,lownoise-Aug_2002,False,False,False
15972,"33, pp. 743–752, May 1998.
",False,lownoise-Aug_2002,False,False,False
15973,"Jung-Suk Goo (S’97–M’02) was born in Seoul,
",False,lownoise-Aug_2002,False,False,False
15974,"Korea, in 1966. He received the B.S. degree inelectricalengineeringfromYonseiUniversity,Seoul,
",False,lownoise-Aug_2002,False,False,False
15975,"in 1988 and the M.S. and Ph.D. degrees in electrical
",False,lownoise-Aug_2002,False,False,False
15976,"engineeringfromStanfordUniversity,Stanford,CA,
",False,lownoise-Aug_2002,False,False,False
15977,"in 1997 and 2001, respectively.
",False,lownoise-Aug_2002,False,False,False
15978,"From 1988 to 1989, he was with GoldStar
",False,lownoise-Aug_2002,False,False,False
15979,"Semiconductor Company, Korea, involved in
",False,lownoise-Aug_2002,False,False,False
15980,"EPROM and 4M DRAM projects. Subsequently,
",False,lownoise-Aug_2002,False,False,False
15981,"he was with LG Semiconductor Company, Korea,
",False,lownoise-Aug_2002,False,False,False
15982,"until 1995. During this period, he was engaged in
",False,lownoise-Aug_2002,False,False,False
15983,"next-generation DRAM development such as 64M and 256M and also was
",False,lownoise-Aug_2002,False,False,False
15984,"involved in a Flash memory project. His primary research areas were DRAM
",False,lownoise-Aug_2002,False,False,False
15985,"processdevelopment, deviceevaluation,and reliability modeling,inparticular,
",False,lownoise-Aug_2002,False,False,False
15986,"the hot-carrier effects. In 2001, he was briefly with the Strategic Technology
",False,lownoise-Aug_2002,False,False,False
15987,"Group of Advanced Micro Devices, Sunnyvale, CA, working on nanoscale
",False,lownoise-Aug_2002,False,False,False
15988,"MOSFET technology development, and with Atheros Communications, Inc.,
",False,lownoise-Aug_2002,False,False,False
15989,"Sunnyvale,engagedinIEEE802.11aWLANchipsetdevelopment.In2002,he
",False,lownoise-Aug_2002,False,False,False
15990,"rejoined the Strategic Technology Group of Advanced Micro Devices, where
",False,lownoise-Aug_2002,False,False,False
15991,"he is currently a Member of Technical Staff. He has authored and coauthoredmore than 24 journal and conference papers and holds six U.S. patents. His
",False,lownoise-Aug_2002,False,False,False
15992,"current research interests are the design and modeling of CMOS RF circuits
",False,lownoise-Aug_2002,False,False,False
15993,"and nanoscale MOSFET physics.1002 IEEE JOURNAL OF SOLID-STATE CIRCUITS, VOL. 37, NO. 8, AUGUST 2002
",False,lownoise-Aug_2002,False,False,False
15994,"Hee-TaeAhn (S’98–M’02)receivedtheB.S.degree
",False,lownoise-Aug_2002,False,False,False
15995,"in1987fromHankukAviationUniversity,Korea.He
",False,lownoise-Aug_2002,False,False,False
15996,"received the M.S. and Ph.D degrees in electrical en-
",False,lownoise-Aug_2002,False,False,False
15997,"gineering from Arizona State University, Tempe, in
",False,lownoise-Aug_2002,False,False,False
15998,"1995 and 2000, respectively.
",False,lownoise-Aug_2002,False,False,False
15999,"From 1987 to 1993, he was with the Research
",False,lownoise-Aug_2002,False,False,False
16000,"and Development Center of LG Semiconductor
",False,lownoise-Aug_2002,False,False,False
16001,"Company,Korea,wherehedesignedseveralmemory
",False,lownoise-Aug_2002,False,False,False
16002,"ICs.In1995,hejoinedAT&TBellLaboratories,Al-
",False,lownoise-Aug_2002,False,False,False
16003,"lentown, PA,where he was involved in mixed-signal
",False,lownoise-Aug_2002,False,False,False
16004,"integrated-circuit design, and from 1996 to 1998,
",False,lownoise-Aug_2002,False,False,False
16005,"he was with SUN Microsystems, Palo Alto, CA, where he designed CMOS
",False,lownoise-Aug_2002,False,False,False
16006,"phase-locked-loopintegratedcircuits.From2000to2001,hewaswithNational
",False,lownoise-Aug_2002,False,False,False
16007,"Semiconductor, Santa Clara, CA, where he was working on analog IC design
",False,lownoise-Aug_2002,False,False,False
16008,"including frequency synthesizers and voltage-controlled oscillators. Since2001,hehasbeenwithQualcommInc,SanDiego,CA.Heiscurrentlyworking
",False,lownoise-Aug_2002,False,False,False
16009,"on a transceiver chipset design for CDMA technology. His current research
",False,lownoise-Aug_2002,False,False,False
16010,"interests are high-frequency analog IC design for wireless communication
",False,lownoise-Aug_2002,False,False,False
16011,"applications.
",False,lownoise-Aug_2002,False,False,False
16012,"Donald J. Ladwig was born in Okinawa, Japan, on November 11, 1961. He
",False,lownoise-Aug_2002,False,False,False
16013,"graduated from the DeVry Institute of Technology, Irving, TX, in 1986, as an
",False,lownoise-Aug_2002,False,False,False
16014,"Electronic Technician.
",False,lownoise-Aug_2002,False,False,False
16015,"HejoinedTexasInstrumentsIncorporated,Dallas,TX,in1986,andhasbeen
",False,lownoise-Aug_2002,False,False,False
16016,"workinginthemodelingfieldsince1988.Hespecializesinon-wafercharacter-
",False,lownoise-Aug_2002,False,False,False
16017,"ization of
",False,lownoise-Aug_2002,False,False,False
16018,"/83-parameters, high-frequency and low-frequency noise, and dc and
",False,lownoise-Aug_2002,False,False,False
16019,"capacitance for semiconductor components. He is currently involved in model
",False,lownoise-Aug_2002,False,False,False
16020,"parameter extraction.
",False,lownoise-Aug_2002,False,False,False
16021,"Zhiping Yu (SM’90) received the B.S. degree from
",False,lownoise-Aug_2002,False,False,False
16022,"TsinghuaUniversity,Beijing,China,in1967andthe
",False,lownoise-Aug_2002,False,False,False
16023,"M.S. and Ph.D. degrees from Stanford University,
",False,lownoise-Aug_2002,False,False,False
16024,"Stanford, CA, in 1980 and 1985, respectively.
",False,lownoise-Aug_2002,False,False,False
16025,"He is currently a Senior Research Scientist with
",False,lownoise-Aug_2002,False,False,False
16026,"the Department of Electrical Engineering, Stanford
",False,lownoise-Aug_2002,False,False,False
16027,"University, and also holds a full professorship at
",False,lownoise-Aug_2002,False,False,False
16028,"Tsinghua University. His research interests focus
",False,lownoise-Aug_2002,False,False,False
16029,"on IC process, device and circuit simulation, and in
",False,lownoise-Aug_2002,False,False,False
16030,"particular, the numerical techniques and modeling
",False,lownoise-Aug_2002,False,False,False
16031,"of RF and heterostructure devices. He has been
",False,lownoise-Aug_2002,False,False,False
16032,"involved in efforts to develop a simulation package for optoelectronic devices
",False,lownoise-Aug_2002,False,False,False
16033,"and three-dimensional solid modeling for ICs. He is also a Consultant toHewlett-Packard Computer Systems and Technology Lab, Palo Alto, CA,
",False,lownoise-Aug_2002,False,False,False
16034,"developing advanced transport models for subquarter-micrometer CMOS
",False,lownoise-Aug_2002,False,False,False
16035,"technology, including quantum mechanical effects.
",False,lownoise-Aug_2002,False,False,False
16036,"Dr. Yu currently serves as an Associate Editor of the IEEE T
",False,lownoise-Aug_2002,False,False,False
16037,"RANSACTIONS
",False,lownoise-Aug_2002,False,False,False
16038,"ONCOMPUTER -AIDEDDESIGN OF INTEGRATED CIRCUITS AND SYSTEMS.
",False,lownoise-Aug_2002,False,False,False
16039,"Thomas H. Lee (M’87) received the S.B., S.M.,
",False,lownoise-Aug_2002,False,False,False
16040,"and Sc.D. degrees in electrical engineering from the
",False,lownoise-Aug_2002,False,False,False
16041,"Massachusetts Institute of Technology, Cambridge,
",False,lownoise-Aug_2002,False,False,False
16042,"in 1983, 1985, and 1990, respectively.
",False,lownoise-Aug_2002,False,False,False
16043,"He joined Analog Devices, Norwood, MA,
",False,lownoise-Aug_2002,False,False,False
16044,"in 1990, where he was primarily engaged in thedesign of high-speed clock recovery devices. In
",False,lownoise-Aug_2002,False,False,False
16045,"1992, he joined Rambus Inc., Mountain View, CA,
",False,lownoise-Aug_2002,False,False,False
16046,"where he developed high-speed analog circuitry
",False,lownoise-Aug_2002,False,False,False
16047,"for 500-Mbyte/s CMOS DRAMs. He has also con-
",False,lownoise-Aug_2002,False,False,False
16048,"tributed to the development of phase-locked loops
",False,lownoise-Aug_2002,False,False,False
16049,"in the StrongARM, Alpha, and K6/K7 microprocessors. Since 1994, he has
",False,lownoise-Aug_2002,False,False,False
16050,"been an Assistant Professor of Electrical Engineering at Stanford University,
",False,lownoise-Aug_2002,False,False,False
16051,"Stanford, CA, where his research focus has been on gigahertz-speed wireline
",False,lownoise-Aug_2002,False,False,False
16052,"and wireless integrated circuits built in conventional silicon technologies,particularly CMOS. He holds 12 U.S. patents. He is the author of The Design
",False,lownoise-Aug_2002,False,False,False
16053,"of CMOS Radio-Frequency Integrated Circuits (Cambridge, U.K.: Cambridge
",False,lownoise-Aug_2002,False,False,False
16054,"Univ. Press, 1998) and is a coauthor of two additional books on RF circuit
",False,lownoise-Aug_2002,False,False,False
16055,"design.He is also a cofounder of Matrix Semiconductor, Santa Clara, CA.
",False,lownoise-Aug_2002,False,False,False
16056,"Dr. Lee has twice received the Best Paper Award at the IEEE International
",False,lownoise-Aug_2002,False,False,False
16057,"Solid-State Circuits Conference (ISSCC). He was a coauthor of a Best Student
",False,lownoise-Aug_2002,False,False,False
16058,"Paper at ISSCC, and recently won a Packard Foundation Fellowship. He is aDistinguished Lecturer of the IEEE Solid-State Circuits Society, and was re-
",False,lownoise-Aug_2002,False,False,False
16059,"cently named a Distinguished Microwave Lecturer.
",False,lownoise-Aug_2002,False,False,False
16060,"Robert W. Dutton (S’67–M’70–SM’80–F’84)
",False,lownoise-Aug_2002,False,False,False
16061,"received the B.S., M.S., and Ph.D. degrees from the
",False,lownoise-Aug_2002,False,False,False
16062,"University of California, Berkeley, in 1966, 1967,
",False,lownoise-Aug_2002,False,False,False
16063,"and 1970, respectively.
",False,lownoise-Aug_2002,False,False,False
16064,"He is Professor of Electrical Engineering at Stan-
",False,lownoise-Aug_2002,False,False,False
16065,"ford University, Stanford, CA, and Director of Re-
",False,lownoise-Aug_2002,False,False,False
16066,"search in the Center for Integrated Systems. He heldsummer staff positions at Fairchild, Bell Telephone
",False,lownoise-Aug_2002,False,False,False
16067,"Laboratories, Hewlett-Packard, IBM Research, and
",False,lownoise-Aug_2002,False,False,False
16068,"Matsushitain 1967, 1973,1975, 1977, and 1988, re-
",False,lownoise-Aug_2002,False,False,False
16069,"spectively. He has published more than 200 journal
",False,lownoise-Aug_2002,False,False,False
16070,"articles and graduated more than 48 doctorate students. His research interests
",False,lownoise-Aug_2002,False,False,False
16071,"focusonICprocess,device,andcircuittechnologies,especiallytheuseofcom-
",False,lownoise-Aug_2002,False,False,False
16072,"puter-aided design (CAD) and parallel computational methods.
",False,lownoise-Aug_2002,False,False,False
16073,"Dr. Dutton was Editor of the IEEE T
",False,lownoise-Aug_2002,False,False,False
16074,"RANSACTIONS ON COMPUTER -AIDED
",False,lownoise-Aug_2002,False,False,False
16075,"DESIGN OF INTEGRATED CIRCUITS AND SYSTEMSfrom 1984 to 1986. He was
",False,lownoise-Aug_2002,False,False,False
16076,"the winner of the 1987 IEEE J. J. Ebers Award, the 1988 Guggenheim Fellow-
",False,lownoise-Aug_2002,False,False,False
16077,"shiptostudyinJapan,andtheJackA.MortonAwardfor1996.Hewaselected
",False,lownoise-Aug_2002,False,False,False
16078,to the National Academy of Engineering in 1991.,False,lownoise-Aug_2002,False,False,False
16079,Abstract ,True,LSBert A Simple Framework for Lexical,False,False,True
16080,"1 I NTRODUCTION
",True,LSBert A Simple Framework for Lexical,False,False,True
16081,"2 R ELATED WORK
",True,LSBert A Simple Framework for Lexical,False,False,True
16082,"3 L EXICAL SIMPLIFICATION FRAMEWORK
",True,LSBert A Simple Framework for Lexical,False,False,True
16083,"3.1 Complex Word Identiﬁcation (CWI)
",True,LSBert A Simple Framework for Lexical,False,False,True
16084,"3.2 Substitute Generation (SG)
",True,LSBert A Simple Framework for Lexical,False,False,True
16085,"3.3 Filtering and Substitute Ranking (SR)
",True,LSBert A Simple Framework for Lexical,False,False,True
16086,"1:S Input Sentence
",True,LSBert A Simple Framework for Lexical,False,False,True
16087,"2:t Complexity threshold
",True,LSBert A Simple Framework for Lexical,False,False,True
16088,"3:ignore list Named Entity Identiﬁcation( S)
",True,LSBert A Simple Framework for Lexical,False,False,True
16089,"4:LSBert( S,t,ignore list)
",True,LSBert A Simple Framework for Lexical,False,False,True
16090,"3.4 LSBert Algorithm
",True,LSBert A Simple Framework for Lexical,False,False,True
16091,"4 E XPERIMENTS
",True,LSBert A Simple Framework for Lexical,False,False,True
16092,"1:complex words CWI(S,t)-ignore list
",True,LSBert A Simple Framework for Lexical,False,False,True
16093,"2:ifnumber( complex words )>0then
",True,LSBert A Simple Framework for Lexical,False,False,True
16094,"3:w head(complex words )
",True,LSBert A Simple Framework for Lexical,False,False,True
16095,"4:subs Substitution Generation( S,w)
",True,LSBert A Simple Framework for Lexical,False,False,True
16096,"5:subs Substitute Ranking( subs )
",True,LSBert A Simple Framework for Lexical,False,False,True
16097,"6:top head(subs )
",True,LSBert A Simple Framework for Lexical,False,False,True
16098,"7: iffre(top)>fre (w)orloss (top)<loss (w)then
",True,LSBert A Simple Framework for Lexical,False,False,True
16099,"8: Replace( S,w,top)
",True,LSBert A Simple Framework for Lexical,False,False,True
16100,"9: ignore list.add(w)
",True,LSBert A Simple Framework for Lexical,False,False,True
16101,"10: LSBert( S,t,ignore list)
",True,LSBert A Simple Framework for Lexical,False,False,True
16102,"11: else
",True,LSBert A Simple Framework for Lexical,False,False,True
16103,"12: LSBert( S,t,ignore list)
",True,LSBert A Simple Framework for Lexical,False,False,True
16104,"13: end if
",True,LSBert A Simple Framework for Lexical,False,False,True
16105,"15: return S
",True,LSBert A Simple Framework for Lexical,False,False,True
16106,"16:end if
",True,LSBert A Simple Framework for Lexical,False,False,True
16107,"4.1 Experiment Setup
",True,LSBert A Simple Framework for Lexical,False,False,True
16108,"6. http://ghpaetzold.github.io/data/BenchLS.zip
",True,LSBert A Simple Framework for Lexical,False,False,True
16109,"4.2 Quantitative Evaluation
",True,LSBert A Simple Framework for Lexical,False,False,True
16110,"4.3 Ablation Study of LSBert
",True,LSBert A Simple Framework for Lexical,False,False,True
16111,"4.4 Qualitative Study
",True,LSBert A Simple Framework for Lexical,False,False,True
16112,"5 C ONCLUSION
",True,LSBert A Simple Framework for Lexical,False,False,True
16113,"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 1
",False,LSBert A Simple Framework for Lexical,False,False,True
16114,"LSBert: A Simple Framework for Lexical
",False,LSBert A Simple Framework for Lexical,False,False,True
16115,"Simpliﬁcation
",False,LSBert A Simple Framework for Lexical,False,False,True
16116,"Jipeng Qiang, Yun Li, Yi Zhu, Yunhao Yuan, and Xindong Wu, Fellow, IEEE,
",False,LSBert A Simple Framework for Lexical,False,False,True
16117,"meaning, to simplify the sentence. Recently unsupervised lexical simpliﬁcation approaches only rely on the complex word itself
",False,LSBert A Simple Framework for Lexical,False,False,True
16118,"regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious
",False,LSBert A Simple Framework for Lexical,False,False,True
16119,"candidates. In this paper, we propose a lexical simpliﬁcation framework LSBert based on pretrained representation model Bert, that is
",False,LSBert A Simple Framework for Lexical,False,False,True
16120,"capable of (1) making use of the wider context when both detecting the words in need of simpliﬁcation and generating substitue
",False,LSBert A Simple Framework for Lexical,False,False,True
16121,"candidates, and (2) taking ﬁve high-quality features into account for ranking candidates, including Berts prediction order, Bert-based
",False,LSBert A Simple Framework for Lexical,False,False,True
16122,"language model, and the paraphrase database PPDB, in addition to the word frequency and word similarity commonly used in other LS
",False,LSBert A Simple Framework for Lexical,False,False,True
16123,"methods. We show that our system outputs lexical simpliﬁcations that are grammatically correct and semantically appropriate, and
",False,LSBert A Simple Framework for Lexical,False,False,True
16124,"obtains obvious improvement compared with these baselines, outperforming the state-of-the-art by 29.8 Accuracy points on three
",False,LSBert A Simple Framework for Lexical,False,False,True
16125,"well-known benchmarks.
",False,LSBert A Simple Framework for Lexical,False,False,True
16126,"Index Terms —Lexical simpliﬁcation, BERT, Unsupervised, Pretrained language model.
",False,LSBert A Simple Framework for Lexical,False,False,True
16127,"F
",False,LSBert A Simple Framework for Lexical,False,False,True
16128,"1 I NTRODUCTION
",False,LSBert A Simple Framework for Lexical,False,False,True
16129,"Lexical Simpliﬁcation (LS) aims at replacing complex words
",False,LSBert A Simple Framework for Lexical,False,False,True
16130,"with simpler alternatives, which can help various groups of peo-
",False,LSBert A Simple Framework for Lexical,False,False,True
16131,"ple, including children [1], non-native speakers [2], people with
",False,LSBert A Simple Framework for Lexical,False,False,True
16132,"cognitive disabilities [3], [4], to understand text better. LS is an
",False,LSBert A Simple Framework for Lexical,False,False,True
16133,"effective way of simplifying a text because some work shows that
",False,LSBert A Simple Framework for Lexical,False,False,True
16134,"those who are familiar with the vocabulary of a text can often
",False,LSBert A Simple Framework for Lexical,False,False,True
16135,"understand its meaning even if the grammatical constructs used
",False,LSBert A Simple Framework for Lexical,False,False,True
16136,"are confusing to them. The LS framework is commonly framed
",False,LSBert A Simple Framework for Lexical,False,False,True
16137,"as a pipeline of three steps: complex word identiﬁcation (CWI),
",False,LSBert A Simple Framework for Lexical,False,False,True
16138,"substitute generation (SG) of complex words, and ﬁltering and
",False,LSBert A Simple Framework for Lexical,False,False,True
16139,"substitute ranking (SR). CWI is often treated as an independent
",False,LSBert A Simple Framework for Lexical,False,False,True
16140,"task [5]. Existing LS systems mainly focused on the two steps
",False,LSBert A Simple Framework for Lexical,False,False,True
16141,"(SG and SR) [6].
",False,LSBert A Simple Framework for Lexical,False,False,True
16142,"The popular LS systems still predominantly use a set of rules
",False,LSBert A Simple Framework for Lexical,False,False,True
16143,"for substituting complex words with their frequent synonyms
",False,LSBert A Simple Framework for Lexical,False,False,True
16144,"from carefully handcrafted databases (e.g., WordNet) [7] or au-
",False,LSBert A Simple Framework for Lexical,False,False,True
16145,"tomatically induced from comparable corpora [1] or paraphrase
",False,LSBert A Simple Framework for Lexical,False,False,True
16146,"database [8]. Recent work utilizes word embedding models to
",False,LSBert A Simple Framework for Lexical,False,False,True
16147,"extract substitute candidates for complex words. Given a complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16148,"word, they extracted the top 10 words as substitute candidates from
",False,LSBert A Simple Framework for Lexical,False,False,True
16149,"the word embedding model whose vectors are closer in terms of
",False,LSBert A Simple Framework for Lexical,False,False,True
16150,"cosine similarity with the complex word [2], [5], [9]. Recently,
",False,LSBert A Simple Framework for Lexical,False,False,True
16151,"the LS system REC-LS attempts to generate substitute candidates
",False,LSBert A Simple Framework for Lexical,False,False,True
16152,"by combining linguistic databases and word embedding models.
",False,LSBert A Simple Framework for Lexical,False,False,True
16153,"However, they generated substitute candidates for the complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16154,"word regardless of the context of the complex word, which
",False,LSBert A Simple Framework for Lexical,False,False,True
16155,"will inevitably produce a large number of spurious candidates
",False,LSBert A Simple Framework for Lexical,False,False,True
16156,"J. Qiang, Y. Li, Y. Zhu, and Y. Yuan are with the Department of Computer
",False,LSBert A Simple Framework for Lexical,False,False,True
16157,"Science, Yangzhou, Jiangsu, China.
",False,LSBert A Simple Framework for Lexical,False,False,True
16158,"E-mail:fjpqiang,liyun, zhuyi, yhyuan g@yzu.edu.cn
",False,LSBert A Simple Framework for Lexical,False,False,True
16159,"X. Wu is with Key Laboratory of Knowledge Engineering with Big Data
",False,LSBert A Simple Framework for Lexical,False,False,True
16160,"(Hefei University of Technology), Ministry of Education, Hefei, Anhui,
",False,LSBert A Simple Framework for Lexical,False,False,True
16161,"China, and Mininglamp Academy of Sciences, Minininglamp, Beijing,
",False,LSBert A Simple Framework for Lexical,False,False,True
16162,"China.
",False,LSBert A Simple Framework for Lexical,False,False,True
16163,"E-mail: xwu@hfut.edu.cn
",False,LSBert A Simple Framework for Lexical,False,False,True
16164,"Fig. 1. Comparison of substitute candidates of complex words. Given
",False,LSBert A Simple Framework for Lexical,False,False,True
16165,"one sentence ”John composed these verses.” and complex words ’com-
",False,LSBert A Simple Framework for Lexical,False,False,True
16166,"posed’ and ’verses’, the top three simpliﬁcation candidates for each
",False,LSBert A Simple Framework for Lexical,False,False,True
16167,"complex word are generated by our method LSBert and the state-of-the-
",False,LSBert A Simple Framework for Lexical,False,False,True
16168,"art two baselines (Glava ˇs [9] and REC-LS [6]). The simpliﬁed sentences
",False,LSBert A Simple Framework for Lexical,False,False,True
16169,"by the three LS methods are shown at the bottom.
",False,LSBert A Simple Framework for Lexical,False,False,True
16170,"that confuse the systems employed in the subsequent steps. For
",False,LSBert A Simple Framework for Lexical,False,False,True
16171,"example, if simpler alternatives of the complex word do not exist
",False,LSBert A Simple Framework for Lexical,False,False,True
16172,"in substitute candidates, the ﬁltering and substitute ranking step of
",False,LSBert A Simple Framework for Lexical,False,False,True
16173,"LS is meaningless.
",False,LSBert A Simple Framework for Lexical,False,False,True
16174,"Context plays a central role in fulﬁlling substitute generation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16175,"Here, we give a simple example shown in Figure 1. For complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16176,"words ’composed’ and ’verses’ in the sentence ”John composed
",False,LSBert A Simple Framework for Lexical,False,False,True
16177,"these verses.”, the top three substitute candidates of the two
",False,LSBert A Simple Framework for Lexical,False,False,True
16178,"complex words generated by the state-of-the-art LS systems [6],arXiv:2006.14939v1  [cs.CL]  25 Jun 2020JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 2
",False,LSBert A Simple Framework for Lexical,False,False,True
16179,"[9] are only related with the complex words itself regardless of
",False,LSBert A Simple Framework for Lexical,False,False,True
16180,"the context. For example, the candidates ”consisting, consists,
",False,LSBert A Simple Framework for Lexical,False,False,True
16181,"comprised” is generated by Glava ˇs [9] for the complex word
",False,LSBert A Simple Framework for Lexical,False,False,True
16182,"”composed”, and the candidates ”framed, quieted, planned” is
",False,LSBert A Simple Framework for Lexical,False,False,True
16183,"produced by REC-LS [6].
",False,LSBert A Simple Framework for Lexical,False,False,True
16184,"In contrast to the existing LS methods that only considered
",False,LSBert A Simple Framework for Lexical,False,False,True
16185,"the context in the last step (substitute ranking), we present a novel
",False,LSBert A Simple Framework for Lexical,False,False,True
16186,"LS framework LSBert, which takes the context into account in all
",False,LSBert A Simple Framework for Lexical,False,False,True
16187,"three steps of LS. As word complexity depends on context, LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16188,"uses a novel approach to identify complex words using a sequence
",False,LSBert A Simple Framework for Lexical,False,False,True
16189,"labeling method [10] based on bi-directional long short-term
",False,LSBert A Simple Framework for Lexical,False,False,True
16190,"memory units (BiLSTM). For producing suitable simpliﬁcations
",False,LSBert A Simple Framework for Lexical,False,False,True
16191,"for the complex word, we exploit recent advances in pretrained
",False,LSBert A Simple Framework for Lexical,False,False,True
16192,"unsupervised deep bidirectional representations Bert [11] . More
",False,LSBert A Simple Framework for Lexical,False,False,True
16193,"speciﬁcally, we mask the complex word wof the original sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16194,"Sas a new sentence S0, and concatenate the original sequence
",False,LSBert A Simple Framework for Lexical,False,False,True
16195,"SandS0for feeding into the Bert to obtain the probability
",False,LSBert A Simple Framework for Lexical,False,False,True
16196,"distribution of the vocabulary corresponding to the masked word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16197,"Then we choose the top probability words as substitute candidates.
",False,LSBert A Simple Framework for Lexical,False,False,True
16198,"For ranking the substitutions, we adopt ﬁve high-quality
",False,LSBert A Simple Framework for Lexical,False,False,True
16199,"features including word frequency and word similarity, Berts
",False,LSBert A Simple Framework for Lexical,False,False,True
16200,"prediction order, Bert-based language model, and the paraphrase
",False,LSBert A Simple Framework for Lexical,False,False,True
16201,"database PPDB, to ensure grammaticality and meaning equiva-
",False,LSBert A Simple Framework for Lexical,False,False,True
16202,"lence to the original sentence in the output. LSBert simpliﬁes one
",False,LSBert A Simple Framework for Lexical,False,False,True
16203,"word at a time and is recursively applied to simplify the sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16204,"by taking word complexity in context into account. As shown
",False,LSBert A Simple Framework for Lexical,False,False,True
16205,"in Figure 1, the meaning of the original sentence using Glava ˇs
",False,LSBert A Simple Framework for Lexical,False,False,True
16206,"is changed, and REC-LS does not make the right simpliﬁcation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16207,"LSBert generates the appropriate substitutes and achieves its aim
",False,LSBert A Simple Framework for Lexical,False,False,True
16208,"that replaces complex words with simpler alternatives.
",False,LSBert A Simple Framework for Lexical,False,False,True
16209,"This paper has the following two contributions:
",False,LSBert A Simple Framework for Lexical,False,False,True
16210,"(1) LSBert is a novel Bert-based method for LS, which can
",False,LSBert A Simple Framework for Lexical,False,False,True
16211,"take full advantages of Bert to generate and rank substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16212,"candidates. To our best knowledge, this is the ﬁrst attempt to
",False,LSBert A Simple Framework for Lexical,False,False,True
16213,"apply pretrained transformer language models for LS. In contrast
",False,LSBert A Simple Framework for Lexical,False,False,True
16214,"to existing methods without considering the context in complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16215,"word identiﬁcation and substitute generations, LSBert is easier to
",False,LSBert A Simple Framework for Lexical,False,False,True
16216,"hold cohesion and coherence of a sentence, since LSBert takes the
",False,LSBert A Simple Framework for Lexical,False,False,True
16217,"context into count for each step of LS
",False,LSBert A Simple Framework for Lexical,False,False,True
16218,"(2) LSBert is a simple, effective and complete LS method.
",False,LSBert A Simple Framework for Lexical,False,False,True
16219,"1)Simple: many steps used in existing LS systems have been
",False,LSBert A Simple Framework for Lexical,False,False,True
16220,"eliminated from our method, e.g., morphological transformation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16221,"2) Effective: it obtains new state-of-the-art results on three bench-
",False,LSBert A Simple Framework for Lexical,False,False,True
16222,"marks. 3) Complete: LSBert recursively simpliﬁes all complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16223,"words in a sentence without requiring additional steps.
",False,LSBert A Simple Framework for Lexical,False,False,True
16224,"To facilitate reproducibility, the code of LSBert framework is
",False,LSBert A Simple Framework for Lexical,False,False,True
16225,"available at https://github.com/BERT-LS.
",False,LSBert A Simple Framework for Lexical,False,False,True
16226,"The rest of this paper is organized as follows. In Section 2,
",False,LSBert A Simple Framework for Lexical,False,False,True
16227,"we introduce the related work of text simpliﬁcation. Section 3
",False,LSBert A Simple Framework for Lexical,False,False,True
16228,"describes the framework LSBert. In Section 4, we describe the
",False,LSBert A Simple Framework for Lexical,False,False,True
16229,"experimental setup and evaluate the proposed method LSBert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16230,"Finally, we draw our conclusions in Section 5.
",False,LSBert A Simple Framework for Lexical,False,False,True
16231,"2 R ELATED WORK
",False,LSBert A Simple Framework for Lexical,False,False,True
16232,"Textual simpliﬁcation (TS) is the process of simplifying the
",False,LSBert A Simple Framework for Lexical,False,False,True
16233,"content of the original text as much as possible, while retaining
",False,LSBert A Simple Framework for Lexical,False,False,True
16234,"the meaning and grammaticality so that it can be more easily
",False,LSBert A Simple Framework for Lexical,False,False,True
16235,"read and understood by a wider audience. Textual simpliﬁcation
",False,LSBert A Simple Framework for Lexical,False,False,True
16236,"focuses on simplifying the vocabulary and syntax of the text. Early
",False,LSBert A Simple Framework for Lexical,False,False,True
16237,"systems of TS often used standard statistical machine translationapproaches to learn the simpliﬁcation of a complex sentence into a
",False,LSBert A Simple Framework for Lexical,False,False,True
16238,"simpliﬁed sentence [12]. Recently, TS methods adopted encoder-
",False,LSBert A Simple Framework for Lexical,False,False,True
16239,"decoder model to simplify the text based on parallel corpora [13]–
",False,LSBert A Simple Framework for Lexical,False,False,True
16240,"[15]. All of the above work belong to the supervised TS systems,
",False,LSBert A Simple Framework for Lexical,False,False,True
16241,"whose performance strongly relies on the availability of large
",False,LSBert A Simple Framework for Lexical,False,False,True
16242,"amounts of parallel sentences. Two public parallel benchmarks
",False,LSBert A Simple Framework for Lexical,False,False,True
16243,"WikiSmall [16] and WikiLarge [17] contain a large proportion of:
",False,LSBert A Simple Framework for Lexical,False,False,True
16244,"inaccurate simpliﬁcations (not aligned or only partially aligned) ;
",False,LSBert A Simple Framework for Lexical,False,False,True
16245,"inadequate simpliﬁcations (not much simpler) [18], [19]. These
",False,LSBert A Simple Framework for Lexical,False,False,True
16246,"problems is mainly because designing a good alignment algorithm
",False,LSBert A Simple Framework for Lexical,False,False,True
16247,"for extracting parallel sentences from EW and SEW is very
",False,LSBert A Simple Framework for Lexical,False,False,True
16248,"difﬁcult [20]. Therefore, a number of approaches focusing on the
",False,LSBert A Simple Framework for Lexical,False,False,True
16249,"generation and assessment of lexical simpliﬁcation were proposed.
",False,LSBert A Simple Framework for Lexical,False,False,True
16250,"Lexical simpliﬁcation (LS) only focuses to simplify complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16251,"words of one sentence. LS needs to identify complex words and
",False,LSBert A Simple Framework for Lexical,False,False,True
16252,"ﬁnd the best candidate substitution for these complex words [21],
",False,LSBert A Simple Framework for Lexical,False,False,True
16253,"[22]. The best substitution needs to be more simplistic while
",False,LSBert A Simple Framework for Lexical,False,False,True
16254,"preserving the sentence grammatically and keeping its meaning as
",False,LSBert A Simple Framework for Lexical,False,False,True
16255,"much as possible, which is a very challenging task. The popular
",False,LSBert A Simple Framework for Lexical,False,False,True
16256,"lexical simpliﬁcation approaches were rule-based, in which each
",False,LSBert A Simple Framework for Lexical,False,False,True
16257,"rule contains a complex word and its simple synonyms [8], [23],
",False,LSBert A Simple Framework for Lexical,False,False,True
16258,"[24]. Rule-based systems usually identiﬁed synonyms from Word-
",False,LSBert A Simple Framework for Lexical,False,False,True
16259,"Net or other linguistic databases for a predeﬁned set of complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16260,"words and selected the ”simplest” from these synonyms based on
",False,LSBert A Simple Framework for Lexical,False,False,True
16261,"the frequency of word or length of word [1], [7]. However, there is
",False,LSBert A Simple Framework for Lexical,False,False,True
16262,"a major limitation for the rule-based systems that it is impossible
",False,LSBert A Simple Framework for Lexical,False,False,True
16263,"to give all possible simpliﬁcation rules for each word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16264,"As complex and simpliﬁed parallel corpora are available, LS
",False,LSBert A Simple Framework for Lexical,False,False,True
16265,"systems tried to extract rules from parallel corpora [25]–[27].
",False,LSBert A Simple Framework for Lexical,False,False,True
16266,"Yatskar et al. (2010) identiﬁed lexical simpliﬁcations from the edit
",False,LSBert A Simple Framework for Lexical,False,False,True
16267,"history of simple English Wikipedia (SEW). They utilized a prob-
",False,LSBert A Simple Framework for Lexical,False,False,True
16268,"abilistic method to recognize simpliﬁcation edits distinguishing
",False,LSBert A Simple Framework for Lexical,False,False,True
16269,"from other types of content changes. Biran et al. (2011) considered
",False,LSBert A Simple Framework for Lexical,False,False,True
16270,"every pair of distinct word in the English Wikipedia (EW) and
",False,LSBert A Simple Framework for Lexical,False,False,True
16271,"SEW to be a possible simpliﬁcation pair, and ﬁltered part of
",False,LSBert A Simple Framework for Lexical,False,False,True
16272,"them based on morphological variants and WordNet. Horn et al.
",False,LSBert A Simple Framework for Lexical,False,False,True
16273,"(2014) also generated the candidate rules from the EW and SEW,
",False,LSBert A Simple Framework for Lexical,False,False,True
16274,"and adopted a context-aware binary classiﬁer to decide whether a
",False,LSBert A Simple Framework for Lexical,False,False,True
16275,"candidate rule should be adopted or not in a certain context. The
",False,LSBert A Simple Framework for Lexical,False,False,True
16276,"main limitation of the type of methods relies heavily on parallel
",False,LSBert A Simple Framework for Lexical,False,False,True
16277,"corpora.
",False,LSBert A Simple Framework for Lexical,False,False,True
16278,"To entirely avoid the requirement of lexical resources or paral-
",False,LSBert A Simple Framework for Lexical,False,False,True
16279,"lel corpora, LS systems based on word embeddings were proposed
",False,LSBert A Simple Framework for Lexical,False,False,True
16280,"[9]. They extracted the top 10 words as candidate substitutions
",False,LSBert A Simple Framework for Lexical,False,False,True
16281,"whose vectors are closer in terms of cosine similarity with the
",False,LSBert A Simple Framework for Lexical,False,False,True
16282,"complex word. Instead of a traditional word embedding model,
",False,LSBert A Simple Framework for Lexical,False,False,True
16283,"Paetzold and Specia (2016) adopted context-aware word embed-
",False,LSBert A Simple Framework for Lexical,False,False,True
16284,"dings trained on a large dataset where each word is annotated with
",False,LSBert A Simple Framework for Lexical,False,False,True
16285,"the POS tag. Afterward, they further extracted candidates for the
",False,LSBert A Simple Framework for Lexical,False,False,True
16286,"complex word by combining word embeddings with WordNet and
",False,LSBert A Simple Framework for Lexical,False,False,True
16287,"parallel corpora [5]. REC-LS [6] attempted to generate substitutes
",False,LSBert A Simple Framework for Lexical,False,False,True
16288,"from multiple sources, e.g, WordNet, Big Huge Thesaurus1and
",False,LSBert A Simple Framework for Lexical,False,False,True
16289,"word embeddings.
",False,LSBert A Simple Framework for Lexical,False,False,True
16290,"After examining existing LS methods ranging from rules-
",False,LSBert A Simple Framework for Lexical,False,False,True
16291,"based to embedding-based, the major challenge is that they gen-
",False,LSBert A Simple Framework for Lexical,False,False,True
16292,"erated simpliﬁcation candidates for the complex word regardless
",False,LSBert A Simple Framework for Lexical,False,False,True
16293,"of the context of the complex word, which will inevitably produce
",False,LSBert A Simple Framework for Lexical,False,False,True
16294,"a large number of spurious candidates that confuse the systems
",False,LSBert A Simple Framework for Lexical,False,False,True
16295,"employed in the subsequent steps.
",False,LSBert A Simple Framework for Lexical,False,False,True
16296,"1. https://words.bighugelabs.comJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 3
",False,LSBert A Simple Framework for Lexical,False,False,True
16297,"Fig. 2. Overview of the lexical simpliﬁcation framework LSBert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16298,"In this paper, we will ﬁrst present a LS approach LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16299,"that requires only a sufﬁciently large corpus of raw text without
",False,LSBert A Simple Framework for Lexical,False,False,True
16300,"any manual efforts. Pre-training language models [11], [28], [29]
",False,LSBert A Simple Framework for Lexical,False,False,True
16301,"have attracted wide attention and has shown to be effective for
",False,LSBert A Simple Framework for Lexical,False,False,True
16302,"improving many downstream natural language processing tasks.
",False,LSBert A Simple Framework for Lexical,False,False,True
16303,"Our method exploits recent advances in Bert to generate suitable
",False,LSBert A Simple Framework for Lexical,False,False,True
16304,"simpliﬁcations for complex words. Our method generates the
",False,LSBert A Simple Framework for Lexical,False,False,True
16305,"candidates of the complex word by considering the whole sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16306,"that is easier to hold cohesion and coherence of a sentence. In
",False,LSBert A Simple Framework for Lexical,False,False,True
16307,"this case, many steps used in existing LS methods have been
",False,LSBert A Simple Framework for Lexical,False,False,True
16308,"eliminated from our method, e.g., morphological transformation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16309,"The previous version LSBert was published in artiﬁcial intelli-
",False,LSBert A Simple Framework for Lexical,False,False,True
16310,"gence conference (AAAI) [30], which only focused on substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16311,"generations given the sentence and its complex word using Bert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16312,"In this paper, we propose an LS framework including complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16313,"word identiﬁcation, substitute generations, substitute ranking. The
",False,LSBert A Simple Framework for Lexical,False,False,True
16314,"framework can simplify one sentence recursively. One recent
",False,LSBert A Simple Framework for Lexical,False,False,True
16315,"work for LS based Bert [31] was almost simultaneously proposed
",False,LSBert A Simple Framework for Lexical,False,False,True
16316,"with our previous version, which also only focused on substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16317,"generations.
",False,LSBert A Simple Framework for Lexical,False,False,True
16318,"3 L EXICAL SIMPLIFICATION FRAMEWORK
",False,LSBert A Simple Framework for Lexical,False,False,True
16319,"In this section, we outline each step of our lexical simpliﬁca-
",False,LSBert A Simple Framework for Lexical,False,False,True
16320,"tion framework LSBert as presented in Figure 2, which includes
",False,LSBert A Simple Framework for Lexical,False,False,True
16321,"the following three steps: complex word identiﬁcation, substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16322,"generation, ﬁltering and substitute ranking. LSBert simliﬁes one
",False,LSBert A Simple Framework for Lexical,False,False,True
16323,"complex word at a time, and is recursively applied to simplify the
",False,LSBert A Simple Framework for Lexical,False,False,True
16324,"sentence. We will give the details of each step below.
",False,LSBert A Simple Framework for Lexical,False,False,True
16325,"3.1 Complex Word Identiﬁcation (CWI)
",False,LSBert A Simple Framework for Lexical,False,False,True
16326,"Identifying complex words from one sentence has been studied
",False,LSBert A Simple Framework for Lexical,False,False,True
16327,"for years, whose goal is to select the words in a given sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16328,"which should be simpliﬁed [32], [33].CWI was framed as a sequence labeling task [10] and an
",False,LSBert A Simple Framework for Lexical,False,False,True
16329,"approach SEQ based on bi-directional long short-term memory
",False,LSBert A Simple Framework for Lexical,False,False,True
16330,"units (BiLSTM) is trained to predict the binary complexity of
",False,LSBert A Simple Framework for Lexical,False,False,True
16331,"words as annotated in the dataset of [34]. In contrast to the other
",False,LSBert A Simple Framework for Lexical,False,False,True
16332,"CWI models, the SEQ model has the following two advantages:
",False,LSBert A Simple Framework for Lexical,False,False,True
16333,"takes word context into account and helps avoid the necessity of
",False,LSBert A Simple Framework for Lexical,False,False,True
16334,"extensive feature engineering, because SEQ only relies on word
",False,LSBert A Simple Framework for Lexical,False,False,True
16335,"embeddings as the only input information.
",False,LSBert A Simple Framework for Lexical,False,False,True
16336,"The SEQ approach labels each word with a lexical complexity
",False,LSBert A Simple Framework for Lexical,False,False,True
16337,"score ( p) which represents the likelihood of each word belonging
",False,LSBert A Simple Framework for Lexical,False,False,True
16338,"to the complex class. Giving a predeﬁned threshold p, if the
",False,LSBert A Simple Framework for Lexical,False,False,True
16339,"lexical complexity of one word is greater than the threshold, it
",False,LSBert A Simple Framework for Lexical,False,False,True
16340,"will be treated as a complex word. For example, the example
",False,LSBert A Simple Framework for Lexical,False,False,True
16341,"”John composed 0:55these verses 0:76” is showed in Figure 2. If
",False,LSBert A Simple Framework for Lexical,False,False,True
16342,"the complexity threshold is set to 0.5, the two words ”composed”
",False,LSBert A Simple Framework for Lexical,False,False,True
16343,"and ”verses” will be the complex words to be simpliﬁed.
",False,LSBert A Simple Framework for Lexical,False,False,True
16344,"LSBert starts with the word ”verses” with the highest pvalue
",False,LSBert A Simple Framework for Lexical,False,False,True
16345,"above the predeﬁned threshold to simplify. After completing the
",False,LSBert A Simple Framework for Lexical,False,False,True
16346,"simpliﬁcation process, we will recalculate the complexity of each
",False,LSBert A Simple Framework for Lexical,False,False,True
16347,"word in the sentence, excluding words that have been simpliﬁed.
",False,LSBert A Simple Framework for Lexical,False,False,True
16348,"In addition, we exclude the simpliﬁcation of entity words by
",False,LSBert A Simple Framework for Lexical,False,False,True
16349,"performing named entity identiﬁcation on the sentence.
",False,LSBert A Simple Framework for Lexical,False,False,True
16350,"3.2 Substitute Generation (SG)
",False,LSBert A Simple Framework for Lexical,False,False,True
16351,"Given a sentence Sand the complex word w, the aim of
",False,LSBert A Simple Framework for Lexical,False,False,True
16352,"substitution generation (SG) is to produce the substitute candi-
",False,LSBert A Simple Framework for Lexical,False,False,True
16353,"dates for the complex word w. LSBert produces the substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16354,"candidates for the complex word using pretrained language model
",False,LSBert A Simple Framework for Lexical,False,False,True
16355,"Bert. we brieﬂy summarize the Bert model, and then describe how
",False,LSBert A Simple Framework for Lexical,False,False,True
16356,"we extend it to do lexical simpliﬁcation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16357,"Bert [11] is a self-supervised method for pretrained a deep
",False,LSBert A Simple Framework for Lexical,False,False,True
16358,"transformer encoder, which is optimized by two training ob-
",False,LSBert A Simple Framework for Lexical,False,False,True
16359,"jectives: masked language modeling (MLM) and next sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16360,"prediction (NSP). Unlike a traditional language modeling ob-
",False,LSBert A Simple Framework for Lexical,False,False,True
16361,"jective of predicting the next word in a sequence given the
",False,LSBert A Simple Framework for Lexical,False,False,True
16362,"history, MLM predicts missing tokens in a sequence given its
",False,LSBert A Simple Framework for Lexical,False,False,True
16363,"left and right context. Bert accomplishes NSP task by prepending
",False,LSBert A Simple Framework for Lexical,False,False,True
16364,"every sentence with a special classiﬁcation token, [CLS], and by
",False,LSBert A Simple Framework for Lexical,False,False,True
16365,"combining sentences with a special separator token, [SEP]. The
",False,LSBert A Simple Framework for Lexical,False,False,True
16366,"ﬁnal hidden state corresponding to the [CLS] token is used as the
",False,LSBert A Simple Framework for Lexical,False,False,True
16367,"total sequence representation from which we predict a label for
",False,LSBert A Simple Framework for Lexical,False,False,True
16368,"classiﬁcation tasks, or which may otherwise be overlooked.
",False,LSBert A Simple Framework for Lexical,False,False,True
16369,"Due to the fundamental nature of MLM, we mask the complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16370,"word wof the sentence Sand get the probability distribution of
",False,LSBert A Simple Framework for Lexical,False,False,True
16371,"the vocabulary p(jSnfwg)corresponding to the masked word w.
",False,LSBert A Simple Framework for Lexical,False,False,True
16372,"Therefore, we can try to use MLM for substitute generation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16373,"For the complex word win a sentence S, we mask the word
",False,LSBert A Simple Framework for Lexical,False,False,True
16374,"wofSusing special symbol ”[MASK]” as a new sequence S0. If
",False,LSBert A Simple Framework for Lexical,False,False,True
16375,"we directly feed S0into MLM, the probability of the vocabulary
",False,LSBert A Simple Framework for Lexical,False,False,True
16376,"p(jS0nftig)corresponding to the complex word wonly considers
",False,LSBert A Simple Framework for Lexical,False,False,True
16377,"the context regardless of the inﬂuence of the complex word w.
",False,LSBert A Simple Framework for Lexical,False,False,True
16378,"Considering that Bert is adept at dealing with sentence pairs due
",False,LSBert A Simple Framework for Lexical,False,False,True
16379,"to the NSP task adopted by Bert. We concatenate the original
",False,LSBert A Simple Framework for Lexical,False,False,True
16380,"sequence SandS0as a sentence pair, and feed the sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16381,"pair (S; S0)into the Bert to obtain the probability distribution of
",False,LSBert A Simple Framework for Lexical,False,False,True
16382,"the vocabulary p(jS; S0nfwg)corresponding to the mask word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16383,"Thus, the higher probability words in p(jS; S0nfwg)correspond-
",False,LSBert A Simple Framework for Lexical,False,False,True
16384,"ing to the mask word not only consider the complex word itself,
",False,LSBert A Simple Framework for Lexical,False,False,True
16385,"but also ﬁt the context of the complex word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16386,"Finally, we select the top 10 words from p(jS; S0nfwg)as
",False,LSBert A Simple Framework for Lexical,False,False,True
16387,"substitution candidates, excluding the morphological derivationsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 4
",False,LSBert A Simple Framework for Lexical,False,False,True
16388,",
",False,LSBert A Simple Framework for Lexical,False,False,True
16389,"Fig. 3. Substitution generation of LSBert for the target complex word prediction, or cloze task. The input text is ”the cat perched on the mat” with
",False,LSBert A Simple Framework for Lexical,False,False,True
16390,"complex word ”perched”. [MASK], [CLS] and [SEP] are thress special symbols in Bert, where [MASK] is used to mask the word, [CLS] is added in
",False,LSBert A Simple Framework for Lexical,False,False,True
16391,"front of each input instance and [SEP] is a special separator token.
",False,LSBert A Simple Framework for Lexical,False,False,True
16392,"ofw. In addition, considering that the contextual information of
",False,LSBert A Simple Framework for Lexical,False,False,True
16393,"the complex word is used twice, we randomly mask a certain
",False,LSBert A Simple Framework for Lexical,False,False,True
16394,"percentage of words in Sexcluding wfor appropriately reducing
",False,LSBert A Simple Framework for Lexical,False,False,True
16395,"the impact of contextual information.
",False,LSBert A Simple Framework for Lexical,False,False,True
16396,"See Figure 3 for an illustration. Suppose that there is a sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16397,"”the cat perched on the mat” and the complex word ”perched”, we
",False,LSBert A Simple Framework for Lexical,False,False,True
16398,"get the top three substitute candidates ”sat, seated, hopped”. We
",False,LSBert A Simple Framework for Lexical,False,False,True
16399,"can see that the three candidates not only have a strong correlation
",False,LSBert A Simple Framework for Lexical,False,False,True
16400,"with the complex word, but also hold the cohesion and coherence
",False,LSBert A Simple Framework for Lexical,False,False,True
16401,"properties of the sentence. If we adopt the existing state-of-the-art
",False,LSBert A Simple Framework for Lexical,False,False,True
16402,"methods [9] and [6], the top three substitution words are ”atop,
",False,LSBert A Simple Framework for Lexical,False,False,True
16403,"overlooking, precariously” and ”put, lighted, lay”, respectively.
",False,LSBert A Simple Framework for Lexical,False,False,True
16404,"Very obviously, our method generates better substitute candidates
",False,LSBert A Simple Framework for Lexical,False,False,True
16405,"for the complex word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16406,"3.3 Filtering and Substitute Ranking (SR)
",False,LSBert A Simple Framework for Lexical,False,False,True
16407,"Giving substitute candidates C=fc1; c2; :::; c ng, the substi-
",False,LSBert A Simple Framework for Lexical,False,False,True
16408,"tution ranking of the lexical simpliﬁcation framework is to decide
",False,LSBert A Simple Framework for Lexical,False,False,True
16409,"which one of the candidate substitutions that ﬁts the context of
",False,LSBert A Simple Framework for Lexical,False,False,True
16410,"complex word is the simplest [22], where nis the number of
",False,LSBert A Simple Framework for Lexical,False,False,True
16411,"substitute candidates. First, threshold-based ﬁltering is performed
",False,LSBert A Simple Framework for Lexical,False,False,True
16412,"by LSBert, which is used to remove some complex substitutes.
",False,LSBert A Simple Framework for Lexical,False,False,True
16413,"Substitutes are removed from consideration if their Zipf values
",False,LSBert A Simple Framework for Lexical,False,False,True
16414,"below 3 using Frequency features. Then, LSBert computes var-
",False,LSBert A Simple Framework for Lexical,False,False,True
16415,"ious rankings according to their scores for each of the features.
",False,LSBert A Simple Framework for Lexical,False,False,True
16416,"After obtaining all rankings for each feature, LSBert scores each
",False,LSBert A Simple Framework for Lexical,False,False,True
16417,"candidate by averaging all its rankings. Finally, we choose the
",False,LSBert A Simple Framework for Lexical,False,False,True
16418,"candidate with the highest ranking as the best substitute.
",False,LSBert A Simple Framework for Lexical,False,False,True
16419,"Previous work for this step is based on the following fea-
",False,LSBert A Simple Framework for Lexical,False,False,True
16420,"tures: word frequency, contextual simplicity and Ngram language
",False,LSBert A Simple Framework for Lexical,False,False,True
16421,"modeling, etc. In contrast to previous work, in addition to the
",False,LSBert A Simple Framework for Lexical,False,False,True
16422,"word frequency and word similarity commonly used in other LS
",False,LSBert A Simple Framework for Lexical,False,False,True
16423,"methods, LSBert considers three additional high-quality features:
",False,LSBert A Simple Framework for Lexical,False,False,True
16424,"two features about Bert and one feature about PPDB (A Paraphrase
",False,LSBert A Simple Framework for Lexical,False,False,True
16425,"Database for Simpliﬁcation).
",False,LSBert A Simple Framework for Lexical,False,False,True
16426,"Bert prediction order. On this step of substitute generation,
",False,LSBert A Simple Framework for Lexical,False,False,True
16427,"we obtain the probability distribution of the vocabulary corre-
",False,LSBert A Simple Framework for Lexical,False,False,True
16428,"sponding to the mask word. Because LSBert already incorporates
",False,LSBert A Simple Framework for Lexical,False,False,True
16429,"the context information on the step of substitution generation, theword order of Bert prediction is a crucial feature which includes
",False,LSBert A Simple Framework for Lexical,False,False,True
16430,"the information of both the context and the complex word itself.
",False,LSBert A Simple Framework for Lexical,False,False,True
16431,"The higher the probability, the more relevant the candidate for the
",False,LSBert A Simple Framework for Lexical,False,False,True
16432,"original sentence.
",False,LSBert A Simple Framework for Lexical,False,False,True
16433,"Language model feature. A substitution candidate should ﬁt
",False,LSBert A Simple Framework for Lexical,False,False,True
16434,"into the sequence of words preceding and following the original
",False,LSBert A Simple Framework for Lexical,False,False,True
16435,"word. We cannot directly compute the probability of a sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16436,"or sequence of words using Bert like traditional n-gram language
",False,LSBert A Simple Framework for Lexical,False,False,True
16437,"models. Let W=w m; :::; w 1; w; w 1; :::; w mbe the context
",False,LSBert A Simple Framework for Lexical,False,False,True
16438,"of the original word w. We adopt a new strategy to compute the
",False,LSBert A Simple Framework for Lexical,False,False,True
16439,"likelihood of W. We ﬁrst replace the original word wwith the
",False,LSBert A Simple Framework for Lexical,False,False,True
16440,"substitution candidate. We then mask one word of Wfrom front
",False,LSBert A Simple Framework for Lexical,False,False,True
16441,"to back and feed into Bert to compute the cross-entropy loss of
",False,LSBert A Simple Framework for Lexical,False,False,True
16442,"the mask word. Finally, we rank all substitute candidates based on
",False,LSBert A Simple Framework for Lexical,False,False,True
16443,"the average loss of W. The lower the loss, the substitute candidate
",False,LSBert A Simple Framework for Lexical,False,False,True
16444,"is a good substitution for the original word. We use as context a
",False,LSBert A Simple Framework for Lexical,False,False,True
16445,"symmetric window of size ﬁve around the complex word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16446,"Semantic similarity. The similarity between the complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16447,"word and the substitution candidate is widely used as a feature
",False,LSBert A Simple Framework for Lexical,False,False,True
16448,"for SR. In general, word embedding models are used to obtain the
",False,LSBert A Simple Framework for Lexical,False,False,True
16449,"vector representation and the cosine similarity metric is chosen to
",False,LSBert A Simple Framework for Lexical,False,False,True
16450,"compute the similarity. Here, we choose the pretrained fastText
",False,LSBert A Simple Framework for Lexical,False,False,True
16451,"model2as word embedding modeling. The higher the similarity
",False,LSBert A Simple Framework for Lexical,False,False,True
16452,"value, the higher the ranking.
",False,LSBert A Simple Framework for Lexical,False,False,True
16453,"Frequency feature. Frequency-based candidate ranking
",False,LSBert A Simple Framework for Lexical,False,False,True
16454,"strategies are one of the most popular choices by lexical simpliﬁ-
",False,LSBert A Simple Framework for Lexical,False,False,True
16455,"cation and quite effective. In general, the more frequency a word
",False,LSBert A Simple Framework for Lexical,False,False,True
16456,"is used, the most familiar it is to readers. We adopt the Zipf scale
",False,LSBert A Simple Framework for Lexical,False,False,True
16457,"created from the SUBTLEX lists [35], because some experiments
",False,LSBert A Simple Framework for Lexical,False,False,True
16458,"[22] revealed that word frequencies from this corpus correlate with
",False,LSBert A Simple Framework for Lexical,False,False,True
16459,"human judgments on simplicity than many other more widely used
",False,LSBert A Simple Framework for Lexical,False,False,True
16460,"corpora, such as Wikipedia. SUBTLEX3is composed of over six
",False,LSBert A Simple Framework for Lexical,False,False,True
16461,"million sentences extracted from subtitles of assorted movies. The
",False,LSBert A Simple Framework for Lexical,False,False,True
16462,"Zipf frequency of a word is the base-10 logarithm of the number
",False,LSBert A Simple Framework for Lexical,False,False,True
16463,"of times it appears per billion words.
",False,LSBert A Simple Framework for Lexical,False,False,True
16464,"2. https://dl.fbaipublicﬁles.com/fasttext/vectors-english/crawl-300d-2M-
",False,LSBert A Simple Framework for Lexical,False,False,True
16465,"subword.zip
",False,LSBert A Simple Framework for Lexical,False,False,True
16466,"3. http://subtlexus.lexique.orgJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 5
",False,LSBert A Simple Framework for Lexical,False,False,True
16467,"Algorithm 1 Lexical simpliﬁcation framework
",False,LSBert A Simple Framework for Lexical,False,False,True
16468,"1:S Input Sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16469,"2:t Complexity threshold
",False,LSBert A Simple Framework for Lexical,False,False,True
16470,"3:ignore list Named Entity Identiﬁcation( S)
",False,LSBert A Simple Framework for Lexical,False,False,True
16471,"4:LSBert( S,t,ignore list)
",False,LSBert A Simple Framework for Lexical,False,False,True
16472,"PPDB feature. Some LS methods generated substitute candi-
",False,LSBert A Simple Framework for Lexical,False,False,True
16473,"dates from PPDB or its subset SimplePPDB [8], [36]. PPDB is a
",False,LSBert A Simple Framework for Lexical,False,False,True
16474,"collection of more than 100 million English paraphrase pairs [37].
",False,LSBert A Simple Framework for Lexical,False,False,True
16475,"These pairs were extracted using a bilingual pivoting technique,
",False,LSBert A Simple Framework for Lexical,False,False,True
16476,"which assumes that two English phrases that translate to the
",False,LSBert A Simple Framework for Lexical,False,False,True
16477,"same foreign phrase have the same meaning. Since LSBert has a
",False,LSBert A Simple Framework for Lexical,False,False,True
16478,"better substitution generation than PPDB and SimplePPDB, they
",False,LSBert A Simple Framework for Lexical,False,False,True
16479,"cannot help improve the performance of substitution generation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16480,"Considering PPDB owns useful information about paraphrase, we
",False,LSBert A Simple Framework for Lexical,False,False,True
16481,"try to use PPDB as a feature to rank the candidate substitutions.
",False,LSBert A Simple Framework for Lexical,False,False,True
16482,"We adopt a simple strategy for PPDB to rank the candidates. For
",False,LSBert A Simple Framework for Lexical,False,False,True
16483,"each candidate ciinCofw, the ranking of ciis 1 if the pair
",False,LSBert A Simple Framework for Lexical,False,False,True
16484,"(w; ci)exists in PPDB. Otherwise, the ranking number of ciis
",False,LSBert A Simple Framework for Lexical,False,False,True
16485,"n=3.
",False,LSBert A Simple Framework for Lexical,False,False,True
16486,"3.4 LSBert Algorithm
",False,LSBert A Simple Framework for Lexical,False,False,True
16487,"Following CWI, substitute generation, ﬁltering and substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16488,"ranking steps, the overall simpliﬁcation algorithm LSBert is shown
",False,LSBert A Simple Framework for Lexical,False,False,True
16489,"in Algorithm 1 and Algorithm 2. Given the sentence Sand
",False,LSBert A Simple Framework for Lexical,False,False,True
16490,"complexity threshold t, we ﬁrst identify named entity using entity
",False,LSBert A Simple Framework for Lexical,False,False,True
16491,"identiﬁcation system4. We add entities into ignore list which
",False,LSBert A Simple Framework for Lexical,False,False,True
16492,"means these words do not need to be simpliﬁed.
",False,LSBert A Simple Framework for Lexical,False,False,True
16493,"In LSBert, we identify all complex words in sentence s
",False,LSBert A Simple Framework for Lexical,False,False,True
16494,"using CWI step excluding ignore list(line 1). If the number of
",False,LSBert A Simple Framework for Lexical,False,False,True
16495,"complex words in the sentence sis larger than 0 (line 2), LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16496,"will try to simplify the top complex word w(line 3). LSBert calls
",False,LSBert A Simple Framework for Lexical,False,False,True
16497,"substitute generation (line 4) and substitute ranking (line 5) in turn.
",False,LSBert A Simple Framework for Lexical,False,False,True
16498,"LSBert chooses the top substitute (line 6). One important thing to
",False,LSBert A Simple Framework for Lexical,False,False,True
16499,"notice is whether LSBert performs the simpliﬁcation only if the
",False,LSBert A Simple Framework for Lexical,False,False,True
16500,"top candidate tophas a higher frequency (Frequency feature) or
",False,LSBert A Simple Framework for Lexical,False,False,True
16501,"lower loss (Language model feature) than the original word (line
",False,LSBert A Simple Framework for Lexical,False,False,True
16502,"7). When LSBert performs the simpliﬁcation, it will replace winto
",False,LSBert A Simple Framework for Lexical,False,False,True
16503,"top(line 8) and add the word topintoignore list(line 9). After
",False,LSBert A Simple Framework for Lexical,False,False,True
16504,"completing the simpliﬁcation of one word, we will iteratively call
",False,LSBert A Simple Framework for Lexical,False,False,True
16505,"LSBert (line 10 and line 12). If the number of complex words in
",False,LSBert A Simple Framework for Lexical,False,False,True
16506,"Sequals to 0, we will stop calling LSBert (line 15).
",False,LSBert A Simple Framework for Lexical,False,False,True
16507,"4 E XPERIMENTS
",False,LSBert A Simple Framework for Lexical,False,False,True
16508,"We design experiments to answer the following questions:
",False,LSBert A Simple Framework for Lexical,False,False,True
16509,"Q1. The effectiveness of substitute candidates and ranking:
",False,LSBert A Simple Framework for Lexical,False,False,True
16510,"Does the simpliﬁcation candidate generation of LSBert outper-
",False,LSBert A Simple Framework for Lexical,False,False,True
16511,"forms the substitution generation of the state-of-the-art competi-
",False,LSBert A Simple Framework for Lexical,False,False,True
16512,"tors?
",False,LSBert A Simple Framework for Lexical,False,False,True
16513,"Q2. The effectiveness of the LS system: Do the of LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16514,"outperforms the full pipeline of the state-of-the-art competitors?
",False,LSBert A Simple Framework for Lexical,False,False,True
16515,"Q3. The factors of affecting the LSBert: Experiments on
",False,LSBert A Simple Framework for Lexical,False,False,True
16516,"different parameters and models verify the impact on the LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16517,"system.
",False,LSBert A Simple Framework for Lexical,False,False,True
16518,"Q4. The qualitative study of the LSBert: We do more
",False,LSBert A Simple Framework for Lexical,False,False,True
16519,"experiments to analyze the advantages and the disadvantages of
",False,LSBert A Simple Framework for Lexical,False,False,True
16520,"LSBert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16521,"4. https://spacy.io/Algorithm 2 LSBert ( S,t,ignore list)
",False,LSBert A Simple Framework for Lexical,False,False,True
16522,"1:complex words CWI(S,t)-ignore list
",False,LSBert A Simple Framework for Lexical,False,False,True
16523,"2:ifnumber( complex words )>0then
",False,LSBert A Simple Framework for Lexical,False,False,True
16524,"3:w head(complex words )
",False,LSBert A Simple Framework for Lexical,False,False,True
16525,"4:subs Substitution Generation( S,w)
",False,LSBert A Simple Framework for Lexical,False,False,True
16526,"5:subs Substitute Ranking( subs )
",False,LSBert A Simple Framework for Lexical,False,False,True
16527,"6:top head(subs )
",False,LSBert A Simple Framework for Lexical,False,False,True
16528,"7: iffre(top)>fre (w)orloss (top)<loss (w)then
",False,LSBert A Simple Framework for Lexical,False,False,True
16529,"8: Replace( S,w,top)
",False,LSBert A Simple Framework for Lexical,False,False,True
16530,"9: ignore list.add(w)
",False,LSBert A Simple Framework for Lexical,False,False,True
16531,"10: LSBert( S,t,ignore list)
",False,LSBert A Simple Framework for Lexical,False,False,True
16532,"11: else
",False,LSBert A Simple Framework for Lexical,False,False,True
16533,"12: LSBert( S,t,ignore list)
",False,LSBert A Simple Framework for Lexical,False,False,True
16534,"13: end if
",False,LSBert A Simple Framework for Lexical,False,False,True
16535,"14:else
",False,LSBert A Simple Framework for Lexical,False,False,True
16536,"15: return S
",False,LSBert A Simple Framework for Lexical,False,False,True
16537,"16:end if
",False,LSBert A Simple Framework for Lexical,False,False,True
16538,"Dataset . We choose the following datasets to evaluate our
",False,LSBert A Simple Framework for Lexical,False,False,True
16539,"framework LSBert from lexical simpliﬁcation datasets and text
",False,LSBert A Simple Framework for Lexical,False,False,True
16540,"simpliﬁcation dataset.
",False,LSBert A Simple Framework for Lexical,False,False,True
16541,"(1) We use three widely used lexical simpliﬁcation datasets
",False,LSBert A Simple Framework for Lexical,False,False,True
16542,"(LexMTurk5[27], BenchLS6[2], NNSeval7[22]) to do experi-
",False,LSBert A Simple Framework for Lexical,False,False,True
16543,"ments. The details of the three datasets are illustrated in this paper
",False,LSBert A Simple Framework for Lexical,False,False,True
16544,"[22]. Notice that, because these datasets already offer the target
",False,LSBert A Simple Framework for Lexical,False,False,True
16545,"words regarded complex by human annotators, we do not address
",False,LSBert A Simple Framework for Lexical,False,False,True
16546,"complex word identiﬁcation task in our evaluations using the three
",False,LSBert A Simple Framework for Lexical,False,False,True
16547,"datasets. These datasets contain instances composed of a sentence,
",False,LSBert A Simple Framework for Lexical,False,False,True
16548,"a target complex word, and a set of suitable substitutions provided
",False,LSBert A Simple Framework for Lexical,False,False,True
16549,"and ranked by humans with respect to their simplicity.
",False,LSBert A Simple Framework for Lexical,False,False,True
16550,"(2) We use one widely used text simpliﬁcation dataset (Wik-
",False,LSBert A Simple Framework for Lexical,False,False,True
16551,"iLarge) to do experiments [17]. The training/development/test set
",False,LSBert A Simple Framework for Lexical,False,False,True
16552,"in WikiLarge have 296,402/2000/359 sentence pairs, respectively.
",False,LSBert A Simple Framework for Lexical,False,False,True
16553,"WikiLarge is a set of automatically aligned complex-simple sen-
",False,LSBert A Simple Framework for Lexical,False,False,True
16554,"tence pairs from English Wikipedia (EW) and Simple English
",False,LSBert A Simple Framework for Lexical,False,False,True
16555,"Wikipedia (SEW). Its validation and test sets are taken from Turk-
",False,LSBert A Simple Framework for Lexical,False,False,True
16556,"corpus, where each original sentence has 8 human simpliﬁcations
",False,LSBert A Simple Framework for Lexical,False,False,True
16557,"created by Amazon Mechanical Turk workers.
",False,LSBert A Simple Framework for Lexical,False,False,True
16558,"4.1 Experiment Setup
",False,LSBert A Simple Framework for Lexical,False,False,True
16559,"We choose the following baselines to comparison:.
",False,LSBert A Simple Framework for Lexical,False,False,True
16560,"(1) Linguistic databases. Devlin [7] extracts synonyms of
",False,LSBert A Simple Framework for Lexical,False,False,True
16561,"complex words from WordNet. Yamamoto [38] is proposed for
",False,LSBert A Simple Framework for Lexical,False,False,True
16562,"Japanese based on dictionary deﬁnitions to extract substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16563,"candidates. Here, Yamamoto is adapted for English by using the
",False,LSBert A Simple Framework for Lexical,False,False,True
16564,"Merriam Dictionary to extract deﬁnitions of complex words.
",False,LSBert A Simple Framework for Lexical,False,False,True
16565,"(2) Parallel corpus. Biran [25] and Horn [27] perform substi-
",False,LSBert A Simple Framework for Lexical,False,False,True
16566,"tute generation (SG) through parallel corpora EW and SEW.
",False,LSBert A Simple Framework for Lexical,False,False,True
16567,"(3) Paraphrase database. SimplePPDB [8] performs SG with
",False,LSBert A Simple Framework for Lexical,False,False,True
16568,"a ﬁltered paraphrase database (PPDB).
",False,LSBert A Simple Framework for Lexical,False,False,True
16569,"(4) Word embeddings. Glava ˇs[9] performs SG with typical
",False,LSBert A Simple Framework for Lexical,False,False,True
16570,"word embeddings. Paetzold-CA [2] performs SG with context-
",False,LSBert A Simple Framework for Lexical,False,False,True
16571,"aware word embeddigns.
",False,LSBert A Simple Framework for Lexical,False,False,True
16572,"(5) Multipe source. PaetzoldNE [5] performs SG with parallel
",False,LSBert A Simple Framework for Lexical,False,False,True
16573,"corpora and context-aware word embeddigns. REC-LS [6] per-
",False,LSBert A Simple Framework for Lexical,False,False,True
16574,"forms SG with typical word embeedings and linguistic databases.
",False,LSBert A Simple Framework for Lexical,False,False,True
16575,"5. http://www.cs.pomona.edu/ dkauchak/simpliﬁcation/lex.mturk.14
",False,LSBert A Simple Framework for Lexical,False,False,True
16576,"6. http://ghpaetzold.github.io/data/BenchLS.zip
",False,LSBert A Simple Framework for Lexical,False,False,True
16577,"7. http://ghpaetzold.github.io/data/NNSeval.zipJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 6
",False,LSBert A Simple Framework for Lexical,False,False,True
16578,"LexMTurk BenchLS NNSeval
",False,LSBert A Simple Framework for Lexical,False,False,True
16579,"PRE RE F1 PRE RE F1 PRE RE F1
",False,LSBert A Simple Framework for Lexical,False,False,True
16580,"Yamamoto 0.056 0.079 0.065 0.032 0.087 0.047 0.026 0.061 0.037
",False,LSBert A Simple Framework for Lexical,False,False,True
16581,"Devlin 0.164 0.092 0.118 0.133 0.153 0.143 0.092 0.093 0.092
",False,LSBert A Simple Framework for Lexical,False,False,True
16582,"Biran 0.153 0.098 0.119 0.130 0.144 0.136 0.084 0.079 0.081
",False,LSBert A Simple Framework for Lexical,False,False,True
16583,"Horn 0.153 0.134 0.143 0.235 0.131 0.168 0.134 0.088 0.106
",False,LSBert A Simple Framework for Lexical,False,False,True
16584,"Glava ˇs 0.151 0.122 0.135 0.142 0.191 0.163 0.105 0.141 0.121
",False,LSBert A Simple Framework for Lexical,False,False,True
16585,"Paetzold-CA 0.177 0.140 0.156 0.180 0.252 0.210 0.118 0.161 0.136
",False,LSBert A Simple Framework for Lexical,False,False,True
16586,"Paetzold-NE 0.310 0.142 0.195 0.270 0.209 0.236 0.186 0.136 0.157
",False,LSBert A Simple Framework for Lexical,False,False,True
16587,"REC-LS 0.151 0.154 0.152 0.129 0.246 0.170 0.103 0.155 0.124
",False,LSBert A Simple Framework for Lexical,False,False,True
16588,"Bert-mask 0.254 0.197 0.222 0.176 0.239 0.203 0.138 0.185 0.158
",False,LSBert A Simple Framework for Lexical,False,False,True
16589,"Bert 0.256 0.199 0.224 0.210 0.285 0.242 0.154 0.205 0.176
",False,LSBert A Simple Framework for Lexical,False,False,True
16590,"Bert-dropout 0.255 0.198 0.223 0.204 0.277 0.235 0.153 0.204 0.175
",False,LSBert A Simple Framework for Lexical,False,False,True
16591,"LSBert pre 0.287 0.223 0.251 0.231 0.314 0.267 0.185 0.246 0.211
",False,LSBert A Simple Framework for Lexical,False,False,True
16592,"LSBert 0.306 0.238 0.268 0.244 0.331 0.281 0.194 0.260 0.222
",False,LSBert A Simple Framework for Lexical,False,False,True
16593,"TABLE 1
",False,LSBert A Simple Framework for Lexical,False,False,True
16594,"Evaluation results of substitute generation on three datasets.
",False,LSBert A Simple Framework for Lexical,False,False,True
16595,"(6) Methods based on Bert. Here, we give multiple strategies to
",False,LSBert A Simple Framework for Lexical,False,False,True
16596,"perform SG using Bert. Bert-mask : we directly mask the complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16597,"word of the sentence and feed it into Bert. Bert : we directly
",False,LSBert A Simple Framework for Lexical,False,False,True
16598,"feed the sentence into Bert to generate substitute generates. Bert-
",False,LSBert A Simple Framework for Lexical,False,False,True
16599,"dropout [31] applied dropout to the complex word’s embeddings
",False,LSBert A Simple Framework for Lexical,False,False,True
16600,"for partially masking the word. These Bert-based baselines are
",False,LSBert A Simple Framework for Lexical,False,False,True
16601,"based on the single sentence that uses to feed into Bert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16602,"(7) Our proposed methods. LSBert preis our previous version.
",False,LSBert A Simple Framework for Lexical,False,False,True
16603,"LSBert is the proposed method in this paper. Our two methods
",False,LSBert A Simple Framework for Lexical,False,False,True
16604,"LSBert preand LSBert feed two sentences for Bert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16605,"The experimental results of Devlin, Yamamoto, Biran, Horn,
",False,LSBert A Simple Framework for Lexical,False,False,True
16606,"and SimplePPDB, Glava ˇs, Paetzold-CA, and Paetzold-NE are
",False,LSBert A Simple Framework for Lexical,False,False,True
16607,"from these two papers [5], [22]. For REC-LS method, we use the
",False,LSBert A Simple Framework for Lexical,False,False,True
16608,"code proposed by the authors. Bert-dropout was re-implemented
",False,LSBert A Simple Framework for Lexical,False,False,True
16609,"based on the original paper. In all experiments for methods based
",False,LSBert A Simple Framework for Lexical,False,False,True
16610,"on Bert, we use BERT-Large, Uncased (Whole Word Masking)
",False,LSBert A Simple Framework for Lexical,False,False,True
16611,"pre-trained on BooksCorpus and English Wikipedia8.
",False,LSBert A Simple Framework for Lexical,False,False,True
16612,"4.2 Quantitative Evaluation
",False,LSBert A Simple Framework for Lexical,False,False,True
16613,"(1) Evaluation of Substitute Candidates
",False,LSBert A Simple Framework for Lexical,False,False,True
16614,"The following three widely used metrics are used for evalua-
",False,LSBert A Simple Framework for Lexical,False,False,True
16615,"tion [2], [22], [39].
",False,LSBert A Simple Framework for Lexical,False,False,True
16616,"Precision (PRE) : The proportion of generated candidates that
",False,LSBert A Simple Framework for Lexical,False,False,True
16617,"are in the gold standard.
",False,LSBert A Simple Framework for Lexical,False,False,True
16618,"Recall (RE) : The proportion of gold-standard substitutions
",False,LSBert A Simple Framework for Lexical,False,False,True
16619,"that are included in the generated substitutions.
",False,LSBert A Simple Framework for Lexical,False,False,True
16620,"F1: The harmonic mean between Precision and Recall.
",False,LSBert A Simple Framework for Lexical,False,False,True
16621,"The results are shown in Table 1. As can be seen, our model
",False,LSBert A Simple Framework for Lexical,False,False,True
16622,"LSBert obtains the highest Recall and F1 scores on three datasets,
",False,LSBert A Simple Framework for Lexical,False,False,True
16623,"largely outperforming the previous best baseline Paetzold-NE,
",False,LSBert A Simple Framework for Lexical,False,False,True
16624,"increasing 37.4%, 19.1% and 41.4% using F1 metric. The base-
",False,LSBert A Simple Framework for Lexical,False,False,True
16625,"line Paetzold-NE by combining the Newsela parallel corpus and
",False,LSBert A Simple Framework for Lexical,False,False,True
16626,"context-aware word embeddings obtains better results on PRE
",False,LSBert A Simple Framework for Lexical,False,False,True
16627,"than LSBert, because it uses a different calculation method. If
",False,LSBert A Simple Framework for Lexical,False,False,True
16628,"one candidate exists in the gold standard, different morphological
",False,LSBert A Simple Framework for Lexical,False,False,True
16629,"derivations of the candidate in substitute candidates are all counted
",False,LSBert A Simple Framework for Lexical,False,False,True
16630,"into the PRE metric. Because of considering the context, the
",False,LSBert A Simple Framework for Lexical,False,False,True
16631,"substitute candidates of Bert based methods are normally different
",False,LSBert A Simple Framework for Lexical,False,False,True
16632,"words.
",False,LSBert A Simple Framework for Lexical,False,False,True
16633,"We note that the Bert based model is not only able to out-
",False,LSBert A Simple Framework for Lexical,False,False,True
16634,"perform other systems on all datasets using F1, but it also has
",False,LSBert A Simple Framework for Lexical,False,False,True
16635,"two clear practical advantages: (1) the only input information it
",False,LSBert A Simple Framework for Lexical,False,False,True
16636,"uses at run time is Bert without requiring linguistic database and
",False,LSBert A Simple Framework for Lexical,False,False,True
16637,"8. https://github.com/google-research/bertLexMTurk BenchLS NNSeval
",False,LSBert A Simple Framework for Lexical,False,False,True
16638,"PRE ACC PRE ACC PRE ACC
",False,LSBert A Simple Framework for Lexical,False,False,True
16639,"Yamamoto 0.066 0.066 0.044 0.041 0.444 0.025
",False,LSBert A Simple Framework for Lexical,False,False,True
16640,"Biran 0.714 0.034 0.124 0.123 0.121 0.121
",False,LSBert A Simple Framework for Lexical,False,False,True
16641,"Devlin 0.368 0.366 0.309 0.307 0.335 0.117
",False,LSBert A Simple Framework for Lexical,False,False,True
16642,"PaetzoldCA 0.578 0.396 0.423 0.423 0.297 0.297
",False,LSBert A Simple Framework for Lexical,False,False,True
16643,"Horn 0.761 0.663 0.546 0.341 0.364 0.172
",False,LSBert A Simple Framework for Lexical,False,False,True
16644,"Glava ˇs 0.710 0.682 0.480 0.252 0.456 0.197
",False,LSBert A Simple Framework for Lexical,False,False,True
16645,"PaetzoldNE 0.676 0.676 0.642 0.434 0.544 0.335
",False,LSBert A Simple Framework for Lexical,False,False,True
16646,"REC-LS 0.786 0.256 0.734 0.335 0.665 0.218
",False,LSBert A Simple Framework for Lexical,False,False,True
16647,"LSBert pre 0.770 0.770 0.604 0.604 0.420 0.420
",False,LSBert A Simple Framework for Lexical,False,False,True
16648,"LSBert 0.864 0.792 0.697 0.616 0.526 0.436
",False,LSBert A Simple Framework for Lexical,False,False,True
16649,"TABLE 2
",False,LSBert A Simple Framework for Lexical,False,False,True
16650,"The evaluation results using Precision (PRE) and Accuracy (ACC) on
",False,LSBert A Simple Framework for Lexical,False,False,True
16651,"three datasets.
",False,LSBert A Simple Framework for Lexical,False,False,True
16652,"comparable corpus, (2) the substitute candidates using Bert do not
",False,LSBert A Simple Framework for Lexical,False,False,True
16653,"require morphological transformation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16654,"For these baselines based on a single sentence (Bert-mask, Bert
",False,LSBert A Simple Framework for Lexical,False,False,True
16655,"and Bert-dropout), the gap between them is very small. Compared
",False,LSBert A Simple Framework for Lexical,False,False,True
16656,"with Bert based on a single sentence, our method LSBert preand
",False,LSBert A Simple Framework for Lexical,False,False,True
16657,"LSBert have better results, which verify that our strategy based
",False,LSBert A Simple Framework for Lexical,False,False,True
16658,"on sentence pairs ﬁts for lexical simpliﬁcation. In conclusion, the
",False,LSBert A Simple Framework for Lexical,False,False,True
16659,"results clearly show that LSBert provides a good balance precision
",False,LSBert A Simple Framework for Lexical,False,False,True
16660,"and recall using only Bert.
",False,LSBert A Simple Framework for Lexical,False,False,True
16661,"(2) Evaluation of SG and SR
",False,LSBert A Simple Framework for Lexical,False,False,True
16662,"In this section, we evaluate the performance of various LS
",False,LSBert A Simple Framework for Lexical,False,False,True
16663,"systems that combines SG and SR. We adopt the following two
",False,LSBert A Simple Framework for Lexical,False,False,True
16664,"well-known metrics used by these work [22], [27].
",False,LSBert A Simple Framework for Lexical,False,False,True
16665,"Precision (PRE) : The proportion with which the replacement
",False,LSBert A Simple Framework for Lexical,False,False,True
16666,"of the original word is either the original word itself or is in the
",False,LSBert A Simple Framework for Lexical,False,False,True
16667,"gold standard.
",False,LSBert A Simple Framework for Lexical,False,False,True
16668,"Accuracy (ACC) : The proportion with which the replacement
",False,LSBert A Simple Framework for Lexical,False,False,True
16669,"of the original word is not the original word and is in the gold
",False,LSBert A Simple Framework for Lexical,False,False,True
16670,"standard.
",False,LSBert A Simple Framework for Lexical,False,False,True
16671,"It can be seen from these two metrices that if no simpliﬁcation
",False,LSBert A Simple Framework for Lexical,False,False,True
16672,"is carried out, the PRE value is 1 and the ACC value is 0. If all
",False,LSBert A Simple Framework for Lexical,False,False,True
16673,"complex words are replaced by the substitutions, the PRE and
",False,LSBert A Simple Framework for Lexical,False,False,True
16674,"ACC vaule have the same value.
",False,LSBert A Simple Framework for Lexical,False,False,True
16675,"The results are shown in Table 2. We can see that our method
",False,LSBert A Simple Framework for Lexical,False,False,True
16676,"LSBert attains the highest Accuracy on three datasets, which
",False,LSBert A Simple Framework for Lexical,False,False,True
16677,"has an average increase of 29.8% over the former state-of-the-
",False,LSBert A Simple Framework for Lexical,False,False,True
16678,"art baseline (Paetzold-NE). It suggests that LSBert is the most
",False,LSBert A Simple Framework for Lexical,False,False,True
16679,"proﬁcient in promoting simplicity. Paetzold-NE obtains higher
",False,LSBert A Simple Framework for Lexical,False,False,True
16680,"than LSBert on Precision on NNSeval, which also means that
",False,LSBert A Simple Framework for Lexical,False,False,True
16681,"many complex words are replaced by the original word itself, due
",False,LSBert A Simple Framework for Lexical,False,False,True
16682,"to the shortage of simpliﬁcation rules in parallel corpora. REC-LSJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 7
",False,LSBert A Simple Framework for Lexical,False,False,True
16683,"Methods SARI FRES
",False,LSBert A Simple Framework for Lexical,False,False,True
16684,"TS methodsDRESS-LS (2017) 37.27 75.33
",False,LSBert A Simple Framework for Lexical,False,False,True
16685,"EditNTS (2019) 38.22 73.81
",False,LSBert A Simple Framework for Lexical,False,False,True
16686,"PBMT (2020) 39.08 76.50
",False,LSBert A Simple Framework for Lexical,False,False,True
16687,"Access (2019) 41.87 81.55
",False,LSBert A Simple Framework for Lexical,False,False,True
16688,"LS methodsGlava ˇs 30.70 81.82
",False,LSBert A Simple Framework for Lexical,False,False,True
16689,"REC-LS 37.11 69.58
",False,LSBert A Simple Framework for Lexical,False,False,True
16690,"LSBert 39.37 77.07
",False,LSBert A Simple Framework for Lexical,False,False,True
16691,"TABLE 3
",False,LSBert A Simple Framework for Lexical,False,False,True
16692,"Comparison of text simpliﬁcation methods on WikiLarge dataset.
",False,LSBert A Simple Framework for Lexical,False,False,True
16693,"obtains the best PRE and poor ACC, because it prefers the original
",False,LSBert A Simple Framework for Lexical,False,False,True
16694,"word as the substitute word.
",False,LSBert A Simple Framework for Lexical,False,False,True
16695,"In conclusion, although LSBert only uses raw text for pre-
",False,LSBert A Simple Framework for Lexical,False,False,True
16696,"trained Bert without using any resources, LSBert remains the best
",False,LSBert A Simple Framework for Lexical,False,False,True
16697,"lexical simpliﬁcation method. The results are in accordance with
",False,LSBert A Simple Framework for Lexical,False,False,True
16698,"the conclusions of the Substitute Generation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16699,"(3)Evaluation of LS system for sentence simpliﬁcation
",False,LSBert A Simple Framework for Lexical,False,False,True
16700,"Lexical simpliﬁcation evaluation needs to be provided with
",False,LSBert A Simple Framework for Lexical,False,False,True
16701,"the sentence and the speciﬁed complex word. Here, we try to
",False,LSBert A Simple Framework for Lexical,False,False,True
16702,"simplify one sentence instead of one word, and choose a sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16703,"simpliﬁcation dataset (WikiLarge) for evaluation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16704,"Since most LS methods only focused on one or two steps (SR
",False,LSBert A Simple Framework for Lexical,False,False,True
16705,"or SR) of LS, they cannot directly simplify one setence. Here,
",False,LSBert A Simple Framework for Lexical,False,False,True
16706,"we choose two complete LS systems (Glava ˇs [9] and REC-LS
",False,LSBert A Simple Framework for Lexical,False,False,True
16707,"[6] ) to comparison. In additional, we choose four state-of-the-art
",False,LSBert A Simple Framework for Lexical,False,False,True
16708,"text simpliﬁcation (TS) methods DRESS-LS [17], EditNTS [15],
",False,LSBert A Simple Framework for Lexical,False,False,True
16709,"PBMT [40], and Access [41]. The ﬁrst three TS methods except
",False,LSBert A Simple Framework for Lexical,False,False,True
16710,"PBMT are sequence-to-sequence modelings and all need training
",False,LSBert A Simple Framework for Lexical,False,False,True
16711,"data sets to learn. PBMT is an unsupervised text simpliﬁcation
",False,LSBert A Simple Framework for Lexical,False,False,True
16712,"system based on phrase-based machine translation system. For
",False,LSBert A Simple Framework for Lexical,False,False,True
16713,"LS methods, they only use the testset to output the simpliﬁed
",False,LSBert A Simple Framework for Lexical,False,False,True
16714,"sentences. For LSBert and Rec-LS, the complexity threshold of
",False,LSBert A Simple Framework for Lexical,False,False,True
16715,"CWI is 0.5. For Glava ˇs method, it tries to simplify all content
",False,LSBert A Simple Framework for Lexical,False,False,True
16716,"words (noun, verb, adjective, or adverb) of one sentence.
",False,LSBert A Simple Framework for Lexical,False,False,True
16717,"Following previous work, two widely used metrics (SARI and
",False,LSBert A Simple Framework for Lexical,False,False,True
16718,"FRES) in text simpliﬁcation are chosen in this paper [42], [43].
",False,LSBert A Simple Framework for Lexical,False,False,True
16719,"SARI [17] is a text-simpliﬁcation metric by comparing the output
",False,LSBert A Simple Framework for Lexical,False,False,True
16720,"against the simple and complex simpliﬁcations9. Flesch reading
",False,LSBert A Simple Framework for Lexical,False,False,True
16721,"ease score (FRES) measures the readability of the output [44]. A
",False,LSBert A Simple Framework for Lexical,False,False,True
16722,"higher FRES represents simpler output.
",False,LSBert A Simple Framework for Lexical,False,False,True
16723,"Table 3 shows the results of all models on WikiLarge dataset.
",False,LSBert A Simple Framework for Lexical,False,False,True
16724,"Our model LSBert obtains a SARI score of 39.37 and a FRES
",False,LSBert A Simple Framework for Lexical,False,False,True
16725,"score of 77.07, even outperforming these three supervised TS
",False,LSBert A Simple Framework for Lexical,False,False,True
16726,"systems (DRESS-LS, EditNTS and PBMT), which indicates that
",False,LSBert A Simple Framework for Lexical,False,False,True
16727,"the model has indeed learned to simplify the complex sentences.
",False,LSBert A Simple Framework for Lexical,False,False,True
16728,"Compared with LS methods Glava ˇs and REC-LS, LSBert achieves
",False,LSBert A Simple Framework for Lexical,False,False,True
16729,"the best results. The two methods go to two different extremes, in
",False,LSBert A Simple Framework for Lexical,False,False,True
16730,"which Glava ˇs simpliﬁes almost all content words of one sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16731,"and REC-LS prefers to save the original word. On the FRES
",False,LSBert A Simple Framework for Lexical,False,False,True
16732,"metric, we can see that Glava ˇs outperforms LSBert, which is also
",False,LSBert A Simple Framework for Lexical,False,False,True
16733,"because it simpliﬁes almost all content words without caring for
",False,LSBert A Simple Framework for Lexical,False,False,True
16734,"the equivalent meaning with the original sentence. Compared with
",False,LSBert A Simple Framework for Lexical,False,False,True
16735,"Access, our model is highly competitive, because LSBert does not
",False,LSBert A Simple Framework for Lexical,False,False,True
16736,"need a parallel dataset to learn and only focuses on simplifying
",False,LSBert A Simple Framework for Lexical,False,False,True
16737,"the words. In conclusion, we can see that LSBert outperforms
",False,LSBert A Simple Framework for Lexical,False,False,True
16738,"previous LS baselines, even some supervised TS baselines, which
",False,LSBert A Simple Framework for Lexical,False,False,True
16739,"indicate that our method is effective at creating simpler output.
",False,LSBert A Simple Framework for Lexical,False,False,True
16740,"9. We used the implementation of SARI in [43].LexMTurk BenchLS NNSeval
",False,LSBert A Simple Framework for Lexical,False,False,True
16741,"PRE ACC PRE ACC PRE ACC
",False,LSBert A Simple Framework for Lexical,False,False,True
16742,"LSBert 0.864 0.792 0.697 0.616 0.526 0.436
",False,LSBert A Simple Framework for Lexical,False,False,True
16743,"w/o Bert 0.828 0.774 0.680 0.629 0.456 0.406
",False,LSBert A Simple Framework for Lexical,False,False,True
16744,"w/o Language 0.828 0.744 0.670 0.610 0.527 0.418
",False,LSBert A Simple Framework for Lexical,False,False,True
16745,"w/o Similarity 0.820 0.768 0.659 0.607 0.452 0.397
",False,LSBert A Simple Framework for Lexical,False,False,True
16746,"w/o Frequency 0.842 0.694 0.713 0.554 0.556 0.393
",False,LSBert A Simple Framework for Lexical,False,False,True
16747,"w/o PPDB 0.852 0.784 0.698 0.622 0.502 0.422
",False,LSBert A Simple Framework for Lexical,False,False,True
16748,"TABLE 4
",False,LSBert A Simple Framework for Lexical,False,False,True
16749,"Ablation study results of the ranking features.
",False,LSBert A Simple Framework for Lexical,False,False,True
16750,"SG SR
",False,LSBert A Simple Framework for Lexical,False,False,True
16751,"PRE RE F1 PRE ACC
",False,LSBert A Simple Framework for Lexical,False,False,True
16752,"LexMTurkBase 0.317 0.246 0.277 0.744 0.704
",False,LSBert A Simple Framework for Lexical,False,False,True
16753,"Large 0.333 0.259 0.291 0.792 0.750
",False,LSBert A Simple Framework for Lexical,False,False,True
16754,"WWM 0.306 0.238 0.268 0.864 0.792
",False,LSBert A Simple Framework for Lexical,False,False,True
16755,"BenchLSBase 0.233 0.317 0.269 0.586 0.537
",False,LSBert A Simple Framework for Lexical,False,False,True
16756,"Large 0.252 0.342 0.290 0.636 0.589
",False,LSBert A Simple Framework for Lexical,False,False,True
16757,"WWM 0.244 0.331 0.281 0.697 0.616
",False,LSBert A Simple Framework for Lexical,False,False,True
16758,"NNSevalBase 0.172 0.230 0.197 0.393 0.347
",False,LSBert A Simple Framework for Lexical,False,False,True
16759,"Large 0.185 0.247 0.211 0.402 0.360
",False,LSBert A Simple Framework for Lexical,False,False,True
16760,"WWM 0.194 0.260 0.222 0.526 0.436
",False,LSBert A Simple Framework for Lexical,False,False,True
16761,"TABLE 5
",False,LSBert A Simple Framework for Lexical,False,False,True
16762,"Inﬂuence of different Bert models.
",False,LSBert A Simple Framework for Lexical,False,False,True
16763,"4.3 Ablation Study of LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16764,"To further analyze the advantages and the disadvantages of
",False,LSBert A Simple Framework for Lexical,False,False,True
16765,"LSBert, we do more experiments in this section.
",False,LSBert A Simple Framework for Lexical,False,False,True
16766,"(1) Inﬂuence of Ranking Features
",False,LSBert A Simple Framework for Lexical,False,False,True
16767,"To determine the importance of each ranking feature, we make
",False,LSBert A Simple Framework for Lexical,False,False,True
16768,"an ablation study by removing one feature in turn. The results are
",False,LSBert A Simple Framework for Lexical,False,False,True
16769,"presented in Table 4. We can see that LSBert combining all ﬁve
",False,LSBert A Simple Framework for Lexical,False,False,True
16770,"features achieves the best results, which means all features have a
",False,LSBert A Simple Framework for Lexical,False,False,True
16771,"positive effect. LSBert removing frequency feature achieves better
",False,LSBert A Simple Framework for Lexical,False,False,True
16772,"results on PRE metric, but it decreases the values of ACC. These
",False,LSBert A Simple Framework for Lexical,False,False,True
16773,"features have different contributions for LSBert’s performance, for
",False,LSBert A Simple Framework for Lexical,False,False,True
16774,"example, PPDB feature brings the least impact on the performance
",False,LSBert A Simple Framework for Lexical,False,False,True
16775,"of LSBert compared with the other features. In this paper, LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16776,"thinks all features are equally important, that may not be the best
",False,LSBert A Simple Framework for Lexical,False,False,True
16777,"option. In the future, we can improve LSBert by combining these
",False,LSBert A Simple Framework for Lexical,False,False,True
16778,"features using different weights.
",False,LSBert A Simple Framework for Lexical,False,False,True
16779,"(2) Inﬂuence of Different Bert Modeling for Substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16780,"Generation
",False,LSBert A Simple Framework for Lexical,False,False,True
16781,"Pretrained Bert plays one vital role in LSBert. Bert has differ-
",False,LSBert A Simple Framework for Lexical,False,False,True
16782,"ent versions based on the parameter scale and training strategy.
",False,LSBert A Simple Framework for Lexical,False,False,True
16783,"Here, we attempt to investigate the inﬂuence of different Bert
",False,LSBert A Simple Framework for Lexical,False,False,True
16784,"versions on the performance of LSBert. We choose the following
",False,LSBert A Simple Framework for Lexical,False,False,True
16785,"three Bert models:
",False,LSBert A Simple Framework for Lexical,False,False,True
16786,"(1) Bert-based, uncased (Base): 12-layer, 768-hidden, 12-
",False,LSBert A Simple Framework for Lexical,False,False,True
16787,"heads, 110M parameters.
",False,LSBert A Simple Framework for Lexical,False,False,True
16788,"(2) Bert-large, uncased (Large): 24-layer, 1024-hidden, 16-
",False,LSBert A Simple Framework for Lexical,False,False,True
16789,"heads, 340M parameters.
",False,LSBert A Simple Framework for Lexical,False,False,True
16790,"(3) Bert-large, uncased, Whole Word Masking (WWM): 24-
",False,LSBert A Simple Framework for Lexical,False,False,True
16791,"layer, 1024-hidden, 16-heads, 340M parameters. The above two
",False,LSBert A Simple Framework for Lexical,False,False,True
16792,"Bert models randomly select WordPiece tokens to mask. Whole
",False,LSBert A Simple Framework for Lexical,False,False,True
16793,"Word Masking always masks all of the tokens corresponding to a
",False,LSBert A Simple Framework for Lexical,False,False,True
16794,"word at once.
",False,LSBert A Simple Framework for Lexical,False,False,True
16795,"Table 5 shows the results of the experiments using different
",False,LSBert A Simple Framework for Lexical,False,False,True
16796,"Bert models on three datasets. From Table 5, we can see that the
",False,LSBert A Simple Framework for Lexical,False,False,True
16797,"WWM model obtains the highest accuracy and precision over the
",False,LSBert A Simple Framework for Lexical,False,False,True
16798,"two other models. Besides, the Large model outperforms the Base
",False,LSBert A Simple Framework for Lexical,False,False,True
16799,"model. It can be concluded that a better Bert model can help to
",False,LSBert A Simple Framework for Lexical,False,False,True
16800,"improve the performance of LSBert system. If in the future a betterJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 8
",False,LSBert A Simple Framework for Lexical,False,False,True
16801,"Bert model is available, one can try to replace the Bert model in
",False,LSBert A Simple Framework for Lexical,False,False,True
16802,"this paper to further improve the performance of LS system.
",False,LSBert A Simple Framework for Lexical,False,False,True
16803,"(3) Inﬂuence of the Number of Substitute Candidates
",False,LSBert A Simple Framework for Lexical,False,False,True
16804,"In this part, we try to investigate the inﬂuence of the number
",False,LSBert A Simple Framework for Lexical,False,False,True
16805,"of simpliﬁcation candidates to the performance of LSBert. The
",False,LSBert A Simple Framework for Lexical,False,False,True
16806,"number of candidates ranges from 5 to 60, respectively. Figure 4
",False,LSBert A Simple Framework for Lexical,False,False,True
16807,"shows the performance of substitute candidates (Precision, Recall
",False,LSBert A Simple Framework for Lexical,False,False,True
16808,"and F1), SR and SR (Precision, Accuracy) varying the number of
",False,LSBert A Simple Framework for Lexical,False,False,True
16809,"candidates on three benchmarks. When increasing the number of
",False,LSBert A Simple Framework for Lexical,False,False,True
16810,"candidates, the score of precision decreases and the score of recall
",False,LSBert A Simple Framework for Lexical,False,False,True
16811,"increases. When increasing the number of candidates, the score of
",False,LSBert A Simple Framework for Lexical,False,False,True
16812,"F1 ﬁrst increases, and declines ﬁnally. The best performance of
",False,LSBert A Simple Framework for Lexical,False,False,True
16813,"LSBert through the experiments is achieved by setting the number
",False,LSBert A Simple Framework for Lexical,False,False,True
16814,"of candidates equals 10 for a good trade-off between precision and
",False,LSBert A Simple Framework for Lexical,False,False,True
16815,"recall. The score of the accuracy and precison of the SG and SR
",False,LSBert A Simple Framework for Lexical,False,False,True
16816,"(full) ﬁrst increase and converge ﬁnally, which means that the SG
",False,LSBert A Simple Framework for Lexical,False,False,True
16817,"and SR is less sensitive to the number of candidates.
",False,LSBert A Simple Framework for Lexical,False,False,True
16818,"4.4 Qualitative Study
",False,LSBert A Simple Framework for Lexical,False,False,True
16819,"All of the above experiments are quantitative analyses of
",False,LSBert A Simple Framework for Lexical,False,False,True
16820,"LSBert. Here, we also qualitatively evaluate our model from three
",False,LSBert A Simple Framework for Lexical,False,False,True
16821,"aspects: substitute generation, substitute ranking and sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16822,"simpliﬁcation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16823,"(1) The analysis of substitute generation results
",False,LSBert A Simple Framework for Lexical,False,False,True
16824,"When the number of substitute candidates is set to 10, the
",False,LSBert A Simple Framework for Lexical,False,False,True
16825,"proportion of LSBert that generates at least one valid substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16826,"candidate is 98.6% on Lexmturk dataset, namely, LSBert only
",False,LSBert A Simple Framework for Lexical,False,False,True
16827,"produces no effective substitute word in only 8 sentences. When
",False,LSBert A Simple Framework for Lexical,False,False,True
16828,"the number of generated candidates is 15, LSBert cannot generate
",False,LSBert A Simple Framework for Lexical,False,False,True
16829,"any valid candidates on only 4 sentences. When the number of
",False,LSBert A Simple Framework for Lexical,False,False,True
16830,"generated candidates is 30, only one sentence cannot be generated
",False,LSBert A Simple Framework for Lexical,False,False,True
16831,"valid candidate by LSBert. In this section, we will analyze the 8
",False,LSBert A Simple Framework for Lexical,False,False,True
16832,"sentences on Table 6.
",False,LSBert A Simple Framework for Lexical,False,False,True
16833,"We can see that LSBert can generate one or two valid substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16834,"candidates on these sentences (sent4, sent5, sent7 and sent8),
",False,LSBert A Simple Framework for Lexical,False,False,True
16835,"e.g, ”senior- >powerful”, ”full-ﬂedged- >development”, ”kinetic-
",False,LSBert A Simple Framework for Lexical,False,False,True
16836,">dynamic”, and ”edited- >altered”. Since the labels are provided
",False,LSBert A Simple Framework for Lexical,False,False,True
16837,"by humans, it is impossible to provide all suitable substitutes
",False,LSBert A Simple Framework for Lexical,False,False,True
16838,"in labels. LSBert fail to produce any valid candidate word on
",False,LSBert A Simple Framework for Lexical,False,False,True
16839,"the other sentences. When we analyze these wrong substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16840,"candidates, we can ﬁnd that they can ﬁt the context. We can guess
",False,LSBert A Simple Framework for Lexical,False,False,True
16841,"that LSBert mainly focuses more on the context and ignores the
",False,LSBert A Simple Framework for Lexical,False,False,True
16842,"meaning of the original word on these wrong examples.
",False,LSBert A Simple Framework for Lexical,False,False,True
16843,"(2) The analysis of substitute ranking results
",False,LSBert A Simple Framework for Lexical,False,False,True
16844,"LSBert can ﬁnd one or more suitable alternatives for almost all
",False,LSBert A Simple Framework for Lexical,False,False,True
16845,"samples, but the ﬁnal system results do not always select the most
",False,LSBert A Simple Framework for Lexical,False,False,True
16846,"suitable candidate as the ﬁnal substitute. In this section, we will
",False,LSBert A Simple Framework for Lexical,False,False,True
16847,"analyze the possible reasons for this question. In Table 7, we give
",False,LSBert A Simple Framework for Lexical,False,False,True
16848,"some examples that LSBert cannot produce the right substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16849,"ranking.
",False,LSBert A Simple Framework for Lexical,False,False,True
16850,"From sent1 and sent3, we can see that the substitute ranking
",False,LSBert A Simple Framework for Lexical,False,False,True
16851,"(SR) chooses the best substitute, but LSBert still chooses the
",False,LSBert A Simple Framework for Lexical,False,False,True
16852,"original word. This is because the Zipf value of ”divided” is
",False,LSBert A Simple Framework for Lexical,False,False,True
16853,"3.65 and the Zipf value of ”classiﬁed” is 3.83, LSBert considers
",False,LSBert A Simple Framework for Lexical,False,False,True
16854,"”classiﬁed” to be simpler than ”divided”. It is the same reason for
",False,LSBert A Simple Framework for Lexical,False,False,True
16855,"sent3 in which the Zipf value of ”noted” is 3.68 and the Zipf value
",False,LSBert A Simple Framework for Lexical,False,False,True
16856,"of ”reported” is 4.18. Consequently, in sent1 and sent3, the best
",False,LSBert A Simple Framework for Lexical,False,False,True
16857,"substitutes of SR cannot be used as the ﬁnal substitutes.
",False,LSBert A Simple Framework for Lexical,False,False,True
16858,"The second case is that the best substitution of the SR step
",False,LSBert A Simple Framework for Lexical,False,False,True
16859,"is not from the labels provided by humans. In sent2 and sent4,LSBert chooses ”maintained” as a simpler for ”retained” and
",False,LSBert A Simple Framework for Lexical,False,False,True
16860,"”never” as a simpler for ”rarely”. We can ﬁnd that these words
",False,LSBert A Simple Framework for Lexical,False,False,True
16861,"”maintained” and ”never” are also suitable substitutes, but do not
",False,LSBert A Simple Framework for Lexical,False,False,True
16862,"appear in the labels.
",False,LSBert A Simple Framework for Lexical,False,False,True
16863,"(3) The analysis of sentence simpliﬁcation results
",False,LSBert A Simple Framework for Lexical,False,False,True
16864,"The above qualitative study for LSBert need to provide the
",False,LSBert A Simple Framework for Lexical,False,False,True
16865,"complex word by humans. In this experiment, we try to verify
",False,LSBert A Simple Framework for Lexical,False,False,True
16866,"the results of LS methods on sentence simpliﬁcation. We also
",False,LSBert A Simple Framework for Lexical,False,False,True
16867,"choose the two methods Glava ˇs and REC-LS to comparison.
",False,LSBert A Simple Framework for Lexical,False,False,True
16868,"Table 8 shows some examples from the WikiLarge dataset to be
",False,LSBert A Simple Framework for Lexical,False,False,True
16869,"simpliﬁed. We note that we draw the same conclusions from these
",False,LSBert A Simple Framework for Lexical,False,False,True
16870,"examples with LS system for sentence simpliﬁcation. Glava ˇs tries
",False,LSBert A Simple Framework for Lexical,False,False,True
16871,"to simplify every content word in the sentence ignoring the aim
",False,LSBert A Simple Framework for Lexical,False,False,True
16872,"of LS. LS aims to replace complex words in a given sentence
",False,LSBert A Simple Framework for Lexical,False,False,True
16873,"with simpler alternatives of equivalent meaning. Rec-LS can make
",False,LSBert A Simple Framework for Lexical,False,False,True
16874,"the right simpliﬁcations, e.g., sentence 2. But, for sentence 1 and
",False,LSBert A Simple Framework for Lexical,False,False,True
16875,"sentence 3, Rec-LS outputs the original sentence. LSBert replaces
",False,LSBert A Simple Framework for Lexical,False,False,True
16876,"complex words with simpler alternatives and makes the most
",False,LSBert A Simple Framework for Lexical,False,False,True
16877,"reasonable simpliﬁcation. This veriﬁes that our framework LSBert
",False,LSBert A Simple Framework for Lexical,False,False,True
16878,"ﬁts for lexical simpliﬁcation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16879,"5 C ONCLUSION
",False,LSBert A Simple Framework for Lexical,False,False,True
16880,"We propose a simple BERT-based framework LSBert for
",False,LSBert A Simple Framework for Lexical,False,False,True
16881,"lexical simpliﬁcation (LS) by leveraging the idea of masking
",False,LSBert A Simple Framework for Lexical,False,False,True
16882,"language model of Bert. The existing LS methods only consider
",False,LSBert A Simple Framework for Lexical,False,False,True
16883,"the context of the complex word on the last step (substitute
",False,LSBert A Simple Framework for Lexical,False,False,True
16884,"ranking) of LS. LSBert focuses on the context of the complex
",False,LSBert A Simple Framework for Lexical,False,False,True
16885,"word on all steps of lexical simpliﬁcation without relying on the
",False,LSBert A Simple Framework for Lexical,False,False,True
16886,"parallel corpus or linguistic databases. Experiment results have
",False,LSBert A Simple Framework for Lexical,False,False,True
16887,"shown that our approach LSBert achieves the best performance
",False,LSBert A Simple Framework for Lexical,False,False,True
16888,"on three well-known benchmarks. Since Bert can be trained in
",False,LSBert A Simple Framework for Lexical,False,False,True
16889,"raw text, our method can be applied to many languages for
",False,LSBert A Simple Framework for Lexical,False,False,True
16890,"lexical simpliﬁcation. One limitation of our method is that it only
",False,LSBert A Simple Framework for Lexical,False,False,True
16891,"generates a single-word replacement for the complex word, but
",False,LSBert A Simple Framework for Lexical,False,False,True
16892,"we plan to extend it to support multi-word expressions. In the
",False,LSBert A Simple Framework for Lexical,False,False,True
16893,"future, the pretrained Bert model can be ﬁne-tuned with just simple
",False,LSBert A Simple Framework for Lexical,False,False,True
16894,"English corpus (e.g., Newsela), and then we will use ﬁne-tuned
",False,LSBert A Simple Framework for Lexical,False,False,True
16895,"Bert for lexical simpliﬁcation.
",False,LSBert A Simple Framework for Lexical,False,False,True
16896,"ACKNOWLEDGEMENT
",False,LSBert A Simple Framework for Lexical,False,False,True
16897,"This research is partially supported by the National Natu-
",False,LSBert A Simple Framework for Lexical,False,False,True
16898,"ral Science Foundation of China under grants 61703362 and
",False,LSBert A Simple Framework for Lexical,False,False,True
16899,"91746209; the National Key Research and Development Pro-
",False,LSBert A Simple Framework for Lexical,False,False,True
16900,"gram of China under grant 2016YFB1000900; the Program for
",False,LSBert A Simple Framework for Lexical,False,False,True
16901,"Changjiang Scholars and Innovative Research Team in Uni-
",False,LSBert A Simple Framework for Lexical,False,False,True
16902,"versity (PCSIRT) of the Ministry of Education, China, under
",False,LSBert A Simple Framework for Lexical,False,False,True
16903,"grant IRT17R32; and the Natural Science Foundation of Jiangsu
",False,LSBert A Simple Framework for Lexical,False,False,True
16904,"Province of China under grant BK20170513. This manuscript
",False,LSBert A Simple Framework for Lexical,False,False,True
16905,"is an extended version of the conference paper, titled Lexical
",False,LSBert A Simple Framework for Lexical,False,False,True
16906,"Simpliﬁcation with Pretrained Encoders, published in the Thirty-
",False,LSBert A Simple Framework for Lexical,False,False,True
16907,"Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI), New
",False,LSBert A Simple Framework for Lexical,False,False,True
16908,"York, February 7-12, 2020.
",False,LSBert A Simple Framework for Lexical,False,False,True
16909,"REFERENCES
",False,LSBert A Simple Framework for Lexical,False,False,True
16910,"[1] J. De Belder, M.-F. Moens, Text simpliﬁcation for children, In Proceed-
",False,LSBert A Simple Framework for Lexical,False,False,True
16911,"ings of the 2010 SIGIR Workshop on Accessible Search Systems (2010)
",False,LSBert A Simple Framework for Lexical,False,False,True
16912,"19–26.
",False,LSBert A Simple Framework for Lexical,False,False,True
16913,"[2] G. H. Paetzold, L. Specia, Unsupervised lexical simpliﬁcation for non-
",False,LSBert A Simple Framework for Lexical,False,False,True
16914,"native speakers., in: AAAI, 2016, pp. 3761–3767.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 9
",False,LSBert A Simple Framework for Lexical,False,False,True
16915,"Fig. 4. Inﬂuence of number of substitute candidates.
",False,LSBert A Simple Framework for Lexical,False,False,True
16916,"Sent1 Much of the water carried by these streams is diverted .
",False,LSBert A Simple Framework for Lexical,False,False,True
16917,"Labels drawn away, redirected, changed, turned, moved, rerouted, led away, sent away, separated, switched, split, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16918,"LSBert reclaimed, displaced, transferred, derived, pumped, routed, converted, recycled, discarded, drained
",False,LSBert A Simple Framework for Lexical,False,False,True
16919,"Sent2 ... , every person born into the world is enslaved to the service of sin and , apart from the efﬁcacious or prevenient grace of God, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16920,"Labels ever, present, showy, useful, effective, capable, strong, valuable, powerful, active, efﬁcient, helpful, generous, power, kindness, effect, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16921,"LSBert benevolent, exemplary, abundant, extraordinary, essential, inspired, ubiquitous, irresistible, exclusive, inclusive
",False,LSBert A Simple Framework for Lexical,False,False,True
16922,"Sent3 The Amazon Basin is the part of South America drained by the Amazon River and its tributaries .
",False,LSBert A Simple Framework for Lexical,False,False,True
16923,"Labels streams, branches, riverlets, adjacent, smaller rivers, channels, rivers, brooks, ditches, children creeks, offshoots, creeks
",False,LSBert A Simple Framework for Lexical,False,False,True
16924,"LSBert basins, drains, derivatives, headwaters, components, subsidiaries, minions, rays, sources, forks
",False,LSBert A Simple Framework for Lexical,False,False,True
16925,"Sent4 He held several senior positions in the Royal Flying Corps during World War I, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16926,"Labels high-level, older, upper, top, higher, high, superior, important, veteran, head, advance, top-level, advanced, leader, chief, principal, big
",False,LSBert A Simple Framework for Lexical,False,False,True
16927,"LSBert junior, signiﬁcant, prestigious, leadership, civil, command, formal, prominent, subordinate, powerful
",False,LSBert A Simple Framework for Lexical,False,False,True
16928,"Sent5 On 1 October 1983 the pilot project began operations as a full-ﬂedged bank and was renamed the Grameen Bank to ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16929,"Labels real, developed, fully operating, legitimate, total, complete, full, qualiﬁed, whole, major, working, full-service, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16930,"LSBert commercial, development, national, community, central, formal, private, public, bangladeshi, chartered
",False,LSBert A Simple Framework for Lexical,False,False,True
16931,"Sent6 The principal greenhouse , in an art nouveau style with ... , resembles the mid-19th century Crystal Palace in London .
",False,LSBert A Simple Framework for Lexical,False,False,True
16932,"Labels is similar to, looks like, looks-like, mimics, represents, matches, shows, mirrors, echos, look like, favors, appears like, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16933,"LSBert recalls, suggests, approaches, echoes, references, parallels, appears, depicts, incorporates, follows
",False,LSBert A Simple Framework for Lexical,False,False,True
16934,"Sent7 A perfectly elastic collision is deﬁned as one in which there is no loss of kinetic energy in the collision .
",False,LSBert A Simple Framework for Lexical,False,False,True
16935,"Labels active, moving, movement, motion, static, motive, innate, kinetic, real, strong, driving, motion related, motion-, living, powerful, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16936,"LSBert mechanical, rotational, dynamic, total, thermal, momentum, physical, the, potential, energetic
",False,LSBert A Simple Framework for Lexical,False,False,True
16937,"Sent8 None of your watched items were edited in the time period displayed .
",False,LSBert A Simple Framework for Lexical,False,False,True
16938,"Labels changed, looked at, reﬁned, revise, ﬁnished, ﬁxed, revised, revised, scanned, shortened
",False,LSBert A Simple Framework for Lexical,False,False,True
16939,"LSBert altered, incorporated, appropriate, modiﬁed, organized, ﬁltered, included, blended, amended, enhanced
",False,LSBert A Simple Framework for Lexical,False,False,True
16940,"TABLE 6
",False,LSBert A Simple Framework for Lexical,False,False,True
16941,"The examples of substitute candidates that do not contain one valid substitution provided by humans on LexMTurk. The complex word of each
",False,LSBert A Simple Framework for Lexical,False,False,True
16942,"sentence are shown in bold.
",False,LSBert A Simple Framework for Lexical,False,False,True
16943,"Sent1 Triangles can also be classiﬁed according to their internal angles, measured here in degrees.
",False,LSBert A Simple Framework for Lexical,False,False,True
16944,"Labels grouped, categorized, arranged, labeled, divided, organized, separated, deﬁned, described, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16945,"LSBert SR divided ,described , separated, designated, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16946,"LSBert Substitute classiﬁed
",False,LSBert A Simple Framework for Lexical,False,False,True
16947,"Sent2 ...; he retained the conductorship of the Vienna Philharmonic until 1927.
",False,LSBert A Simple Framework for Lexical,False,False,True
16948,"Labels kept, held, had, got
",False,LSBert A Simple Framework for Lexical,False,False,True
16949,"LSBert SR maintained, held,kept , remained, continued, shared, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16950,"LSBert Substitute maintained
",False,LSBert A Simple Framework for Lexical,False,False,True
16951,"Sent3 ..., and a Venetian in Paris in 1528 also reported that she was said to be beautiful
",False,LSBert A Simple Framework for Lexical,False,False,True
16952,"Labels said, told, stated, wrote, declared, indicated, noted, claimed, announced, mentioned
",False,LSBert A Simple Framework for Lexical,False,False,True
16953,"LSBert SR noted , conﬁrmed, described, claimed, recorded, said, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16954,"LSBert Substitute reported
",False,LSBert A Simple Framework for Lexical,False,False,True
16955,"Sent4 ..., the king will rarely play an active role in the development of an offensive or ....
",False,LSBert A Simple Framework for Lexical,False,False,True
16956,"Labels infrequently, hardly, uncommonly, barely, seldom, unlikely, sometimes, not, seldomly, ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16957,"LSBert SR never, usually, seldom ,not,barely ,hardly , ...
",False,LSBert A Simple Framework for Lexical,False,False,True
16958,"LSBert Substitute never
",False,LSBert A Simple Framework for Lexical,False,False,True
16959,"TABLE 7
",False,LSBert A Simple Framework for Lexical,False,False,True
16960,"The examples that the ﬁnal substitute generated by LSBert is not from the labels. The words in the substitute ranking belonging to the labels are
",False,LSBert A Simple Framework for Lexical,False,False,True
16961,"shown in bold.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 10
",False,LSBert A Simple Framework for Lexical,False,False,True
16962,"1Sentence Admission to Tsinghua is exceedingly competitive.
",False,LSBert A Simple Framework for Lexical,False,False,True
16963,"Label Entrance to Tsinghua is very very difﬁcult.
",False,LSBert A Simple Framework for Lexical,False,False,True
16964,"Glava ˇs Offers toQinghua isvery exciting .
",False,LSBert A Simple Framework for Lexical,False,False,True
16965,"REC-LS Admission to Tsinghua is exceedingly competitive.
",False,LSBert A Simple Framework for Lexical,False,False,True
16966,"LSBert Entrance to Tsinghua is very tough .
",False,LSBert A Simple Framework for Lexical,False,False,True
16967,"2Sentence Many species had vanished by the end of the nineteenth century, with European settlement.
",False,LSBert A Simple Framework for Lexical,False,False,True
16968,"Label With Euopean settlement many species have been vanished.
",False,LSBert A Simple Framework for Lexical,False,False,True
16969,"Glava ˇs Some birds wasgone by the time of the twentieth history , with world land .
",False,LSBert A Simple Framework for Lexical,False,False,True
16970,"REC-LS Many species had disappeared by the end of the 19th century, with European settlement.
",False,LSBert A Simple Framework for Lexical,False,False,True
16971,"LSBert Many animals haddisappeared by the end of the nineteenth century, with European settlement.
",False,LSBert A Simple Framework for Lexical,False,False,True
16972,"3Sentence In 1987 Wexler was inducted into the Rock and Roll Hall of Fame.
",False,LSBert A Simple Framework for Lexical,False,False,True
16973,"Label In 1987 Wexler was inducted into the Rock and Roll Hall of Fame.
",False,LSBert A Simple Framework for Lexical,False,False,True
16974,"Glava ˇs In 1987 Livingston wasfame into the rock and youhall of hall.
",False,LSBert A Simple Framework for Lexical,False,False,True
16975,"REC-LS In 1987 Wexler was inducted into the Rock and Roll Hall of Fame.
",False,LSBert A Simple Framework for Lexical,False,False,True
16976,"LSBert In 1987 Wexler was elected into the Rock and Roll Hall of Honor .
",False,LSBert A Simple Framework for Lexical,False,False,True
16977,"4Sentence Oregano is an indispensable ingredient in Greek cuisine.
",False,LSBert A Simple Framework for Lexical,False,False,True
16978,"Label Oregano is a necessary ingredient in Greek cuisine.
",False,LSBert A Simple Framework for Lexical,False,False,True
16979,"Glava ˇs Garlic is an essential ingredient in Greek cooking .
",False,LSBert A Simple Framework for Lexical,False,False,True
16980,"REC-LS Oregano is an essential element in Greek cuisine.
",False,LSBert A Simple Framework for Lexical,False,False,True
16981,"LSBert Oregano is an important element in Greek food.
",False,LSBert A Simple Framework for Lexical,False,False,True
16982,"5Sentence Their eyes are quite small, and their visual acuity is poor.
",False,LSBert A Simple Framework for Lexical,False,False,True
16983,"Label Their eyes are quite small, and their visual acuity is poor.
",False,LSBert A Simple Framework for Lexical,False,False,True
16984,"Glava ˇs Their eyes have very little , and their musical visual isbad.
",False,LSBert A Simple Framework for Lexical,False,False,True
16985,"REC-LS Their eyes are quite small, and their ocular acuteness is poor.
",False,LSBert A Simple Framework for Lexical,False,False,True
16986,"LSBert Their eyes are quite small, and their visual ability isbad.
",False,LSBert A Simple Framework for Lexical,False,False,True
16987,"TABLE 8
",False,LSBert A Simple Framework for Lexical,False,False,True
16988,"The simpliﬁed sentences are shown using three different LS methods on WikiLarge dataset. Substitutions are shown in bold.
",False,LSBert A Simple Framework for Lexical,False,False,True
16989,"[3] L. Feng, Automatic readability assessment for people with intellectual
",False,LSBert A Simple Framework for Lexical,False,False,True
16990,"disabilities, ACM SIGACCESS accessibility and computing (93) (2009)
",False,LSBert A Simple Framework for Lexical,False,False,True
16991,"84–91.
",False,LSBert A Simple Framework for Lexical,False,False,True
16992,"[4] H. Saggion, Automatic text simpliﬁcation, Synthesis Lectures on Human
",False,LSBert A Simple Framework for Lexical,False,False,True
16993,"Language Technologies 10 (1) (2017) 1–137.
",False,LSBert A Simple Framework for Lexical,False,False,True
16994,"[5] G. Paetzold, L. Specia, Lexical simpliﬁcation with neural ranking, in:
",False,LSBert A Simple Framework for Lexical,False,False,True
16995,"ACL: V olume 2, Short Papers, 2017, pp. 34–40.
",False,LSBert A Simple Framework for Lexical,False,False,True
16996,"[6] S. Gooding, E. Kochmar, Recursive context-aware lexical simpliﬁcation,
",False,LSBert A Simple Framework for Lexical,False,False,True
16997,"in: Proceedings of the 2019 Conference on Empirical Methods in Natural
",False,LSBert A Simple Framework for Lexical,False,False,True
16998,"Language Processing and the 9th International Joint Conference on
",False,LSBert A Simple Framework for Lexical,False,False,True
16999,"Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 4855–4865.
",False,LSBert A Simple Framework for Lexical,False,False,True
17000,"[7] S. Devlin, J. Tait, The use of a psycholinguistic database in the simpli
",False,LSBert A Simple Framework for Lexical,False,False,True
17001,"cation of text for aphasic readers, Linguistic Databases 1 (1998) 161173.
",False,LSBert A Simple Framework for Lexical,False,False,True
17002,"[8] E. Pavlick, C. Callison-Burch, Simple ppdb: A paraphrase database for
",False,LSBert A Simple Framework for Lexical,False,False,True
17003,"simpliﬁcation, in: ACL: V olume 2, Short Papers, 2016, pp. 143–148.
",False,LSBert A Simple Framework for Lexical,False,False,True
17004,"[9] G. Glava ˇs, S. ˇStajner, Simplifying lexical simpliﬁcation: do we need
",False,LSBert A Simple Framework for Lexical,False,False,True
17005,"simpliﬁed corpora?, in: ACL, 2015, pp. 63–68.
",False,LSBert A Simple Framework for Lexical,False,False,True
17006,"[10] S. Gooding, E. Kochmar, Complex word identiﬁcation as a sequence
",False,LSBert A Simple Framework for Lexical,False,False,True
17007,"labelling task, in: Proceedings of the 57th Annual Meeting of the
",False,LSBert A Simple Framework for Lexical,False,False,True
17008,"Association for Computational Linguistics, 2019, pp. 1148–1153.
",False,LSBert A Simple Framework for Lexical,False,False,True
17009,"[11] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
",False,LSBert A Simple Framework for Lexical,False,False,True
17010,"bidirectional transformers for language understanding, arXiv preprint
",False,LSBert A Simple Framework for Lexical,False,False,True
17011,"arXiv:1810.04805.
",False,LSBert A Simple Framework for Lexical,False,False,True
17012,"[12] W. Coster, D. Kauchak, Simple english wikipedia: a new text simpliﬁca-
",False,LSBert A Simple Framework for Lexical,False,False,True
17013,"tion task, in: ACL, 2011, pp. 665–669.
",False,LSBert A Simple Framework for Lexical,False,False,True
17014,"[13] T. Wang, P. Chen, K. Amaral, J. Qiang, An experimental study of
",False,LSBert A Simple Framework for Lexical,False,False,True
17015,"lstm encoder-decoder model for text simpliﬁcation, arXiv preprint
",False,LSBert A Simple Framework for Lexical,False,False,True
17016,"arXiv:1609.03663.
",False,LSBert A Simple Framework for Lexical,False,False,True
17017,"[14] S. Nisioi, S. ˇStajner, S. P. Ponzetto, L. P. Dinu, Exploring neural text
",False,LSBert A Simple Framework for Lexical,False,False,True
17018,"simpliﬁcation models, in: ACL, V ol. 2, 2017, pp. 85–91.
",False,LSBert A Simple Framework for Lexical,False,False,True
17019,"[15] Y . Dong, Z. Li, M. Rezagholizadeh, J. C. K. Cheung, Editnts: An
",False,LSBert A Simple Framework for Lexical,False,False,True
17020,"neural programmer-interpreter model for sentence simpliﬁcation through
",False,LSBert A Simple Framework for Lexical,False,False,True
17021,"explicit editing, in: Proceedings of the 57th Annual Meeting of the
",False,LSBert A Simple Framework for Lexical,False,False,True
17022,"Association for Computational Linguistics, 2019, pp. 3393–3402.
",False,LSBert A Simple Framework for Lexical,False,False,True
17023,"[16] Z. Zhu, D. Bernhard, I. Gurevych, A monolingual tree-based translation
",False,LSBert A Simple Framework for Lexical,False,False,True
17024,"model for sentence simpliﬁcation, in: Proceedings of the 23rd interna-
",False,LSBert A Simple Framework for Lexical,False,False,True
17025,"tional conference on computational linguistics, 2010, pp. 1353–1361.
",False,LSBert A Simple Framework for Lexical,False,False,True
17026,"[17] X. Zhang, M. Lapata, Sentence simpliﬁcation with deep reinforcement
",False,LSBert A Simple Framework for Lexical,False,False,True
17027,"learning, arXiv preprint arXiv:1703.10931.
",False,LSBert A Simple Framework for Lexical,False,False,True
17028,"[18] W. Xu, C. Callison-Burch, C. Napoles, Problems in current text simpliﬁ-
",False,LSBert A Simple Framework for Lexical,False,False,True
17029,"cation research: New data can help, TACL 3 (1) (2015) 283–297.
",False,LSBert A Simple Framework for Lexical,False,False,True
17030,"[19] S. ˇStajner, H. B ´echara, H. Saggion, A deeper exploration of the standard
",False,LSBert A Simple Framework for Lexical,False,False,True
17031,"pb-smt approach to text simpliﬁcation and its evaluation, in: ACL, 2015,
",False,LSBert A Simple Framework for Lexical,False,False,True
17032,"pp. 823–828.
",False,LSBert A Simple Framework for Lexical,False,False,True
17033,"[20] W. Hwang, H. Hajishirzi, M. Ostendorf, W. Wu, Aligning sentences from
",False,LSBert A Simple Framework for Lexical,False,False,True
17034,"standard wikipedia to simple wikipedia, in: ACL, 2015, pp. 211–217.[21] M. Shardlow, A survey of automated text simpliﬁcation, International
",False,LSBert A Simple Framework for Lexical,False,False,True
17035,"Journal of Advanced Computer Science and Applications 4 (1) (2014)
",False,LSBert A Simple Framework for Lexical,False,False,True
17036,"58–70.
",False,LSBert A Simple Framework for Lexical,False,False,True
17037,"[22] G. H. Paetzold, L. Specia, A survey on lexical simpliﬁcation, in: Journal
",False,LSBert A Simple Framework for Lexical,False,False,True
17038,"of Artiﬁcial Intelligence Research, V ol. 60, 2017, pp. 549–593.
",False,LSBert A Simple Framework for Lexical,False,False,True
17039,"[23] M. Lesk, Automatic sense disambiguation using machine readable dictio-
",False,LSBert A Simple Framework for Lexical,False,False,True
17040,"naries: How to tell a pine cone from an ice cream cone, in: Proceedings
",False,LSBert A Simple Framework for Lexical,False,False,True
17041,"of the 5th Annual International Conference on Systems Documentation,
",False,LSBert A Simple Framework for Lexical,False,False,True
17042,"1986, pp. 24–26.
",False,LSBert A Simple Framework for Lexical,False,False,True
17043,"1986, pp. 24–26.
",False,LSBert A Simple Framework for Lexical,False,False,True
17044,"Abstract
",True,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17045,"1 Introduction
",True,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17046,"2 Data
",True,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17047,"3.1 Classiﬁers
",True,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17048,"3.2 Features
",True,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17049,"Proceedings of SemEval-2016 , pages 996–1000,
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17050,"San Diego, California, June 16-17, 2016. c
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17051,"2016 Association for Computational Linguistics
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17052,"LTG at SemEval-2016 Task 11: Complex Word Identiﬁcation with Classiﬁer
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17053,"Ensembles
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17054,"Shervin Malmasi1Mark Dras1Marcos Zampieri2,3
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17055,"1Macquarie University, Sydney, NSW, Australia
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17056,"2Saarland University, Germany
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17057,"3German Research Center for Artiﬁcial Intelligence, Germany
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17058,"{first.last}@mq.edu.au, marcos.zampieri@dfki.de
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17059,"We present the description of the LTG entry
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17060,"in the SemEval-2016 Complex Word Identi-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17061,"ﬁcation (CWI) task, which aimed to develop
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17062,"systems for identifying complex words in En-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17063,"glish sentences. Our entry focused on the use
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17064,"of contextual language model features and the
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17065,"application of ensemble classiﬁcation meth-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17066,"ods. Both of our systems achieved good per-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17067,"formance, ranking in 2ndand 3rdplace overall
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17068,"in terms of F-Score.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17069,"1 Introduction
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17070,"Complex Word Identiﬁcation (CWI) is the task of
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17071,"identifying complex words in texts using computa-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17072,"tional methods (Shardlow, 2013). The task is usu-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17073,"ally carried out as part of lexical and text simpli-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17074,"ﬁcation systems. Shardlow (2014) considers CWI
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17075,"as the ﬁrst processing step in lexical simpliﬁcation
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17076,"pipelines. Complex or difﬁcult words should ﬁrst be
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17077,"identiﬁed so they can be later substituted by simpler
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17078,"ones to improve text readability.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17079,"CWI has gained more importance in the last
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17080,"decade as lexical and text simpliﬁcation systems
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17081,"have been developed or tailored for a number of pur-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17082,"poses. They have been applied to make texts more
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17083,"accessible to language learners (Petersen and Os-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17084,"tendorf, 2007); other researchers have explored text
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17085,"simpliﬁcation strategies targeted at populations with
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17086,"low literacy skills (Alu ´ısio et al., 2008). Finally, an-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17087,"other relevant application of text simpliﬁcation are
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17088,"people with dyslexia (Rello et al., 2013).
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17089,"The SemEval 2016 Task 11: Complex Word Iden-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17090,"tiﬁcation (CWI) provides an interesting opportunityto evaluate methods and approaches for this task.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17091,"The organizers proposed a binary text classiﬁcation
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17092,"task in which participants were required label words
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17093,"in English sentences as either complex ( 1) or simple
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17094,"(0). The task organizers provided participants with a
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17095,"training set containing sentences annotated with this
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17096,"information, followed by an unlabeled test set for
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17097,"evaluation. The assessment of whether words in a
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17098,"sentence are complex or simple was performed by
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17099,"human annotators required to label the data.1
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17100,"2 Data
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17101,"Based on the information available at the shared
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17102,"task’s website2: “400 annotators were presented
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17103,"with several sentences and asked to select which
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17104,"words they did not understand their meaning.”
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17105,"The CWI task dataset was divided as follows:
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17106,"•Training set: 2,237 judgments by 20 annota-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17107,"tors over 200 sentences. A word is considered
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17108,"complex if at least one of the 20 annotators as-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17109,"signed it as so.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17110,"•Test set: 88,221 judgments made over 9,000
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17111,"sentences (1 annotator per sentence).
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17112,"1Here the term complex is used as a synonym for difﬁcult.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17113,"Unlike the Morphology term complex (antonym of simplex )
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17114,"that deﬁnes compound words or words composed of multiple
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17115,"morphs (Adams, 2001).
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17116,"2http://alt.qcri.org/semeval2016/task11/9963 Methodology
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17117,"The primary focus of our team’s entry was the use of
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17118,"judgements from different annotators to create train-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17119,"ing data. We looked at how adjusting the thresh-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17120,"old for inter-annotator agreement would affect the
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17121,"results and whether the combination of data created
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17122,"using different threshold values could improve per-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17123,"formance.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17124,"Initially, the training data released by the organiz-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17125,"ers was labeled in a way that a word was marked as
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17126,"complex if any annotator judged it so. During the
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17127,"course of the shared task the organizers released ad-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17128,"ditional information about the training data, chieﬂy
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17129,"the individual judgements of the 20 annotators that
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17130,"were used to derive the ﬁnal labels for each word.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17131,"We attempted to use this data in our system. Dur-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17132,"ing development we noted that by increasing this
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17133,"threshold to two, the performance of our system un-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17134,"der cross-validation improved by a small amount.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17135,"Accordingly, we pursued this direction as the main
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17136,"focus of our experiments.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17137,"3.1 Classiﬁers
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17138,"We utilize a decision tree classiﬁer, which we found
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17139,"to perform better than Support Vector Machine
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17140,"(SVM) and Na ¨ıve Bayes classiﬁers for this data.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17141,"3.2 Features
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17142,"Our core set of features are based on estimating n-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17143,"gram probabilities using web-scale language mod-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17144,"els. More speciﬁcally, this data was sourced from
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17145,"the Microsoft Web N-Gram Service3, although we
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17146,"should note that this service has been deprecated
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17147,"and replaced since the shared task.4These language
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17148,"models are trained on web-scale corpora collected
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17149,"by Microsoft’s Bing search engine from crawling
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17150,"English web pages.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17151,"Given a target word wt, we extract several prob-
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17152,"ability estimates to use as classiﬁcation features.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17153,"These estimates, which we describe below, use the
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17154,"target word as well its preceding and following
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17155,"words, as shown in Figure 1.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17156,"3http://weblm.research.microsoft.com/
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17157,"4It has been replaced by Microsoft’s Project Oxford:
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17158,"https://www.projectoxford.ai/weblm3.2.1 Word Probability
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17159,"This is an estimate of how likely the target word
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17160,"is to occur in the language model:
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17161,"P(wt)
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17162,"Rarer words would be assigned lower values and
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17163,"thus this feature can help quantify word frequency
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17164,"for the classiﬁer.
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17165,"3.2.2 Conditional Probability
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17166,"3.2.2 Conditional Probability
",False,LTG at SemEval-2016 Task 11 Complex Word Identification with Classifier,False,False,True
17167,"ABSTRACT
",True,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17168,"ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
",True,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17169,"1. INTRODUCTION
",True,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17170,"2. PROPOSED METHOD
",True,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17171,"2.1 Detector Analysis
",True,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17172,"Multi-view Face Detection Using Deep Convolutional
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17173,"Neural Networks
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17174,"Sachin Sudhakar Farfade
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17175,"Y ahoo
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17176,"fsachin@yahoo-inc.comMohammad Saberian
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17177,"Y ahoo
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17178,"saberian@yahoo-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17179,"inc.comLi-Jia Li
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17180,"Y ahoo
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17181,"lijiali.vision@gmail.com
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17182,"In this paper we consider the problem of multi-view face de-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17183,"tection. While there has been signicant research on this
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17184,"problem, current state-of-the-art approaches for this task
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17185,"require annotation of facial landmarks, e.g. TSM [25], or
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17186,"annotation of face poses [28, 22]. They also require training
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17187,"dozens of models to fully capture faces in all orientations,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17188,"e.g. 22 models in HeadHunter method [22]. In this paper we
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17189,"propose Deep Dense Face Detector (DDFD), a method that
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17190,"does not require pose/landmark annotation and is able to de-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17191,"tect faces in a wide range of orientations using a single model
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17192,"based on deep convolutional neural networks. The proposed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17193,"method has minimal complexity; unlike other recent deep
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17194,"learning object detection methods [9], it does not require
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17195,"additional components such as segmentation, bounding-box
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17196,"regression, or SVM classiers. Furthermore, we analyzed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17197,"scores of the proposed face detector for faces in dierent ori-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17198,"entations and found that 1) the proposed method is able to
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17199,"detect faces from dierent angles and can handle occlusion to
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17200,"some extent, 2) there seems to be a correlation between dis-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17201,"tribution of positive examples in the training set and scores
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17202,"of the proposed face detector. The latter suggests that the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17203,"proposed method's performance can be further improved by
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17204,"using better sampling strategies and more sophisticated data
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17205,"augmentation techniques. Evaluations on popular face de-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17206,"tection benchmark datasets show that our single-model face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17207,"detector algorithm has similar or better performance com-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17208,"pared to the previous methods, which are more complex and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17209,"require annotations of either dierent poses or facial land-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17210,"marks.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17211,"Categories and Subject Descriptors
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17212,"I.4 [IMAGE PROCESSING AND COMPUTER VI-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17213,"SION ]: Applications
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17214,"General Terms
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17215,"Application
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17216,"Permission to make digital or hard copies of all or part of this work for personal or
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17217,"classroom use is granted without fee provided that copies are not made or distributed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17218,"for proﬁt or commercial advantage and that copies bear this notice and the full cita-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17219,"tion on the ﬁrst page. Copyrights for components of this work owned by others than
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17220,"publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17221,"and/or a fee. Request permissions from permissions@acm.org.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17222,"ICMR’15, June 23–26, 2015, Shanghai, China.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17223,"Copyright is held by the owner/author(s). Publication rights licensed to ACM.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17224,"ACM 978-1-4503-3274-3/15/06 ...$15.00.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17225,"DOI: http://dx.doi.org/10.1145/2671188.2749408 .
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17226,"Figure 1: An example of user generated photos on
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17227,"social networks that contains faces in various poses,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17228,"illuminations and occlusions. The bounding-boxes
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17229,"and corresponding scores show output of our pro-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17230,"posed face detector.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17231,"Keywords
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17232,"Face Detection, Convolutional Neural Network, Deep Learn-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17233,"ing
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17234,"1. INTRODUCTION
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17235,"With the wide spread use of smartphones and fast mobile
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17236,"networks, millions of photos are uploaded everyday to the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17237,"cloud storages such as Dropbox or social networks such as
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17238,"Facebook, Twitter, Instagram, Google+, and Flicker. Orga-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17239,"nizing and retrieving relevant information from these photos
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17240,"is very challenging and directly impact user experience on
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17241,"those platforms. For example, users commonly look for pho-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17242,"tos that were taken at a particular location, at a particular
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17243,"time, or with a particular friend. The former two queries
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17244,"are fairly straightforward, as almost all of today's cameras
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17245,"embed time and GPS location into photos. The last query,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17246,"i.e. contextual query, is more challenging as there is no ex-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17247,"plicit signal about the identities of people in the photos.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17248,"The key for this identication is the detection of human
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17249,"faces. This has made low complexity, rapid and accurate
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17250,"face detection an essential component for cloud based photo
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17251,"sharing/storage platforms.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17252,"For the past two decades, face detection has always been
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17253,"an active research area in the vision community. The semi-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17254,"nal work of Viola and Jones [40] made it possible to rapidly
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17255,"detect up-right faces in real-time with very low computa-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17256,"tional complexity. Their detector, called detector cascade,arXiv:1502.02766v3  [cs.CV]  20 Apr 2015consists of a sequence of simple-to-complex face classiers
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17257,"and has attracted extensive research eorts. Moreover, de-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17258,"tector cascade has been deployed in many commercial prod-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17259,"ucts such as smartphones and digital cameras. While cas-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17260,"cade detectors can accurately nd visible up-right faces, they
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17261,"often fail to detect faces from dierent angles, e.g. side
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17262,"view or partially occluded faces. This failure can signif-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17263,"icantly impact the performance of photo organizing soft-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17264,"ware/applications since user generated content often con-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17265,"tains faces from dierent angles or faces that are not fully
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17266,"visible; see for example Figure 1. This has motivated many
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17267,"works on the problem of multi-view face detection over the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17268,"past two decades. Current solutions can be summarized into
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17269,"three categories:
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17270,"Cascade-based: These methods extend the Viola and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17271,"Jones detector cascade. For example, [41] proposed to
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17272,"train a detector cascade for each view of the face and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17273,"combined their results at the test time. Recently, [22]
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17274,"combined this method with integral channel features
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17275,"[3] and soft-cascade [1], and showed that by using 22
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17276,"cascades, it is possible to obtain state-of-the-art per-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17277,"formance for multi-view face detection. This approach,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17278,"however, requires face orientation annotations. More-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17279,"over its complexity in training and testing increases
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17280,"linearly with the number of models. To address the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17281,"computational complexity issue, Viola and Jones [39]
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17282,"proposed to rst estimate the face pose using a tree
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17283,"classier and then run the cascade of corresponding
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17284,"face pose to verify the detection. While improving the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17285,"detection speed, this method degrades the accuracy
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17286,"because mistakes of the initial tree classier are irre-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17287,"versible. This method is further improved by [13, 12]
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17288,"where, instead of one detector cascade, several detec-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17289,"tors are used after the initial classier. Finally, [35] and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17290,"[28] combined detector cascade with multiclass boost-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17291,"ing and proposed a method for multiclass/multi-view
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17292,"object detection.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17293,"DPM-based: These methods are based on the deformable
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17294,"part models technique [5] where a face is dened as
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17295,"a collection of its parts. The parts are dened via
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17296,"unsupervised or supervised training, and a classier,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17297,"latent SVM, is trained to nd those parts and their
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17298,"geometric relationship. These detectors are robust to
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17299,"partial occlusion because they can detect faces even
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17300,"when some of the parts are not present. These meth-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17301,"ods are, however, computationally intensive because 1)
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17302,"they require solving a latent SVM for each candidate
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17303,"location and 2) multiple DPMs have to be trained and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17304,"combined to achieve the state-of-the-art performance
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17305,"[22, 25]. Moreover, in some cases DPM-based models
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17306,"require annotation of facial landmarks for training, e.g
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17307,"[25].
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17308,"Neural-Network-based: There is a long history of using
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17309,"neural networks for the task of face detection [38, 37,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17310,"27, 8, 7, 6, 26, 11, 24, 23]. In particular, [38] trained
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17311,"a two-stage system based on convolutional neural net-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17312,"works. The rst network locates rough positions of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17313,"faces and the second network veries the detection and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17314,"makes more accurate localization. In [27], the authors
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17315,"trained multiple face detection networks and combined
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17316,"their output to improve the performance. [8] traineda single multi-layer network for face detection. The
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17317,"trained network is able to partially handle dierent
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17318,"poses and rotation angles. More recently, [23] proposed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17319,"to train a neural network jointly for face detection and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17320,"pose estimation. They showed that this joint learning
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17321,"scheme can signicantly improve performance of both
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17322,"detection and pose estimation. Our method follows
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17323,"the works in [8, 23] but constructs a deeper CNN for
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17324,"face detection.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17325,"The key challenge in multi-view face detection, as pointed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17326,"out by Viola and Jones [39], is that learning algorithms such
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17327,"as Boosting or SVM and image features such as HOG or
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17328,"Haar wavelets are not strong enough to capture faces of dif-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17329,"ferent poses and thus the resulted classiers are hopelessly
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17330,"inaccurate . However, with recent advances in deep learn-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17331,"ing and GPU computation, it is possible to utilize the high
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17332,"capacity of deep convolutional neural networks for feature
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17333,"extraction/classication, and train a single model for the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17334,"task of multi-view face detection.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17335,"Deep convolutional neural network has recently demon-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17336,"strated outstanding performance in a variety of vision tasks
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17337,"such as face recognition [34, 30], object classication [19, 31],
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17338,"and object detection [9, 29, 18, 32]. In particular [19] trained
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17339,"an 8-layered network, called AlexNet, and showed that deep
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17340,"convolutional neural networks can signicantly outperform
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17341,"other methods for the task of large scale image classica-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17342,"tion. For the task of object detection, [9] proposed R-CNN
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17343,"method that uses an image segmentation technique, selec-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17344,"tive search [36], to nd candidate image regions and classify
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17345,"those candidates using a version of AlexNet that is ne-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17346,"tuned for objects in the PASCAL VOC dataset. More re-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17347,"cently, [33] improved R-CNN by 1) augmenting the selective
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17348,"search proposals with candidate regions from multibox ap-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17349,"proach [4], and 2) replacing 8-layered AlexNet with a much
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17350,"deeper CNN model of GoogLeNet [31]. Despite state-of-the-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17351,"art performance, these methods are computationally sub-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17352,"optimal because they require evaluating a CNN over more
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17353,"than 2 ;000 overlapping candidate regions independently. To
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17354,"address this issue, [18] recently proposed to run the CNN
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17355,"model on the full image once and create a feature pyra-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17356,"mid. The candidate regions, obtained by selective search,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17357,"are then mapped into this feature pyramid space. [18] then
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17358,"uses spatial pyramid pooling [20] and SVM on the mapped
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17359,"regions to classify candidate proposals. Beyond region-based
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17360,"methods, deep convolutional neural networks have also been
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17361,"used with sliding window approach, e.g. OverFeat [29] and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17362,"deformable part models [10] for object detection and [17]
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17363,"for human pose estimation. In general, for object detection
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17364,"these methods still have an inferior performance compared
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17365,"to region-based methods such as R-CNN [9] and [33]. How-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17366,"ever, in our face detection experiments we found that the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17367,"region-based methods are often very slow and result in rel-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17368,"atively weak performance.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17369,"In this paper, we propose a method based on deep learn-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17370,"ing, called Deep Dense Face Detector (DDFD), that does
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17371,"not require pose/landmark annotation and is able to detect
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17372,"faces in a wide range of orientations using a single model.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17373,"The proposed method has minimal complexity because un-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17374,"like recent deep learning object detection methods such as
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17375,"[9], it does not require additional components for segmen-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17376,"tation, bounding-box regression, or SVM classiers. Com-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17377,"pared to previous convolutional neural-network-based face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17378,"detectors such as [8], our network is deeper and is trainedon a signicantly larger training set. In addition, by analyz-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17379,"ing detection condence scores, we show that there seems
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17380,"to be a correlation between the distribution of positive ex-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17381,"amples in the training set and the condence scores of the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17382,"proposed detector. This suggests that the performance of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17383,"our method can be further improved by using better sam-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17384,"pling strategies and more sophisticated data augmentation
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17385,"techniques. In our experiments, we compare the proposed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17386,"method to a deep learning based method, R-CNN, and sev-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17387,"eral cascade and DPM-based methods. We show that DDFD
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17388,"can achieve similar or better performance even without us-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17389,"ing pose annotation or information about facial landmarks.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17390,"2. PROPOSED METHOD
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17391,"In this section, we provide details of the algorithm and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17392,"training process of our proposed face detector, called Deep
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17393,"Dense Face Detector (DDFD). The key ideas are 1) leverage
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17394,"the high capacity of deep convolutional networks for clas-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17395,"sication and feature extraction to learn a single classier
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17396,"for detecting faces from multiple views and 2) minimize the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17397,"computational complexity by simplifying the architecture of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17398,"the detector.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17399,"We start by ne-tuning AlexNet [19] for face detection.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17400,"For this we extracted training examples from the AFLW
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17401,"dataset [21], which consists of 21K images with 24K face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17402,"annotations. To increase the number of positive examples,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17403,"we randomly sampled sub-windows of the images and used
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17404,"them as positive examples if they had more than a 50% IOU
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17405,"(intersection over union) with the ground truth. For further
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17406,"data augmentation, we also randomly 
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17407,"ipped these training
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17408,"examples. This resulted in a total number of 200K posi-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17409,"tive and and 20 millions negative training examples. These
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17410,"examples were then resized to 227 227 and used to ne-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17411,"tune a pre-trained AlexNet model [19]. For ne-tuning, we
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17412,"used 50K iterations and batch size of 128 images, where each
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17413,"batch contained 32 positive and 96 negative examples.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17414,"Using this ne-tuned deep network, it is possible to take
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17415,"either region-based or sliding window approaches to obtain
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17416,"the nal face detector. In this work we selected a sliding
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17417,"window approach because it has less complexity and is in-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17418,"dependent of extra modules such as selective search. Also,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17419,"as discussed in the experiment section, this approach leads
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17420,"to better results as compared to R-CNN.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17421,"Our face classier, similar to AlexNet [19], consists of 8
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17422,"layers where the rst 5 layers are convolutional and the last
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17423,"3 layers are fully-connected. We rst converted the fully-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17424,"connected layers into convolutional layers by reshaping layer
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17425,"parameters [14]. This made it possible to eciently run the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17426,"CNN on images of any size and obtain a heat-map of the face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17427,"classier. An example of a heat-map is shown in Figure 2-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17428,"right. Each point in the heat-map shows the CNN response,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17429,"the probability of having a face, for its corresponding 227 
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17430,"227 region in the original image. The detected regions were
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17431,"then processed by non-maximal suppression to accurately
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17432,"localize the faces. Finally, to detect faces of dierent sizes,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17433,"we scaled the images up/down and obtained new heat-maps.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17434,"We tried dierent scaling schemes and found that rescaling
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17435,"image 3 times per octave gives reasonably good performance.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17436,"This is interesting as many of the other methods such as
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17437,"[22, 2] requires a signicantly larger number of resizing per
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17438,"octave, e.g. 8. Note that, unlike R-CNN [9], which uses
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17439,"SVM classier to obtain the nal score, we removed the SVMmodule and found that the network output are informative
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17440,"enough for the task of face detection.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17441,"Face localization can be further improved by using a bounding-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17442,"box regression module similar to [29, 9]. In our exper-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17443,"iment, however, adding this module degraded the perfor-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17444,"mance. Therefore, compared to the other methods such as
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17445,"R-CNN [9], which uses selective search, SVM and bounding-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17446,"box regression, or DenseNet [10], which is based on the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17447,"deformable part models, our proposed method (DDFD) is
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17448,"fairly simple. Despite its simplicity, as shown in the exper-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17449,"iments section, DDFD can achieve state-of-the-art perfor-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17450,"mance for face detection.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17451,"2.1 Detector Analysis
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17452,"In this section, we look into the scores of the proposed face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17453,"detector and observe that there seems to be a correlation
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17454,"between those scores and the distribution of positive exam-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17455,"ples in the training set. We can later use this hypothesis to
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17456,"obtain better training set or to design better data augmen-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17457,"tation procedures and improve performance of DDFD.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17458,"We begin by running our detector on a variety of faces
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17459,"with dierent in-plane and out-of-plane rotations, occlusions
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17460,"and lighting conditions (see for example Figure 1, Figure 2-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17461,"left and Figure 3). First, note that in all cases our detector is
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17462,"able to detect the faces except for the two highly occluded
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17463,"ones in Figure 1. Second, for almost all of the detected
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17464,"faces, the detector's condence score is pretty high, close
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17465,"to 1. Also as shown in the heat-map of Figure 2-right, the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17466,"scores are close to zero for all other regions. This shows that
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17467,"DDFD has very strong discriminative power, and its output
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17468,"can be used directly without any post-processing steps such
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17469,"as SVM, which is used in R-CNN [9]. Third, if we com-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17470,"pare the detector scores for faces in Figure 2-left, it is clear
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17471,"that the up-right frontal face in the bottom has a very high
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17472,"score of 0 :999 while faces with more in-plane rotation have
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17473,"less score. Note that these scores are output of a sigmoid
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17474,"function, i.e. probability (soft-max) layer in the CNN, and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17475,"thus small changes in them re
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17476,"ects much larger changes in
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17477,"the output of the previous layer. It is interesting to see that
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17478,"the scores decrease as the in-plane rotation increases. We
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17479,"can see the same trend for out-of-plane rotated faces and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17480,"occluded faces in Figures 1 and 3. We hypothesize that this
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17481,"trend in the scores is not because detecting rotated face are
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17482,"more dicult but it is because of lack of good training ex-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17483,"amples to represent such faces in the training process.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17484,"To examine this hypothesis, we looked into the face an-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17485,"notations for AFLW dataset [21]. Figure 4 shows the distri-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17486,"bution of the annotated faces with regards to their in-plane,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17487,"pitch (up and down) and yaw (left to right) rotations. As
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17488,"shown in this gure, the number of faces with more than
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17489,"30 degrees out-of-plane rotation is signicantly lower than
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17490,"the faces with less than 30 degree rotation. Similarly, the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17491,"number of faces with yaw or pitch less than 50 degree is
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17492,"signicantly larger than the other ones. Given this skewed
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17493,"training set, it not surprising that the ne-tuned CNN is
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17494,"more condent about up-right faces. This is because the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17495,"CNN is trained to minimize the risk of the soft-max loss
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17496,"function
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17497,"R=X
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17498,"xi2Blog [prob(yijxi)]; (1)
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17499,"whereBis the example batch that is used in an iteration
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17500,"of stochastic gradient descent and yiis the label of example
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17501,"xi. The sampling method for selecting examples in Bcan0.00.10.20.30.40.50.60.70.80.9Figure 2: left) an example image with faces in dierent in-plane rotations. It also shows output of our
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17502,"proposed face detector after NMS along with corresponding condence score for each detection. right)
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17503,"heat-map for the response of DDFD scores over the image.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17504,"signicantly hurt performance of the nal detector. In an
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17505,"extreme case ifBnever contains any example of a certain
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17506,"class, the CNN classier will never learn the attributes of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17507,"that class.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17508,"In our implementation jBj= 128 and it is collected by ran-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17509,"domly sampling the training set. However, since the number
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17510,"of negative examples are 100 times more than the number
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17511,"of positive examples, a uniform sampling will result in only
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17512,"about 2 positive examples per batch. This signicantly de-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17513,"grades the chance of the CNN to distinguish faces from non-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17514,"faces. To address this issue, we enforced one quarter of each
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17515,"batch to be positive examples, where the positive examples
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17516,"are uniformly sampled from the pool of positive training
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17517,"samples. But, as illustrated in Figure 4, this pool is highly
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17518,"skewed in dierent aspects, e.g. in-plane and out-of-plane
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17519,"rotations. The CNN is therefore getting exposed with more
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17520,"up-right faces; it is thus not surprising that the ne-tuned
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17521,"CNN is more condent about the up-right faces than the ro-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17522,"tated ones. This analysis suggests that the key for improv-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17523,"ing performance of DDFD is to ensure that all categories of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17524,"the training examples have similar chances to contribute in
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17525,"optimizing the CNN. This can be accomplished by enforc-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17526,"ing population-based sampling strategies such as increasing
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17527,"selection probability for categories with low population.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17528,"Similarly, as shown in Figure 1, the current face detector
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17529,"still fails to detect faces with heavy occlusions. Similar to
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17530,"the issue with rotated faces, we believe that this problem can
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17531,"also be addressed through modication of the training set.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17532,"In fact, most of the face images in the AFLW dataset [21]
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17533,"are not occluded, which makes it dicult for a CNN to learn
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17534,"that faces can be occluded. This issue can be addressed by
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17535,"using more sophisticated data augmentation techniques such
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17536,"as occluding parts of positive examples. Note that simply
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17537,"covering parts of positive examples with black/white or noise
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17538,"blocks is not useful as the CNN may learn those articial
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17539,"patterns.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17540,"To summarize, the proposed face detector based on deep
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17541,"CNN is able to detect faces from dierent angles and handle
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17542,"occlusion to some extent. However, since the training set is
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17543,"skewed, the network is more condent about up-right faces
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17544,"and better results can be achieved by using better sampling
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17545,"strategies and more sophisticated data augmentation tech-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17546,"niques.3. EXPERIMENTS
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17547,"We implemented the proposed face detector using the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17548,"Cae library [16] and used its pre-trained Alexnet [19] model
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17549,"for ne-tuning. For further details on the training process of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17550,"our proposed face detector please see section 2. After con-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17551,"verting fully-connected layers to convolutional layers [14], it
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17552,"is possible to get the network response (heat-map) for the
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17553,"whole input image in one call to Cae code. The heat-map
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17554,"shows the scores of the CNN for every 227 227 window
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17555,"with a stride of 32 pixels in the original image. We directly
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17556,"used this response for classifying a window as face or back-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17557,"ground. To detect faces of smaller or larger than 227 227,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17558,"we scaled the image up or down respectively.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17559,"We tested our face detection approach on PASCAL Face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17560,"[42], AFW [25] and FDDB [15] datasets. For selecting and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17561,"tuning parameters of the proposed face detector we used
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17562,"the PASCAL Face dataset. PASCAL Face dataset consists
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17563,"of 851 images and 1341 annotated faces, where annotated
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17564,"faces can be as small as 35 pixels. AFW dataset is built using
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17565,"Flickr images. It has 205 images with 473 annotated faces,
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17566,"and its images tend to contain cluttered background with
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17567,"large variations in both face viewpoint and appearance (ag-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17568,"ing, sunglasses, make-ups, skin color, expression etc.). Sim-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17569,"ilarly, FDDB dataset [15] consists of 5171 annotated faces
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17570,"with 2846 images and contains occluded, out-of-focus, and
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17571,"low resolution faces. For evaluation, we used the toolbox
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17572,"provide by [22] with corrected annotations for PASCAL Face
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17573,"and AFW datasets and the original annotations of FDDB
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17574,"dataset.
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17575,"We started by nding the optimal number of scales for
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17576,"the proposed detector using PASCAL dataset. We upscaled
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17577,"images by factor of 5 to detect faces as small as 227 =5 = 45
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17578,"pixels. We then down scaled the image with by a fac-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17579,"tor,fs, and repeated the process until the minimum im-
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17580,"age dimension is less than 227 pixels. For the choice of
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17581,"fs, we chose fs2fp
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17582,"0:5 = 0 :7071;3p
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17583,"0:5 = 0 :7071;3p
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,False,False,True
17584,"0:5 = 0 :7071;3p
",False,Multi-view Face Detection Using Deep Convolutional Neural Networks,True,False,True
17585,"Abstract
",True,neekhara21a,False,False,True
17586,"1. Introduction
",True,neekhara21a,False,False,True
17587,"2. Background and Related Work
",True,neekhara21a,False,False,True
17588,"3. Methodology
",True,neekhara21a,False,False,True
17589,"3.1. Speaker Encoder
",True,neekhara21a,False,False,True
17590,"3.2. Mel-Spectrogram Synthesizer
",True,neekhara21a,False,False,True
17591,"4. Experiments
",True,neekhara21a,False,False,True
17592,"4.1. Datasets and Training
",True,neekhara21a,False,False,True
17593,"2 - LSTM layers 
",True,neekhara21a,False,False,True
17594,"2-layer 
",True,neekhara21a,False,False,True
17595,"4.3. Cloning Tasks
",True,neekhara21a,False,False,True
17596,"4.4. Results
",True,neekhara21a,False,False,True
17597,"5. Broader Impact
",True,neekhara21a,False,False,True
17598,"6. Conclusion and Future Work
",True,neekhara21a,False,False,True
17599,"Proceedings of Machine Learning Research 157:{, 2021 ACML 2021
",False,neekhara21a,False,False,True
17600,"Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
17601,"Paarth Neekhara* pneekhar@eng.ucsd.edu
",False,neekhara21a,False,False,True
17602,"Shehzeen Hussain* ssh028@eng.ucsd.edu
",False,neekhara21a,False,False,True
17603,"Shlomo Dubnov sdubnov@ucsd.edu
",False,neekhara21a,False,False,True
17604,"Farinaz Koushanfar fkoushanfar@eng.ucsd.edu
",False,neekhara21a,False,False,True
17605,"Julian McAuley jmcauley@eng.ucsd.edu
",False,neekhara21a,False,False,True
17606,"University of California, San Diego, 9500 Gilman Dr, La Jolla, CA 92093
",False,neekhara21a,False,False,True
17607,"* Denotes Equal Contribution
",False,neekhara21a,False,False,True
17608,"Editors: Vineeth N Balasubramanian and Ivor Tsang
",False,neekhara21a,False,False,True
17609,"Voice cloning is the task of learning to synthesize the voice of an unseen speaker from a few
",False,neekhara21a,False,False,True
17610,"samples. While current voice cloning methods achieve promising results in Text-to-Speech
",False,neekhara21a,False,False,True
17611,"(TTS) synthesis for a new voice, these approaches lack the ability to control the expressive-
",False,neekhara21a,False,False,True
17612,"ness of synthesized audio. In this work, we propose a controllable voice cloning method that
",False,neekhara21a,False,False,True
17613,"allows ne-grained control over various style aspects of the synthesized speech for an unseen
",False,neekhara21a,False,False,True
17614,"speaker. We achieve this by explicitly conditioning the speech synthesis model on a speaker
",False,neekhara21a,False,False,True
17615,"encoding, pitch contour and latent style tokens during training. Through both quantitative
",False,neekhara21a,False,False,True
17616,"and qualitative evaluations, we show that our framework can be used for various expres-
",False,neekhara21a,False,False,True
17617,"sive voice cloning tasks using only a few transcribed or untranscribed speech samples for
",False,neekhara21a,False,False,True
17618,"a new speaker. These cloning tasks include style transfer from a reference speech, synthe-
",False,neekhara21a,False,False,True
17619,"sizing speech directly from text, and ne-grained style control by manipulating the style
",False,neekhara21a,False,False,True
17620,"conditioning variables during inference.1
",False,neekhara21a,False,False,True
17621,"Keywords: TTS, Voice Cloning, Expressive TTS, Deep Learning
",False,neekhara21a,False,False,True
17622,"1. Introduction
",False,neekhara21a,False,False,True
17623,"Recent research eorts in voice cloning have focused on synthesizing a person's voice from
",False,neekhara21a,False,False,True
17624,"only a few reference audio samples. While such a system can generate speech from text for
",False,neekhara21a,False,False,True
17625,"a new speaker, it leaves out control over various style aspects of speech. Explicit control
",False,neekhara21a,False,False,True
17626,"over the style aspects of cloned speech is desirable for several applications, such as: voice-
",False,neekhara21a,False,False,True
17627,"overs in animated lms, synthesizing realistic and expressive speech for DeepFake videos,
",False,neekhara21a,False,False,True
17628,"translating speech from one language to another while preserving speaking style and speaker
",False,neekhara21a,False,False,True
17629,"identity, advertisement campaigns with expressive speech in multiple voices and languages
",False,neekhara21a,False,False,True
17630,"(etc.). Expressive voice cloning systems can also help create personalized speech interfaces
",False,neekhara21a,False,False,True
17631,"with voice assistants in smartphones, cars, and home assistants. Since speech serves as a
",False,neekhara21a,False,False,True
17632,"primary communication interface between machine learning agents and humans, the ability
",False,neekhara21a,False,False,True
17633,"to speak expressively is a very desirable quality for voice cloning systems. Furthermore,
",False,neekhara21a,False,False,True
17634,"such systems can potentially empower individuals who have lost their ability to speak.
",False,neekhara21a,False,False,True
17635,"1. Audio examples: https://expressivecloning.github.io/
",False,neekhara21a,False,False,True
17636,"Interactive Demo: https://expressivecloning.github.io/app.html
",False,neekhara21a,False,False,True
17637,"©2021 P. Neekhara*, S. Hussain*, S. Dubnov, F. Koushanfar & J. McAuley.Neekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
17638,"The goal of voice cloning is commonly formulated as learning to synthesize the voice of
",False,neekhara21a,False,False,True
17639,"an unseen speaker using only a few seconds of transcribed or untranscribed speech. This
",False,neekhara21a,False,False,True
17640,"is typically done by embedding speaker-dependent information from the available speech
",False,neekhara21a,False,False,True
17641,"samples of the new speaker, and conditioning a trained multi-speaker Text-to-Speech (TTS)
",False,neekhara21a,False,False,True
17642,"model on the derived speaker embedding Arik et al. (2018); Jia et al. (2018). While such
",False,neekhara21a,False,False,True
17643,"a system can achieve promising results in closely retaining speaker-specic characteristics
",False,neekhara21a,False,False,True
17644,"in the cloned speech, it does not oer control over other aspects of speech that are not
",False,neekhara21a,False,False,True
17645,"contained in the text or the speaker-specic embedding. These aspects include variation in
",False,neekhara21a,False,False,True
17646,"tone, speaking rate, emphasis and emotions.
",False,neekhara21a,False,False,True
17647,"Several past works have focused on the problem of expressive TTS synthesis by learning
",False,neekhara21a,False,False,True
17648,"latent variables for controlling the style aspects of speech synthesized for a given text Wang
",False,neekhara21a,False,False,True
17649,"et al. (2018); Skerry-Ryan et al. (2018). Such models are usually trained on a single-speaker
",False,neekhara21a,False,False,True
17650,"expressive speech dataset to learn meaningful latent codes for various style aspects of the
",False,neekhara21a,False,False,True
17651,"speech. Recent works Stanton et al. (2018); Valle et al. (2020), have extended the idea of
",False,neekhara21a,False,False,True
17652,"learning style representations to a multi-speaker setting by conditioning the TTS synthesis
",False,neekhara21a,False,False,True
17653,"model on both speaker identity and style encodings. Such techniques show promise in
",False,neekhara21a,False,False,True
17654,"disentangling style and speaker specic information, and generate dierent style variants of
",False,neekhara21a,False,False,True
17655,"synthesized speech for the same text and speaker. However, these methods are limited by
",False,neekhara21a,False,False,True
17656,"the speakers used in the training set and cannot be directly used for synthesizing voices of
",False,neekhara21a,False,False,True
17657,"speakers not seen during training.
",False,neekhara21a,False,False,True
17658,"Adapting multi-speaker TTS models for voice cloning requires scaling up model training
",False,neekhara21a,False,False,True
17659,"to a large multi-speaker TTS dataset, containing several minutes of transcribed speech from
",False,neekhara21a,False,False,True
17660,"thousands of speakers. High speaker diversity in the training data is important to achieve
",False,neekhara21a,False,False,True
17661,"generalization on unseen speakers Arik et al. (2018); Jia et al. (2018). The goal of our
",False,neekhara21a,False,False,True
17662,"work is to perform TTS synthesis for an unseen speaker with control over the style aspects
",False,neekhara21a,False,False,True
17663,"of generated speech . As a rst step in this direction, we train a TTS model conditioned
",False,neekhara21a,False,False,True
17664,"on speaker encodings and latent style tokens Wang et al. (2018) on a large multi-speaker
",False,neekhara21a,False,False,True
17665,"dataset. While this model is able to generate voices for unseen speakers, we nd that the
",False,neekhara21a,False,False,True
17666,"results fall short in terms of speech naturalness and style control during synthesis. Our
",False,neekhara21a,False,False,True
17667,"results suggest that learning meaningful latent style aspects is dicult when training on a
",False,neekhara21a,False,False,True
17668,"large multi-speaker dataset containing speech with mostly neutral style and expressions.
",False,neekhara21a,False,False,True
17669,"To address problem of disentangling style and speaker characteristics on a large multi-
",False,neekhara21a,False,False,True
17670,"speaker dataset containing mostly style-neutral speech, we propose a voice cloning model
",False,neekhara21a,False,False,True
17671,"that is conditioned on both latent and heuristically derived style information. Specically,
",False,neekhara21a,False,False,True
17672,"we condition our TTS synthesis model on (i) text, (ii) speaker encoding (iii) pitch contour of
",False,neekhara21a,False,False,True
17673,"the target speech and (iv) latent style tokens Wang et al. (2018). By conditioning synthesis
",False,neekhara21a,False,False,True
17674,"on various style aspects and speaker embeddings derived from the target speech, we are
",False,neekhara21a,False,False,True
17675,"able to train a model that oers ne-grained style control for synthesized speech. To adapt
",False,neekhara21a,False,False,True
17676,"inference for an unseen speaker, we can either perform zero-shot inference or ne-tune the
",False,neekhara21a,False,False,True
17677,"synthesis model on the limited text and speech pairs for the new speaker. Through both
",False,neekhara21a,False,False,True
17678,"quantitative and qualitative evaluations, we demonstrate that our proposed model can make
",False,neekhara21a,False,False,True
17679,"a new voice express, emote, sing or copy the style of a given reference speech.
",False,neekhara21a,False,False,True
17680,"The main contributions of this study are as follows:
",False,neekhara21a,False,False,True
17681,"•We introduce the problem of expressive voice cloning | Synthesizing a person's voice
",False,neekhara21a,False,False,True
17682,"from a few audio samples and allowing control over the style aspects of the synthesizedExpressive Neural Voice Cloning
",False,neekhara21a,False,False,True
17683,"speech. While past works have studied the problem of expressive TTS, they have not
",False,neekhara21a,False,False,True
17684,"investigated the problem of cloning a new speaker's voice in an expressive manner.
",False,neekhara21a,False,False,True
17685,"•We propose an expressive voice cloning framework by training a controllable TTS
",False,neekhara21a,False,False,True
17686,"model on a large multi-speaker dataset with mostly style-neutral speech. By allowing
",False,neekhara21a,False,False,True
17687,"explicit control over the speaker encoding, pitch and rhythm of the synthesized speech,
",False,neekhara21a,False,False,True
17688,"we are able to generate expressive speech for a new speaker that generalizes beyond
",False,neekhara21a,False,False,True
17689,"the distribution of the training data.
",False,neekhara21a,False,False,True
17690,"•We develop three benchmark tasks and dene metrics to evaluate expressive voice
",False,neekhara21a,False,False,True
17691,"cloning systems in terms of speaker similarity, style similarity and naturalness of
",False,neekhara21a,False,False,True
17692,"synthesized speech. We demonstrate that our proposed framework signicantly out-
",False,neekhara21a,False,False,True
17693,"performs baseline models that do not use explicit pitch contours for training.
",False,neekhara21a,False,False,True
17694,"2. Background and Related Work
",False,neekhara21a,False,False,True
17695,"Neural TTS: State-of-the-art neural approaches for natural TTS synthesis Ping et al.
",False,neekhara21a,False,False,True
17696,"(2018a); Shen et al. (2018) typically decompose the waveform synthesis pipeline into two
",False,neekhara21a,False,False,True
17697,"steps: (1) Synthesizing perceptually informed mel-spectrograms from language using an
",False,neekhara21a,False,False,True
17698,"attention based sequence-to-sequence model like Tacotron Wang et al. (2017) or Tacotron
",False,neekhara21a,False,False,True
17699,"2 Shen et al. (2018). (2) Vocoding the synthesized spectrograms to audible waveforms using
",False,neekhara21a,False,False,True
17700,"a neural vocoder van den Oord et al. (2016); Prenger et al. (2018); Neekhara et al. (2019) or
",False,neekhara21a,False,False,True
17701,"heuristic methods like the Grin-Lim Grin et al. (1984) algorithm. Multi-speaker TTS
",False,neekhara21a,False,False,True
17702,"models Gibiansky et al. (2017); Ping et al. (2018b) extend this line of work by additionally
",False,neekhara21a,False,False,True
17703,"conditioning the spectrogram synthesis model on speaker embeddings, which are trained
",False,neekhara21a,False,False,True
17704,"end-to-end using the speaker labels in the TTS dataset. While these approaches achieve
",False,neekhara21a,False,False,True
17705,"promising results in synthesizing speech for multiple speakers for a given text, they cannot
",False,neekhara21a,False,False,True
17706,"be directly used to synthesize voices of speakers not seen during training.
",False,neekhara21a,False,False,True
17707,"Voice Cloning: Voice cloning focuses on generative modeling of speech conditioned on
",False,neekhara21a,False,False,True
17708,"a speaker encoding derived from a few reference speaker audio samples. While speech
",False,neekhara21a,False,False,True
17709,"synthesis models exist van den Oord et al. (2016); Wang et al. (2017), it has been a challenge
",False,neekhara21a,False,False,True
17710,"to adapt these voice models to new speakers with limited data. Recent eorts have been
",False,neekhara21a,False,False,True
17711,"made in designing systems that can learn to synthesize a person's voice from only a few
",False,neekhara21a,False,False,True
17712,"audio samples Arik et al. (2018); Chen et al. (2019); Cooper et al. (2020); Huang et al.
",False,neekhara21a,False,False,True
17713,"(2020); Jia et al. (2018). They train a separate speaker encoding network to condition a
",False,neekhara21a,False,False,True
17714,"multi-speaker TTS model on speaker dependent information. Since the speaker encoding
",False,neekhara21a,False,False,True
17715,"network operates on waveforms, it can be used for zero-shot voice cloning from untranscribed
",False,neekhara21a,False,False,True
17716,"utterances of a target speaker. Additionally, the authors of Arik et al. (2018) demonstrate
",False,neekhara21a,False,False,True
17717,"that the synthesis model can be ne-tuned on limited text and audio pairs of a new speaker
",False,neekhara21a,False,False,True
17718,"to improve the speaker similarity of the cloned speech.
",False,neekhara21a,False,False,True
17719,"Expressive Speech Synthesis: Prior works Wang et al. (2018); Stanton et al. (2018);
",False,neekhara21a,False,False,True
17720,"Skerry-Ryan et al. (2018) on expressive speech synthesis focus on models that can be con-
",False,neekhara21a,False,False,True
17721,"ditioned on text and a latent embedding for style or prosody. During training, the style
",False,neekhara21a,False,False,True
17722,"embeddings are derived using a learnable module called Global Style Tokens (GST) , that
",False,neekhara21a,False,False,True
17723,"operates on the target speech for a given phrase and derives a style embedding through
",False,neekhara21a,False,False,True
17724,"attention over a dictionary of learnable vectors. During inference, the synthesizer can beNeekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
17725,"conditioned on dierent reference audios to produce style variants of speech for the same
",False,neekhara21a,False,False,True
17726,"text. Manipulating these latent style variables during inference oers some coarse control
",False,neekhara21a,False,False,True
17727,"over the style of the synthesized speech. Recently proposed Mellotron model Valle et al.
",False,neekhara21a,False,False,True
17728,"(2020) uses a combination of explicit and latent style variables to oer more ne-grained
",False,neekhara21a,False,False,True
17729,"control over the expressive characteristics of synthesized speech. Specically, Mellotron
",False,neekhara21a,False,False,True
17730,"conditions the spectrogram synthesis network on pitch contour, GSTs Wang et al. (2018)
",False,neekhara21a,False,False,True
17731,"and speaker ID during training. During inference, the synthesizer can be conditioned on
",False,neekhara21a,False,False,True
17732,"the melodic information|pitch and rhythm of a reference speech and synthesize speech
",False,neekhara21a,False,False,True
17733,"in the voice of a given speaker in the training set. The authors demonstrate that explicit
",False,neekhara21a,False,False,True
17734,"conditioning on pitch contour during training phase, makes it possible to generalize the
",False,neekhara21a,False,False,True
17735,"inference on various melodic pitch contours. While Mellotron allows expressive TTS for
",False,neekhara21a,False,False,True
17736,"speakers in the training dataset, since it uses a xed size speaker embedding matrix for
",False,neekhara21a,False,False,True
17737,"speaker conditioning, it cannot be used to generate speech for new speakers.
",False,neekhara21a,False,False,True
17738,"3. Methodology
",False,neekhara21a,False,False,True
17739,"Our expressive voice cloning framework is a multi-speaker TTS model that is conditioned on
",False,neekhara21a,False,False,True
17740,"speaker encodings and style aspects of speech. Style conditioning in expressive TTS models
",False,neekhara21a,False,False,True
17741,"is popularly done by learning a dictionary of latent style vectors called Global Style Tokens
",False,neekhara21a,False,False,True
17742,"(GST) Wang et al. (2018). While GSTs can learn meaningful latent codes when trained on
",False,neekhara21a,False,False,True
17743,"a dataset with high variation in expressions, we empirically nd that it oers limited style
",False,neekhara21a,False,False,True
17744,"control when trained on a large multi-speaker dataset with mostly neutral prosody.
",False,neekhara21a,False,False,True
17745,"Signal processing heuristics like the Yin algorithm De Cheveign e and Kawahara (2002)
",False,neekhara21a,False,False,True
17746,"can derive the fundamental frequency contour (pitch contour) and voicing decisions from
",False,neekhara21a,False,False,True
17747,"speech, which can be useful for expressive speech synthesis. We nd that using a combina-
",False,neekhara21a,False,False,True
17748,"tion of latent and heuristically derived style information in the TTS model not only provides
",False,neekhara21a,False,False,True
17749,"ne-grained control over the style aspects of synthesized speech, but also scales up to a large
",False,neekhara21a,False,False,True
17750,"multi-speaker dataset to produce more natural sounding audio for an unseen speaker. A
",False,neekhara21a,False,False,True
17751,"high level overview of our expressive voice cloning framework is shown in Figure 1. Similar
",False,neekhara21a,False,False,True
17752,"to past works on voice cloning Arik et al. (2018); Jia et al. (2018), the three main compo-
",False,neekhara21a,False,False,True
17753,"nents Speaker Encoder ,Mel Spectrogram Synthesizer andVocoder are all trained separately.
",False,neekhara21a,False,False,True
17754,"We describe the individual components of our framework and their training objectives in
",False,neekhara21a,False,False,True
17755,"the following sections.
",False,neekhara21a,False,False,True
17756,"3.1. Speaker Encoder
",False,neekhara21a,False,False,True
17757,"Speaker conditioning in multi-speaker TTS models is usually done using a lookup in the
",False,neekhara21a,False,False,True
17758,"speaker embedding matrix which is randomly initialized and trained end-to-end with the
",False,neekhara21a,False,False,True
17759,"synthesizer. While such a framework learns speaker-specic information via the embedding
",False,neekhara21a,False,False,True
17760,"vectors, synthesis cannot be generalized to unseen speakers. To adapt the multi-speaker
",False,neekhara21a,False,False,True
17761,"TTS model for the goal of voice cloning, the speaker embedding layer can be replaced with
",False,neekhara21a,False,False,True
17762,"a speaker encoder that derives speaker specic information from the target waveform. In
",False,neekhara21a,False,False,True
17763,"this setting, the speaker encoder can obtain embeddings for speakers not seen during train-
",False,neekhara21a,False,False,True
17764,"ing using a few reference speech samples. To obtain meaningful embeddings, the speaker
",False,neekhara21a,False,False,True
17765,"encoder should be trained to discriminate between dierent speakers for the task of speaker
",False,neekhara21a,False,False,True
17766,"verication Wan et al. (2017).Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
17767,"Concat Text
",False,neekhara21a,False,False,True
17768,"GST
",False,neekhara21a,False,False,True
17769,"Pitch Contour Target Waveform 
",False,neekhara21a,False,False,True
17770,"Encoder Attention Decoder Speaker 
",False,neekhara21a,False,False,True
17771,"Encoder 
",False,neekhara21a,False,False,True
17772,"Vocoder 
",False,neekhara21a,False,False,True
17773,"Synthesized 
",False,neekhara21a,False,False,True
17774,"Waveform Log-mel 
",False,neekhara21a,False,False,True
17775,"spectrogram 
",False,neekhara21a,False,False,True
17776,"Synthesizer 
",False,neekhara21a,False,False,True
17777,"Figure 1: Expressive Voice Cloning Model: Tacotron-2 TTS model conditioned on speaker
",False,neekhara21a,False,False,True
17778,"and style characteristics derived from the target audio of a given text. At inference
",False,neekhara21a,False,False,True
17779,"time, the model can be provided independent references for style and speaker
",False,neekhara21a,False,False,True
17780,"encodings to achieve expressive voice cloning.
",False,neekhara21a,False,False,True
17781,"We follow the speaker encoder architecture described in Wan et al. (2017); Louppe
",False,neekhara21a,False,False,True
17782,"(2019b). The network is a stack of 3 LSTM layers with 256 cells in each layer that operate on
",False,neekhara21a,False,False,True
17783,"mel-spectrograms with 40 channels. The nal speaker embedding is obtained by projecting
",False,neekhara21a,False,False,True
17784,"the LSTM output at the last layer to 256 dimensions followed by L2normalization. Note
",False,neekhara21a,False,False,True
17785,"that ours is a smaller model than that used in Jia et al. (2018) which had 768 cells in
",False,neekhara21a,False,False,True
17786,"each LSTM layer. The speaker encoder is trained to optimize a generalized end-to-end
",False,neekhara21a,False,False,True
17787,"speaker verication loss Wan et al. (2017), that encourages high cosine similarity between
",False,neekhara21a,False,False,True
17788,"embeddings from same speaker and low similarity between dierent speaker embeddings.
",False,neekhara21a,False,False,True
17789,"During inference, each utterance is broken into smaller segments of 1,600 ms with 1,000 ms
",False,neekhara21a,False,False,True
17790,"overlap between consecutive segments. The nal embedding is estimated by averaging the
",False,neekhara21a,False,False,True
17791,"embedding of each individual segment.
",False,neekhara21a,False,False,True
17792,"3.2. Mel-Spectrogram Synthesizer
",False,neekhara21a,False,False,True
17793,"The goal of our synthesis model is to disentangle the style and speaker-specic information
",False,neekhara21a,False,False,True
17794,"in speech by conditioning our TTS synthesis model on the speaker encoding and various
",False,neekhara21a,False,False,True
17795,"style aspects. To this end, we adapt the synthesis model used in Mellotron Valle et al.
",False,neekhara21a,False,False,True
17796,"(2020) for the task of voice cloning. Mellotron is a multi-speaker TTS model that extends
",False,neekhara21a,False,False,True
17797,"Tacotron 2 GST Wang et al. (2018) by additional conditioning on pitch contours and speaker
",False,neekhara21a,False,False,True
17798,"embeddings. To adapt Mellotron for voice cloning, we remove the speaker embedding layer
",False,neekhara21a,False,False,True
17799,"and replace it with the speaker encoder network described in Section 3.1.
",False,neekhara21a,False,False,True
17800,"At its core, our synthesis model based on Tacotron 2 Shen et al. (2018), is an LSTM
",False,neekhara21a,False,False,True
17801,"based sequence-to-sequence model composed of an encoder that operates on a sequence
",False,neekhara21a,False,False,True
17802,"of characters and a decoder that generates the individual frames of the mel spectrogram
",False,neekhara21a,False,False,True
17803,"while attending over the encoded representations. Along with the encoded representation
",False,neekhara21a,False,False,True
17804,"for text, we concatenate the speaker encoding (obtained from the speaker encoder) and the
",False,neekhara21a,False,False,True
17805,"GST embedding at each time-step. The GST embedding is obtained by querying a dictio-
",False,neekhara21a,False,False,True
17806,"nary of latent style vectors with the target mel-spectrogram using a multi-headed attention
",False,neekhara21a,False,False,True
17807,"mechanism described in Wang et al. (2018). Decoding occurs in an autoregressive manner
",False,neekhara21a,False,False,True
17808,"where we synthesize one mel spectrogram frame at a time by providing the fundamental
",False,neekhara21a,False,False,True
17809,"frequency (from the pitch contour) and the mel spectrogram of the previous frame as theNeekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
17810,"input to the decoder. The pitch contours are derived from the target speech using the Yin
",False,neekhara21a,False,False,True
17811,"algorithm with harmonicity thresholds between 0.1 and 0.25.
",False,neekhara21a,False,False,True
17812,"In this way, we can factor mel-spectrogram synthesis into the following variables: text
",False,neekhara21a,False,False,True
17813,"(t),speaker encoding ( s),pitch contour ( f0)and latent style embedding obtained from GST
",False,neekhara21a,False,False,True
17814,"(z). Formally, our synthesizer is a generative model g(t;s;f 0;z;W) that is parameterized
",False,neekhara21a,False,False,True
17815,"by trainable weights W, trained to optimize a loss function Lthat penalizes the dierences
",False,neekhara21a,False,False,True
17816,"between the generated and ground truth mel spectrogram. That is,
",False,neekhara21a,False,False,True
17817,"min
",False,neekhara21a,False,False,True
17818,"WE(ti;ai)DfL(g(ti;si;f0i;zi;W);meli)g (1)
",False,neekhara21a,False,False,True
17819,"whereDis the dataset containing text and audio pairs ( ti;ai). The variables ( si;f0i;zi;meli)
",False,neekhara21a,False,False,True
17820,"are all derived from the target waveform ai. For the loss function L, we use the L2 loss
",False,neekhara21a,False,False,True
17821,"between the generated and ground truth mel spectrograms.
",False,neekhara21a,False,False,True
17822,"During training, the synthesizer learns another latent variable: the attention map be-
",False,neekhara21a,False,False,True
17823,"tween the encoder and decoder states which captures the alignment between text and audio.
",False,neekhara21a,False,False,True
17824,"Following the notation used in Valle et al. (2020), we call this latent variable rhythm , since it
",False,neekhara21a,False,False,True
17825,"controls the timing aspects of synthesized speech. Note that unlike other style aspects which
",False,neekhara21a,False,False,True
17826,"can be obtained directly from ai, deriving rhythm requires both text and audio ( ti;ai). In
",False,neekhara21a,False,False,True
17827,"our experiments, we obtain the rhythm by using our synthesizer as a forced-aligner. That
",False,neekhara21a,False,False,True
17828,"is, for a given text and audio pair, we derive the attention map between the encoder and
",False,neekhara21a,False,False,True
17829,"decoder states by doing a forward pass through our model using teacher forcing. Therefore,
",False,neekhara21a,False,False,True
17830,"during inference, our synthesizer gcan be explicitly conditioned on rhythm rderived from
",False,neekhara21a,False,False,True
17831,"some text and audio pair: g(t;s;f 0;z;r;W).
",False,neekhara21a,False,False,True
17832,"While the style aspects are obtained from the target waveform of the same speaker
",False,neekhara21a,False,False,True
17833,"during training, we can use a dierent reference audio and text pair during inference. For
",False,neekhara21a,False,False,True
17834,"example, we can transfer the pitch contour and rhythm of a style reference audio Sfrom a
",False,neekhara21a,False,False,True
17835,"dierent speaker to the voice of a given target speaker Tas follows:
",False,neekhara21a,False,False,True
17836,"mel=g(tS;sT;f0S;zT;rS;W) (2)
",False,neekhara21a,False,False,True
17837,"The output melshould have the same pitch and rhythm as the style reference Sand should
",False,neekhara21a,False,False,True
17838,"retain the latent style aspects and voice of the target speaker T. In our work we focus on
",False,neekhara21a,False,False,True
17839,"three dierent cloning tasks with dierent sources of style conditioning information which
",False,neekhara21a,False,False,True
17840,"we discuss in Section 4.3.
",False,neekhara21a,False,False,True
17841,"Additionally, to assess the importance of pitch contours during training, we train another
",False,neekhara21a,False,False,True
17842,"TTS model that is conditioned only on the latent style aspects obtained using GST. We use
",False,neekhara21a,False,False,True
17843,"the same Tacotron2 architecture and GST module as our proposed model. Formally, this
",False,neekhara21a,False,False,True
17844,"alternative synthesizer g(t;s;z ;W) is trained to optimize the same objective as Equation 1:
",False,neekhara21a,False,False,True
17845,"min
",False,neekhara21a,False,False,True
17846,"WE(ti;ai)DfL(g(ti;si;zi;W);meli)g (3)
",False,neekhara21a,False,False,True
17847,"We refer to this alternative model as Tacotron2 + GST in our experiments. Similar to our
",False,neekhara21a,False,False,True
17848,"proposed system, this model can also be additionally conditioned on rhythm. Since we are
",False,neekhara21a,False,False,True
17849,"not explicitly conditioning the model on pitch contours, we expect the pitch variation in
",False,neekhara21a,False,False,True
17850,"speech to be captured as part of the latent style tokens. We empirically demonstrate that
",False,neekhara21a,False,False,True
17851,"using only latent style representation on a large multi-speaker dataset with neutral prosody
",False,neekhara21a,False,False,True
17852,"oers limited style control and audio naturalness.Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
17853,"Vocoder: For decoding the synthesized mel-spectrograms into listenable waveforms,
",False,neekhara21a,False,False,True
17854,"we use the WaveGlow Prenger et al. (2018) model trained on the single speaker Sally
",False,neekhara21a,False,False,True
17855,"dataset Valle et al. (2020). An advantage of WaveGlow over WaveNet van den Oord et al.
",False,neekhara21a,False,False,True
17856,"(2016) is that it allows real-time inference, while being competitive in terms of audio nat-
",False,neekhara21a,False,False,True
17857,"uralness. The same vocoder model is used across all experiments and datasets. We nd
",False,neekhara21a,False,False,True
17858,"that the vocoder model trained on a single speaker generalizes well across all speakers in
",False,neekhara21a,False,False,True
17859,"our datasets.
",False,neekhara21a,False,False,True
17860,"3.3. Cloning Techniques: Zero-Shot and Model Adaptation
",False,neekhara21a,False,False,True
17861,"We adopt the following two approaches for cloning the voice of a new speaker from a few
",False,neekhara21a,False,False,True
17862,"transcribed or untranscribed speech samples:
",False,neekhara21a,False,False,True
17863,"Zero-Shot: For zero-shot voice cloning, we derive the speaker embedding by taking the
",False,neekhara21a,False,False,True
17864,"mean followed by L-2 normalization of the speaker encodings of the individual samples of
",False,neekhara21a,False,False,True
17865,"the target speaker. Since speaker encodings are obtained directly from the waveforms, we
",False,neekhara21a,False,False,True
17866,"do not require audio transcriptions of the new speaker for zero-shot voice cloning.
",False,neekhara21a,False,False,True
17867,"Model Adaptation: When transcribed samples of a new speaker are available, we
",False,neekhara21a,False,False,True
17868,"can ne-tune our synthesis model using the text and audio pairs. As shown in Neural
",False,neekhara21a,False,False,True
17869,"Voice Cloning Arik et al. (2018), ne-tuning can signicantly improve the speaker similarity
",False,neekhara21a,False,False,True
17870,"metrics of the cloned speech. Also, the authors of Arik et al. (2018) observe that ne-tuning
",False,neekhara21a,False,False,True
17871,"the whole synthesis model is faster and more eective than ne-tuning only the speaker
",False,neekhara21a,False,False,True
17872,"embedding layer since more degrees of freedom are allowed in the whole model adaptation.
",False,neekhara21a,False,False,True
17873,"Our preliminary experiments on model adaptation suggested the same. We hypothesize the
",False,neekhara21a,False,False,True
17874,"reason for this is that ne-tuning the last-few layers of the synthesis model is essential, if not
",False,neekhara21a,False,False,True
17875,"sucient, to adapt the synthesizer to the speaker-specic speech characteristics. Therefore,
",False,neekhara21a,False,False,True
17876,"we study the following two model adaptation techniques: Adaptation whole - Fine-tune
",False,neekhara21a,False,False,True
17877,"all the parameters of the synthesis model on the text and audio pairs of the new speaker.
",False,neekhara21a,False,False,True
17878,"Adaptation decoder - Fine-tune only the decoder parameters of the synthesis model. The
",False,neekhara21a,False,False,True
17879,"advantage of only adapting the decoder parameters is that it requires fewer speaker-specic
",False,neekhara21a,False,False,True
17880,"model parameters and a shared encoder can be used across all speakers in a real-world
",False,neekhara21a,False,False,True
17881,"deployment setting. In both of the above adaptation settings, we ne-tune our model for
",False,neekhara21a,False,False,True
17882,"100 to 200 iterations using Adam optimizer with a learning rate of 1e-4. Model adaptation
",False,neekhara21a,False,False,True
17883,"takes up to 6 minutes for ne-tuning on 1 to 20 samples of the target speaker on a single
",False,neekhara21a,False,False,True
17884,"Nvidia Titan 1080 GPU.
",False,neekhara21a,False,False,True
17885,"4. Experiments
",False,neekhara21a,False,False,True
17886,"4.1. Datasets and Training
",False,neekhara21a,False,False,True
17887,"We train our mel-spectrogram synthesis model on the clean subset of the publicly available
",False,neekhara21a,False,False,True
17888,"Libri-TTS Zen et al. (2019) dataset| train-clean-100 andtrain-clean-360 . This clean subset
",False,neekhara21a,False,False,True
17889,"contains around 245 hours of speech across 1151 speakers sampled at 24 kHz. Past works
",False,neekhara21a,False,False,True
17890,"on voice cloning Wan et al. (2017); Arik et al. (2018) trained their synthesis models on the
",False,neekhara21a,False,False,True
17891,"LibriSpeech dataset Panayotov et al. (2015) and empirically demonstrated the importance
",False,neekhara21a,False,False,True
17892,"of a speaker-diverse training dataset for the task of voice cloning. We lter out utterances
",False,neekhara21a,False,False,True
17893,"longer than 10 seconds and resample waveforms to 22050 Hz.Neekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
17894,"For training the synthesizer, we warm start our model using the pre-trained Mellotron
",False,neekhara21a,False,False,True
17895,"checkpoint which is trained on a subset of LibriTTS containing 123 speakers. The speaker
",False,neekhara21a,False,False,True
17896,"embedding layer is replaced with our speaker encoding network which is kept frozen during
",False,neekhara21a,False,False,True
17897,"training. We use a validation set with 250 examples and train the model using a batch
",False,neekhara21a,False,False,True
17898,"size of 32 and an initial learning rate of 5e-4. We use an Adam optimizer Kingma and
",False,neekhara21a,False,False,True
17899,"Ba (2015) to update the weights and anneal the learning rate to half its value every 50k
",False,neekhara21a,False,False,True
17900,"mini-batch iterations. We include details of our model architecture and hyper-parameters
",False,neekhara21a,False,False,True
17901,"in Section 4.2 and point to our codebase2for precise model implementation.
",False,neekhara21a,False,False,True
17902,"For the Tacotron 2 + GST model, we use the same Tacotron 2 architecture and GST
",False,neekhara21a,False,False,True
17903,"hyper-parameters as our proposed model. Training for the proposed model and the Tacotron
",False,neekhara21a,False,False,True
17904,"2 + GST model converged in 210,000 and 185,000 mini-batch iterations respectively and
",False,neekhara21a,False,False,True
17905,"took around 4 seconds per iteration on a single Nvidia Titan 1080 GPU. The Resemblyzer
",False,neekhara21a,False,False,True
17906,"speaker encoder Louppe (2019b,a) used in our experiments is trained on the VoxCeleb Na-
",False,neekhara21a,False,False,True
17907,"grani et al. (2019), VoxCeleb2 Chung et al. (2018) and LibriSpeech-other Panayotov et al.
",False,neekhara21a,False,False,True
17908,"(2015) datasets containing a total of 8.4k speakers. The authors of Louppe (2019a) report a
",False,neekhara21a,False,False,True
17909,"4.5% Equal Error Rate (EER) for the task of speaker verication using this speaker encoder
",False,neekhara21a,False,False,True
17910,"on their internal test set.
",False,neekhara21a,False,False,True
17911,"4.2. Model architecture and hyper-parameter details
",False,neekhara21a,False,False,True
17912,"Our spectrogram synthesizer is closely follows the Tacotron-2 architecture Shen et al. (2018).
",False,neekhara21a,False,False,True
17913,"The model is composed of an encoder and decoder with attention. The encoder processes
",False,neekhara21a,False,False,True
17914,"the input sequence using a stack of 3 1-d convolution layers followed by a bi-directional
",False,neekhara21a,False,False,True
17915,"LSTM. The input tokens are embedded using a 512 dimensional embedding layer. Each
",False,neekhara21a,False,False,True
17916,"convolutional layer has 512 lters with a kernel size of 5 followed by batch normalization and
",False,neekhara21a,False,False,True
17917,"ReLU activation functions. The single bidirectional LSTM consists of 256 hidden units in
",False,neekhara21a,False,False,True
17918,"each direction leading to a 512 dimensional embedding at each time-step. At each time-step
",False,neekhara21a,False,False,True
17919,"of the encoder output, we concatenate the speaker encoding and the GST embedding. The
",False,neekhara21a,False,False,True
17920,"GST module we use follows the same architecture as that proposed in Wang et al. (2018)
",False,neekhara21a,False,False,True
17921,"except that we use 8 attention heads instead of 4.
",False,neekhara21a,False,False,True
17922,"The attention module follows the location sensitive attention procedure and hyper-
",False,neekhara21a,False,False,True
17923,"parameters described in Shen et al. (2018). The decoder network is an autoregressive
",False,neekhara21a,False,False,True
17924,"network that takes in as input the spectrogam frame and fundamental frequency of the
",False,neekhara21a,False,False,True
17925,"previous time-step. The spectrogram frame goes through a pre-net module which consists
",False,neekhara21a,False,False,True
17926,"of 2 fully connected layers with 256 hidden units and ReLU activation. The pre-net module
",False,neekhara21a,False,False,True
17927,"acts as an information bottleneck which is essential for learning attention Shen et al. (2018).
",False,neekhara21a,False,False,True
17928,"The decoder is a stack of 2 convolutional layers with 1024 units in each layer. The nal
",False,neekhara21a,False,False,True
17929,"predicted mel-spectrogram is passed through a 5 layer convolutional post-net which predicts
",False,neekhara21a,False,False,True
17930,"a residual that can be added to improve overall reconstruction error. Each post-net layer
",False,neekhara21a,False,False,True
17931,"uses 512 lters with kernel width of 5 followed by batch normalization and tanh activation
",False,neekhara21a,False,False,True
17932,"function.
",False,neekhara21a,False,False,True
17933,"We use Adam Kingma and Ba (2015) optimizer with hyper-parameters with an initial
",False,neekhara21a,False,False,True
17934,"learning rate 1e-4 and 1= 0:9;2= 0:999. During training, we use a mini-batch size of 32
",False,neekhara21a,False,False,True
17935,"and anneal learning rate to half its value every 50k mini-batch iterations.
",False,neekhara21a,False,False,True
17936,"2. https://expressivecloning.github.io/Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
17937,"Character 
",False,neekhara21a,False,False,True
17938,"Embedding 3 Conv Layers Bidirectional 
",False,neekhara21a,False,False,True
17939,"LSTM Attention 
",False,neekhara21a,False,False,True
17940,"Layer Concat Speaker Encoding 
",False,neekhara21a,False,False,True
17941,"GST embedding 
",False,neekhara21a,False,False,True
17942,"2 - LSTM layers 
",False,neekhara21a,False,False,True
17943,"Decoder Previous mel-spectrogram frame 
",False,neekhara21a,False,False,True
17944,"Previous F0Linear Projection 5 Conv layer 
",False,neekhara21a,False,False,True
17945,"Post-net +Output mel-spectrogram 
",False,neekhara21a,False,False,True
17946,"2-layer 
",False,neekhara21a,False,False,True
17947,"Pre-net 
",False,neekhara21a,False,False,True
17948,"Figure 2: Mel-spectrogram synthesizer model architecture
",False,neekhara21a,False,False,True
17949,"4.3. Cloning Tasks
",False,neekhara21a,False,False,True
17950,"In this section, we discuss the three main tasks for which we evaluate our voice cloning
",False,neekhara21a,False,False,True
17951,"methods. When cloning the voice of a new speaker, we require a few audio samples of the
",False,neekhara21a,False,False,True
17952,"speaker to obtain the speaker encoding. We refer to these samples as target speaker samples .
",False,neekhara21a,False,False,True
17953,"We perform voice cloning for the speakers in the VCTK dataset Veaux et al. (2017). The
",False,neekhara21a,False,False,True
17954,"VCTK dataset contains speech sampled at 48 KHz from 108 native English speakers, the
",False,neekhara21a,False,False,True
17955,"majority of which have British accents. We down-sampled the audio to 22,050 KHz to
",False,neekhara21a,False,False,True
17956,"make it consistent with our training data. To synthesize the speech for a given speaker
",False,neekhara21a,False,False,True
17957,"encoding and text, our synthesis model additionally requires various style conditioning
",False,neekhara21a,False,False,True
17958,"variables described in Section 3.2. While the latent GST embedding can be obtained from
",False,neekhara21a,False,False,True
17959,"thetarget speaker samples , pitch contour and rhythm information needs to be derived from
",False,neekhara21a,False,False,True
17960,"astyle reference audio that corresponds to the given text. In case we do not have a style
",False,neekhara21a,False,False,True
17961,"reference audio available, we can synthesize one using a single speaker TTS system. To
",False,neekhara21a,False,False,True
17962,"evaluate our cloning techniques objectively in terms of style and speaker disentanglement,
",False,neekhara21a,False,False,True
17963,"and also assess their usefulness in real world settings, we perform the following cloning
",False,neekhara21a,False,False,True
17964,"tasks:
",False,neekhara21a,False,False,True
17965,"1. Text Cloning speech directly from text: For cloning speech directly from text, we
",False,neekhara21a,False,False,True
17966,"rst synthesize speech for the given text using a single speaker TTS model: Tacotron 2 +
",False,neekhara21a,False,False,True
17967,"WaveGlow trained on the LJ Speech Ito (2017) dataset. We then derive the pitch contour
",False,neekhara21a,False,False,True
17968,"of the synthetic speech using the Yin algorithm De Cheveign e and Kawahara (2002) and
",False,neekhara21a,False,False,True
17969,"scale the pitch contour linearly to have the same mean pitch as that of the target speaker
",False,neekhara21a,False,False,True
17970,"samples . For deriving rhythm, we use our proposed synthesis model as a forced aligner
",False,neekhara21a,False,False,True
17971,"between the text and Tacotron2-synthesized speech. We use the target speaker samples for
",False,neekhara21a,False,False,True
17972,"obtaining the GST embedding for both our proposed model and the baseline Tacotron2 +
",False,neekhara21a,False,False,True
17973,"GST model.
",False,neekhara21a,False,False,True
17974,"2. Imitation - Reconstruct a sample from the target speaker: In this setup, we use a text
",False,neekhara21a,False,False,True
17975,"and audio pair of the target speaker (not contained in the target speaker samples ), and try
",False,neekhara21a,False,False,True
17976,"to reconstruct the audio from its factorized representation using our synthesis model. All of
",False,neekhara21a,False,False,True
17977,"the style conditioning variables - pitch, rhythm and GST embedding are derived from theNeekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
17978,"speech sample we are trying to imitate. The imitation task is a toy experiment that allows
",False,neekhara21a,False,False,True
17979,"quantitative evaluation of style similarity metrics between the synthesized speech and style
",False,neekhara21a,False,False,True
17980,"reference.
",False,neekhara21a,False,False,True
17981,"3. Style Transfer - Transfer the pitch and rhythm of speech from a dierent expressive
",False,neekhara21a,False,False,True
17982,"speaker: The goal of this task is to transfer the pitch and rhythm from some expressive
",False,neekhara21a,False,False,True
17983,"speech to the cloned speech for the target speaker. For this task, we use examples from the
",False,neekhara21a,False,False,True
17984,"single speaker Blizzard 2013 dataset King and Karaiskos (2013) as style references. This
",False,neekhara21a,False,False,True
17985,"dataset contains expressive audio book readings from a single speaker with high variation
",False,neekhara21a,False,False,True
17986,"in emotion and pitch. For our proposed model, we use this style reference audio to extract
",False,neekhara21a,False,False,True
17987,"the pitch and rhythm. Similar to the Text task, we scale the pitch contour to have the same
",False,neekhara21a,False,False,True
17988,"mean as that of the target speaker samples . In-order to retain speaker-specic latent style
",False,neekhara21a,False,False,True
17989,"aspects, we use target speaker samples to extract the GST embedding. For the Tacotron2
",False,neekhara21a,False,False,True
17990,"+ GST model, which does not have explicit pitch conditioning, we use the style reference
",False,neekhara21a,False,False,True
17991,"audio for obtaining the GST embedding and the rhythm.
",False,neekhara21a,False,False,True
17992,"4.4. Results
",False,neekhara21a,False,False,True
17993,"For the above described cloning tasks, we evaluate three aspects of the cloned speech: i)
",False,neekhara21a,False,False,True
17994,"speaker similarity to the target speaker, ii) style similarity to the reference style and iii)
",False,neekhara21a,False,False,True
17995,"speech naturalness. We encourage the readers to listen to our audio examples referenced in
",False,neekhara21a,False,False,True
17996,"the footnote of the rst page to contextualize the following results.
",False,neekhara21a,False,False,True
17997,"Speaker Classication Accuracy: We train a speaker classier on the VCTK dataset
",False,neekhara21a,False,False,True
17998,"to classify a given utterance as one of the 108 speakers. The speaker classier is a two layer
",False,neekhara21a,False,False,True
17999,"neural network with 256 hidden units that takes as input the speaker encoding obtained
",False,neekhara21a,False,False,True
18000,"through our pre-trained speaker encoder network. Similar to Arik et al. (2018), our speaker
",False,neekhara21a,False,False,True
18001,"classier achieves 100% accuracy on a hold out set containing 200 examples from the VCTK
",False,neekhara21a,False,False,True
18002,"dataset. However, since our classication model and training dataset for the synthesizer
",False,neekhara21a,False,False,True
18003,"are not the same as Arik et al. (2018) (1,151 speakers in ours vs. 2,481 speakers in Arik
",False,neekhara21a,False,False,True
18004,"et al. (2018)), we do not make direct comparisons with their work. We conduct our speaker
",False,neekhara21a,False,False,True
18005,"classication evaluations on all 108 speakers of the VCTK dataset. We clone 25 speech
",False,neekhara21a,False,False,True
18006,"samples per speaker for each task described in Section 4.3. Figure 3 (left) shows the speaker
",False,neekhara21a,False,False,True
18007,"classication accuracy curves for all cloning tasks and techniques with respect to the number
",False,neekhara21a,False,False,True
18008,"of target speaker samples. Our results are consistent with the following ndings of Arik et al.
",False,neekhara21a,False,False,True
18009,"(2018)|Model adaptation signicantly outperforms the zero-shot voice cloning technique
",False,neekhara21a,False,False,True
18010,"since it allows the model to adjust to the speaker characteristics of the new speaker. More
",False,neekhara21a,False,False,True
18011,"target speaker samples helps improve speaker classication accuracy, although in the zero-
",False,neekhara21a,False,False,True
18012,"shot scenario we do not observe much improvement after 10 target speaker samples.
",False,neekhara21a,False,False,True
18013,"For zero-shot voice cloning, both Tacotron2-GST and our proposed model achieve similar
",False,neekhara21a,False,False,True
18014,"speaker classication accuracy for Text and Style Transfer cloning tasks. The accuracy of
",False,neekhara21a,False,False,True
18015,"our proposed model is slightly higher for the imitation task as compared to other tasks
",False,neekhara21a,False,False,True
18016,"for both model adaptation and zero-shot voice cloning. This implies that conditioning on
",False,neekhara21a,False,False,True
18017,"the actual pitch contour of the target speaker improves speaker specic characteristics of
",False,neekhara21a,False,False,True
18018,"the cloned speech. While linear scaling of a reference style pitch contour works well, our
",False,neekhara21a,False,False,True
18019,"ndings motivate future research on predicting speaker-specic pitch contours from text
",False,neekhara21a,False,False,True
18020,"and speaker encodings.Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
18021,"1510200102030405060708090Speaker Classification Accuracy ( %)Number of Target Speaker Samples  Tacotron2 + GST Zeroshot - Text Tacotron2 + GST Zeroshot - Imitation Tacotron2 + GST Zeroshot - Style Transfer Zero-shot - Text Zero-shot - Imitation Zero-shot - Style Transfer Adaption Decoder - Text Adaption Decoder - Imitation Adaption Decoder - Style Transfer Adaption Whole - Text Adaption Whole - Imitation Adaption Whole - Style Transfer
",False,neekhara21a,False,False,True
18022,"15102002468101214161820SV -Equal Error Rate (%)Number of Target Speaker SamplesVCTK Real
",False,neekhara21a,False,False,True
18023,"Figure 3: Speaker similarity evaluation of each cloning technique for dierent voice cloning
",False,neekhara21a,False,False,True
18024,"tasks in terms of Speaker Classication Accuracy and Speaker Verication Equal
",False,neekhara21a,False,False,True
18025,"Error Rate (SV-EER).
",False,neekhara21a,False,False,True
18026,"Speaker verication Equal Error Rate (SV-EER): SV-EER is another objective
",False,neekhara21a,False,False,True
18027,"metric used to evaluate speaker similarity between the cloned audio and the ground-truth
",False,neekhara21a,False,False,True
18028,"reference audio. We use a speaker verication system that scores the speaker similarity
",False,neekhara21a,False,False,True
18029,"between two utterances based on the cosine similarity of the encodings obtained using the
",False,neekhara21a,False,False,True
18030,"speaker encoder described in Section 3.1. Equal Error Rate (EER) is the point when the
",False,neekhara21a,False,False,True
18031,"false acceptance rate and false rejection rate of the speaker verication system are equal.
",False,neekhara21a,False,False,True
18032,"We perform speaker verication evaluations on randomly selected 20 speakers in the
",False,neekhara21a,False,False,True
18033,"VCTK dataset. We enroll 5 speech samples per speaker in the speaker verication system
",False,neekhara21a,False,False,True
18034,"and synthesize 50 speech samples per speaker for each cloning task. EERs are estimated
",False,neekhara21a,False,False,True
18035,"by pairing each sample of the same speaker with another sample from a dierent speaker.
",False,neekhara21a,False,False,True
18036,"Figure 3 shows the plots of SV-EER for dierent cloning techniques and tasks using our
",False,neekhara21a,False,False,True
18037,"proposed model and also the those estimated using real data. Our observations on the
",False,neekhara21a,False,False,True
18038,"SV-EER metric are similar to those on the accuracy metric. Model adaptation outperforms
",False,neekhara21a,False,False,True
18039,"zero-shot cloning techniques and with more than 10 cloning samples achieves similar EER
",False,neekhara21a,False,False,True
18040,"as the real data. Additionally, we include human evaluation scores on speaker similarity in
",False,neekhara21a,False,False,True
18041,"our supplementary material.
",False,neekhara21a,False,False,True
18042,"Style Similarity: In order to evaluate the similarity between the style of synthesized
",False,neekhara21a,False,False,True
18043,"and reference audio, we perform quantitative evaluation on the Imitation task described
",False,neekhara21a,False,False,True
18044,"in Section 4.3. We use the following error metrics: Gross Pitch Error (GPE) Nakatani
",False,neekhara21a,False,False,True
18045,"et al. (2008), Voicing Decision Error (VDE) Nakatani et al. (2008) and F0 Frame Error
",False,neekhara21a,False,False,True
18046,"(FFE) Chu and Alwan (2009). Results are presented in Table 1 in which we compare the
",False,neekhara21a,False,False,True
18047,"error values for dierent approaches when using 10 target speaker samples for cloning. We
",False,neekhara21a,False,False,True
18048,"synthesize 25 speech samples per speaker for all speakers in the VCTK dataset to estimate
",False,neekhara21a,False,False,True
18049,"the reported error values. Our proposed models signicantly outperform the Tacotron 2 +
",False,neekhara21a,False,False,True
18050,"GST baseline, clearly indicating the importance of pitch contour conditioning for accurate
",False,neekhara21a,False,False,True
18051,"style transfer.Neekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
18052,"Imitation Style Transfer
",False,neekhara21a,False,False,True
18053,"Approach GPE VDE FFE Style-MOS
",False,neekhara21a,False,False,True
18054,"Tacotron2 + GST - Zero-shot 20.37% 26.39% 29.47% 2:690:11
",False,neekhara21a,False,False,True
18055,"Proposed Model - Zero-shot 3.72% 10.65% 11.74% 3:150:11
",False,neekhara21a,False,False,True
18056,"Proposed Model - Adaptation Whole 2.97% 12.58% 13.60% 3:400:10
",False,neekhara21a,False,False,True
18057,"Proposed Model - Adaptation Decoder 2.39% 11.60% 12.51% 3:290:10
",False,neekhara21a,False,False,True
18058,"Table 1: Style similarity evaluations for the imitation and style transfer tasks. We use
",False,neekhara21a,False,False,True
18059,"three objective error metrics (lower values are better). For the style transfer task
",False,neekhara21a,False,False,True
18060,"we present the mean opinion scores on style similarity (Style-MOS) with 95%
",False,neekhara21a,False,False,True
18061,"condence interval.
",False,neekhara21a,False,False,True
18062,"We also conduct a crowd-sourced listening test on Amazon Mechanical Turk (AMT)
",False,neekhara21a,False,False,True
18063,"for the style transfer task in which we ask the listeners to rate the style similarity between
",False,neekhara21a,False,False,True
18064,"the ground truth style reference and synthesized audio on a 5 point scale (interface for
",False,neekhara21a,False,False,True
18065,"this study is included in the supplementary material). For each cloning technique (using
",False,neekhara21a,False,False,True
18066,"10 target speaker samples), we synthesize 25 audio samples per speaker for 20 speakers
",False,neekhara21a,False,False,True
18067,"in the VCTK dataset leading to 500 evaluations of each technique. We present the style
",False,neekhara21a,False,False,True
18068,"similarity Mean Opinion Scores (Style-MOS) in Table 1. It can be seen that our proposed
",False,neekhara21a,False,False,True
18069,"models signicantly outperform the Tacotron 2 + GST model. Model adaptation techniques
",False,neekhara21a,False,False,True
18070,"perform slightly better than zero-shot method suggesting that ne-tuning improves the
",False,neekhara21a,False,False,True
18071,"model predictions for an unseen speaker encoding.
",False,neekhara21a,False,False,True
18072,"Naturalness: To assess speech naturalness, we conducted a crowd-sourced listening
",False,neekhara21a,False,False,True
18073,"test on AMT and asked listeners to rate each audio utterance on a 5-point naturalness
",False,neekhara21a,False,False,True
18074,"scale to collect Mean Opinion Scores (MOS). Similar to the above mentioned user study,
",False,neekhara21a,False,False,True
18075,"we use 10 target speaker samples for each cloning technique. All evaluations are conducted
",False,neekhara21a,False,False,True
18076,"on randomly selected 20 VCTK speakers with 25 audio samples synthesized per speaker.
",False,neekhara21a,False,False,True
18077,"Each sample is rated independently by a single listener leading to 500 evaluations for each
",False,neekhara21a,False,False,True
18078,"technique per cloning task. We report the MOS of Real data and audio synthesized using
",False,neekhara21a,False,False,True
18079,"dierent cloning techniques in Table 2. Our proposed model signicantly outperforms the
",False,neekhara21a,False,False,True
18080,"baseline Tacotron2 + GST model for both zero-shot and model adaptation techniques.
",False,neekhara21a,False,False,True
18081,"This suggests that pitch contour conditioning in a multi-speaker setting helps improve
",False,neekhara21a,False,False,True
18082,"speech naturalness in addition to providing higher style similarity. It can be seen that the
",False,neekhara21a,False,False,True
18083,"naturalness is even further improved with model adaptation techniques since it allows the
",False,neekhara21a,False,False,True
18084,"generative model to adjust for the unseen speaker encodings.
",False,neekhara21a,False,False,True
18085,"5. Broader Impact
",False,neekhara21a,False,False,True
18086,"Speech interfaces enable hands-free operation and can assist users who are visually or phys-
",False,neekhara21a,False,False,True
18087,"ically impaired. Research into machine generation of speech is driven by the prospect of
",False,neekhara21a,False,False,True
18088,"oering services where humans interact solely with machines, thereby eliminating the cost
",False,neekhara21a,False,False,True
18089,"of live agents and signicantly reducing the cost of providing services. Since speech serves
",False,neekhara21a,False,False,True
18090,"as a primary communication interface between machine learning agents and humans, the
",False,neekhara21a,False,False,True
18091,"ability to speak expressively in a new voice can help create more personalized machine assis-Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
18092,"Cloning Task
",False,neekhara21a,False,False,True
18093,"Approach Text Imitation Style Transfer
",False,neekhara21a,False,False,True
18094,"Real data VCTK 4 :110:08
",False,neekhara21a,False,False,True
18095,"Real data Blizzard 4 :070:08
",False,neekhara21a,False,False,True
18096,"Tacotron2 + GST - Zero-shot 2 :670:10 2 :510:10 3 :020:09
",False,neekhara21a,False,False,True
18097,"Proposed Model - Zero-shot 3 :560:09 3 :540:10 3 :530:10
",False,neekhara21a,False,False,True
18098,"Proposed Model - Adaptation Whole 3 :750:09 3 :710:09 3 :600:09
",False,neekhara21a,False,False,True
18099,"Proposed Model - Adaptation Decoder 3 :610:09 3 :570:09 3 :450:09
",False,neekhara21a,False,False,True
18100,"Table 2: Mean Opinion Score (MOS) for speech naturalness with 95% condence intervals.
",False,neekhara21a,False,False,True
18101,"tants. Furthermore, such systems can also empower individuals who have lost their ability
",False,neekhara21a,False,False,True
18102,"to speak.
",False,neekhara21a,False,False,True
18103,"Explicit control over the style aspects of cloned speech is also desirable for several
",False,neekhara21a,False,False,True
18104,"multimedia applications. These include: voice overs in animated lms, synthesizing realistic
",False,neekhara21a,False,False,True
18105,"and expressive speech for videos, translating speech from one language to another while
",False,neekhara21a,False,False,True
18106,"preserving the speaking style and speaker identity, advertisement and political campaigns
",False,neekhara21a,False,False,True
18107,"with expressive speech in multiple voices or languages, etc.
",False,neekhara21a,False,False,True
18108,"Our intent for generating expressive speech is to advance the research of synthetic audio
",False,neekhara21a,False,False,True
18109,"generation, such that it can aid in the accessibility of speech interfaces and support users
",False,neekhara21a,False,False,True
18110,"with speech impairments, as well as contribute to mainstream use in movies, digital story-
",False,neekhara21a,False,False,True
18111,"telling and modern-day streaming services. This work provides us with an opportunity to
",False,neekhara21a,False,False,True
18112,"collaborate with researchers for advancing multi-disciplinary investigation of AI techniques.
",False,neekhara21a,False,False,True
18113,"However, any emerging technology can also be abused. Realistic voice cloning technology
",False,neekhara21a,False,False,True
18114,"can be used to create voice-overs for subjects of DeepFake videos, and has the potential to
",False,neekhara21a,False,False,True
18115,"be used maliciously to spread disinformation or for creating inappropriate content. Also,
",False,neekhara21a,False,False,True
18116,"the technology can be abused for circumventing speech based user authentication systems
",False,neekhara21a,False,False,True
18117,"in smart devices. We seek to discourage the unethical use of our technology. Upon the
",False,neekhara21a,False,False,True
18118,"release of public access to our voice cloning app, we plan to incorporate techniques to wa-
",False,neekhara21a,False,False,True
18119,"termark the speech generated by our model. This will allow us to thwart the misuse of our
",False,neekhara21a,False,False,True
18120,"technology and curb any spread of misinformation using our platform. It is our intention
",False,neekhara21a,False,False,True
18121,"to develop Expressive Voice Cloning in a way that its potential for abuse is minimized and
",False,neekhara21a,False,False,True
18122,"maximise its use as a tool for learning, education and experimentation.
",False,neekhara21a,False,False,True
18123,"6. Conclusion and Future Work
",False,neekhara21a,False,False,True
18124,"In this work we introduce an expressive voice cloning and dene three benchmark tasks
",False,neekhara21a,False,False,True
18125,"to evaluate such systems. We empirically nd that learning only latent style tokens is
",False,neekhara21a,False,False,True
18126,"insucient to capture expressiveness in speech when training the synthesis model on a
",False,neekhara21a,False,False,True
18127,"speaker-diverse dataset with mostly neutral prosody. Our proposed model uses a combina-
",False,neekhara21a,False,False,True
18128,"tion of heuristically derived and latent style information, which not only oers ne-grained
",False,neekhara21a,False,False,True
18129,"control over style aspects but also improves speech naturalness. We demonstrate that our
",False,neekhara21a,False,False,True
18130,"proposed model can successfully extract and transfer style and speaker characteristics fromNeekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
18131,"unseen audio references to the synthesized speech. We recommend future works on models
",False,neekhara21a,False,False,True
18132,"for predicting speaker specic pitch contours directly from style labels (like happy ,sad,
",False,neekhara21a,False,False,True
18133,"neutral etc) and text to allow control over expressions of the synthesized speech when a
",False,neekhara21a,False,False,True
18134,"style reference audio is not available.
",False,neekhara21a,False,False,True
18135,"References
",False,neekhara21a,False,False,True
18136,"Sercan Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. Neural voice cloning
",False,neekhara21a,False,False,True
18137,"with a few samples. In NeurIPS . 2018.
",False,neekhara21a,False,False,True
18138,"Mengnan Chen, Minchuan Chen, Shuang Liang, Jun Ma, Lei Chen, Shaojun Wang, and
",False,neekhara21a,False,False,True
18139,"Jing Xiao. Cross-lingual, multi-speaker text-to-speech synthesis using neural speaker
",False,neekhara21a,False,False,True
18140,"embedding. In INTERSPEECH , 2019.
",False,neekhara21a,False,False,True
18141,"Wei Chu and Abeer Alwan. Reducing f0 frame error of f0 tracking algorithms under noisy
",False,neekhara21a,False,False,True
18142,"conditions with an unvoiced/voiced classication frontend. In ICASSP . IEEE, 2009.
",False,neekhara21a,False,False,True
18143,"J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In
",False,neekhara21a,False,False,True
18144,"INTERSPEECH , 2018.
",False,neekhara21a,False,False,True
18145,"E. Cooper, C. Lai, Y. Yasuda, F. Fang, X. Wang, N. Chen, and J. Yamagishi. Zero-
",False,neekhara21a,False,False,True
18146,"shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings. In
",False,neekhara21a,False,False,True
18147,"ICASSP , 2020.
",False,neekhara21a,False,False,True
18148,"Alain De Cheveign e and Hideki Kawahara. Yin, a fundamental frequency estimator for
",False,neekhara21a,False,False,True
18149,"speech and music. 2002.
",False,neekhara21a,False,False,True
18150,"Andrew Gibiansky, Sercan Arik, Gregory Diamos, John Miller, Kainan Peng, Wei Ping,
",False,neekhara21a,False,False,True
18151,"Jonathan Raiman, and Yanqi Zhou. Deep voice 2: Multi-speaker neural text-to-speech.
",False,neekhara21a,False,False,True
18152,"InNIPS , 2017.
",False,neekhara21a,False,False,True
18153,"Daniel W. Grin, Jae, S. Lim, and Senior Member. Signal estimation from modied short-
",False,neekhara21a,False,False,True
18154,"time Fourier transform. IEEE Trans. Acoustics, Speech and Sig. Proc , 1984.
",False,neekhara21a,False,False,True
18155,"Y. Huang, L. He, W. Wei, W. Gale, J. Li, and Y. Gong. Using personalized speech synthesis
",False,neekhara21a,False,False,True
18156,"and neural language generator for rapid speaker adaptation. In ICASSP , 2020.
",False,neekhara21a,False,False,True
18157,"Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/ , 2017.
",False,neekhara21a,False,False,True
18158,"Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, zhifeng Chen, Patrick
",False,neekhara21a,False,False,True
18159,"Nguyen, Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu. Transfer learning from
",False,neekhara21a,False,False,True
18160,"speaker verication to multispeaker text-to-speech synthesis. In NeurIPS . 2018.
",False,neekhara21a,False,False,True
18161,"Simon J. King and Vasilis Karaiskos. The blizzard challenge 2013. In In Blizzard Challenge
",False,neekhara21a,False,False,True
18162,"Workshop , 2013.
",False,neekhara21a,False,False,True
18163,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR ,
",False,neekhara21a,False,False,True
18164,"2015.
",False,neekhara21a,False,False,True
18165,"Gilles Louppe. Master thesis : Automatic multispeaker voice cloning. 2019a.Expressive Neural Voice Cloning
",False,neekhara21a,False,False,True
18166,"Gilles Louppe. Resemblyzer - https://github.com/resemble-ai/Resemblyzer/ , 2019b. URL
",False,neekhara21a,False,False,True
18167,"https://github.com/resemble-ai/Resemblyzer/ .
",False,neekhara21a,False,False,True
18168,"Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. Voxceleb: Large-scale
",False,neekhara21a,False,False,True
18169,"speaker verication in the wild. Computer Science and Language , 2019.
",False,neekhara21a,False,False,True
18170,"Tomohiro Nakatani, Shigeaki Amano, Toshio Irino, Kentaro Ishizuka, and Tadahisa Kondo.
",False,neekhara21a,False,False,True
18171,"A method for fundamental frequency estimation and voicing decision: Application to
",False,neekhara21a,False,False,True
18172,"infant utterances recorded in real acoustical environments. Speech Communication , 2008.
",False,neekhara21a,False,False,True
18173,"Paarth Neekhara, Chris Donahue, Miller Puckette, Shlomo Dubnov, and Julian McAuley.
",False,neekhara21a,False,False,True
18174,"Expediting TTS synthesis with adversarial vocoding. In INTERSPEECH , 2019.
",False,neekhara21a,False,False,True
18175,"Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an
",False,neekhara21a,False,False,True
18176,"asr corpus based on public domain audio books. In ICASSP . IEEE, 2015.
",False,neekhara21a,False,False,True
18177,"Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang,
",False,neekhara21a,False,False,True
18178,"Jonathan Raiman, and John Miller. Deep Voice 3: Scaling text-to-speech with convolu-
",False,neekhara21a,False,False,True
18179,"tional sequence learning. In ICLR , 2018a.
",False,neekhara21a,False,False,True
18180,"Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O. Arik, Ajay Kannan, Sharan Narang,
",False,neekhara21a,False,False,True
18181,"Jonathan Raiman, and John L. Miller. Deep voice 3: 2000-speaker neural text-to-speech.
",False,neekhara21a,False,False,True
18182,"InICLR , 2018b.
",False,neekhara21a,False,False,True
18183,"Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A 
",False,neekhara21a,False,False,True
18184,"ow-based generative
",False,neekhara21a,False,False,True
18185,"network for speech synthesis. In ICASSP , 2018.
",False,neekhara21a,False,False,True
18186,"Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng
",False,neekhara21a,False,False,True
18187,"Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural TTS
",False,neekhara21a,False,False,True
18188,"synthesis by conditioning WaveNet on mel spectrogram predictions. In ICASSP , 2018.
",False,neekhara21a,False,False,True
18189,"R. J. Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor,
",False,neekhara21a,False,False,True
18190,"Ron J. Weiss, Rob Clark, and Rif A. Saurous. Towards end-to-end prosody transfer for
",False,neekhara21a,False,False,True
18191,"expressive speech synthesis with tacotron. arXiv:1803.09047 , 2018.
",False,neekhara21a,False,False,True
18192,"Daisy Stanton, Yuxuan Wang, and R. J. Skerry-Ryan. Predicting expressive speaking style
",False,neekhara21a,False,False,True
18193,"from text in end-to-end speech synthesis. arXiv:1803.09017 , 2018.
",False,neekhara21a,False,False,True
18194,"Rafael Valle, Jason Li, Ryan Prenger, and Bryan Catanzaro. Mellotron: Multispeaker
",False,neekhara21a,False,False,True
18195,"expressive voice synthesis by conditioning on rhythm, pitch and global style tokens.
",False,neekhara21a,False,False,True
18196,"ICASSP , 2020.
",False,neekhara21a,False,False,True
18197,"A aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex
",False,neekhara21a,False,False,True
18198,"Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A gener-
",False,neekhara21a,False,False,True
18199,"ative model for raw audio. arXiv:1609.03499 , 2016.
",False,neekhara21a,False,False,True
18200,"Christophe Veaux, Junichi Yamagishi, and Kirsten Macdonald. Cstr vctk corpus: English
",False,neekhara21a,False,False,True
18201,"multi-speaker corpus for cstr voice cloning toolkit. 2017.
",False,neekhara21a,False,False,True
18202,"Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. Generalized end-to-end loss
",False,neekhara21a,False,False,True
18203,"for speaker verication. arXiv:1710.10467 , 2017.Neekhara* Hussain* Dubnov Koushanfar McAuley
",False,neekhara21a,False,False,True
18204,"Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly,
",False,neekhara21a,False,False,True
18205,"Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-
",False,neekhara21a,False,False,True
18206,"to-end speech synthesis. In INTERSPEECH , 2017.
",False,neekhara21a,False,False,True
18207,"Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying
",False,neekhara21a,False,False,True
18208,"Xiao, Fei Ren, Ye Jia, and Rif A. Saurous. Style tokens: Unsupervised style modeling,
",False,neekhara21a,False,False,True
18209,"control and transfer in end-to-end speech synthesis. arXiv:1803.09017 , 2018.
",False,neekhara21a,False,False,True
18210,"Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and
",False,neekhara21a,False,False,True
18211,"Yonghui Wu. LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. In
",False,neekhara21a,False,False,True
18212,"INTERSPEECH , 2019. doi: 10.21437/Interspeech.2019-2441.",False,neekhara21a,False,False,True
18213,"Abstract
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18214,"1 Introduction
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18215,"2 Related Work
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18216,"3.1 Speaker adaptation
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18217,"3.2 Speaker encoding
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18218,"3Multi -speaker 
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18219,"3.3 Discriminative models for evaluation
",True,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18220,"Neural Voice Cloning with a Few Samples
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18221,"Sercan Ö. Arık∗
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18222,"sercanarik@baidu.comJitong Chen∗
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18223,"chenjitong01@baidu.comKainan Peng∗
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18224,"pengkainan@baidu.com
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18225,"Wei Ping∗
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18226,"pingwei01@baidu.comYanqi Zhou
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18227,"yanqiz@baidu.com
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18228,"Baidu Research
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18229,"1195 Bordeaux Dr. Sunnyvale, CA 94089
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18230,"V oice cloning is a highly desired feature for personalized speech interfaces. We
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18231,"introduce a neural voice cloning system that learns to synthesize a person’s voice
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18232,"from only a few audio samples. We study two approaches: speaker adaptation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18233,"and speaker encoding. Speaker adaptation is based on ﬁne-tuning a multi-speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18234,"generative model. Speaker encoding is based on training a separate model to
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18235,"directly infer a new speaker embedding, which will be applied to a multi-speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18236,"generative model. In terms of naturalness of the speech and similarity to the original
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18237,"speaker, both approaches can achieve good performance, even with a few cloning
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18238,"audios.2While speaker adaptation can achieve slightly better naturalness and
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18239,"similarity, cloning time and required memory for the speaker encoding approach
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18240,"are signiﬁcantly less, making it more favorable for low-resource deployment.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18241,"1 Introduction
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18242,"Generative models based on deep neural networks have been successfully applied to many domains
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18243,"such as image generation [e.g., Oord et al. ,2016b ,Karras et al. ,2017 ], speech synthesis [e.g., Oord
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18244,"et al. ,2016a ,Arik et al. ,2017a ,Wang et al. ,2017 ], and language modeling [e.g., Jozefowicz et al. ,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18245,"2016 ]. Deep neural networks are capable of modeling complex data distributions and can be further
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18246,"conditioned on external inputs to control the content and style of generated samples.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18247,"In speech synthesis, generative models can be conditioned on text and speaker identity [e.g., Arik
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18248,"et al. ,2017b ]. While text carries linguistic information and controls the content of the generated
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18249,"speech, speaker identity captures characteristics such as pitch, speech rate and accent. One approach
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18250,"for multi-speaker speech synthesis is to jointly train a generative model and speaker embeddings
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18251,"on triplets of text, audio and speaker identity [e.g., Ping et al. ,2018 ]. The idea is to encode the
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18252,"speaker-dependent information with low-dimensional embeddings, while sharing the majority of the
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18253,"model parameters across all speakers. One limitation of such methods is that they can only generate
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18254,"speech for observed speakers during training. An intriguing task is to learn the voice of an unseen
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18255,"speaker from a few speech samples, a.k.a. voice cloning , which corresponds to few-shot generative
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18256,"modeling of speech conditioned on the speaker identity. While a generative model can be trained
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18257,"from scratch with a large amount of audio samples3, we focus on voice cloning of a new speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18258,"with a few minutes or even few seconds data. It is challenging as the model has to learn the speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18259,"characteristics from very limited amount of data, and still generalize to unseen texts.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18260,"∗Equal contribution
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18261,"2Cloned audio samples can be found in https://audiodemos.github.io
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18262,"3A single speaker model can require ∼20 hours of training data [e.g., Arik et al. ,2017a ,Wang et al. ,2017 ],
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18263,"while a multi-speaker model for 108 speakers [ Arik et al. ,2017b ] requires about ∼20 minutes data per speaker.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18264,"32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.In this paper, we investigate voice cloning in sequence-to-sequence neural speech synthesis sys-
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18265,"tems [ Ping et al. ,2018 ]. Our contributions are the following:
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18266,"1.We demonstrate and analyze the strength of speaker adaption approaches for voice cloning,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18267,"based on ﬁne-tuning a pre-trained multi-speaker model for an unseen speaker using a few
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18268,"samples.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18269,"2.We propose a novel speaker encoding approach, which provides comparable naturalness
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18270,"and similarity in subjective evaluations while yielding signiﬁcantly less cloning time and
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18271,"computational resource requirements.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18272,"3.We propose automated evaluation methods for voice cloning based on neural speaker classiﬁ-
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18273,"cation and speaker veriﬁcation.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18274,"4.We demonstrate voice morphing for gender and accent transformation via embedding manip-
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18275,"ulations.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18276,"2 Related Work
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18277,"Our work builds upon the state-of-the-art in neural speech synthesis and few-shot generative modeling.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18278,"Neural speech synthesis: Recently, there is a surge of interest in speech synthesis with neu-
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18279,"ral networks, including Deep V oice 1 [ Arik et al. ,2017a ], Deep V oice 2 [ Arik et al. ,2017b ],
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18280,"Deep V oice 3 [ Ping et al. ,2018 ], WaveNet [ Oord et al. ,2016a ], SampleRNN [ Mehri et al. ,2016 ],
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18281,"Char2Wav [ Sotelo et al. ,2017 ], Tacotron [ Wang et al. ,2017 ] and V oiceLoop [ Taigman et al. ,2018 ].
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18282,"Among these methods, sequence-to-sequence models [ Ping et al. ,2018 ,Wang et al. ,2017 ,Sotelo
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18283,"et al. ,2017 ] with attention mechanism have much simpler pipeline and can produce more natural
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18284,"speech [e.g., Shen et al. ,2017 ]. In this work, we use Deep V oice 3 as the baseline multi-speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18285,"model, because of its simple convolutional architecture and high efﬁciency for training and fast model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18286,"adaptation. It should be noted that our techniques can be seamlessly applied to other neural speech
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18287,"synthesis models.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18288,"Few-shot generative modeling: Humans can learn new generative tasks from only a few examples,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18289,"which motivates research on few-shot generative models. Early studies mostly focus on Bayesian
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18290,"methods. For example, hierarchical Bayesian models are used to exploit compositionality and
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18291,"causality for few-shot generation of characters [ Lake et al. ,2013 ,2015 ] and words in speech [ Lake
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18292,"et al. ,2014 ]. Recently, deep neural networks achieve great successes in few-shot density estimation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18293,"and conditional image generation [e.g., Rezende et al. ,2016 ,Reed et al. ,2017 ,Azadi et al. ,2017 ],
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18294,"because of the great potential for composition in their learned representation. In this work, we
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18295,"investigate few-shot generative modeling of speech conditioned on a particular speaker. We train a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18296,"separate speaker encoding network to directly predict the parameters of multi-speaker generative
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18297,"model by only taking unsubscribed audio samples as inputs.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18298,"Speaker-dependent speech processing: Speaker-dependent modeling has been widely studied for
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18299,"automatic speech recognition (ASR), with the goal of improving the performance by exploiting
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18300,"speaker characteristics. In particular, there are two groups of methods in neural ASR, in alignment
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18301,"with our two voice cloning approaches. The ﬁrst group is speaker adaptation for the whole-model [ Yu
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18302,"et al. ,2013 ], a portion of the model [ Miao and Metze ,2015 ,Cui et al. ,2017 ], or merely to a speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18303,"embedding [ Abdel-Hamid and Jiang ,2013 ,Xue et al. ,2014 ]. Speaker adaptation for voice cloning is
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18304,"in the same vein as these approaches, but differences arise when text-to-speech vs. speech-to-text
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18305,"are considered [ Yamagishi et al. ,2009 ]. The second group is based on training ASR models jointly
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18306,"with embeddings. Extraction of the embeddings can be based on i-vectors [Miao et al. ,2015 ], or
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18307,"bottleneck layers of neural networks trained with a classiﬁcation loss [ Li and Wu ,2015 ]. Although
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18308,"the general idea of speaker encoding is also based on extracting the embeddings directly, as a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18309,"major distinction, our speaker encoder models are trained with an objective function that is directly
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18310,"related to speech synthesis. Lastly, speaker-dependent modeling is essential for multi-speaker speech
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18311,"synthesis. Using i-vectors to represent speaker-dependent characteristics is one approach [ Wu et al. ,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18312,"2015 ], however, they have the limitation of being separately trained, with an objective that is not
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18313,"directly related to speech synthesis. Also they may not be accurately extracted with small amount of
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18314,"audio [ Miao et al. ,2015 ]. Another approach for multi-speaker speech synthesis is using trainable
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18315,"speaker embeddings [ Arik et al. ,2017b ], which are randomly initialized and jointly optimized from a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18316,"generative loss function.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18317,"2Voice conversion: A closely related task of voice cloning is voice conversion. The goal of voice
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18318,"conversion is to modify an utterance from source speaker to make it sound like the target speaker,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18319,"while keeping the linguistic contents unchanged. Unlike voice cloning, voice conversion systems
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18320,"do not need to generalize to unseen texts. One common approach is dynamic frequency warping,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18321,"to align spectra of different speakers. Agiomyrgiannakis and Roupakia [2016 ] proposes a dynamic
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18322,"programming algorithm that simultaneously estimates the optimal frequency warping and weighting
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18323,"transform while matching source and target speakers using a matching-minimization algorithm. Wu
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18324,"et al. [2016 ] uses a spectral conversion approach integrated with the locally linear embeddings for
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18325,"manifold learning. There are also approaches to model spectral conversion using neural networks
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18326,"[Desai et al. ,2010 ,Chen et al. ,2014 ,Hwang et al. ,2015 ]. Those models are typically trained with a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18327,"large amount of audio pairs of target and source speakers.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18328,"3 From Multi-Speaker Generative Modeling to Voice Cloning
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18329,"We consider a multi-speaker generative model, f(ti,j,si;W,esi), which takes a text ti,jand a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18330,"speaker identity si. The trainable parameters in the model is parameterized by W, andesi. The
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18331,"latter denotes the trainable speaker embedding corresponding to si. BothWandesiare optimized
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18332,"by minimizing a loss function Lthat penalizes the difference between generated and ground-truth
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18333,"audios (e.g. a regression loss for spectrograms):
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18334,"min
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18335,"W,eEsi∼S,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18336,"(ti,j,ai,j)∼Tsi{L(f(ti,j,si;W,esi),ai,j)} (1)
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18337,"whereSis a set of speakers, Tsiis a training set of text-audio pairs for speaker si, andai,jis the
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18338,"ground-truth audio for ti,jof speaker si. The expectation is estimated over text-audio pairs of all
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18339,"training speakers. We use /hatwiderWand/hatwideeto denote the trained parameters and embeddings. Speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18340,"embeddings have been shown to effectively capture speaker characteristics with low-dimensional vec-
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18341,"tors [ Arik et al. ,2017b ,Ping et al. ,2018 ]. Despite training with only a generative loss, discriminative
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18342,"properties (e.g. gender and accent) are observed in the speaker embedding space [ Arik et al. ,2017b ].
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18343,"For voice cloning, we extract the speaker characteristics for an unseen speaker skfrom a set of cloning
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18344,"audiosAsk, and generate an audio given any text for that speaker. The two performance metrics for
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18345,"the generated audio are speech naturalness and speaker similarity (i.e., whether the generated audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18346,"sounds like it is pronounced by the target speaker). The two approaches for neural voice cloning are
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18347,"summarized in Fig. 1and explained in the following sections.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18348,"3.1 Speaker adaptation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18349,"The idea of speaker adaptation is to ﬁne-tune a trained multi-speaker model for an unseen speaker
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18350,"using a few audio-text pairs. Fine-tuning can be applied to either the speaker embedding [ Taigman
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18351,"et al. ,2018 ] or the whole model. For embedding-only adaptation, we have the following objective:
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18352,"min
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18353,"eskE(tk,j,ak,j)∼Tsk/braceleftBig
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18354,"L/parenleftBig
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18355,"f(tk,j,sk;/hatwiderW,esk),ak,j/parenrightBig/bracerightBig
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18356,", (2)
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18357,"whereTskis a set of text-audio pairs for the target speaker sk. For whole model adaptation, we have
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18358,"the following objective:
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18359,"min
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18360,"W,eskE(tk,j,ak,j)∼Tsk{L(f(tk,j,sk;W,esk),ak,j)}. (3)
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18361,"Although the entire model provides more degrees of freedom for speaker adaptation, its optimization
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18362,"is challenging for small amount of cloning data. Early stopping is required to avoid overﬁtting.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18363,"3.2 Speaker encoding
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18364,"We propose a speaker encoding method to directly estimate the speaker embedding from audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18365,"samples of an unseen speaker. Such a model does not require any ﬁne-tuning during voice cloning.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18366,"Thus, the same model can be used for all unseen speakers. The speaker encoder, g(Ask;Θ), takes a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18367,"set of cloning audio samples Askand estimates eskfor speaker sk. The model is parametrized by
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18368,"Θ. Ideally, the speaker encoder can be jointly trained with the multi-speaker generative model from
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18369,"scratch, with a loss function deﬁned for the generated audio:
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18370,"min
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18371,"W,ΘEsi∼S,
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18372,"(ti,j,ai,j)∼Tsi{L(f(ti,j,si;W,g(Asi;Θ)),ai,j)}. (4)
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18373,"3Multi -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18374,"generative model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18375,"Speaker embeddingText AudioSpeaker encoder 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18376,"modelSpeaker embedding
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18377,"Cloning audioMulti -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18378,"generative model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18379,"Speaker embeddingText Audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18380,"Speaker encoder 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18381,"model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18382,"Cloning audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18383,"Speaker embedding
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18384,"Speaker encoder 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18385,"model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18386,"Cloning audioTrainingMulti -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18387,"generative model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18388,"Speaker embeddingText Audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18389,"Multi -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18390,"generative model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18391,"Speaker embedding
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18392,"Multi -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18393,"generative model
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18394,"Speaker embeddingCloning
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18395,"text
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18396,"Cloning
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18397,"textCloning 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18398,"audioCloning 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18399,"audioCloningor
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18400,"Multi -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18401,"generative modelText Audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18402,"Speaker embeddingAudio generationSpeaker adaptation Speaker encodingTrainable
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18403,"Fixed
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18404,"Multi -speaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18405,"generative modelText Audio
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18406,"Speaker embedding
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18407,"Figure 1: Illustration of speaker adaptation and speaker encoding approaches for voice cloning.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18408,"Note that the speaker encoder is trained with the speakers for the multi-speaker generative model.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18409,"During training, a set of cloning audio samples Asiare randomly sampled for training speaker si.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18410,"During inference, audio samples from the target speaker sk,Ask, are used to compute g(Ask;Θ).
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18411,"We observed optimization challenges with joint training from scratch: the speaker encoder tends
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18412,"to estimate an average voice to minimize the overall generative loss. One possible solution is to
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18413,"introduce discriminative loss functions for intermediate embeddings4or generated audios5. In our
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18414,"case, however, such approaches only slightly improve speaker differences. Instead, we propose a
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18415,"separate training procedure for speaker encoder. Speaker embeddings /hatwideesiare extracted from a trained
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18416,"multi-speaker generative model f(ti,j,si;W,esi). Then, the speaker encoder g(Ask;Θ)is trained
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18417,"with an L1 loss to predict the embeddings from sampled cloning audios:
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18418,"min
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18419,"ΘEsi∼S{|g(Asi;Θ)−/hatwideesi)|}. (5)
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18420,"Eventually, entire model can be jointly ﬁne-tuned following Eq. 4, with pre-trained /hatwiderWand/hatwideΘas the
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18421,"initialization. Fine-tuning encourages the generative model to compensate for embedding estimation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18422,"errors, and may reduce attention problems. However, generative loss still dominates learning and
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18423,"speaker differences in generated audios may be slightly reduced as well (see Section 4.3for details).
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18424,"×NconvMel 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18425,"spectrogramsPrenet Conv. bloc k Mean poolin g Self attentionSpeaker 
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18426,"embedding
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18427,"Figure 2: Speaker encoder architecture. See Appendix Afor details.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18428,"For speaker encoder g(Ask;Θ), we propose a neural network architecture comprising three parts (as
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18429,"shown in Fig. 2):
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18430,"(i)Spectral processing : We input mel-spectrograms of cloning audio samples to prenet, which
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18431,"contains fully-connected layers with exponential linear unit for feature transformation.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18432,"(ii)Temporal processing : To utilize long-term context, we use convolutional layers with gated linear
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18433,"unit and residual connections, average pooling is applied to summarize the whole utterance.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18434,"4We have experimented classiﬁcation loss by mapping the embeddings to labels via a softmax layer.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18435,"5We have experimented integrating a pre-trained classiﬁer to encourage discrimination in generated audios.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18436,"4(iii) Cloning sample attention : Considering that different cloning audios contain different amount of
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18437,"speaker information, we use a multi-head self-attention mechanism [ Vaswani et al. ,2017 ] to
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18438,"compute the weights for different audios and get aggregated embeddings.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18439,"3.3 Discriminative models for evaluation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18440,"Besides human evaluations, we propose two evaluation methods using discriminative models for
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18441,"voice cloning performance.
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18442,"3.3.1 Speaker classiﬁcation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18443,"3.3.1 Speaker classiﬁcation
",False,NeurIPS-2018-neural-voice-cloning-with-a-few-samples-Paper,False,False,True
18444,"   Abstract   
",True,OpenCV Geometrix Transformations,False,False,False
18445,"I. INTRODUCTION  
",True,OpenCV Geometrix Transformations,False,False,False
18446,"II. LITERATURE SURVEY  
",True,OpenCV Geometrix Transformations,False,False,False
18447,"IV. IMAGE ROTATION AND AF FINE TRANSFORMATION USING OPENCV -PYTHON  
",True,OpenCV Geometrix Transformations,False,False,False
18448,"VI. IMAGE THRESHOLDING US ING OPENCV -PYTHON  
",True,OpenCV Geometrix Transformations,False,False,False
18449,"VII. CONCLUSION AND FUTURE  WORK  
",True,OpenCV Geometrix Transformations,False,False,False
18450,"GRD Journal  for Engineering | Vol ume 2 | Issue 11 | October  2017  
",False,OpenCV Geometrix Transformations,False,False,False
18451,"  ISSN: 2455 -5703  
",False,OpenCV Geometrix Transformations,False,False,False
18452," 
",False,OpenCV Geometrix Transformations,False,False,False
18453,"All rights reserved by www.grdjournals.com  49 Geometric Transformations and Thresholding of 
",False,OpenCV Geometrix Transformations,False,False,False
18454,"Images using Opencv -Python  
",False,OpenCV Geometrix Transformations,False,False,False
18455," 
",False,OpenCV Geometrix Transformations,False,False,False
18456,"R. Devi  
",False,OpenCV Geometrix Transformations,False,False,False
18457,"Assistant Professor  
",False,OpenCV Geometrix Transformations,False,False,False
18458,"Department of Electron ic and Communication Engineering  
",False,OpenCV Geometrix Transformations,False,False,False
18459,"Saranathan  College of Engineering,Trichy, India  
",False,OpenCV Geometrix Transformations,False,False,False
18460,"  
",False,OpenCV Geometrix Transformations,False,False,False
18461," 
",False,OpenCV Geometrix Transformations,False,False,False
18462,"A geometric change is any bijection of a set having some geometric structure to itself or another such set. Specifically, a g eometric 
",False,OpenCV Geometrix Transformations,False,False,False
18463,"change is a capacity whose space and range are sets of focuses. In this paper to apply diverse geometric change to picture s like 
",False,OpenCV Geometrix Transformations,False,False,False
18464,"interpretation, turn, relative change utilizing opencv -Python is presented. Geometric changes can be grouped by the measurement 
",False,OpenCV Geometrix Transformations,False,False,False
18465,"of their operand sets (along these lines recognizing planar changes and those of space, for instance). They can likewise be ordered 
",False,OpenCV Geometrix Transformations,False,False,False
18466,"by the properties they save . Displacements preserve distances and oriented angles ; Iso-metries preserve angles and distances; 
",False,OpenCV Geometrix Transformations,False,False,False
18467,"Similarities preserve angles and ratios between distances ; affine transformations preserve parallelism ; projective tran sformations 
",False,OpenCV Geometrix Transformations,False,False,False
18468,"preserve collinearity . 
",False,OpenCV Geometrix Transformations,False,False,False
18469,"Keywords - Transformations,  Scaling,  Translations,  Affine   
",False,OpenCV Geometrix Transformations,False,False,False
18470," 
",False,OpenCV Geometrix Transformations,False,False,False
18471,"I. INTRODUCTION  
",False,OpenCV Geometrix Transformations,False,False,False
18472,"In computer graphics and digital imaging, image scaling refers to the resizing of a digital image. In video technology, the 
",False,OpenCV Geometrix Transformations,False,False,False
18473,"magnification of digita l material is known as up -scaling or resolution enhancement. When scaling a vector graphic image, the 
",False,OpenCV Geometrix Transformations,False,False,False
18474,"graphic primitives that make up the image can be scaled using geometric transformations, with no loss of image quality. When 
",False,OpenCV Geometrix Transformations,False,False,False
18475,"scaling a raster graphics ima ge, a new image with a higher or lower number of pixels must be generated. In the case of decreasing 
",False,OpenCV Geometrix Transformations,False,False,False
18476,"the pixel number (scaling down) this usually results in a visible quality loss. From the standpoint of digital signal process ing, the 
",False,OpenCV Geometrix Transformations,False,False,False
18477,"scaling of raster gra phics is a two -dimensional example of sample -rate conversion, the conversion of a discrete signal from a 
",False,OpenCV Geometrix Transformations,False,False,False
18478,"sampling rate (in this case the local sampling rate) to another.  
",False,OpenCV Geometrix Transformations,False,False,False
18479,"The translate operator performs a geometric transformation which maps the position of each picture element in an input 
",False,OpenCV Geometrix Transformations,False,False,False
18480,"image into a new position in an output image, where the dimensionality of the two images often is, but need not necessarily b e, the 
",False,OpenCV Geometrix Transformations,False,False,False
18481,"same. Under translation, an image element located at in the original is shifted to a new po sition in the corresponding output image 
",False,OpenCV Geometrix Transformations,False,False,False
18482,"by displacing it through a user -specified translation. The treatment of elements near image edges varies with implementation. 
",False,OpenCV Geometrix Transformations,False,False,False
18483,"Translation is used to improve visualization of an image, but also has a role as a prepro cessor in applications where registration of 
",False,OpenCV Geometrix Transformations,False,False,False
18484,"two or more images is required. Translation is a special case of affine transformation. The rotation operator performs a geom etric 
",False,OpenCV Geometrix Transformations,False,False,False
18485,"transform which maps the position of a picture element in an input image onto a position in an output image by rotating it through 
",False,OpenCV Geometrix Transformations,False,False,False
18486,"a user -specified angle about an origin. In most implementations, output locations which are outside the boundary of the image are 
",False,OpenCV Geometrix Transformations,False,False,False
18487,"ignored. Rotation is most commonly used to improve the visual appearance of  an image, although it can be useful as a preprocessor 
",False,OpenCV Geometrix Transformations,False,False,False
18488,"in applications where directional operators are involved. Rotation is a special case of affine transformation.  
",False,OpenCV Geometrix Transformations,False,False,False
18489,"II. LITERATURE SURVEY  
",False,OpenCV Geometrix Transformations,False,False,False
18490,"Writing Survey In recent years, the recognition of Objects continuously and Image preparing has turned into a dynamic region of 
",False,OpenCV Geometrix Transformations,False,False,False
18491,"research and a few new methodologies have been proposed. A few scientists have directed many examinations about Object 
",False,OpenCV Geometrix Transformations,False,False,False
18492,"discovery 1. S.V. Viraktamath, Mukund Katti, Aditya Khatawkar and Pavan Kulkarni has led an investigation of openCV and 
",False,OpenCV Geometrix Transformations,False,False,False
18493,"furthermore have distributed an IEEE paper for Face Detection and Tracking utilizing OpenCV. Their work is connected with 
",False,OpenCV Geometrix Transformations,False,False,False
18494,"changing over web cam caught 2D Images and change over them into 3D Images identified with huma n faces by building 3D 
",False,OpenCV Geometrix Transformations,False,False,False
18495,"Geometry information yields [1]. 2. Ashish Pant, Arjun Arora, Sunnet Kumar and Prof. R.P. Arora shape DIT Dehradun have 
",False,OpenCV Geometrix Transformations,False,False,False
18496,"looked into about Image Processing and scrambling an Image with a specific end goal to exchange securely finished  the systems. 
",False,OpenCV Geometrix Transformations,False,False,False
18497,"3. They entitled their work as Sophisticated Image Encryption Using OpenCV [2]. 4. Kevinhughes, a first class individual in 
",False,OpenCV Geometrix Transformations,False,False,False
18498,"Opencv region has composed various sites containing ventures instructional exercises around there and ventures for int roducing 
",False,OpenCV Geometrix Transformations,False,False,False
18499,"different virtual products [3]. 5. Serge Belongie and Jitendra Malik, individuals from IEEE have done a wast contemplate in t he 
",False,OpenCV Geometrix Transformations,False,False,False
18500,"field of Shape Matching and Object Matching Based on their shapes, separating two question in light of the distinction in their 
",False,OpenCV Geometrix Transformations,False,False,False
18501,"shapes. 6. Orlando J. Tobias, and Rui Seara, Member, IEEE, have put their awesome endeavors studding the ways and strategies 
",False,OpenCV Geometrix Transformations,False,False,False
18502,"for Image Segmentation and histogram Thresholding.  Geometric Transformations and Thresholding of Images using Opencv -Python   
",False,OpenCV Geometrix Transformations,False,False,False
18503,"(GRDJE/ Volu me 2 / Issue 11 / 007 ) 
",False,OpenCV Geometrix Transformations,False,False,False
18504," 
",False,OpenCV Geometrix Transformations,False,False,False
18505," 
",False,OpenCV Geometrix Transformations,False,False,False
18506," All rights reserved by www.grdjournals.com  
",False,OpenCV Geometrix Transformations,False,False,False
18507," 
",False,OpenCV Geometrix Transformations,False,False,False
18508,"50 III. TRANSFORMATION OF IMA GES 
",False,OpenCV Geometrix Transformations,False,False,False
18509,"OpenCV gives two change capacities, cv2.warpAf fine and cv2.warpPerspective, with which you can have a wide range of changes. 
",False,OpenCV Geometrix Transformations,False,False,False
18510,"cv2.warpAffine takes a 2x3 change grid while cv2.warpPerspective takes a 3x3 change framework as information. Scaling is 
",False,OpenCV Geometrix Transformations,False,False,False
18511,"simply resizing of the picture. OpenCV accompanies a cap acity cv2.resize() for this reason. The extent of the picture can be 
",False,OpenCV Geometrix Transformations,False,False,False
18512,"indicated physically, or you can determine the scaling factor. Diverse addition strategies are utilized. Best introduction te chniques 
",False,OpenCV Geometrix Transformations,False,False,False
18513,"are cv2.INTER_AREA for contracting and cv2.INTER_CUBI C (moderate) and cv2.INTER_LINEAR for zooming. As a matter 
",False,OpenCV Geometrix Transformations,False,False,False
18514,"of course, insertion strategy utilized is cv2.INTER_LINEAR for all resizing purposes. Interpretation is the moving of questio n's 
",False,OpenCV Geometrix Transformations,False,False,False
18515,"area. On the off chance that you know the move in (x,y) course, let it be , you can make the change grid as takes after  
",False,OpenCV Geometrix Transformations,False,False,False
18516," 
",False,OpenCV Geometrix Transformations,False,False,False
18517,"You can take make it into a Numpy array of type np.float32 and pass it into cv2.warpAffine() function. This figure 1.1 Shows 
",False,OpenCV Geometrix Transformations,False,False,False
18518,"simulation results for scaling of image using opencv -python.  
",False,OpenCV Geometrix Transformations,False,False,False
18519," 
",False,OpenCV Geometrix Transformations,False,False,False
18520,"Fig. 1.1:  Scaling of Image  
",False,OpenCV Geometrix Transformations,False,False,False
18521,"IV. IMAGE ROTATION AND AF FINE TRANSFORMATION USING OPENCV -PYTHON  
",False,OpenCV Geometrix Transformations,False,False,False
18522,"Rotation of an image for an angle is achieved by the transformation matrix of the form  
",False,OpenCV Geometrix Transformations,False,False,False
18523," 
",False,OpenCV Geometrix Transformations,False,False,False
18524,"But OpenCV provides scaled rotation with adjustable center of rotation so that you can rotate at any location you prefer. Modified 
",False,OpenCV Geometrix Transformations,False,False,False
18525,"transformation matrix is given by  
",False,OpenCV Geometrix Transformations,False,False,False
18526," 
",False,OpenCV Geometrix Transformations,False,False,False
18527,"Where , 
",False,OpenCV Geometrix Transformations,False,False,False
18528," 
",False,OpenCV Geometrix Transformations,False,False,False
18529,"To find this transformation matrix, OpenCV provides a function, cv2.getRotationMatrix2D . 
",False,OpenCV Geometrix Transformations,False,False,False
18530," 
",False,OpenCV Geometrix Transformations,False,False,False
18531,"Fig. 1.2: Rotation of Images  
",False,OpenCV Geometrix Transformations,False,False,False
18532,"Geometric Transformations and Thresholding of Images using Opencv -Python   
",False,OpenCV Geometrix Transformations,False,False,False
18533,"(GRDJE/ Volu me 2 / Issue 11 / 007 ) 
",False,OpenCV Geometrix Transformations,False,False,False
18534," 
",False,OpenCV Geometrix Transformations,False,False,False
18535," 
",False,OpenCV Geometrix Transformations,False,False,False
18536," All rights reserved by www.grdjournals.com  
",False,OpenCV Geometrix Transformations,False,False,False
18537," 
",False,OpenCV Geometrix Transformations,False,False,False
18538,"51 V. AFFINE TRANSFORMATION  & PRESPECTI VE TRANSFORMATION OF  IMAGES USING OPENCV  
",False,OpenCV Geometrix Transformations,False,False,False
18539,"In affine transformation, all parallel lines in the original image will still be parallel in the output image. To find the tr ansformation 
",False,OpenCV Geometrix Transformations,False,False,False
18540,"matrix, we need three points from input image and their corresponding locatio ns in output image. Then cv2.getAffineTransform 
",False,OpenCV Geometrix Transformations,False,False,False
18541,"will create a 2x3 matrix which is to be passed to cv2.warpAffine.  
",False,OpenCV Geometrix Transformations,False,False,False
18542," 
",False,OpenCV Geometrix Transformations,False,False,False
18543,"Fig. 1.3: Affine Transformation of Images  
",False,OpenCV Geometrix Transformations,False,False,False
18544,"For perspective transformation, you need a 3x3 transformation matrix. Straight lines will remain straight even after the 
",False,OpenCV Geometrix Transformations,False,False,False
18545,"transformation. To find this transformation matrix, you need 4 points on the input image and corresponding points on the outp ut 
",False,OpenCV Geometrix Transformations,False,False,False
18546,"image. Among these 4 points, 3 of them should not be collinear.  
",False,OpenCV Geometrix Transformations,False,False,False
18547,"Then transformation matrix can be found by the function cv2.getPerspectiveTransform.  
",False,OpenCV Geometrix Transformations,False,False,False
18548,"Then apply cv2.warpPerspective with this 3x3 transformation matrix.  
",False,OpenCV Geometrix Transformations,False,False,False
18549,"VI. IMAGE THRESHOLDING US ING OPENCV -PYTHON  
",False,OpenCV Geometrix Transformations,False,False,False
18550,"The function used is cv2.threshold. First argument is the source image, which should be a grayscale imag e. Second argument is 
",False,OpenCV Geometrix Transformations,False,False,False
18551,"the threshold value which is used to classify the pixel values. Third argument is the maxVal which represents the value to be  given 
",False,OpenCV Geometrix Transformations,False,False,False
18552,"if pixel value is more than (sometimes less than) the threshold value. OpenCV provides different style s of thresholding and it is 
",False,OpenCV Geometrix Transformations,False,False,False
18553,"decided by the fourth parameter of the function. Different types are:  
",False,OpenCV Geometrix Transformations,False,False,False
18554,"– cv2.THRESH_BINARY  
",False,OpenCV Geometrix Transformations,False,False,False
18555,"– cv2.THRESH_BINARY_INV  
",False,OpenCV Geometrix Transformations,False,False,False
18556,"– cv2.THRESH_TRUNC  
",False,OpenCV Geometrix Transformations,False,False,False
18557,"– cv2.THRESH_TOZERO  
",False,OpenCV Geometrix Transformations,False,False,False
18558,"– cv2.THRESH_TOZERO_INV  
",False,OpenCV Geometrix Transformations,False,False,False
18559,"In the previous section, we used a global value as threshold value. But it may not be good in all the conditions where 
",False,OpenCV Geometrix Transformations,False,False,False
18560,"image has different lighting conditions in different areas. In that case, we go for adaptive thresholding. In this, the algor ithm 
",False,OpenCV Geometrix Transformations,False,False,False
18561,"calcula te the threshold for a small regions of the image. So we get different thresholds for different regions of the same image and  
",False,OpenCV Geometrix Transformations,False,False,False
18562,"it gives us better results for images with varying illumination.  
",False,OpenCV Geometrix Transformations,False,False,False
18563,"It has three ‘special’ input parameters and only one output argum ent. 
",False,OpenCV Geometrix Transformations,False,False,False
18564,"Adaptive Method - It decides how thresholding value is calculated.  
",False,OpenCV Geometrix Transformations,False,False,False
18565,"cv2.ADAPTIVE_THRESH_MEAN_C:Threshold value is the mean of neighbourhood area.  
",False,OpenCV Geometrix Transformations,False,False,False
18566,"cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where 
",False,OpenCV Geometrix Transformations,False,False,False
18567,"weights  are a gaussian window.  
",False,OpenCV Geometrix Transformations,False,False,False
18568,"Block Size - It decides the size of neighbourhood area.  
",False,OpenCV Geometrix Transformations,False,False,False
18569,"C - It is just a constant which is subtracted from the mean or weighted mean calculated  
",False,OpenCV Geometrix Transformations,False,False,False
18570,"Geometric Transformations and Thresholding of Images using Opencv -Python   
",False,OpenCV Geometrix Transformations,False,False,False
18571,"(GRDJE/ Volu me 2 / Issue 11 / 007 ) 
",False,OpenCV Geometrix Transformations,False,False,False
18572," 
",False,OpenCV Geometrix Transformations,False,False,False
18573," 
",False,OpenCV Geometrix Transformations,False,False,False
18574," All rights reserved by www.grdjournals.com  
",False,OpenCV Geometrix Transformations,False,False,False
18575," 
",False,OpenCV Geometrix Transformations,False,False,False
18576,"52  
",False,OpenCV Geometrix Transformations,False,False,False
18577,"Fig. 1.4: Adaptive Thresholding  
",False,OpenCV Geometrix Transformations,False,False,False
18578,"VII. CONCLUSION AND FUTURE  WORK  
",False,OpenCV Geometrix Transformations,False,False,False
18579,"Image Thresholding and geom entric transformation of images is done by using opencv –python.  Geomrntric transformations of 
",False,OpenCV Geometrix Transformations,False,False,False
18580,"images is done by using scaling,  rotation,  transformations of images is presented here.  Image thresholding is done by using 
",False,OpenCV Geometrix Transformations,False,False,False
18581,"adaptive thresholding.This is used to  reduce the noise of images and increases the contrast of the images.  
",False,OpenCV Geometrix Transformations,False,False,False
18582,"REFERENCES  
",False,OpenCV Geometrix Transformations,False,False,False
18583,"[1] Viraktamath SV, Mukund Katti, Aditya Khatawkar, Pavan Kulkarni, “Face Detection and Tracking using OpenCV,” The SIJ Transacti on on Computer 
",False,OpenCV Geometrix Transformations,False,False,False
18584,"Networks & Communication Engineerin g (CNCE), 2013, 1(3).  
",False,OpenCV Geometrix Transformations,False,False,False
18585,"[2] Pant A, Arora A, Kumar S, Arora RP. “Sophisticated Image Encryption Using OpenCV,” International Journal of Advances Research  in Computer Science 
",False,OpenCV Geometrix Transformations,False,False,False
18586,"and Software Engineering, 2012, 2(1).  
",False,OpenCV Geometrix Transformations,False,False,False
18587,"[3] Kevin Hughes – One more robot learn to see (http ://kevinhughes.ca)  
",False,OpenCV Geometrix Transformations,False,False,False
18588,"[4] Belongie S, Malik J, Puzicha J. “Shape Matching and Object Recognition using shape contexts,” IEEE Transactions on Pattern An alysis and Machine 
",False,OpenCV Geometrix Transformations,False,False,False
18589,"Intelligence, 2002; 24(4):509 -522,  
",False,OpenCV Geometrix Transformations,False,False,False
18590,"[5] Tobias OJ, Seara R. “Image Segmentation by Histogram Thr esholding Using Fuzzy Sets,” IEEE Transactions on Image Processing, 2002; 11(12):1457 -1465.  
",False,OpenCV Geometrix Transformations,False,False,False
18591,"[6] http://www.opencv.org  
",False,OpenCV Geometrix Transformations,False,False,False
18592," 
",False,OpenCV Geometrix Transformations,False,False,False
18593," 
",False,OpenCV Geometrix Transformations,False,False,False
18594," 
",False,OpenCV Geometrix Transformations,False,False,False
18595," 
",False,OpenCV Geometrix Transformations,False,False,False
18596," 
",False,OpenCV Geometrix Transformations,False,False,False
18597," 
",False,OpenCV Geometrix Transformations,False,False,False
18598,"Abstract
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18599,"1 Introduction
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18600,"2 Complex Word Identiﬁcation
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18601,"2.1 A User Study on Word Complexity
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18602,"2.2 A Dataset for Lexical Simpliﬁcation
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18603,"3 Substitution Generation
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18604,"3.1 Context-Aware Word Embedding Models
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18605,"1or GloVe2.
",True,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18606,"Unsupervised Lexical Simpliﬁcation for Non-Native Speakers
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18607,"Gustavo H. Paetzold and Lucia Specia
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18608,"University of Shefﬁeld
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18609,"Western Bank, South Yorkshire S10 2TN
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18610,"Shefﬁeld, United Kingdom
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18611,"Lexical Simpliﬁcation is the task of replacing com-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18612,"plex words with simpler alternatives. We propose a
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18613,"novel, unsupervised approach for the task. It relies ontwo resources: a corpus of subtitles and a new type of
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18614,"word embeddings model that accounts for the ambigu-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18615,"ity of words. We compare the performance of our ap-proach and many others over a new evaluation dataset,
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18616,"which accounts for the simpliﬁcation needs of 400 non-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18617,"native English speakers. The experiments show that ourapproach outperforms state-of-the-art work in Lexical
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18618,"Simpliﬁcation.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18619,"1 Introduction
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18620,"V ocabulary acquisition is a process inherent to human lan-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18621,"guage learning that determines the rate at which an individ-ual becomes familiarised with the lexicon of a given lan-guage. Word recognition, however, is described as a seriesof linguistic sub-processes that establishes one’s capabil-ity of identifying and comprehending individual words ina text. It has been shown that individuals with low-literacylevels or who suffer from certain clinical conditions, suchas Dyslexia (Ellis 2014), Aphasia (Devlin and Tait 1998)and some forms of Autism (Barbu et al. 2015), can face im-pairments in either or both processes, often hindering themincapable of recognising and/or understanding the meaningof texts. Impairments that cause the narrowing of one’s vo-cabulary can be severely crippling: the results obtained by(Hirsh, Nation, and others 1992) show that one must be fa-miliar with at least 95% of the vocabulary of a text in order
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18622,"to understand it, and 98% to read it for leisure.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18623,"Lexical Simpliﬁcation (LS) aims to address this problem
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18624,"by replacing words that may challenge a certain target au-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18625,"dience with simpler alternatives. It was ﬁrst introduced in
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18626,"the work of (Devlin and Tait 1998), who inspired further re-search. The biggest challenge in LS is performing replace-ments without compromising the grammaticality or chang-ing the meaning of the sentence being simpliﬁed.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18627,"Most strategies in the literature take LS as the series of
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18628,"cognitive processes illustrated by the pipeline in Figure 1.By following this model, the performance of LS systems
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18629,"Copyright c/circlecopyrt2016, Association for the Advancement of Artiﬁcial
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18630,"Intelligence (www.aaai.org). All rights reserved.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18631,"Figure 1: Lexical Simpliﬁcation Pipeline
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18632,"has considerably increased in recent years. The approach of(Horn, Manduca, and Kauchak 2014) offers an improvementof62.9% in accuracy over the earlier work of (Biran, Brody,
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18633,"and Elhadad 2011). However, most recent work is limited toexploiting linguistic resources that are scarce and/or expen-sive to produce, such as WordNets and Simple Wikipedia.In this paper, we describe an LS approach that focuses onthe simpliﬁcation needs of non-native English speakers. We
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18634,"propose an unsupervised strategy for Substitution Genera-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18635,"tion, Selection and Ranking. Instead of relying on complexand expensive resources, our approach uses a new context-aware model for word embeddings, which can be easilytrained over large corpora, as well as n-gram frequencies ex-tracted from a corpus of movie subtitles. We also introduce anew domain-speciﬁc dataset for the task, which accounts forthe simpliﬁcation needs of non-native English speakers. Weevaluate the performance of our approaches for each step ofthe pipeline both individually and jointly, comparing them
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18636,"to several other approaches in the literature.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18637,"2 Complex Word Identiﬁcation
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18638,"Complex Word Identiﬁcation (CWI) is the task of deter-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18639,"mining which words in a text should be simpliﬁed, giventhe needs of a certain target audience. It is commonly per-formed before any simpliﬁcation occurs, and aims to pre-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18640,"vent an LS system from making unnecessary substitutions.Most existing work, however, do not provide an explicit so-lution to CWI, and instead model it implicitly (Biran, Brody,and Elhadad 2011; Horn, Manduca, and Kauchak 2014;
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18641,"Glava ˇs and ˇStajner 2015). In order to address CWI and still
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18642,"be able to compare our LS approach to others, we have cho-Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18643,"3761sen to create a dataset that accounts for the simpliﬁcation
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18644,"needs of non-native English speakers.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18645,"In previous work focusing on the evaluation of LS sys-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18646,"tems, (De Belder and Moens 2012) and (Horn, Manduca,and Kauchak 2014) introduce the LSeval and LexMTurkdatasets. The instances in both datasets, 930total, are com-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18647,"posed of a sentence, a target word, and candidate substi-tutions ranked by simplicity. Using different metrics, oneis able to evaluate each step of the LS pipeline over thesedatasets. There is, however, no way of knowing the proﬁleof the annotators who produced these datasets. In both ofthem, the candidate substitutions were suggested and rankedby English speakers from the U.S., who are unlikely to benon-native speakers of English in their majority. This lim-itation renders these datasets unsuitable for the evaluationof our approach because i) the target words used may notbe considered complex by non-native speakers ii) the can-didate substitutions suggested may be deemed complex bynon-native speakers. In order to reuse these resources andcreate a more reliable dataset, we have conducted a userstudy to learn more about word complexity for non-nativespeakers.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18648,"2.1 A User Study on Word Complexity
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18649,"400 non-native speakers participated in the experiment,
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18650,"all university students or staff. They were asked to judgewhether or not they could understand the meaning of eachcontent word (nouns, verbs, adjectives and adverbs, astagged by Freeling (Padr and Stanilovsky 2012)) in a set ofsentences, each of which was judged independently. V olun-teers were instructed to annotate all words that they couldnot understand individually, even if they could comprehendthe meaning of the sentence as a whole.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18651,"All sentences used were taken from Wikipedia, LSeval
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18652,"and LexMTurk. A total of 35,958distinct words from 9,200
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18653,"sentences were annotated (232, 481total), of which 3,854
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18654,"distinct words (6, 388total) were deemed as complex by at
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18655,"least one annotator.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18656,"2.2 A Dataset for Lexical Simpliﬁcation
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18657,"Using the data produced in the user study, we ﬁrst assessedreliability of the LSeval and LexMTurk datasets in evaluat-ing LS systems for non-native speakers. We found that theproportion of target words deemed complex by at least one
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18658,"annotator was only 30.8% for LexMTurk, and 15% for LSe-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18659,"val. As for the candidate substitutions, 21.7% of the ones in
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18660,"LSeval and 13.4% in LexMTurk were deemed complex by
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18661,"at least one annotator.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18662,"These results show that, although they may not be used
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18663,"in their entirety, both datasets contain instances that are suit-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18664,"able for our purposes. To create our dataset, we ﬁrst used the
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18665,"Text Adorning module of LEXenstein (Paetzold and Specia
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18666,"2015; Burns 2013) to inﬂect all candidate verbs and nounsin both datasets to the same tense as the target word. We
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18667,"then used the Spelling Correction module of LEXenstein to
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18668,"correct any misspelled words among the candidates of bothdatasets. Next, we removed all candidate substitutes whichwere deemed complex by at least one annotator in our userstudy. Finally, we discarded all instances in which the tar-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18669,"get word was not deemed complex by any of our annotators.The resulting dataset, which we refer to as NNSeval, con-tains 239instances.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18670,"3 Substitution Generation
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18671,"The goal of Substitution Generation (SG) is to generate can-didate substitutions for complex words. Most LS approachesin the literature do so by extracting synonyms, hyper-nyms and paraphrases from thesauri (Devlin and Tait 1998;De Belder and Moens 2010; Bott et al. 2012). The generatorsdescribed in (Paetzold and Specia 2013; Paetzold 2013) and(Horn, Manduca, and Kauchak 2014) do not use thesauri, butinstead extract candidate substitutions from aligned parallelsentences from Wikipedia in Simple Wikipedia. Althoughparallel corpora can be produced automatically, they still of-fer limited coverage of complex words, and can thus limitthe potential of a simpliﬁer.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18672,"The recent work of (Glava ˇs and ˇStajner 2015) aims to ad-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18673,"dress these limitations by exploiting word embedding mod-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18674,"els (Mikolov et al. 2013), which require only large corpora tobe produced. Given a target word, they extract the 10words
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18675,"from the model for which the embedding vectors have the
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18676,"highest cosine similarity to the one of the complex word it-self. Traditional embedding models suffer, however, from avery severe limitation: they do not accommodate ambiguouswords’ meanings. In other words, all possible meanings of aword are represented by a single numerical vector. We pro-pose a new type of embeddings model that addresses this
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18677,"limitation.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18678,"3.1 Context-Aware Word Embedding Models
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18679,"In order to learn context-aware word embeddings, we resortto annotating the training corpus from which the model islearned. If one is able to assign a sense label to all words inthe training corpus, then a distinct numerical vector wouldbe assigned to each sense of a word. But as shown by
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18680,"(Navigli 2009), state-of-the-art Word Sense Disambiguation
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18681,"(WSD) systems make mistakes more than 25% of the time,
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18682,"are language dependent and can be quite slow to run. Con-
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18683,"sidering that the training corpora used often contain billionsof words, this strategy becomes impractical.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18684,"We instead compromise by using Part-Of-Speech (POS)
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18685,"tags as surrogates for sense labels. Although they do not con-vey the same degree of information, they account for someof the ambiguity inherent to words which can take multi-ple grammatical forms. Annotating the corpus with raw POStags in Treebank format (Marcus, Marcinkiewicz, and San-torini 1993), however, could introduce a lot of sparsity toour model, since it would generate a different tag for eachinﬂection of nouns and verbs, for example. To avoid spar-sity, we generalise all tags related to nouns, verbs, adjectivesand adverbs to N, V , J and R, respectively.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18686,"Once the words in the training corpus are annotated with
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18687,"their generalised POS tags, the model can be trained withany tool available, such as word2vec
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18688,"1or GloVe2.
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18689,"1https://code.google.com/p/word2vec/
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18690,"2http://nlp.stanford.edu/projects/glove/
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18691,"37623.2 Candidate Generation Algorithm
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18692,"37623.2 Candidate Generation Algorithm
",False,Paetzold and Specia - 2016 - Unsupervised Lexical Simplification for Non-Native (1),False,False,True
18693,"ABSTRACT
",True,PROTECT PERSONAL PRIVACY,False,False,True
18694,"1. INTRODUCTION
",True,PROTECT PERSONAL PRIVACY,False,False,True
18695,"2. VARIOUS METHODS FOR SPAM DETECTION
",True,PROTECT PERSONAL PRIVACY,False,False,True
18696,"3. MATHEMATICAL ANALYSIS
",True,PROTECT PERSONAL PRIVACY,False,False,True
18697,"4. ANALYSIS AND ASSESSING 
",True,PROTECT PERSONAL PRIVACY,False,False,True
18698,"5. RESULT AND CONCLUSION
",True,PROTECT PERSONAL PRIVACY,False,False,True
18699,"42
",False,PROTECT PERSONAL PRIVACY,False,False,True
18700,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18701,"PROTECT PERSONAL PRIV ACY 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18702,"AND WASTING TIME USING 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18703,"NLP: A COMPARATIVE 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18704,"APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18705,"Arun Velu, 1 Dr. Pawan Whig 2
",False,PROTECT PERSONAL PRIVACY,False,False,True
18706,"Director Equifax and Researcher, Atlanta , USA 1
",False,PROTECT PERSONAL PRIVACY,False,False,True
18707,"Senior IEEE Member, Dean Research, Vivekananda Institute of 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18708,"Professional Studies, New Delhi, India 2Vivekananda Journal of Research
",False,PROTECT PERSONAL PRIVACY,False,False,True
18709,"October, 2021, V ol. 10, Special Issue, Pg No.42-52
",False,PROTECT PERSONAL PRIVACY,False,False,True
18710,"ISSN 2319-8702(Print)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18711,"ISSN 2456-7574(Online)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18712,"Peer Reviewed Refereed Journal
",False,PROTECT PERSONAL PRIVACY,False,False,True
18713,"© Vivekananda Institute of Professional Studies
",False,PROTECT PERSONAL PRIVACY,False,False,True
18714,"http://www.vips.edu/vjr.php
",False,PROTECT PERSONAL PRIVACY,False,False,True
18715,"In one of the   research Study   Joe  et.  al  proposed  that   33% People classify SMS 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18716,"Spam as annoying  while about 25%  wasting time, and  nearly 22 % violating personal 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18717,"privacy . Spam is the  unwanted irritating email systems to indiscriminately send out 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18718,"spontaneous posts in wholesale. While e-mail spam is most well known, the term is 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18719,"used in other media  for similar abuses. Text  spam, generally unwanted bulk message 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18720,"with some business interest, is similar in the context of email spams. SMS spam is 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18721,"used to spread phishing links and commercial advertising. Commercial spammers use 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18722,"malware that is illegal in most countries to send SMS spam. Transfer unwanted spam 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18723,"on an endangered machine reduces the danger to the spammer, which complicates 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18724,"the origin of junk mail. The presence of known words, phrases, abbreviations and 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18725,"idioms can greatly influence the detection of SMS spam. The objective of this paper 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18726,"is to analyse and evaluate different classification techniques based on their accuracy, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18727,"recall and CAP curves. There was a comparison between traditional methods of 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18728,"machine learning and deep learning.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18729,"Keywords: machine learning , spammer, malware, SMS
",False,PROTECT PERSONAL PRIVACY,False,False,True
18730,"1. INTRODUCTION
",False,PROTECT PERSONAL PRIVACY,False,False,True
18731,"Whenever SMS spam reaches the inbox of a user, the mobile phone alerts the user. If the user 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18732,"realizes that the message is unwanted, they are disappointed, and also some mobile telephone storage is 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18733,"carried out by SMS spam. The detection of SMS spam is an important task for identifying and filtering 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18734,"spam text messages. More SMS messages are sent daily so that a operator can  correlate newer SMS 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18735,"posts in the situation of previously conventional SMS messages. It is also very challenging. We thus 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18736,"attempt to advance a web-based SMS text junk mail or ham gauge by using the knowledge of artificial 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18737,"intelligence by combining machine learning and data mining. 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18738,"Electronic mail remains a very economical communication method, but hackers use it as a method 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18739,"for spreading the virus, phishing and malicious code, etc. Email is very useful and convenient to use, but 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18740,"many people misuse it too. Though there has been a steady increase in email use as well as the number 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18741,"of spams, other communication media like Facebook, Twitter, and chat programs. There has been an 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18742,"exponential increase in e-mail service. The average increase in the number of messages sent daily during 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18743,"2019 was 538.1 million since 2010. Spam volume during 2018 was 89-92%. 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18744,"Spam emails become one of the most serious issues for email users and affect email users 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18745,"negatively. HAM is a real email to receive and SPAM is a spurious mail to thousands of users with 43
",False,PROTECT PERSONAL PRIVACY,False,False,True
18746,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18747,"bad intentions from unreliable sources in bulk. Spam is deliberately sent to users where it is not meant 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18748,"to be received by the recipient. Spam operators attempt by luring them with attractive offers to extract 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18749,"sensitive user data.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18750,"Because of the high number of methods for separating HAM and SPAM, a single method which 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18751,"can lead to a very small positive rate is difficult to narrow. Although many methods and algorithms are 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18752,"available to combat Spam Problem, there is no perfect single method or algorithm. During the isolation 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18753,"process, many HAM mails are marked as spam. This situation is called False by misrepresenting HAM 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18754,"as SPAM.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18755,"This research paper is organized in various sections. The first area focuses on associated work 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18756,"in the machine and non-machine learning processes used to classify e-mails as HAM or SPAM. The 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18757,"second section provides an insight into the collaborative approach and related work in this field. Spam 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18758,"control. The third section discusses the techniques of various machine learning algorithms   to find out 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18759,"comparative analysis and   to conclude which algorithm is best  in terms of accuracy and other factors.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18760,"2. VARIOUS METHODS FOR SPAM DETECTION
",False,PROTECT PERSONAL PRIVACY,False,False,True
18761," ML and Non ML methods
",False,PROTECT PERSONAL PRIVACY,False,False,True
18762,"Machine learning and non-machine learning are two major categories for spam filtering. Non-
",False,PROTECT PERSONAL PRIVACY,False,False,True
18763,"machine learning methods use a spam filter procedure such as White-Listing, Black-List and keyword 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18764,"search . Non-machine teaching is easy to implement and to experiment, so spammers are highly likely 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18765,"to bypass non-machine teaching. However, a strong search for keywords and continuous updating of the 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18766,"whitelist and blacklist may still be more successful.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18767,"The approach to machine learning coincides greatly with the characterization of text and thus 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18768,"iterates researchers’ interest. Researchers have used many approaches in machine learning such as vector 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18769,"support, memory based learning, Ripper rules-based learning, boosting decision-making processes, hard 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18770,"sets, neural networks and Bayesian classifiers. Most approaches to email classification are based on a 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18771,"single text classification algorithm. Rough theory or rough theory-based methods for email classification 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18772,"are most popular among researchers. Emails are not written as they should be written when the malicious 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18773,"task is used. In such circumstances, emails are difficult to categorise, but the raw theory is quite inaccurate, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18774,"inconsistent and unprecedented. Some researchers have used data mining methods, such as SVM, neural 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18775,"network, naive Bayesian and theoretical rough-set. The problem, however, was that these methods were 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18776,"employed with a set of rules. Researchers such as Chouchoulas, Zhao and Zhang, Zhao and Zhu have 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18777,"only experimented with spam rule generation keyword frequency.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18778," Cooperation approach
",False,PROTECT PERSONAL PRIVACY,False,False,True
18779," Many solutions are available on standalone servers to classify spam. The solutions on stand-alone 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18780,"servers include Counter Attack, Opt-Outlist and Spam filter. Standalone servers have limited computing 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18781,"power and speed, so a new type of spam mail cannot be filtered by standalone servers. Any new mail 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18782,"type requires the creation and use of new rules. The collaborative approach has the necessary computing 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18783,"power and speed to produce and use new anti-spam rules as required.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18784," Spam prevention
",False,PROTECT PERSONAL PRIVACY,False,False,True
18785,"SPAM may be handled in two different ways, i.e. at the place of origin, check and block spam 44
",False,PROTECT PERSONAL PRIVACY,False,False,True
18786,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18787,"or during mail receipt. Spammers are aimed at servers that allow another server to use them as an 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18788,"intermediary channel. These low-security unattended servers are called . Because spammers use Botnets 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18789,"and the location of their origin constantly changes, spam can be easily checked at its origin. Some 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18790,"servers are black listing for SPAM spreading or used as a SPAM spreading channel. Since the source 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18791,"is not confident, the mail is called spam directly. On the internet, there are many open proxy servers. 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18792,"Spammers will also spread SPAM on these servers. When an open proxy server is used by spammers, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18793,"the actual e-mail source is hard to identify. Once the email is sent to the mail server, the second method 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18794,"of verification and classification is HAM and SPAM.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18795,"PEAS Representation
",False,PROTECT PERSONAL PRIVACY,False,False,True
18796,"PM : Performance Metrics  means  indicates How systematizes the AI distinguish it’s doing?
",False,PROTECT PERSONAL PRIVACY,False,False,True
18797,"E: Environment tells  what situation in which the agent relates?
",False,PROTECT PERSONAL PRIVACY,False,False,True
18798,"A: Actuators describes the  effect prepares AI have on surroundings?
",False,PROTECT PERSONAL PRIVACY,False,False,True
18799,"S: Sensors is used  to gather AI data starting its surroundings?
",False,PROTECT PERSONAL PRIVACY,False,False,True
18800,"The following is a PEAS summary of the classifier’s task environment:
",False,PROTECT PERSONAL PRIVACY,False,False,True
18801,"Table 1 PEAS representation of AI Problem
",False,PROTECT PERSONAL PRIVACY,False,False,True
18802," Description of algorithms
",False,PROTECT PERSONAL PRIVACY,False,False,True
18803,"ML processes can be used to recognize junk mail or ham by appropriate scheme to the relevant 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18804,"labels and to make predictions or classifications by means of a trained model as the objective of the 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18805,"project will be to categorize the texts into junk mail or ham, which is the problems of Dual Grouping.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18806," Sack-up or Bagging
",False,PROTECT PERSONAL PRIVACY,False,False,True
18807,"One of the important algorithm Bagging practices a modest method that appears repeatedly in 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18808,"statistical analyses — to improve one’s estimates by combining many estimates. Bagging constructs n 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18809,"classification trees by sampling bootstrap data and combines predictions in order to yield a concluding 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18810,"prediction.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18811," Random Forest (RF)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18812,"RF or forests of haphazard decision are a group technique of learning to classify the regression 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18813,"and to do other tasks by creating a multitude of decision-making trees and delivering the classes that 45
",False,PROTECT PERSONAL PRIVACY,False,False,True
18814,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18815,"fashion the individual trees for classes (classification) or mean (regression). Random decision forests 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18816,"have the habit of overfitting their training set for decision-making bodies
",False,PROTECT PERSONAL PRIVACY,False,False,True
18817," Naïve Bavarian (NB)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18818,"As their name implies, this algorithm assumes that each of the data sets’ variables is “Naïve,” 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18819,"i.e. not interconnected. The calculation of each hypothesis is simplified in order to make the calculation 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18820,"tractable. Naïve Bayes or Idiot Bayes are called. Naïve Bayes is an algorithm for viral classification 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18821,"which is used mainly to achieve the dataset’s basis accuracy. While the algorithm is naive, it is quicker 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18822,"than substantial data sets for SMS information to be classified on the basis of probability. This reduces 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18823,"the burden on the AI arrangement and retains the productivity efficient and controllable. Because of the 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18824,"lack of a well-trained Bayes classifier, they are relaxed and fast to implement. This allows real data to 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18825,"be tested without outlay much time and change emerging the model. They are prepared to predict when 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18826,"applied.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18827," Classification of Extra Trees
",False,PROTECT PERSONAL PRIVACY,False,False,True
18828,"The Random Forest is like this. It produces several trees and splits nodes by using random sub-
",False,PROTECT PERSONAL PRIVACY,False,False,True
18829,"sets of characteristics, but it does not bootstrap observations with two key differences (that is, samples 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18830,"with no substitution), and nodes are split in random splits and not in best divisions.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18831," Support Vector Machine (SVM)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18832,"Support-vector machines are supervised learning models that analyse data used for classification 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18833,"and regression analysis, with their associated learning algorithms. SVMs are also supported vector 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18834,"networks. In light of a number of training examples, each of which is marked as one category of two, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18835,"and an SVM training algorithm creates a model that assigns new examples to a single category or another 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18836,"and makes it not likely to be a binary linear classification. SVM is a great way to classify binary data, as 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18837,"SVM categorizes facts by result the best plane and distinguishes facts points from the other class.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18838," KNN
",False,PROTECT PERSONAL PRIVACY,False,False,True
18839,"Classification based on KNN K-Nearest in neighbouring countries is a kind of lethargic learning, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18840,"because it fixes not attempt to shape a general internal model, but only stores training data instances. 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18841,"Classification of the k shall be calculated by simple majority vote of the closest neighbours. This algorithm 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18842,"is easy to implement, robust, bright and useful when there are extensive training data. The drawback is, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18843,"however, that the value of K must be established and the calculation costs are high, since the detachment 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18844,"of each occurrence from all the training examples must be calculated.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18845," Decision  Tree 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18846,"The division criteria standard lies behind the intellect of any classification of the tab. Decision 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18847,"trees are similarly obtainable to a tree-structured current chart where the instances are classified according 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18848,"to their properties. An instance, the results of the trial labeled by the outlet and the leaf bump represent 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18849,"the class label, represents a node in a Decision Tree.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18850,"3. MATHEMATICAL ANALYSIS
",False,PROTECT PERSONAL PRIVACY,False,False,True
18851,"Information about dataset used can be given by using  following command 46
",False,PROTECT PERSONAL PRIVACY,False,False,True
18852,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18853,"Bar graph representation of above data is obtained by using following commands
",False,PROTECT PERSONAL PRIVACY,False,False,True
18854,"#Palette
",False,PROTECT PERSONAL PRIVACY,False,False,True
18855,"cols= [“#E1F16B”, “#E598D8”] 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18856,"#first of all let us evaluate the target and find out if our data is imbalanced or not
",False,PROTECT PERSONAL PRIVACY,False,False,True
18857,"plt.figure(figsize=( 12,8))
",False,PROTECT PERSONAL PRIVACY,False,False,True
18858,"fg = sns.countplot(x= data[ “target” ], palette= cols)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18859,"fg.set_title( “Count Plot of Classes” , color=”#58508d”)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18860,"fg.set_xlabel( “Classes”, color=”#58508d”)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18861,"fg.set_ylabel( “Number of Data points” , color=”#58508d”)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18862,"after executing above commands we get 47
",False,PROTECT PERSONAL PRIVACY,False,False,True
18863,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18864,"Figure 1  classiffication  of  data
",False,PROTECT PERSONAL PRIVACY,False,False,True
18865,"The description of data  is  represented by 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18866,"data[ “No_of_Characters” ] = data[ “text” ].apply(len)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18867,"data[ “No_of_Words”]=data.apply( lambda  row: nltk.word_tokenize(row[ “text” ]), axis=1).
",False,PROTECT PERSONAL PRIVACY,False,False,True
18868,"apply(len)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18869,"data[ “No_of_sentence” ]=data.apply( lambda  row: nltk.sent_tokenize(row[ “text” ]), axis=1).
",False,PROTECT PERSONAL PRIVACY,False,False,True
18870,"apply(len)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18871,"data.describe().T
",False,PROTECT PERSONAL PRIVACY,False,False,True
18872,"Top 10 Ham and Spam Words used are given by 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18873,"#for counting frequently occurence of spam and ham.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18874,"count1 = Counter( “ “.join(data[data[ ‘target’]==’ham’][“text” ]).split()).most_common( 10)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18875,"data1 = pd.DataFrame.from_dict(count1)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18876,"data1 = data1.rename(columns={ 0: “words of ham”, 1 : “count” })48
",False,PROTECT PERSONAL PRIVACY,False,False,True
18877,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18878,"count2 = Counter( “ “.join(data[data[ ‘target’]==’spam’][“text” ]).split()).most_common( 10)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18879,"data2 = pd.DataFrame.from_dict(count2)
",False,PROTECT PERSONAL PRIVACY,False,False,True
18880,"data2 = data2.rename(columns={ 0: “words of spam”, 1 : “count_” })
",False,PROTECT PERSONAL PRIVACY,False,False,True
18881,"Figure 2 Frequency of ham words49
",False,PROTECT PERSONAL PRIVACY,False,False,True
18882,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18883,"Figure 3  frequency  of spam words
",False,PROTECT PERSONAL PRIVACY,False,False,True
18884,"4. ANALYSIS AND ASSESSING 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18885,"The following metrics were used to determine certain evaluation metrics:
",False,PROTECT PERSONAL PRIVACY,False,False,True
18886,"We used accuracy as the primary evaluation criteria for our classifiers because it is an intuitive 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18887,"metric with a simple interpretation: simply counting correctly classified messages.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18888," Important Matrices 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18889,"To get the required information to validate any proposal or comparison, it is significant to select 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18890,"the precise performance metrics for setup. We consider the following known metrics in order to analyse 50
",False,PROTECT PERSONAL PRIVACY,False,False,True
18891,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18892,"and compare the detection capacities of the classifiers considered:
",False,PROTECT PERSONAL PRIVACY,False,False,True
18893," Accuracy 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18894,"Accuracy is the most intuitive performance measurement and simply a proportion with all the 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18895,"observations that have been accurately predicted. One could believe that if it is highly accurate, our 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18896,"model is best. Yes, accuracy is only a big step if the values of false positive and negative are almost 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18897,"identical, but only when you have symmetrical data sets. Therefore, you need to look at other parameters 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18898,"to assess the performance of your model.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18899," 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18900,"  
",False,PROTECT PERSONAL PRIVACY,False,False,True
18901,"The ratio of correctly predicted positive observations to the total predicted positive observations 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18902,"is known as precision. The question that this measure answers is how many of the passengers who were 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18903,"labelled as having survived really did. The low false positive rate is related to high precision. 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18904,"Precision  = 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18905," Recall  
",False,PROTECT PERSONAL PRIVACY,False,False,True
18906,"It is also  called  sensitivity  and  defined as the correctly predicted ratio of positive observations 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18907,"to all actual class observations.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18908," 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18909,"Table 2 Comparisons among  various algorithm
",False,PROTECT PERSONAL PRIVACY,False,False,True
18910,"Algorithm 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18911,"Used True Positive False Negative False Positive True Negative
",False,PROTECT PERSONAL PRIVACY,False,False,True
18912,"Naive Bayes 1425 15 14  218
",False,PROTECT PERSONAL PRIVACY,False,False,True
18913,"SVM 1303 141 136 92
",False,PROTECT PERSONAL PRIVACY,False,False,True
18914,"KNeighbors 1439 0 233 0
",False,PROTECT PERSONAL PRIVACY,False,False,True
18915,"Decision Tree 1421 27 18 206
",False,PROTECT PERSONAL PRIVACY,False,False,True
18916,"Extra Tree 1434 25 5 208
",False,PROTECT PERSONAL PRIVACY,False,False,True
18917,"Random Forest 1437 32 2 201
",False,PROTECT PERSONAL PRIVACY,False,False,True
18918,"AdaBoost 1417 36 22 197
",False,PROTECT PERSONAL PRIVACY,False,False,True
18919,"Bagging 1422 27 17 206
",False,PROTECT PERSONAL PRIVACY,False,False,True
18920,"Table 3 Comparison in terms of Accuracy ,Precision and Recall51
",False,PROTECT PERSONAL PRIVACY,False,False,True
18921,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18922,"Algorithm Accuracy Precision Recall AR Posi-
",False,PROTECT PERSONAL PRIVACY,False,False,True
18923,"tive 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18924,"Naïve bayes 98.7 0.98 0.93 0.92 0.94
",False,PROTECT PERSONAL PRIVACY,False,False,True
18925,"SVM 98.9 0.96 0.98 0.93 0.98
",False,PROTECT PERSONAL PRIVACY,False,False,True
18926,"KNeighbors 98.7 0.94 0.92 0.96 0.98
",False,PROTECT PERSONAL PRIVACY,False,False,True
18927,"Decision Tree 98.5 0.97 0.96 0.96 0.94
",False,PROTECT PERSONAL PRIVACY,False,False,True
18928,"Random Forest 98.0 0.95 0.98 0.93 0.95
",False,PROTECT PERSONAL PRIVACY,False,False,True
18929,"AdaBoost 98.1 0.94 0.99 0.98 0.97
",False,PROTECT PERSONAL PRIVACY,False,False,True
18930,"CNN 99.2 0.99 0.94 0.94 0.89
",False,PROTECT PERSONAL PRIVACY,False,False,True
18931,"Bagging 98.2 0.94 0.99 0.99 0.99
",False,PROTECT PERSONAL PRIVACY,False,False,True
18932,"5. RESULT AND CONCLUSION
",False,PROTECT PERSONAL PRIVACY,False,False,True
18933,"This research  studies  focused on debating about how to protect personal privacy and wasting 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18934,"time  with the application of AI enabled technology and testing ML techniques for junk mail or Junk 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18935,"SMS detection.  Several algorithms  compared with different classifiers to see which one was the best. 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18936,"The results of our classifier evaluation show that the CNN Classifier achieves the highest accuracy of  99  
",False,PROTECT PERSONAL PRIVACY,False,False,True
18937,"percent and  about 98 percent, respectively, for the  given dataset, with AR values of 0.99 . While CNN 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18938,"has been widely used in image classification, it has shown substantial improvements over predictable 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18939,"classifiers and achieves the maximum accurateness among them for word-based data as well. This 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18940,"achievement by CNN  greatly expanded the research area of its presentation to text-related classification 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18941,"issues, such as review classification and sentiment prediction. SVM and NB, as predicted, perform well 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18942,"among conventional classifiers, coming close to CNN for both datasets. Noteworthy outcomes have 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18943,"been got as a result of this work, indicating that this study can be applied in the real world to detect spams 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18944,"SMS. This research Study can be   very useful for the researchers  working in the same field.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18945," Future Scope
",False,PROTECT PERSONAL PRIVACY,False,False,True
18946,"This research study can be helpful for the researchers working in the same field. Some  New 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18947,"Machine learning   or deep learning algorithm can  be   applied in order  to get more accurate result.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18948,"REFERENCE
",False,PROTECT PERSONAL PRIVACY,False,False,True
18949,"[1]  H. Najadat, N. Abdulla, R. Abooraig, and S. Nawasrah, “Mobile SMS Spam Filtering based on 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18950,"Mixing Classifiers,” International Journal of Advanced Computing Research, vol. 1, 2014
",False,PROTECT PERSONAL PRIVACY,False,False,True
18951,"[2]  J. Deng, H. Xia, Y. Fu, J. Zhou, and Q. Xia, “Intelligent spam filtering for massive short message 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18952,"stream,” COMPEL-The international journal for computation and mathematics in electrical and 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18953,"electronic engineering, vol. 32, pp. 586-596, 2013.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18954,"[3]  K. Yadav, P. Kumaraguru, A. Goyal, A. Gupta, and V. Naik, “SMSAssassin: Crowdsourcing 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18955,"Driven Mobile-based System for SMS Spam Filtering”, HotMobile’11 Proceedings of the 12th 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18956,"Workshop on Mobile Computing Systems and Applications, pp. 1-6, 2011.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18957,"[4]  B. S.-E. Kim, J.-T. Jo, and S.-H. Choi, “SMS Spam Filtering Using Keyword Frequency Ratio,” 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18958,"International Journal of Security and Its Applications, vol. 9, pp. 329-336, 2015.52
",False,PROTECT PERSONAL PRIVACY,False,False,True
18959,"PROTECT PERSONAL PRIV ACY AND WASTING TIME USING NLP: A COMPARATIVE APPROACH USING AI 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18960,"[5]  T. M. Mahmoud and A. M. Mahfouz, “SMS Spam Filtering Technique Based on Artificial 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18961,"Immune,” IJCSI International Journal of Computer Science Issues, vol. 9, 2012.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18962,"[6]  Y . Yang and S. Elfayoumy, “Anti-Spam Filtering Using Neural Networks and Bayesian 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18963,"Classifiers”, Proceedings of the 2007 IEEE International Symposium on Computational 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18964,"Intelligence in Robotics and Automation, Jacksonville, FL, USA, June 20-23, 2007.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18965,"[7]  David Ndumiyana, Munyaradzi Magomelo, and Lucy Sakala, “Spam Detection using a Neural 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18966,"Network Classifier,” Online Journal of Physical and Environmental Science Research, vol. 2, 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18967,"issue 2, pp. 28-37, April 2013.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18968,"[8]  T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami, “Contributions to the Study of SMS Spam 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18969,"Filtering: New Collection and Results (preprint),” Proceedings of the 11th ACM symposium on 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18970,"Document engineering, Mountain View, California, USA, pp. 259-262, 2011.
",False,PROTECT PERSONAL PRIVACY,False,False,True
18971,"[9]  Pawan Whig and S. N Ahmad,” Modelling and Simulation of economical Water Quality 
",False,PROTECT PERSONAL PRIVACY,False,False,True
18972,"Monitoring Device “, Journal of aquaculture & Marine Biology, 2016, Vol.4,Issue 6, pp.1-
",False,PROTECT PERSONAL PRIVACY,False,False,True
18973,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,False,False,True
18974,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,False,False,True
18975,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18976,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18977,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18978,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18979,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18980,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18981,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18982,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18983,"6(Scopus).  ISSN: 2378-3184
",False,PROTECT PERSONAL PRIVACY,True,False,True
18984,Abstract,True,pub07,False,False,False
18985,"I. INTRODUCTION
",True,pub07,False,False,False
18986,"II. P
",True,pub07,False,False,False
18987,"III. T
",True,pub07,False,False,False
18988,"IV. ANALYSIS OF THE PRECORRECTED -FFT A LGORITHM
",True,pub07,False,False,False
18989,"V. R
",True,pub07,False,False,False
18990,"VI. R
",True,pub07,False,False,False
18991,"VII. C
",True,pub07,False,False,False
18992,"IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997 1059
",False,pub07,False,False,False
18993,"A Precorrected-FFT Method for Electrostatic
",False,pub07,False,False,False
18994,"Analysis of Complicated 3-D Structures
",False,pub07,False,False,False
18995,"Joel R. Phillips and Jacob K. White, Associate Member, IEEE
",False,pub07,False,False,False
18996,"celerating the potential calculation which occurs in the innerloop of iterative algorithms for solving electromagnetic boundaryintegral equations. Such integral equations arise, for example,in the extraction of coupling capacitances in three-dimensional(3-D) geometries. We present extensive experimental compar-isons with the capacitance extraction code FASTCAP [1] anddemonstrate that, for a wide variety of geometries commonlyencountered in integrated circuit packaging, on-chip interconnectand micro-electro-mechanical systems, the new “precorrected-FFT” algorithm is superior to the fast multipole algorithm used
",False,pub07,False,False,False
18997,"in FASTCAP in terms of execution time and memory use. At
",False,pub07,False,False,False
18998,"engineering accuracies, in terms of a speed-memory product, thenewalgorithmcanbesuperiortothefastmultipolebasedschemesby more than an order of magnitude.
",False,pub07,False,False,False
18999,"Index Terms— Capacitance extraction, dense matrix algebra,
",False,pub07,False,False,False
19000,"electrostatic analysis, fast Fourier transform, integral equation.
",False,pub07,False,False,False
19001,"I. INTRODUCTION
",False,pub07,False,False,False
19002,"APPLICATIONS as diverse as analysis of signal integrity
",False,pub07,False,False,False
19003,"in integrated circuit interconnect, characterization of
",False,pub07,False,False,False
19004,"electrical packaging, and design of microelectromechanical
",False,pub07,False,False,False
19005,"systems [2] require accurate electrostatic analysis of com-plicated three-dimensional (3-D) structures. Recent work ontechniques for rapid electrostatic analysis for capacitance
",False,pub07,False,False,False
19006,"extraction have been based on random-walk methods [3],
",False,pub07,False,False,False
19007,"partitioning heuristics combined with techniques from matrixextension theory [4], [5], ﬁnite-difference [6], [7] or ﬁnite-element methods [8], [9], or method-of-moments [10]
",False,pub07,False,False,False
19008,"techniques.
",False,pub07,False,False,False
19009,"Algorithms using method of moments [10] or weighted
",False,pub07,False,False,False
19010,"residuals [11], [12] based discretizations of integral equa-tion formulations, also known as boundary-element methods
",False,pub07,False,False,False
19011,"[13], are commonly used to perform electrostatic analyzes,
",False,pub07,False,False,False
19012,"but such approaches generate dense matrix problems whichare computationally expensive to solve, and this limits thecomplexity of problems which can be analyzed. Multipole-
",False,pub07,False,False,False
19013,"accelerated iterative methods [1], [14]–[16] have recently been
",False,pub07,False,False,False
19014,"Manuscript received February 15, 1996; revised April 18, 1997. This
",False,pub07,False,False,False
19015,"work was supported by the Defense Advanced Research Project Agency
",False,pub07,False,False,False
19016,"Contract J-FBI-95-215 and Subgrant 536040-52223, by the National Science
",False,pub07,False,False,False
19017,"Foundation Contract MIP-8858764 A02, by an NDSEG Fellowship, and by
",False,pub07,False,False,False
19018,"grants from Digital Equipment Corporation, IBM, and the Consortium for
",False,pub07,False,False,False
19019,"Superconducting Electronics. This paper was recommended by Associate
",False,pub07,False,False,False
19020,"Editor K. Mayaram.
",False,pub07,False,False,False
19021,"J. R. Phillips was with the Research Laboratory of Electronics, Massachu-
",False,pub07,False,False,False
19022,"settsInstituteofTechnology,Cambridge,MA02139USA.HeisnowwithCa-dence Design Systems, San Jose, CA 95134 USA (e-mail: jrp@cadence.com).
",False,pub07,False,False,False
19023,"J. K. White is with the Research Laboratory of Electronics, Massachusetts
",False,pub07,False,False,False
19024,"Institute of Technology, Cambridge, MA 02139 USA.
",False,pub07,False,False,False
19025,"Publisher Item Identiﬁer S 0278-0070(97)09247-6.used to reduce the computational cost of boundary-element
",False,pub07,False,False,False
19026,"based methods, but these techniques are still computationallyexpensive and, of more immediate consequence, memoryexhausting.
",False,pub07,False,False,False
19027,"In [1], a fast algorithm for electrostatic analysis of
",False,pub07,False,False,False
19028,"3-D structures was presented. The computation time for thealgorithm was shown to grow nearly as
",False,pub07,False,False,False
19029,", where is the
",False,pub07,False,False,False
19030,"number of panels used to discretize the conductor surfaces,and
",False,pub07,False,False,False
19031,"is the number of conductors. The algorithm of [1]
",False,pub07,False,False,False
19032,"was based on the hierarchical multipole algorithm [15], which
",False,pub07,False,False,False
19033,"can perform the dense matrix-vector product associated with
",False,pub07,False,False,False
19034,"discretized potential integral equations in order- ()
",False,pub07,False,False,False
19035,"time and memory. In this paper, we describe a precorrected-FFT approach which can replace the fast multipole algorithmfor accelerating the Coulomb potential calculation needed toperform the matrix-vector product. The central idea of thealgorithm is to represent the long-range part of the Coulomb
",False,pub07,False,False,False
19036,"potential by point charges lying on a uniform grid, rather
",False,pub07,False,False,False
19037,"than by series expansions as in fast multipole algorithms [15].This grid representation allows the fast Fourier transform(FFT) [17]–[19] to be used to efﬁciently perform potentialcomputations.Becauseonlythelong-rangepartofthepotentialis represented by the grid, the grid is not coupled to the
",False,pub07,False,False,False
19038,"underlying discretization of the structure. Decoupling the long
",False,pub07,False,False,False
19039,"and short range parts of the potentials allows the algorithm tosolve problems which may be discretized in a very irregularfashion in nearly optimal time.
",False,pub07,False,False,False
19040,"Numerous algorithms exist for the “n-body problem” of
",False,pub07,False,False,False
19041,"evaluating the potential of a set of charges at all the other
",False,pub07,False,False,False
19042,"charge points, such as the “particle-mesh” methods (see [20]
",False,pub07,False,False,False
19043,"for extensive references), the fast multipole method (FMM)[15],andmultigridmethods[21].Thevariousalgorithmsdifferin the way the long range potential is approximated and inthe way local interactions are treated. We have attemptedto develop an algorithm which, like particle-mesh methods,
",False,pub07,False,False,False
19044,"exploits the availability of efﬁcient discrete Fourier transform
",False,pub07,False,False,False
19045,"implementations while at the same time preserves the higheraccuracy of the multipole-based schemes, but is also (likemultigrid schemes) easily adapted to a broad class of kernels.In addition, the algorithm is, in the way local interactions aretreated, particularly adapted to boundary-integral solvers.
",False,pub07,False,False,False
19046,"The precorrected-FFT method, described below, is at best
",False,pub07,False,False,False
19047,"an
",False,pub07,False,False,False
19048,"algorithm. It is possible to construct geometries
",False,pub07,False,False,False
19049,"for which the performance of the precorrected-FFT algorithmis inferior to the fast multipole methods, but we demon-strate that for many structures associated with packaging,on-chip interconnect, and micro-electro-mechanical systems,
",False,pub07,False,False,False
19050,"0278–0070/97$10.00 1997 IEEE1060 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19051,"precorrected-FFT methods are faster and use substantially less
",False,pub07,False,False,False
19052,"memory.
",False,pub07,False,False,False
19053,"The outline of the paper is as follows. The boundary-
",False,pub07,False,False,False
19054,"element formulation and a standard iterative algorithm for
",False,pub07,False,False,False
19055,"solving the generated matrix problem are brieﬂy reviewed
",False,pub07,False,False,False
19056,"in Section II. The precorrected-FFT method is described inSection III and some analysis of the algorithm performed inSection IV. Simple examples are examined in Section V to
",False,pub07,False,False,False
19057,"show various aspects of the algorithm. In Section VI, our
",False,pub07,False,False,False
19058,"precorrected-FFT method is compared to FASTCAP on avariety of realistic examples, and is shown to be faster and usesubstantially less memory. Finally, in Section VII, we discuss
",False,pub07,False,False,False
19059,"some possible extensions of the algorithm and strategies to
",False,pub07,False,False,False
19060,"reduce the computational complexity for very inhomogeneousproblems.
",False,pub07,False,False,False
19061,"II. P
",False,pub07,False,False,False
19062,"ROBLEM FORMULATION
",False,pub07,False,False,False
19063,"The capacitance of an -conductor geometry can then be
",False,pub07,False,False,False
19064,"summarized by an symmetric matrix , where the
",False,pub07,False,False,False
19065,"entry represents capacitive coupling between conductors
",False,pub07,False,False,False
19066,"and.Todeterminethe thcolumnofthecapacitancematrix,
",False,pub07,False,False,False
19067,"one need only solve for the surface charges on each conductor
",False,pub07,False,False,False
19068,"produced by raising conductor to one volt while grounding
",False,pub07,False,False,False
19069,"the rest. If the conductors are embedded in a homogeneousdielectric, these
",False,pub07,False,False,False
19070,"potential problems can be solved using
",False,pub07,False,False,False
19071,"an equivalent free space formulation in which the conductor-
",False,pub07,False,False,False
19072,"dielectric interfaces are replaced by a charge layer of density
",False,pub07,False,False,False
19073,"[22], [23]. The charge layer in the free space problem will
",False,pub07,False,False,False
19074,"be the induced charge in the original problem if satisﬁes the
",False,pub07,False,False,False
19075,"integral equation
",False,pub07,False,False,False
19076,"surfaces (1)
",False,pub07,False,False,False
19077,"where is the known conductor surface potential, is
",False,pub07,False,False,False
19078,"the differential conductor surface area, is the
",False,pub07,False,False,False
19079,"dielectric constant, and is the usual Euclidean length of
",False,pub07,False,False,False
19080,". This approach may be extended to the case of piecewise-
",False,pub07,False,False,False
19081,"constant dielectrics [23].
",False,pub07,False,False,False
19082,"A standard approach [10] to numerically solving (1) for
",False,pub07,False,False,False
19083,"is to use a piecewise constant collocation scheme. That is,the conductor surfaces are broken into
",False,pub07,False,False,False
19084,"small panels, and it
",False,pub07,False,False,False
19085,"is assumed that on each panel , a charge, , is uniformly
",False,pub07,False,False,False
19086,"distributed, as in Fig. 1. Then for each panel, an equation iswritten which relates the known potential at the center of that
",False,pub07,False,False,False
19087,"th panel, denoted , to the sum of the contributions to that
",False,pub07,False,False,False
19088,"potential from the charge distributions on all panels [23].
",False,pub07,False,False,False
19089,"The result is a dense linear system
",False,pub07,False,False,False
19090,"(2)
",False,pub07,False,False,False
19091,"where , is the vector of panel charges,
",False,pub07,False,False,False
19092,"is the vector of known panel potentials, and
",False,pub07,False,False,False
19093,"(3)
",False,pub07,False,False,False
19094,"where the collocation point is the center of the th panel
",False,pub07,False,False,False
19095,"andis the area of the th panel. The dense linear system
",False,pub07,False,False,False
19096,"Fig. 1. Piecewise-constantcollocationdiscretizationoftwoconductors.Con-
",False,pub07,False,False,False
19097,"ductor surfaces are discretized into panels which support a constant charge
",False,pub07,False,False,False
19098,"density.
",False,pub07,False,False,False
19099,"of (2) can be solved to compute panel charges from a given
",False,pub07,False,False,False
19100,"set of panel potentials and the capacitances can be derived bysumming the panel charges.
",False,pub07,False,False,False
19101,"The direct approach of solving (2) via Gaussian elimination,
",False,pub07,False,False,False
19102,"which requires
",False,pub07,False,False,False
19103,"operations and storage, becomes
",False,pub07,False,False,False
19104,"computationally intractable if the number of panels exceedsseveral hundred.
",False,pub07,False,False,False
19105,"III. T
",False,pub07,False,False,False
19106,"HEPRECORRECTED -FFT A PPROACH
",False,pub07,False,False,False
19107,"If, instead of Gaussian elimination, an iterative algorithm
",False,pub07,False,False,False
19108,"such as GMRES [24] is used to solve (2), then each iterationof GMRES will cost
",False,pub07,False,False,False
19109,"operations. This is because the
",False,pub07,False,False,False
19110,"matrix in (2) is dense, and therefore evaluating candidate
",False,pub07,False,False,False
19111,"solution vectors involves a dense matrix-vector multiply. Sev-eral sparsiﬁcation techniques for
",False,pub07,False,False,False
19112,"are based on the idea of
",False,pub07,False,False,False
19113,"directly computing only those portions of associated with
",False,pub07,False,False,False
19114,"interactions between panels which are close to each other. The
",False,pub07,False,False,False
19115,"rest of is then somehow approximated to accelerate the
",False,pub07,False,False,False
19116,"computation [15], [5], [21].
",False,pub07,False,False,False
19117,"To develop a faster approach to computing the matrix-
",False,pub07,False,False,False
19118,"vector product, consider the parallelepiped which contains a
",False,pub07,False,False,False
19119,"3-D problem after it has been discretized into panels. The
",False,pub07,False,False,False
19120,"parallelepiped containing the problem could be subdividedinto an
",False,pub07,False,False,False
19121,"array of small cubes so that each small
",False,pub07,False,False,False
19122,"cube contains only a few panels. Fig. 2(a) shows a discretized
",False,pub07,False,False,False
19123,"sphere, with the associated space subdivided into a 3 3
",False,pub07,False,False,False
19124,"3 array of cubes. We refer to these small cubes as cells.
",False,pub07,False,False,False
19125,"A possible approach to computing distant interactions is to
",False,pub07,False,False,False
19126,"exploit the fact that potentials at evaluation points distant from
",False,pub07,False,False,False
19127,"a cell can be accurately computed by representing the givencell’s charge distribution using a small number of weightedpoint charges. If the point charges all lie on a uniform grid,
",False,pub07,False,False,False
19128,"for example at the cell vertices, then the computation of the
",False,pub07,False,False,False
19129,"potential at the grid points due to the grid charges is a discreteconvolution which can be performed using the FFT. Fig. 2(b)shows a possible set of grid charges for the cell subdivisions
",False,pub07,False,False,False
19130,"showninFig. 2(a).Thus,afourstepmethodforapproximating
",False,pub07,False,False,False
19131,"is:
",False,pub07,False,False,False
19132,"1) project the panel charges onto a uniform grid of point
",False,pub07,False,False,False
19133,"charges;
",False,pub07,False,False,False
19134,"2) compute the grid potentials due to grid charges using an
",False,pub07,False,False,False
19135,"FFT;PHILLIPS AND WHITE: PRECORRECTED-FFT METHOD 1061
",False,pub07,False,False,False
19136,"(a) (b)
",False,pub07,False,False,False
19137,"Fig. 2. (a) Side view of a sphere discretized into 320 panels, with spatial decomposition into a 3 /23 /23 array of cells. (b) Superimposed grid charges
",False,pub07,False,False,False
19138,"corresponding to the cell decomposition of (a), with /112 /61 /51. In each cell, a 3 /23 /23 array of grid charges is used to represent the long range potential of
",False,pub07,False,False,False
19139,"the charged panels in the cell. Some of the grid charges are shared among cells. Note that the grid is “coarser” than the triangular panels used to discre tize
",False,pub07,False,False,False
19140,"the sphere. The grid extends outside the problem domain because the number of grid points is required to be a factor of two.
",False,pub07,False,False,False
19141,"Fig. 3. A 2-D pictorial representation of the four steps of the precor-
",False,pub07,False,False,False
19142,"rected-FFT algorithm. Interactions with nearby panels (in the grey area) arecomputed directly, interactions between distant panels are computed using
",False,pub07,False,False,False
19143,"the grid.
",False,pub07,False,False,False
19144,"3) interpolate the grid potentials onto the panels;
",False,pub07,False,False,False
19145,"4) directly compute nearby interactions.
",False,pub07,False,False,False
19146,"This process is summarized in Fig. 3. We emphasize that the
",False,pub07,False,False,False
19147,"grid of point charges is introduced purely as a computationalaid, it is not related to the underlying discretization of theconductors.
",False,pub07,False,False,False
19148,"A. Notation
",False,pub07,False,False,False
19149,"Given a set of
",False,pub07,False,False,False
19150,"cells which contain the set of panels and
",False,pub07,False,False,False
19151,"deﬁne the grid points, we now describe how to compute the
",False,pub07,False,False,False
19152,"vector of potentials from the vector of panel charges
",False,pub07,False,False,False
19153,". will denote the contribution of the grid
",False,pub07,False,False,False
19154,"charges to the potentials on the panel charges. denotes
",False,pub07,False,False,False
19155,"thenumberofpanelsinacell , therestrictionof
",False,pub07,False,False,False
19156,"the charge vector to the indices whose corresponding panels
",False,pub07,False,False,False
19157,"lie in cell and denotes the similar restriction
",False,pub07,False,False,False
19158,"of the potential vector. The variable denotes the order of grid
",False,pub07,False,False,False
19159,"approximation. is the vector of grid charges, thevector of grid potentials, and , denote
",False,pub07,False,False,False
19160,"the restriction of and, respectively, to grid points of cell
",False,pub07,False,False,False
19161,". We deﬁne to be the indexes of the set of cells which
",False,pub07,False,False,False
19162,"are “near” cell . , yet to be deﬁned, will
",False,pub07,False,False,False
19163,"refer to linear operators which project uniformly distributed
",False,pub07,False,False,False
19164,"panel charges to the grid points, and the linear operator
",False,pub07,False,False,False
19165,"gives grid potentials in terms of grid charges,
",False,pub07,False,False,False
19166,"i.e., . is the nonzero part of
",False,pub07,False,False,False
19167,"corresponding to charge in cell, ; is
",False,pub07,False,False,False
19168,"the similar part of , and is the block of
",False,pub07,False,False,False
19169,"which maps grid charges of cellto grid potentials of
",False,pub07,False,False,False
19170,"cell, . A subscript indicates an index into
",False,pub07,False,False,False
19171,"a matrix or vector, e.g., is theth entry of vector .
",False,pub07,False,False,False
19172,"B. Projecting Onto a Grid
",False,pub07,False,False,False
19173,"The ﬁrst step in the description of the algorithm is to de-
",False,pub07,False,False,False
19174,"scribe the construction of the grid projection operator . For
",False,pub07,False,False,False
19175,"panel charges contained within a given cell, the potentials atevaluation points distant from the given cell can be accurately
",False,pub07,False,False,False
19176,"computed by representing the given cell’s charge distribution
",False,pub07,False,False,False
19177,"with a small number of appropriately weighted point chargesonauniformgridthroughoutthegivencell’svolume.Fig. 2(b)shows the grid imposed on the cell structure of Fig. 2(a) when
",False,pub07,False,False,False
19178,"a3
",False,pub07,False,False,False
19179,"33arrayofgridchargesisusedtorepresentthecharge
",False,pub07,False,False,False
19180,"ineachcell.Notethatbecausethegridisonlyusedtorepresentthe long range part of the panel potentials, the grid may besigniﬁcantly coarser than the actual problem discretization.
",False,pub07,False,False,False
19181,"To motivate a scheme for representing panel charges with
",False,pub07,False,False,False
19182,"weighted point charges lying on a grid, consider a chargedistribution
",False,pub07,False,False,False
19183,"contained entirely within some small volume
",False,pub07,False,False,False
19184,"B. The potential outside due to can be determined from
",False,pub07,False,False,False
19185,"the knowledge of the potential on a surface surrounding
",False,pub07,False,False,False
19186,"[25]. For example, suppose is contained within a sphere
",False,pub07,False,False,False
19187,"of radius , as in Fig. 4. For all ) with , the1062 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19188,"Fig. 4. Potentials /32 /40 /114/60 /114 /99 /59/18 /59/30 /41or /32 /40/126 /114/62 /114 /99 /59 /126/18/59 /126/30 /41for /114/59 /126/114/62 /97may be
",False,pub07,False,False,False
19189,"obtained from the potential at /114 /61 /114 /99.
",False,pub07,False,False,False
19190,"potential can be written as a multipole expansion series
",False,pub07,False,False,False
19191,"(4)
",False,pub07,False,False,False
19192,"wherethe aresphericalharmonicsandthe coefﬁcients
",False,pub07,False,False,False
19193,"of expansion [25]. Since the spherical harmonics are orthogo-
",False,pub07,False,False,False
19194,"nal on a sphere, if the potential is known on any sphere of
",False,pub07,False,False,False
19195,"radius , the multipole moments can be computed as
",False,pub07,False,False,False
19196,"(5)
",False,pub07,False,False,False
19197,"The above observation suggests a scheme for computing
",False,pub07,False,False,False
19198,"the grid charges used to represent charge in a given cell .
",False,pub07,False,False,False
19199,"Suppose a array of grid charges is used to represent
",False,pub07,False,False,False
19200,"the charge in a cell. First, test points are selected on the
",False,pub07,False,False,False
19201,"surface of a sphere of radius whose center is coincident
",False,pub07,False,False,False
19202,"with the center of the given cell. Then, potentials due to the
",False,pub07,False,False,False
19203,"grid charges are forced to match the potential due to the
",False,pub07,False,False,False
19204,"cell’s actual charge distribution (say panel charges) at the
",False,pub07,False,False,False
19205,"test points, i.e.,
",False,pub07,False,False,False
19206,"(6)
",False,pub07,False,False,False
19207,"where is the mapping between grid charges
",False,pub07,False,False,False
19208,"and test point potentials, given by
",False,pub07,False,False,False
19209,"(7)
",False,pub07,False,False,False
19210,"Here andare the positions of the th test point and
",False,pub07,False,False,False
19211,"theth grid point, respectively. By construction, the relative
",False,pub07,False,False,False
19212,"positions of the grid charges and the test points are identicalfor each cell
",False,pub07,False,False,False
19213,", and therefore is the same for each cell.
",False,pub07,False,False,False
19214,"Fig. 5. Two-dimensional pictorial representation of the grid projection
",False,pub07,False,False,False
19215,"scheme. The black points (at /94/120) represent the grid charges ( /94/113) being used to
",False,pub07,False,False,False
19216,"represent the triangular panel’s charge density /27. The white points are the
",False,pub07,False,False,False
19217,"points /120/116where the potential due to the black point charges and the potential
",False,pub07,False,False,False
19218,"due to the triangular panel’s charge density are forced to match. The grid
",False,pub07,False,False,False
19219,"charges approximate the panel potential outside the gray region.
",False,pub07,False,False,False
19220,"is the mapping between panel charges and test
",False,pub07,False,False,False
19221,"point potentials and is given by
",False,pub07,False,False,False
19222,"(8)
",False,pub07,False,False,False
19223,"Since the collocation (6) is linear in the panel and grid charge
",False,pub07,False,False,False
19224,"distributions, the contribution of the th panel in cell to
",False,pub07,False,False,False
19225,"can be represented by a column vector . is
",False,pub07,False,False,False
19226,"given by
",False,pub07,False,False,False
19227,"(9)
",False,pub07,False,False,False
19228,"where denotes the th column of and
",False,pub07,False,False,False
19229,"indicates the generalized Moore–Penrose (or pseudo-) inverse
",False,pub07,False,False,False
19230,"[26] of . The computation of is done using the
",False,pub07,False,False,False
19231,"singular value decomposition. Since this matrix is small and
",False,pub07,False,False,False
19232,"is the same for each cell, the relative computational cost ofperforming the singular value decomposition is insigniﬁcant.
",False,pub07,False,False,False
19233,"For any panel charge
",False,pub07,False,False,False
19234,"in cell, this projection operation
",False,pub07,False,False,False
19235,"generates a subset of the grid charges . The contribution
",False,pub07,False,False,False
19236,"tofrom the charges in cell is generated by summing
",False,pub07,False,False,False
19237,"over all the charges in the cell. Note that panel charges outsidecell
",False,pub07,False,False,False
19238,"may contribute to some of the elements of in the
",False,pub07,False,False,False
19239,"case of shared grid charges. The grid projection scheme is
",False,pub07,False,False,False
19240,"summarized in Fig. 5. For an alternative approach, based onmatching multipole expansion coefﬁcients directly, see [27].A simpler approach based on polynomial interpolation may
",False,pub07,False,False,False
19241,"be found in [28].
",False,pub07,False,False,False
19242,"The accuracy of the above projection scheme hinges on the
",False,pub07,False,False,False
19243,"proper selection of the test points
",False,pub07,False,False,False
19244,". From (5), we expect
",False,pub07,False,False,False
19245,"high accuracy if the test points are chosen to be abscissas of a
",False,pub07,False,False,False
19246,"high-order quadrature rule [29]. It can be shown that the error
",False,pub07,False,False,False
19247,"in potential due to the grid-charge approximation of a chargedistribution contained within a sphere of radius
",False,pub07,False,False,False
19248,", at a distancePHILLIPS AND WHITE: PRECORRECTED-FFT METHOD 1063
",False,pub07,False,False,False
19249,"from the center of the distribution, is of order
",False,pub07,False,False,False
19250,"if the test points are chosen to be the nodes of a quadrature
",False,pub07,False,False,False
19251,"rule accurate to order [30].
",False,pub07,False,False,False
19252,"C. Computing Grid Potentials
",False,pub07,False,False,False
19253,"Once the charge has been projected to a grid, the operation
",False,pub07,False,False,False
19254,", computing the potentials at the grid points due to the grid
",False,pub07,False,False,False
19255,"charges, is a 3-D convolution. We denote this as
",False,pub07,False,False,False
19256,"(10)
",False,pub07,False,False,False
19257,"where and are triplets specifying the grid points
",False,pub07,False,False,False
19258,"and is the inverse distance between
",False,pub07,False,False,False
19259,"grid points and . As will be made clear below,
",False,pub07,False,False,False
19260,"(0, 0, 0) can be arbitrarily deﬁned, and is set to zero. The
",False,pub07,False,False,False
19261,"above convolution can be rapidly computed by using the FFT.
",False,pub07,False,False,False
19262,"In practice, each convolution requires one forward and one
",False,pub07,False,False,False
19263,"inverse 3-D FFT. The discrete Fourier transform of the kernelmatrix
",False,pub07,False,False,False
19264,", denoted , need be computed only once.
",False,pub07,False,False,False
19265,"An efﬁcient FFT implementation is central to the perfor-
",False,pub07,False,False,False
19266,"mance of the precorrected-FFT algorithm. The FFT is a very
",False,pub07,False,False,False
19267,"well-studied algorithm and many possible implementationsexist. Most FFT implementations have a fairly regular nature,thus very efﬁcient optimized code can be developed. Also, the
",False,pub07,False,False,False
19268,"structure of the data in a multidimensional convolution can
",False,pub07,False,False,False
19269,"be exploited for additional performance gains. For example,the use of the FFT to perform a linear multidimensionalconvolution involvesembedding thedata(
",False,pub07,False,False,False
19270,")to betransformed
",False,pub07,False,False,False
19271,"into a larger data space, much of which is zero. The fact that
",False,pub07,False,False,False
19272,"much of the transformed data is zero can be exploited to yielda more efﬁcient transform. In comparison, achieving optimalmachine performance with fast multipole algorithms is more
",False,pub07,False,False,False
19273,"difﬁcult, due to the less regular nature of the algorithms.
",False,pub07,False,False,False
19274,"D. Interpolating Grid Potentials
",False,pub07,False,False,False
19275,"Once the grid potentials have been computed, they must
",False,pub07,False,False,False
19276,"be interpolated to the panels in each cell. This process is
",False,pub07,False,False,False
19277,"essentially the same as the problem of representing charge
",False,pub07,False,False,False
19278,"on the grid, as can be seen from the following result [28].
",False,pub07,False,False,False
19279,"Theorem 1: Given
",False,pub07,False,False,False
19280,"is an operator which
",False,pub07,False,False,False
19281,"projects charge onto a grid of points, then may be
",False,pub07,False,False,False
19282,"interpreted as an operator which interpolates potential at grid
",False,pub07,False,False,False
19283,"pointsontochargecoordinates;conversely,given
",False,pub07,False,False,False
19284,"is an operator which interpolates potential at grid points
",False,pub07,False,False,False
19285,"onto charge coordinates, may be interpreted as an operator
",False,pub07,False,False,False
19286,"which projects charge onto the grid coordinates. In either case,
",False,pub07,False,False,False
19287,"andhave comparable accuracy.
",False,pub07,False,False,False
19288,"Proof:Let be the Green function for a source at
",False,pub07,False,False,False
19289,", evaluated at . Suppose that a unit charge at the point is
",False,pub07,False,False,False
19290,"represented by the vector of grid charges . The approximate
",False,pub07,False,False,False
19291,"potential at a point is given by
",False,pub07,False,False,False
19292,"where is the position of the th grid charge and ,
",False,pub07,False,False,False
19293,". Conversely, suppose there is a unit chargeatand the potential atis to be computed by
",False,pub07,False,False,False
19294,"interpolating potentials produced by this unit charge at thegrid points
",False,pub07,False,False,False
19295,". Then, if is the interpolation operator
",False,pub07,False,False,False
19296,"For a symmetric Green function,
",False,pub07,False,False,False
19297,"and
",False,pub07,False,False,False
19298,"so that
",False,pub07,False,False,False
19299,"Now suppose is assigned the value in order to represent
",False,pub07,False,False,False
19300,"the unit point charge at . Then
",False,pub07,False,False,False
19301,"When a collocation scheme is used to discretize the integralequation, the operator which interpolates potential at gridpoints in cell
",False,pub07,False,False,False
19302,"to a charge also in cell is not
",False,pub07,False,False,False
19303,"deﬁned in (9). Instead, the projection operator for
",False,pub07,False,False,False
19304,"a point charge located at the collocation point is computedwhich gives the interpolation operator
",False,pub07,False,False,False
19305,". However,
",False,pub07,False,False,False
19306,"if a Galerkin scheme is used in the discretization then the
",False,pub07,False,False,False
19307,"interpolation operator is .
",False,pub07,False,False,False
19308,"Thus, projection, followed by convolution, followed by
",False,pub07,False,False,False
19309,"interpolation gives the grid-charge approximation to the
",False,pub07,False,False,False
19310,"potentials which can be represented as
",False,pub07,False,False,False
19311,"(11)
",False,pub07,False,False,False
19312,"If Galerkin methods are used, (11) becomes
",False,pub07,False,False,False
19313,"(12)
",False,pub07,False,False,False
19314,"and therefore the precorrected-FFT method preserves the sym-
",False,pub07,False,False,False
19315,"metry of the Galerkin discretization for free space problems.
",False,pub07,False,False,False
19316,"E. Precorrecting
",False,pub07,False,False,False
19317,"The difﬁculty with the above three steps is that the calcula-
",False,pub07,False,False,False
19318,"tions using the FFT on the grid do not accurately approximate
",False,pub07,False,False,False
19319,"the nearby interactions. In of (11), the portions of
",False,pub07,False,False,False
19320,"associated with neighboring cell interactions have already
",False,pub07,False,False,False
19321,"been computed, though this close interaction has been poorly
",False,pub07,False,False,False
19322,"approximated in the projection/interpolation. A more accurate
",False,pub07,False,False,False
19323,"calculation of interactions between nearby panels is needed,but it is also necessary to remove or avoid the inaccuratecontribution from the use of the grid. This is a general
",False,pub07,False,False,False
19324,"difﬁculty with grid-based potential calculation methods and a
",False,pub07,False,False,False
19325,"variety of correction methods have been proposed [20], [28],[31] the details of which usually depend on the problem beingsolved, the interpolation scheme, and the nature of the grid
",False,pub07,False,False,False
19326,"solver.
",False,pub07,False,False,False
19327,"Because our algorithm works directly with the Green func-
",False,pub07,False,False,False
19328,"tion, and because the iterative solver requires that many1064 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19329,"(a) (b)
",False,pub07,False,False,False
19330,"Fig. 6. (a) A sphere discretized into 960 panels. The discretization is reﬁned by subdividing the spherical triangle deﬁned by the panel vertices into
",False,pub07,False,False,False
19331,"four triangular panels, whose vertices are the midpoints of the edges of the original spherical triangle. (b) Lines show integrated charge error for t he
",False,pub07,False,False,False
19332,"sphere with Dirichlet condition of (17). Solid line shows errors for grid code, (x) /112 /612 (*) /112 /613 (+) /112 /614. Dashed line connecting (o) shows
",False,pub07,False,False,False
19333,"error for /108 /612 multipole scheme.
",False,pub07,False,False,False
19334,"potential evaluations are performed for a given panel conﬁgu-
",False,pub07,False,False,False
19335,"ration, it is possible to treat nearby panel interactions exactly,
",False,pub07,False,False,False
19336,"without sacriﬁcing algorithmic efﬁciency. We accurately treat
",False,pub07,False,False,False
19337,"interactions between panels close together by modifying theway nearby interactions are computed, a step we refer to asprecorrection.
",False,pub07,False,False,False
19338,"In particular, denote as
",False,pub07,False,False,False
19339,"the portion of associated
",False,pub07,False,False,False
19340,"with the interaction between neighboring cells and,
",False,pub07,False,False,False
19341,"and the matrices formed from the columns and
",False,pub07,False,False,False
19342,", respectively, and denote as the panel potentials
",False,pub07,False,False,False
19343,"in cell due to the charges in cell. Then
",False,pub07,False,False,False
19344,"(13)
",False,pub07,False,False,False
19345,"is the grid-approximation to , which is inaccurate. Sub-
",False,pub07,False,False,False
19346,"tracting this approximation and then adding the correct con-tribution
",False,pub07,False,False,False
19347,"(14)
",False,pub07,False,False,False
19348,"produces the accurate result .
",False,pub07,False,False,False
19349,"This may be efﬁciently accomplished by deﬁning
",False,pub07,False,False,False
19350,"(15)
",False,pub07,False,False,False
19351,"to be the “precorrected” direct interaction operator. When
",False,pub07,False,False,False
19352,"used in conjunction with the grid charge representation
",False,pub07,False,False,False
19353,"results in exact calculation of the interactions of panels which
",False,pub07,False,False,False
19354,"are close. Assuming that the product will be computed
",False,pub07,False,False,False
19355,"many times in the inner loop of an iterative algorithm, will
",False,pub07,False,False,False
19356,"be expensive to initially compute, but will cost no more to
",False,pub07,False,False,False
19357,"subsequently apply than .
",False,pub07,False,False,False
19358,"F. Complete Algorithm
",False,pub07,False,False,False
19359,"Combining the above steps leads to the precorrected-FFT
",False,pub07,False,False,False
19360,"algorithm, which rapidly computes the dense matrix-
",False,pub07,False,False,False
19361,"vector product. Using the above notation, the algorithm can
",False,pub07,False,False,False
19362,"be described as two steps. The ﬁrst step is to compute
",False,pub07,False,False,False
19363,"(16)
",False,pub07,False,False,False
19364,"andare sparse interpolation operators and can be
",False,pub07,False,False,False
19365,"represented in a sparse manner via the FFT. The second stepistoaddin thecorrecteddirectinteractions, toobtainthe panel
",False,pub07,False,False,False
19366,"potentials for each cell
",False,pub07,False,False,False
19367,"(17)
",False,pub07,False,False,False
19368,"Becauseforeach ,isasmallsetandeachmatrix
",False,pub07,False,False,False
19369,"is also small, this second step is also a sparse operation. The
",False,pub07,False,False,False
19370,"complete algorithm follows in pseudocode form:
",False,pub07,False,False,False
19371,"Precorrected-FFT Algorithm to Compute
",False,pub07,False,False,False
19372,"/* Projection Step */
",False,pub07,False,False,False
19373,"Set 0
",False,pub07,False,False,False
19374,"For each cell 1t o
",False,pub07,False,False,False
19375,"For each panelin cell,1t o
",False,pub07,False,False,False
19376,"Add
",False,pub07,False,False,False
19377,"/* Convolution Step */
",False,pub07,False,False,False
19378,"Compute FFT()
",False,pub07,False,False,False
19379,"Compute
",False,pub07,False,False,False
19380,"Compute FFT()
",False,pub07,False,False,False
19381,"/* Interpolation Step */
",False,pub07,False,False,False
19382,"Set 0
",False,pub07,False,False,False
19383,"For each cell 1t o
",False,pub07,False,False,False
19384,"For each panelin cell,1t o
",False,pub07,False,False,False
19385,"Add
",False,pub07,False,False,False
19386,"/* Nearby Interactions */
",False,pub07,False,False,False
19387,"For each cell 1t o
",False,pub07,False,False,False
19388,"For each cellin
",False,pub07,False,False,False
19389,"Thus, the effect of this algorithm is to replace the operation
",False,pub07,False,False,False
19390,"where is a dense matrix, with the operationPHILLIPS AND WHITE: PRECORRECTED-FFT METHOD 1065
",False,pub07,False,False,False
19391,"(a) (b)
",False,pub07,False,False,False
19392,"(c) (d)
",False,pub07,False,False,False
19393,"Fig. 7. The cube example. (a) Discretization of the cube. (b) CPU time, in seconds, needed for the fast multipole (dashed line connecting “ /2”) and
",False,pub07,False,False,False
19394,"precorrected-FFT algorithms (“*”) to compute a matrix-vector product. For the precorrected-FFT algorithm, different results are possible depend ing on
",False,pub07,False,False,False
19395,"whether speed or memory usage is to be optimized. The solid line connects runs with grid sizes chosen to minimize memory use. Note the speed-memory
",False,pub07,False,False,False
19396,"product is fairly independent of grid size. (c) Memory, in Mb, needed by the fast multipole and precorrected-FFT algorithms. (d) Product of (b) and (c) .
",False,pub07,False,False,False
19397,"where all the matrices andpossess sparse repre-
",False,pub07,False,False,False
19398,"sentations.
",False,pub07,False,False,False
19399,"G. Grid Selection
",False,pub07,False,False,False
19400,"Before the algorithm has been completely speciﬁed, it
",False,pub07,False,False,False
19401,"necessary to specify how panels are selected for inclusion indirect interaction regions and how the grid size is selected.That is, for each cell
",False,pub07,False,False,False
19402,"the set must be speciﬁed.
",False,pub07,False,False,False
19403,"To insure that interactions between panels which are close
",False,pub07,False,False,False
19404,"together are treated accurately, at a minimum it is necessaryto compute interactions between panels in cells which arenear-neighbors of each other via direct products. The near-
",False,pub07,False,False,False
19405,"neighbors of a cell
",False,pub07,False,False,False
19406,"are deﬁned to be all the cells which have
",False,pub07,False,False,False
19407,"a vertex in common with cell (thus a cell is a near-neighbor
",False,pub07,False,False,False
19408,"of itself). We have included only near-neighbor interactions inthe computational experiments of Sections V and VI.
",False,pub07,False,False,False
19409,"The worst case accuracy of the grid representation is a
",False,pub07,False,False,False
19410,"function of the ratio of thecell radiusto the radiusof the directinteractionregion[30].Thus,oncethedirect-interactionregionhas been speciﬁed to be near-neighbor cells, the selection of
",False,pub07,False,False,False
19411,"the cell size, and hence the grid spacing is purely a matter of
",False,pub07,False,False,False
19412,"computational efﬁciency. The cost of direct interactions willdecrease monotonically as the cells are made smaller, but thenumber of grid points will increase, so the cost of the FFT
",False,pub07,False,False,False
19413,"will increase monotonically. This implies that the total cost ofthe algorithm will have an easily determined global minimumfor some grid spacing. For a given grid spacing and panelconﬁguration, the memory and computation time needed bythe precorrected-FFT algorithm can be estimated cheaply, sothe optimal grid spacing can be obtained by starting with asmall number of grid points and increasing the number until aminimumCPUormemoryestimate,asappropriate,isreached.In addition,we have generallyrequired that the number of gridpoints be a factor of two, in order to exploit the most efﬁcientFFT implementations.
",False,pub07,False,False,False
19414,"It is interesting that the optimal grid size may occasionally
",False,pub07,False,False,False
19415,"be such that the number of grid charges,
",False,pub07,False,False,False
19416,",i slargerthan the
",False,pub07,False,False,False
19417,"original number of panel charges . This may be the case even
",False,pub07,False,False,False
19418,"when thegrid spacing is larger than the underlying panel sizes,that is, when the grid is “coarser” than the panel discretization.Such a case may occur, for example, for a ﬁnely discretizedcube surface, where the grid must ﬁll the 3-D space of thecube’s interior. However, the overall algorithm may still bequite effective, since the cost of the FFT is
",False,pub07,False,False,False
19419,", with
",False,pub07,False,False,False
19420,"a constant factor of . Thus if andis large, the
",False,pub07,False,False,False
19421,"cost of the FFT is less than that of the direct product by afactor of nearly
",False,pub07,False,False,False
19422,", and so the algorithm may have1066 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19423,"(a) (b)
",False,pub07,False,False,False
19424,"(c) (d)
",False,pub07,False,False,False
19425,"Fig. 8. The bus crossing example. (a) Larger problems are generated by adding more bus lines. (b) CPU time, in seconds, needed for the fast multipole
",False,pub07,False,False,False
19426,"(dashed line connecting “ /2”) and precorrected-FFT algorithms (solid line connecting “*”) to compute a matrix-vector product. (c) Memory, in Mb, needed
",False,pub07,False,False,False
19427,"by the fast multipole and precorrected-FFT algorithms. (d) Product of (b) and (c).
",False,pub07,False,False,False
19428,"by a fairly signiﬁcant factor and still possess an advantage
",False,pub07,False,False,False
19429,"over the direct computation.
",False,pub07,False,False,False
19430,"IV. ANALYSIS OF THE PRECORRECTED -FFT A LGORITHM
",False,pub07,False,False,False
19431,"In this section, we analyze the complexity of the
",False,pub07,False,False,False
19432,"precorrected-FFT algorithm and give some comparisonswith other approaches.
",False,pub07,False,False,False
19433,"A. Comparison to Fast Multipole Algorithms
",False,pub07,False,False,False
19434,"First we compare the efﬁciency of the grid representation
",False,pub07,False,False,False
19435,"used in the precorrected-FFT algorithm to the multipole ex-pansions used in the fast multipole method. Both the fast
",False,pub07,False,False,False
19436,"multipolealgorithmandtheprecorrected-FFTalgorithmobtain
",False,pub07,False,False,False
19437,"efﬁciency by representing the long-range part of the potentialof a group of charges by an expression which can be used atmultiple evaluation points, but the algorithms differ in the way
",False,pub07,False,False,False
19438,"theyclustersetsofchargestogethertoformsingleexpressions.
",False,pub07,False,False,False
19439,"Again consider subdividing the parallelepiped containing
",False,pub07,False,False,False
19440,"the entire 3-D problem domain into a
",False,pub07,False,False,False
19441,"array of
",False,pub07,False,False,False
19442,"cells. Then, the collocation approach above can be used to
",False,pub07,False,False,False
19443,"generate point charge approximations for charge distributions
",False,pub07,False,False,False
19444,"in every cell, effectively projecting the charge density onto a3-D grid. For example, if the representative point charges areplaced at the cell vertices, then the panel charge distribution
",False,pub07,False,False,False
19445,"will be projected to a
",False,pub07,False,False,False
19446,"uniform grid.
",False,pub07,False,False,False
19447,"Fastmultipolealgorithmsalsoeffectivelycreateauniformgridby constructing multipole expansions at the center of eachcell, but due to sharing, the point charge approach can be
",False,pub07,False,False,False
19448,"more efﬁcient. For example, when representing the potential
",False,pub07,False,False,False
19449,"of a panel by charges at the cell vertices, there are eight
",False,pub07,False,False,False
19450,"free coefﬁcients which may be varied to obtain an optimalrepresentation, and there will be
",False,pub07,False,False,False
19451,"terms
",False,pub07,False,False,False
19452,"in the entire domain. On average, there is only one grid-charge
",False,pub07,False,False,False
19453,"per cubic cell, since a point charge at a cell vertex is used to
",False,pub07,False,False,False
19454,"represent charge in the eight cells which share that vertex. Bycontrast, as no sharing occurs in the multipole representation,if there are
",False,pub07,False,False,False
19455,"coefﬁcients in the multipole expansion which
",False,pub07,False,False,False
19456,"represents the potential of the charges in the cell, the total
",False,pub07,False,False,False
19457,"number of terms in the domain will be .
",False,pub07,False,False,False
19458,"For an equivalent number of total terms in the domain, weexpectthegridrepresentationtobemoreaccurate.Conversely,
",False,pub07,False,False,False
19459,"for roughly equivalent accuracy, we may choose
",False,pub07,False,False,False
19460,", but
",False,pub07,False,False,False
19461,"then the total number of multipole terms will be signiﬁcantlyhigher than for the grid representation.
",False,pub07,False,False,False
19462,"B. Performance for Homogeneous Problems
",False,pub07,False,False,False
19463,"From the analysis of the preceding section, we expect the
",False,pub07,False,False,False
19464,"grid representation to be locally more efﬁcient than the use of
",False,pub07,False,False,False
19465,"multipoleexpansions. However,ourcurrentimplementation of
",False,pub07,False,False,False
19466,"the precorrected-FFT algorithm may be globally less efﬁcient,as the grid representation is introduced throughout space, evenwhere no panels are present. Thus, whereas for a problem
",False,pub07,False,False,False
19467,"containing
",False,pub07,False,False,False
19468,"panels, the fast multipole algorithm can perform
",False,pub07,False,False,False
19469,"a potential evaluation for all of the panels in operations,
",False,pub07,False,False,False
19470,"regardless of the panel distribution [32], no such guaranteePHILLIPS AND WHITE: PRECORRECTED-FFT METHOD 1067
",False,pub07,False,False,False
19471,"(a) (b)
",False,pub07,False,False,False
19472,"(c) (d)
",False,pub07,False,False,False
19473,"Fig. 9. Several realistic capacitance extraction problems. (a) The woven bus example (woven 5 /25). (b) The comb drive example (comb). (c) The
",False,pub07,False,False,False
19474,"via example (via). (d) The SRAM example (SRAM).
",False,pub07,False,False,False
19475,"is available for the precorrected-FFT algorithm. However, it
",False,pub07,False,False,False
19476,"is possible to establish a weaker complexity result for theprecorrected-FFT method.
",False,pub07,False,False,False
19477,"Theorem 2: For a homogeneous distribution of
",False,pub07,False,False,False
19478,"panels
",False,pub07,False,False,False
19479,"and a given prescribed accuracy, the precorrected-FFT methodrequires
",False,pub07,False,False,False
19480,"operations to perform a potential calcula-
",False,pub07,False,False,False
19481,"tion.
",False,pub07,False,False,False
19482,"Proof:Given that the computational domain is a paral-
",False,pub07,False,False,False
19483,"lelepiped containing panels, again assume space has been
",False,pub07,False,False,False
19484,"divided into an array of cells, and that the panel
",False,pub07,False,False,False
19485,"distribution is homogeneous on the scale of the cell size. Thatis, the number of panels per cell,
",False,pub07,False,False,False
19486,", is bounded independent
",False,pub07,False,False,False
19487,"of, with of order- . Finally, assume that the grid in
",False,pub07,False,False,False
19488,"each cell is a array. There are three components in
",False,pub07,False,False,False
19489,"the cost of the precorrected-FFT algorithm: the cost of direct
",False,pub07,False,False,False
19490,"interactions, the cost of grid projection and interpolation, and
",False,pub07,False,False,False
19491,"the cost of the FFT. The cost of the direct interactions willbe
",False,pub07,False,False,False
19492,". The cost of the grid
",False,pub07,False,False,False
19493,"projection will be . Finally, the cost of the
",False,pub07,False,False,False
19494,"FFT will be . Summing
",False,pub07,False,False,False
19495,"these costs results in the ﬁnal complexity of .
",False,pub07,False,False,False
19496,"Since the grid spacing is typically less ﬁne than the
",False,pub07,False,False,False
19497,"underlying surface discretization, for a typical problem theprecorrected-FFT algorithm has
",False,pub07,False,False,False
19498,"complexity for
",False,pub07,False,False,False
19499,"problems with considerable inhomogeneity in the ﬁne surface
",False,pub07,False,False,False
19500,"discretization,aslongasthepaneldistributionishomogeneousat a very coarse level. As will be seen in Section VI,many structures arising in practice satisfy this “coarsely
",False,pub07,False,False,False
19501,"homogeneous” condition.
",False,pub07,False,False,False
19502,"C. Comparison to Other Grid-Based Methods
",False,pub07,False,False,False
19503,"In order to solve the underlying potential-theoretic prob-
",False,pub07,False,False,False
19504,"lem, the precorrected-FFT algorithm introduces a uniformgrid which covers the problem domain volume and so it is
",False,pub07,False,False,False
19505,"instructive to compare the precorrected-FFT algorithm with
",False,pub07,False,False,False
19506,"other methods that introduce volumetric grids.
",False,pub07,False,False,False
19507,"First, most other methods which use a grid to represent the
",False,pub07,False,False,False
19508,"solution throughout space, such as ﬁnite-difference methods,
",False,pub07,False,False,False
19509,"ﬁnite-element methods, or integral equation methods which
",False,pub07,False,False,False
19510,"directly exploit the convolutional properties of the kernel viathe FFT [33]–[35], introducing a space-ﬁlling grid which mustalso accurately represent the complicated problem geometry.
",False,pub07,False,False,False
19511,"These two conﬂicting requirements generally result in either
",False,pub07,False,False,False
19512,"restricted geometries or a very large number of unknowns thatin turn limits the size of the problem that can be effectivelysolved.
",False,pub07,False,False,False
19513,"In contrast, as shown in Fig. 2, the grid introduced by the
",False,pub07,False,False,False
19514,"precorrected-FFT algorithm is geometrically unrelated to theunderlying surface discretization of the geometry. In generalthe number of panels in a surface discretization is much
",False,pub07,False,False,False
19515,"smaller than the number of elements in a volume representa-
",False,pub07,False,False,False
19516,"tion, so we expect the precorrected-FFT algorithm to be moreefﬁcient, than, for example, ﬁnite-difference approaches.1068 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19517,"TABLE I
",False,pub07,False,False,False
19518,"STATISTICS FOR FASTCAP O RDER-2, GRID-2,3 CODES FOR /49 /61/114GREENFUNCTION.SETUP,SOLVE,ANDCPU T IMESARE INSECONDS ON DEC AXP 3000/900,
",False,pub07,False,False,False
19519,"MEMORY IN MEGABYTES . /109ISNUMBER OF CONDUCTORS IN PROBLEM,EACHCONDUCTOR REQUIRES A SEPARATELINEARSYSTEMSOLUTION
",False,pub07,False,False,False
19520,"Additionally, most other 3-D grid-based approaches neces-
",False,pub07,False,False,False
19521,"sarily have a complexity of ,i fis the number of basis
",False,pub07,False,False,False
19522,"elements along a side. The precorrected-FFT method analyzedhere uses
",False,pub07,False,False,False
19523,"basis elements in the underlying surface
",False,pub07,False,False,False
19524,"discretization, and the complexity is
",False,pub07,False,False,False
19525,"(see Sections IV-B and V-B, and [30]).
",False,pub07,False,False,False
19526,"At50 basis elements per dimension, corresponding only
",False,pub07,False,False,False
19527,"to a 15000 panel problem, exceeds by more than a
",False,pub07,False,False,False
19528,"factor of 20.
",False,pub07,False,False,False
19529,"In short, because of the decoupling of short range interac-
",False,pub07,False,False,False
19530,"tions from the long range interactions treated by the grid, the
",False,pub07,False,False,False
19531,"precorrected-FFT method can efﬁciently utilize fast potential
",False,pub07,False,False,False
19532,"solvers without sacriﬁcing the ability to represent complicatedsurface geometries in a compact manner.
",False,pub07,False,False,False
19533,"V. R
",False,pub07,False,False,False
19534,"EFERENCE EXAMPLES
",False,pub07,False,False,False
19535,"In this section, we examine a variety of simple examples to
",False,pub07,False,False,False
19536,"evaluate the performance of the precorrected-FFT algorithm.
",False,pub07,False,False,False
19537,"We start by examining the errors introduced by the gridprojection method and then we examine the efﬁciency of the
",False,pub07,False,False,False
19538,"overall precorrected method.
",False,pub07,False,False,False
19539,"A. Empirical Error Analysis
",False,pub07,False,False,False
19540,"As described above, in the precorrected-FFT algorithm, the
",False,pub07,False,False,False
19541,"interaction between panels in neighboring cells is computedexactly, but more distant interactions are approximated by
",False,pub07,False,False,False
19542,"extrapolation, convolution, and then interpolation using thegrid. To demonstrate that the errors due to using the grid are
",False,pub07,False,False,False
19543,"well controlled, we present an empirical error study based on
",False,pub07,False,False,False
19544,"ananalyticallysolvablepotentialproblem borrowedfrom[32].If (1) is solved on a sphere with given potential
",False,pub07,False,False,False
19545,"(18)
",False,pub07,False,False,False
19546,"the analytically computable charge distribution is
",False,pub07,False,False,False
19547,"(19)
",False,pub07,False,False,False
19548,"To estimate the error introduced by the grid approximations
",False,pub07,False,False,False
19549,"in the precorrected-FFT method, the sphere can be discretized,
",False,pub07,False,False,False
19550,"as in Fig. 6, and the charges on each panel computed.
",False,pub07,False,False,False
19551,"The approximations introduced by the grid-charge approxi-mation to long-range interactions will become evident as thediscretization is reﬁned, since eventually these errors will
",False,pub07,False,False,False
19552,"dominate over the discretization error. One relative measure
",False,pub07,False,False,False
19553,"of the error is
",False,pub07,False,False,False
19554,"(20)
",False,pub07,False,False,False
19555,"where the sum runs over all panels ,is the centroid of
",False,pub07,False,False,False
19556,"panel,the area of that panel, the charge on the panel,PHILLIPS AND WHITE: PRECORRECTED-FFT METHOD 1069
",False,pub07,False,False,False
19557,"andthe exact total charge on the sphere. Fig. 6(b) shows
",False,pub07,False,False,False
19558,"that for the low-order piecewise-constant collocation scheme,as the discretization of the sphere is reﬁned [see Fig. 6(a)], theintegrated error
",False,pub07,False,False,False
19559,"decreasesproportional to . The multipole
",False,pub07,False,False,False
19560,"or precorrected-FFT approximation errors are evident when
",False,pub07,False,False,False
19561,"ceases to decrease as is increased. For example, for the 2
",False,pub07,False,False,False
19562,"22 grid-charge representation is used in each cell ( 2),
",False,pub07,False,False,False
19563,"ceases to decrease below about 0.05. This indicates that the
",False,pub07,False,False,False
19564,"2 grid charge scheme introduces errors into the integrated
",False,pub07,False,False,False
19565,"charge calculation of about 5%. Similarly, we expect the
",False,pub07,False,False,False
19566,"3 scheme to be accurate to almost a tenth of a percent. Wehave also shown results for the
",False,pub07,False,False,False
19567,"2 multipole approximation,
",False,pub07,False,False,False
19568,"which from this experiment we expect to be intermediate in
",False,pub07,False,False,False
19569,"accuracy between the grid 3 and 2 approximations.
",False,pub07,False,False,False
19570,"B. Effects of Inhomogeneity
",False,pub07,False,False,False
19571,"The fast multipole algorithms used in the FASTCAP pro-
",False,pub07,False,False,False
19572,"gram compute matrix-vector products in operations
",False,pub07,False,False,False
19573,"regardless of the distribution of panels on the discretized
",False,pub07,False,False,False
19574,"surfaces [32], but this is not true of the precorrected-FFTmethod. As described, the use of the FFT implies that the al-gorithm computes matrix-vector products in at best
",False,pub07,False,False,False
19575,"operations, and attains this optimum only for fairly homoge-neous distributions of panels (see Section IV-B). That is, forproblemswherethepanelsaredistributedinaroughly uniformmannerthroughoutspace,theprecorrected-FFTmethodshould
",False,pub07,False,False,False
19576,"be efﬁcient. In contrast, for inhomogeneous problems which
",False,pub07,False,False,False
19577,"consist of clusters of panels separated by large areas ofopen space, inefﬁciency may be expected. Therefore it isimportant to quantify the performance penalty induced in the
",False,pub07,False,False,False
19578,"precorrected-FFT method by problem inhomogeneity.
",False,pub07,False,False,False
19579,"A simple approach to generating an example which is inho-
",False,pub07,False,False,False
19580,"mogeneous is to reﬁne the discretization of a cube. The cubeexample is intended to serve as a model for typical boundary-
",False,pub07,False,False,False
19581,"element discretizations of surfaces. As the discretization is
",False,pub07,False,False,False
19582,"reﬁned, problems with increasing numbers of panels will begenerated. The precorrected-FFT algorithm must place gridcharges in the empty interior of the cube, which causes the
",False,pub07,False,False,False
19583,"CPU time and memory required by the algorithm to increase
",False,pub07,False,False,False
19584,"faster than
",False,pub07,False,False,False
19585,".A sincreases, relatively more panels
",False,pub07,False,False,False
19586,"are near the surfaces of the cube relative to the interior,i.e., the problem inhomogeneity increases. Thus, at some
",False,pub07,False,False,False
19587,"large
",False,pub07,False,False,False
19588,", the fast multipole methods will be superior to the
",False,pub07,False,False,False
19589,"precorrected-FFTmethod.Wewishtodeterminehoweffectivethe precorrected-FFT method is for reasonable size problems,and at what
",False,pub07,False,False,False
19590,"it would become advantageous to use the
",False,pub07,False,False,False
19591,"fast-multipole methods.
",False,pub07,False,False,False
19592,"Fig. 7 shows the comparison of the precorrected-FFT
",False,pub07,False,False,False
19593,"method at 3 to the fast-multipole based code FASTCAP,
",False,pub07,False,False,False
19594,"at2, for the cube example. The discretization of the cube
",False,pub07,False,False,False
19595,"is reﬁned to generate more panels, and the performance of
",False,pub07,False,False,False
19596,"the two codes compared as the problem size increases. Threeﬁgures are shown. Fig. 7(b) shows the time required for eachcode to compute a matrix-vector product, Fig. 7(c) shows the
",False,pub07,False,False,False
19597,"amount of memory needed by each code, and Fig. 7(d) shows
",False,pub07,False,False,False
19598,"a ﬁgure of merit which is the product of required memoryand the time needed for a potential calculation. The productTABLE II
",False,pub07,False,False,False
19599,"COMPARISON OF FASTCAP ANDGRIDCODES.FIGURESARERATIOS OF
",False,pub07,False,False,False
19600,"REQUIREDRESOURCES .“ PRODUCT”ISPRODUCT OF CPUANDMEMORYFIGURE
",False,pub07,False,False,False
19601,"is important to consider when analyzing the precorrected-FFT
",False,pub07,False,False,False
19602,"method because, as is clear from the ﬁgure, speed can be
",False,pub07,False,False,False
19603,"traded for memory by manipulating the size of the regionthe grid-charge approximation covers. The CPU and memory
",False,pub07,False,False,False
19604,"ﬁgures for the precorrected-FFT method are observed to grow
",False,pub07,False,False,False
19605,"irregularly with problem size. This is because our speciﬁcimplementation of the method requires the number of grid-charges along one side of the computational domain to be
",False,pub07,False,False,False
19606,"a power of two. The solid line in the ﬁgures shows results
",False,pub07,False,False,False
19607,"when the number of grid charges along a side was selectedto optimize (see Section III-G) the speed-memory product,which is observed to grow smoothly. Two cases in Fig. 7(a)
",False,pub07,False,False,False
19608,"are evident where the code would have been considerably
",False,pub07,False,False,False
19609,"faster had a different number of grid-charges been used.However, as Fig. 7(b) shows, the memory required would
",False,pub07,False,False,False
19610,"have been greater in each case.
",False,pub07,False,False,False
19611,"Analysis of the trend of Fig. 7(a) reveals that the CPU time
",False,pub07,False,False,False
19612,"needed to solve the cube problem grows as about
",False,pub07,False,False,False
19613,",
",False,pub07,False,False,False
19614,"whereisthenumberofpanels,fasterthanthe expected
",False,pub07,False,False,False
19615,"asymptotically for the fast multipole method. However, for
",False,pub07,False,False,False
19616,"all the problems analyzed, the precorrected-FFT method wassuperior in terms of CPU time and memory required. We mayobtain the approximate point at which the algorithms cross
",False,pub07,False,False,False
19617,"over by extrapolating the data in Fig. 7(c). Assuming that
",False,pub07,False,False,False
19618,"the CPU time and memory of the multipole method grow as
",False,pub07,False,False,False
19619,", and that the CPU time and memory required by the
",False,pub07,False,False,False
19620,"precorrected-FFTmethodgrowsas [30],theninterms
",False,pub07,False,False,False
19621,"of the speed-memory product the precorrected-FFT methodwill be superior to the fast-multipole method until
",False,pub07,False,False,False
19622,"is, at
",False,pub07,False,False,False
19623,"least, several million panels. We estimate over 30 gigabytes of
",False,pub07,False,False,False
19624,"memory would be needed to solve such a problem.
",False,pub07,False,False,False
19625,"The cube example demonstrates that problems exist for
",False,pub07,False,False,False
19626,"which the precorrected-FFT algorithm is inferior to the fast-
",False,pub07,False,False,False
19627,"multipole methods. This example, however, is somewhat ar-
",False,pub07,False,False,False
19628,"tiﬁcial, as very large capacitance extraction problems are not1070 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19629,"TABLE III
",False,pub07,False,False,False
19630,"COMPARISON OF CAPACITANCE EXTRACTION ALGORITHMS .FIGURES IN PARENTHESES AREESTIMATES
",False,pub07,False,False,False
19631,"usually due to very ﬁne discretizations of a few surfaces, but
",False,pub07,False,False,False
19632,"rather by ﬁxing a discretization level, and solving problems
",False,pub07,False,False,False
19633,"which involve increasingly more complicated structures. For asituation which better models problems from VLSI intercon-nect analysis, consider a bus crossing example, as in Fig. 8. In
",False,pub07,False,False,False
19634,"this example, a series of stacked bus problems are solved. The
",False,pub07,False,False,False
19635,"faces of each bus line are broken into quadrilateral sections,and the quadrilaterals discretized by division into a centralpanel and ﬁve edge panels. In order to generate larger and
",False,pub07,False,False,False
19636,"larger problems, we consider
",False,pub07,False,False,False
19637,"levels of bus wires, each level
",False,pub07,False,False,False
19638,"having wires.
",False,pub07,False,False,False
19639,"From Fig. 8 it is clear that the computational cost of
",False,pub07,False,False,False
19640,"the algorithm grows nearly linearly with problem size, as
",False,pub07,False,False,False
19641,"predicted in Section IV-B. For the size problems considered,
",False,pub07,False,False,False
19642,"the precorrected-FFT method with 3 enjoys an advantage
",False,pub07,False,False,False
19643,"of more than a factor of three in terms of computationalcost and roughly a factor of four in memory utilization over
",False,pub07,False,False,False
19644,"the fast multipole method using order-2 expansions. With
",False,pub07,False,False,False
19645,"these parameter values, however, from Fig. 6 we expect theprecorrected-FFT method to be considerably more accurate.
",False,pub07,False,False,False
19646,"VI. R
",False,pub07,False,False,False
19647,"EALISTIC EXAMPLES
",False,pub07,False,False,False
19648,"In this section, we present results comparing the FASTCAP
",False,pub07,False,False,False
19649,"program to the precorrected-FFT method for computing ca-
",False,pub07,False,False,False
19650,"pacitances of several 3-D geometries. As a preconditioner has
",False,pub07,False,False,False
19651,"not yet been implemented in the precorrected-FFT algorithm,all comparisons were performed without FASTCAP’s precon-ditioner. Fig. 9 shows four realistic 3-D structures: a woven
",False,pub07,False,False,False
19652,"bus structure, a bus crossing structure, a via structure, and part
",False,pub07,False,False,False
19653,"of an SRAM memory cell. We have compared the multipole-based code FASTCAP, using multipole expansion order
",False,pub07,False,False,False
19654,"2,
",False,pub07,False,False,False
19655,"to the grid based methods with 2 and 3. To estimate
",False,pub07,False,False,False
19656,"the accuracy of the computed capacitances, we have compared
",False,pub07,False,False,False
19657,"the results to the grid-code run using 6, which we expect
",False,pub07,False,False,False
19658,"to introduce errors into the calculation which are very smallcompared to the
",False,pub07,False,False,False
19659,"2,3 grid codes or the multipole
",False,pub07,False,False,False
19660,"2 code. As a check on this assumption we also performed thecalculations using the fast multipole algorithm and sixth ordermultipole expansions. Taking the
",False,pub07,False,False,False
19661,"6 capacitances to be
",False,pub07,False,False,False
19662,"exact, we have calculated both the maximum relative errors in
",False,pub07,False,False,False
19663,"thecomputedcapacitancecoefﬁcients,aswellasthemaximum
",False,pub07,False,False,False
19664,"over all rows of the capacitance matrix of the largest errorin the row as a fraction of that row’s diagonal capacitance.Table I shows the computation times, memory required, and
",False,pub07,False,False,False
19665,"error estimates for each problem. All experiments were run
",False,pub07,False,False,False
19666,"on a DEC AXP3000/900, with 256 megabytes of physicalmemory.The table indicates that multipole expansions of order 2
",False,pub07,False,False,False
19667,"are usually enough to give relative accuracy of one percent
",False,pub07,False,False,False
19668,"or so in the calculated capacitances. In terms of relativeerrors in the computed capacitances, the
",False,pub07,False,False,False
19669,"2 grid code
",False,pub07,False,False,False
19670,"appears to be comparable to the 2 multipole code, and
",False,pub07,False,False,False
19671,"somewhat inferior when the error is measured as a percentageof the diagonal capacitance. The
",False,pub07,False,False,False
19672,"3 grid code clearly has
",False,pub07,False,False,False
19673,"uniformly superior error properties. These results are in accordwith the sphere example considered previously in Fig. 6.
",False,pub07,False,False,False
19674,"Table II shows explicit performance comparisons of the
",False,pub07,False,False,False
19675,"2multipolecode tothe 2grid code,whichhas comparable
",False,pub07,False,False,False
19676,"accuracy, as well as to the more-accurate 3 grid codes.
",False,pub07,False,False,False
19677,"At3, the precorrected-FFT method can be as much as
",False,pub07,False,False,False
19678,"four times faster and can use as little as one ﬁfth the memoryof FASTCAP. In terms of the speed-memory product, thegrid-based code at
",False,pub07,False,False,False
19679,"3 was superior by a factor ranging
",False,pub07,False,False,False
19680,"from four to 20. At 2, the performance advantage of the
",False,pub07,False,False,False
19681,"grid-code was even more signiﬁcant. The CPU advantage of
",False,pub07,False,False,False
19682,"the method ranged from nearly four to more than eight, thememory advantage from four to six, and the product from 12to 52.
",False,pub07,False,False,False
19683,"ThetwoﬁnalentriesinTable Iareworthyofnote.Usingthe
",False,pub07,False,False,False
19684,"2 grid representation, from which we expect about 2–4%
",False,pub07,False,False,False
19685,"accuracy, it was possible to analyze two very large problems.The ﬁrst is a 15
",False,pub07,False,False,False
19686,"15 wire woven bus crossing, shown in
",False,pub07,False,False,False
19687,"Fig. 10, which has over 80000 panels in the discretization.
",False,pub07,False,False,False
19688,"The second is the cube, discretized into about 125000 panels.The precorrected-FFT method was able to perform a singlesolution (one row in the capacitance matrix) in only about3 min. More importantly, both problems could be solved inthe available physical memory, which was not possible usingFASTCAP.
",False,pub07,False,False,False
19689,"We note that all the examples considered here generate
",False,pub07,False,False,False
19690,"fairly well conditioned linear systems, so no preconditioner
",False,pub07,False,False,False
19691,"was necessary to secure convergence in a reasonable time.However, use of a preconditioner could further reduce thecomputation times required for the linear system solution, andenable solution of less well conditioned problems.
",False,pub07,False,False,False
19692,"Finally, we wish to emphasize that, regardless of whether
",False,pub07,False,False,False
19693,"the precorrected-FFT or fast-multipole based approaches areused, the advantage of the accelerated schemes over tra-
",False,pub07,False,False,False
19694,"ditional algorithms is tremendous. Table III compares the
",False,pub07,False,False,False
19695,"computation time and memory needed by algorithms based onLU-factorization via Gaussian elimination, iterative solutionusing direct (explicit) matrix-vector products, and iterativesolution usingaccelerated matrix-vector products, as describedin this paper. The statistics for the direct algorithms werePHILLIPS AND WHITE: PRECORRECTED-FFT METHOD 1071
",False,pub07,False,False,False
19696,"Fig. 10. The large woven bus structure. The 30 conductor structure is formed from ﬁfteen conductors woven around ﬁfteen straight conductors. The actu al
",False,pub07,False,False,False
19697,"discretization is ﬁner than shown in the above ﬁgure. There are 82080 panels in the actual discretization.
",False,pub07,False,False,False
19698,"estimated by extrapolating timings of computations performed
",False,pub07,False,False,False
19699,"by MATLAB, and by assuming that storage is in doubleprecision ﬂoating point words. For the largest problems, thespeedups can be two orders of magnitude over iterative solu-
",False,pub07,False,False,False
19700,"tion with direct products, and nearly sixorders of magnitude
",False,pub07,False,False,False
19701,"over Gaussian-elimination, with memory savings of a factorof 500.
",False,pub07,False,False,False
19702,"VII. C
",False,pub07,False,False,False
19703,"ONCLUSION
",False,pub07,False,False,False
19704,"In this paper, we presented a precorrected-FFT approach
",False,pub07,False,False,False
19705,"to reducing the CPU time required to compute accurate cou-
",False,pub07,False,False,False
19706,"pling capacitances of complicated 3-D structures. As theexamples above demonstrate, the precorrected-FFT methodis well tuned to the problems associated with integrated
",False,pub07,False,False,False
19707,"circuit packaging, on-chip interconnect, and micro-electro-
",False,pub07,False,False,False
19708,"mechanical systems. In particular, the CPU time-memoryproduct for the precorrected-FFT algorithm can be more thanan order of magnitude smaller than that of the FASTCAP
",False,pub07,False,False,False
19709,"program.
",False,pub07,False,False,False
19710,"A major advantage of this method is that it is based on
",False,pub07,False,False,False
19711,"the FFT and local interpolation operators, rather than onspherical-harmonics based shifting operators as in the fast
",False,pub07,False,False,False
19712,"multipole method. Thus, a range of kernels can be treated in
",False,pub07,False,False,False
19713,"the method while still preserving the high order of accuracyof multipole-based representations. For example, the approachcan be combined with modiﬁed Green function techniques for
",False,pub07,False,False,False
19714,"handling ground planes, symmetry planes, or ﬂat dielectric
",False,pub07,False,False,False
19715,"interfaces, with only minimal modiﬁcation to the algorithm[36]. The algorithm is also capable of handling the Helmholtzkernelassociatedwithfull-waveelectromagneticanalysis[30].
",False,pub07,False,False,False
19716,"The major drawback of our algorithm is its poor perfor-
",False,pub07,False,False,False
19717,"mance on very inhomogeneous problems. The use of the FFTto evaluate the grid potentials is very inefﬁcient in this case,since most of the data in the transform is zero. Future work
",False,pub07,False,False,False
19718,"will focus on more efﬁcient algorithms to evaluate the grid
",False,pub07,False,False,False
19719,"potentials in the very inhomogeneous case, such as applyingthe algorithm of Section III recursively to obtain a multigrid-like algorithm. Such an algorithm would, in principle, inherit
",False,pub07,False,False,False
19720,"the efﬁciency of the grid-based representation discussed inthis paper, while at the same time achieving the near
",False,pub07,False,False,False
19721,"complexity of the fast multipole methods.
",False,pub07,False,False,False
19722,"ACKNOWLEDGMENT
",False,pub07,False,False,False
19723,"The authors are grateful to Dr. K. Nabors for making his
",False,pub07,False,False,False
19724,"FASTCAP program available. Many of the experiments were
",False,pub07,False,False,False
19725,"conducted by using substantial portions of the FASTCAPprogram. In addition, they would like to thank C. L. Bermanand C. Anderson of IBM, Yorktown Heights, NY, and L.
",False,pub07,False,False,False
19726,"Greengard of the Courant Institute for useful discussions.
",False,pub07,False,False,False
19727,"R
",False,pub07,False,False,False
19728,"EFERENCES
",False,pub07,False,False,False
19729,"[1] K. Nabors and J. White, “FASTCAP: A multipole accelerated 3-D
",False,pub07,False,False,False
19730,"capacitance extraction program,” IEEE Trans. Computer-Aided Design ,
",False,pub07,False,False,False
19731,"vol. 10, pp. 1447–1459, Nov. 1991.
",False,pub07,False,False,False
19732,"[2] S. D. Senturia, R. M. Harris, B. P. Johnson, S. Kim, K. Nabors,
",False,pub07,False,False,False
19733,"M. A. Shulman, and J. K. White, “A computer-aided design system
",False,pub07,False,False,False
19734,"for microelectromechanical systems (MEMCAD),” IEEE J. Microelec-
",False,pub07,False,False,False
19735,"tromech. Syst. , vol. 1, pp. 3–13, Mar. 1992.
",False,pub07,False,False,False
19736,"[3] Y. L. Le Coz and R. B. Iverson, “A stochastic algorithm for high speed
",False,pub07,False,False,False
19737,"capacitance extraction in integrated circuits,” Solid State Electron. , vol.
",False,pub07,False,False,False
19738,"35, no. 7, pp. 1005–1012, 1992.
",False,pub07,False,False,False
19739,"[4] P. Dewilde and Z. Q. Ning, Models for Large Integrated Circuits.
",False,pub07,False,False,False
19740,"Boston, MA: Kluwer, 1990.
",False,pub07,False,False,False
19741,"[5] A. J. van Genderen and N. P. VanDer Meijs, “Hierarchical extraction
",False,pub07,False,False,False
19742,"of 3-D interconnect capacitances in large regular VLSI structures,” inProc. Int. Conf. Computer-Aided Design , Nov. 1993.
",False,pub07,False,False,False
19743,"[6] A. H. Zemanian, R. P. Tewarson, C. P. Ju, and J. F. Jen, “Three-
",False,pub07,False,False,False
19744,"dimensional capacitance computations for VLSI/ULSI interconnec-
",False,pub07,False,False,False
19745,"tions,”IEEE Trans. Computer-Aided Design , vol. 8, pp. 1319–1326,
",False,pub07,False,False,False
19746,"Dec. 1989.
",False,pub07,False,False,False
19747,"[7] A. Seidl, H. Klose, M. Svoboda, J. Oberndorfer, and W. R¨ osner,
",False,pub07,False,False,False
19748,"“CAPCAL—A 3-D capacitance solver for support of CAD systems,”
",False,pub07,False,False,False
19749,"IEEE Trans. Computer-Aided Design vol. 7, pp. 549–556, May
",False,pub07,False,False,False
19750,"1988.
",False,pub07,False,False,False
19751,"[8] P. E. Cottrell and E. M. Buturla, “VLSI wiring capacitance,” IBM J.
",False,pub07,False,False,False
19752,"Res. Develop. , vol. 29, pp. 277–287, May 1985.
",False,pub07,False,False,False
19753,"[9] T. Chou and Z. J. Cendes, “Capacitance calculation of IC packages
",False,pub07,False,False,False
19754,"using the ﬁnite element method and planes of symmetry,” IEEE Trans.
",False,pub07,False,False,False
19755,"Computer-Aided Design, vol. 13, pp. 1159–1166, Sept. 1994.
",False,pub07,False,False,False
19756,"[10] R. F. Harrington, Field Computation by Moment Methods. New York:
",False,pub07,False,False,False
19757,"MacMillan, 1968.1072 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS, VOL. 16, NO. 10, OCTOBER 1997
",False,pub07,False,False,False
19758,"[11] S. H. Crandall, Engineering Analysis. New York: McGraw-Hill, 1956.
",False,pub07,False,False,False
19759,"[12] L. Collatz, The Numerical Treatment of Differential Equations ,3rd ed.
",False,pub07,False,False,False
19760,"New York: Springer-Verlag, 1966.
",False,pub07,False,False,False
19761,"[13] C. A. Brebbia, J. C. F. Telles, and L. C. Wrobel, Boundary Element
",False,pub07,False,False,False
19762,"Techniques. Berlin, Germany: Springer-Verlag, 1984.
",False,pub07,False,False,False
19763,"[14] V. Rokhlin, “Rapid solution of integral equations of classical potential
",False,pub07,False,False,False
19764,"theory,”J. Comput. Phys. , vol. 60, pp. 187–207, Sept. 15, 1985.
",False,pub07,False,False,False
19765,"[15] L. Greengard, The Rapid Evaluation of Potential Fields in Particle
",False,pub07,False,False,False
19766,"Systems. Cambridge, MA: MIT Press, 1988.
",False,pub07,False,False,False
19767,"[16] V. Jandhyala, E. Michielssen, and R. Mittra, “Multipole-accelerated
",False,pub07,False,False,False
19768,"capacitance computation for 3-D structures in a stratiﬁed dielectric
",False,pub07,False,False,False
19769,"medium using a closed form Green’s function,” Int. J. Microwave
",False,pub07,False,False,False
19770,"Millimeter-Wave Computer-Aided Eng. , vol. 5, no. 2, pp. 68–78,
",False,pub07,False,False,False
19771,"1995.
",False,pub07,False,False,False
19772,"[17] E. O. Brigham, The Fast Fourier Transform and Its Applications.
",False,pub07,False,False,False
19773,"Englewood Cliffs, NJ: Prentice-Hall, 1988.
",False,pub07,False,False,False
19774,"[18] C. F. van Loan, Computational Frameworks for the Fast Fourier
",False,pub07,False,False,False
19775,"Transform. Philadelphia, PA: SIAM, 1992.
",False,pub07,False,False,False
19776,"[19] J. W. Cooley and J. W. Tukey, “An algorithm for the machine compu-
",False,pub07,False,False,False
19777,"tation of complex Fourier series,” Math. Comp. , vol. 19, pp. 297–301,
",False,pub07,False,False,False
19778,"1965.
",False,pub07,False,False,False
19779,"[20] R. W. Hockney and J. W. Eastwood, Computer Simulation Using
",False,pub07,False,False,False
19780,"Particles. New York: Adam Hilger, 1988.
",False,pub07,False,False,False
19781,"[21] A.BrandtandA.A.Lubrecht,“Multilevel matrixmultiplicationand fast
",False,pub07,False,False,False
19782,"solution of integral equations,” J. Comput. Phys. , vol. 90, pp. 348–370,
",False,pub07,False,False,False
19783,"1990.
",False,pub07,False,False,False
19784,"[22] A. E. Ruehli and P. A. Brennan, “Efﬁcient capacitance calculations
",False,pub07,False,False,False
19785,"for three-dimensional multiconductor systems,” IEEE Trans.Microwave
",False,pub07,False,False,False
19786,"Theory Tech. , vol. MTT-21, pp. 76–82, Feb. 1973.
",False,pub07,False,False,False
19787,"[23] S. M. Rao, T. K. Sarkar, and R. F. Harrington, “The electrostatic ﬁeld of
",False,pub07,False,False,False
19788,"conductingbodies in multipledielectricmedia,” IEEETrans.Microwave
",False,pub07,False,False,False
19789,"Theory Tech. , vol. MTT-32, pp. 1441–1448, Nov. 1984.
",False,pub07,False,False,False
19790,"[24] Y. Saad and M. Schultz, “GMRES: A generalized minimal residual
",False,pub07,False,False,False
19791,"algorithm for solving nonsymmetric linear systems,” SIAM J. Sci. Stat.
",False,pub07,False,False,False
19792,"Comput., vol. 7, pp. 856–869, July 1986.
",False,pub07,False,False,False
19793,"[25] J. D. Jackson, Classical Electrodynamics. New York: Wiley, 1975.
",False,pub07,False,False,False
19794,"[26] R. A. Horn and C. R. Johnson, Matrix Analysis. Cambridge, MA:
",False,pub07,False,False,False
19795,"Cambridge Univ. Press, 1985.
",False,pub07,False,False,False
19796,"[27] L. Berman, “Grid-multipole calculations,” SIAM J. Sci. Stat. Comput. ,
",False,pub07,False,False,False
19797,"vol. 16, pp. 1082–1091, Sept. 1995.
",False,pub07,False,False,False
19798,"[28] A. Brandt, “Multilevel computations of integral transforms and particle
",False,pub07,False,False,False
19799,"interactions with oscillatory kernals,” Comput. Phys. Commun. vol. 65,
",False,pub07,False,False,False
19800,"pp. 24–38, 1991.
",False,pub07,False,False,False
19801,"[29] A. D. McLaren, “Optimal numerical integration on a sphere,” Math.
",False,pub07,False,False,False
19802,"Comput., vol. 17, pp. 361–383, 1963.
",False,pub07,False,False,False
19803,"[30] J. R. Phillips, “Error and complexity analysis for a collocation-grid-
",False,pub07,False,False,False
19804,"projection plus precorrected-FFT algorithm for solving potential integralequations with Laplace or Helmholtz kernels,” in Proc. 1995 Copper
",False,pub07,False,False,False
19805,"Mountain Conf. Multigrid Methods , Apr. 1995.
",False,pub07,False,False,False
19806,"[31] C. R. Anderson, “A method of local corrections for computing the
",False,pub07,False,False,False
19807,"velocity ﬁeld due to a distribution of vortex blobs,” J. Comput. Phys. ,
",False,pub07,False,False,False
19808,"vol. 62, pp. 111–123, 1986.[32] K. Nabors, F. T. Korsmeyer, F. T. Leighton, and J. White, “Multipole
",False,pub07,False,False,False
19809,"accelerated preconditioned iterative methods for three-dimensional po-tential integral equations of the ﬁrst kind,” SIAM J. Sci. Stat. Comput. ,
",False,pub07,False,False,False
19810,"vol. AP-15, pp. 713–735, May 1994.
",False,pub07,False,False,False
19811,"[33] D. T. Borup and O. P. Gandhi, “Calculation of high-resolution SAR
",False,pub07,False,False,False
19812,"distributions in biological bodies using the FFT algorithm and conjugate
",False,pub07,False,False,False
19813,"gradient method,” IEEE Trans. Microwave Theory Tech. , vol. MTT-33,
",False,pub07,False,False,False
19814,"pp. 417–419, 1985.
",False,pub07,False,False,False
19815,"[34] T. K. Sarkar, E. Arvas, and S. M. Rao, “Application of FFT and the
",False,pub07,False,False,False
19816,"conjugate gradient method for the solution of electromagnetic radiation
",False,pub07,False,False,False
19817,"from electrically large and small conducting bodies,” IEEE Trans.
",False,pub07,False,False,False
19818,"Antennas Propagat. , vol. AP-34, pp. 635–640, 1986.
",False,pub07,False,False,False
19819,"[35] J. R. Phillips, M. Kamon, and J. White, “An FFT-based approach to
",False,pub07,False,False,False
19820,"including nonideal ground planes in a fast 3-D inductance extraction
",False,pub07,False,False,False
19821,"program,” in Proc. Custom Int. Circuits Conf. , May 1993.
",False,pub07,False,False,False
19822,"[36] J. R. Phillips and J. K. White, “Efﬁcient capacitance extraction of 3D
",False,pub07,False,False,False
19823,"structures using generalized pre-corrected FFT methods,” in Proc.IEEE
",False,pub07,False,False,False
19824,"3rdTopicalMeetingonElectricalPerformanceofElectronicPackaging ,
",False,pub07,False,False,False
19825,"Nov. 1994.
",False,pub07,False,False,False
19826,"Joel R. Phillips received B.S. degrees in physics
",False,pub07,False,False,False
19827,"and electrical engineering, the M.S. degree in elec-trical engineering, and the Ph.D. degree in electrical
",False,pub07,False,False,False
19828,"engineering and computer science, all from the
",False,pub07,False,False,False
19829,"Massachusetts Institute of Technology, Cambridge,
",False,pub07,False,False,False
19830,"in 1991, 1993, and 1997, respectively.
",False,pub07,False,False,False
19831,"In 1997, he joined Cadence Design Systems, San
",False,pub07,False,False,False
19832,"Jose, CA. His research interests include compu-
",False,pub07,False,False,False
19833,"tational electromagnetics, algorithms for analysis,simulationandmodelingofRFcircuitsandsystems,
",False,pub07,False,False,False
19834,"nonlinear dynamics, and numerical methods for
",False,pub07,False,False,False
19835,"solution of integral equations.
",False,pub07,False,False,False
19836,"Jacob K. White (S’80–M’83–A’88) received the B.S. degree in electri-
",False,pub07,False,False,False
19837,"cal engineering and computer science from the Massachusetts Institute of
",False,pub07,False,False,False
19838,"Technology (MIT), Cambridge, and the S.M. and Ph.D. degrees in electrical
",False,pub07,False,False,False
19839,"engineeringand computersciencefromtheUniversityof California,Berkeley.
",False,pub07,False,False,False
19840,"He worked at the IBM T. J. Watson Research Center, Yorktown Heights,
",False,pub07,False,False,False
19841,"NY, from 1985 to 1987, was the Analog Devices Career Development
",False,pub07,False,False,False
19842,"Assistant Professor at MIT from 1987 to 1989. He is currently a Professor at
",False,pub07,False,False,False
19843,"MIT, and his research interests are in serial and parallel numerical algorithms
",False,pub07,False,False,False
19844,"for problems in circuit, interconnect, device, and microelectromechanical
",False,pub07,False,False,False
19845,"system design.
",False,pub07,False,False,False
19846,"Dr. White was a 1988 Presidential Young Investigator, and an associate
",False,pub07,False,False,False
19847,"editor of the IEEE T
",False,pub07,False,False,False
19848,"RANSACTIONS ON COMPUTER-AIDEDDESIGN OF CIRCUITS
",False,pub07,False,False,False
19849,ANDSYSTEMSfrom 1992 until 1996.,False,pub07,False,False,False
19850,"Abstract. We developed a learning-based question classiﬁer for question an-
",True,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19851,"1 Introduction
",True,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19852,"2 Related Work
",True,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19853,"3 Choosing the Classiﬁer
",True,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19854,"4 Features in Question Classiﬁcation
",True,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19855,"4.1 Lexical Features
",True,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19856,"Question Classiﬁcation by Weighted Combination of
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19857,"Lexical, Syntactic and Semantic Features
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19858,"Babak Loni, Gijs van Tulder, Pascal Wiggers, David M.J. Tax, and Marco Loog
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19859,"Delft University of Technology, P attern Recognition Laboratory,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19860,"P.O. Box 5031, 2600 GA Delft, The Netherlands
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19861,"{b.loni,G.vanTulder}@student.tudelft.nl,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19862,"{P.Wiggers,D.M.J.Tax,M.Loog}@tudelft.nl
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19863,"swering systems. A question classiﬁer tries to predict the entity type of the possi-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19864,"ble answers to a given question written in n atural language. We extracted several
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19865,"lexical, syntactic and semantic features and examined their usefulness for ques-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19866,"tion classiﬁcation. Furthermore we developed a weighting approach to combine
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19867,"features based on their importance. Our result on the well-known TREC questions
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19868,"dataset is competitive with the state-of-the-art on this task.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19869,"1 Introduction
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19870,"One of the most crucial tasks in Question Answering ( QA) systems is question classiﬁ-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19871,"cation. A question classiﬁer predicts the entity type of a possible (factual) answer for a
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19872,"given question. For example, if the system is asked “What is the capital of the Nether-lands?”, the question classiﬁer should assign to this question the label city, since the
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19873,"expected answer is a named entity of type city.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19874,"Determining the class of a question is quite useful for the process of answering the
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19875,"question. Knowing that the question is of a particular type, the search space for possible
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19876,"answers can be narrowed down to a much smalle r space. Furthermore, the question class
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19877,"can be used to rank the candidate answers [5,12].
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19878,"In this work, we developed a feature-driven learning-based question classiﬁer that
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19879,"is competitive with state-of-the-art question classiﬁcation approaches. We extractedknown and new lexical, syntactic and semantic f eatures and compared the classiﬁcation
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19880,"accuracies that can be obtained with these sets. Furthermore, we investigated whether
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19881,"combining feature sets can improve classi ﬁcation accuracy. For this we introduce a
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19882,"weighted combination approach that takes into account the importance of the features.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19883,"This paper is organized as follows. We start with a discussion of related work in
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19884,"section 2. In section 3 we discuss our motivation for choosing support vector machines
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19885,"(
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19886,"SVMs) as our classiﬁer. In section 4 we explain the features that we extracted and their
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19887,"individual classiﬁcation accuracies. We int roduce our approach to combine features in
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19888,"section 5. We end with a conclusion.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19889,"2 Related Work
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19890,"Different approaches to question classiﬁcation have been proposed. Some early stud-ies build question classiﬁers based on matching with hand-crafted rules [15]. However,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19891,"I. Habernal and V . Matou˘ sek (Eds.): TSD 2011, LNAI 6836, pp. 243–250, 2011.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19892,"c/circlecopyrtSpringer-Verlag Berlin Heidelberg 2011244 B. Loni et al.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19893,"these approaches do not generalize well to new domains and do not scale easily. Most re-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19894,"cent studies build question classiﬁers based on machine learning approaches [6,16,9,10]
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19895,"that use features extracted from the question.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19896,"The accuracy of most of the studies, incl uding this work, is usually measured on a
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19897,"well-known taxonomy of question classes proposed by Li and Roth [9]. This taxonomy
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19898,"has two layers consisting of 6 coarse grained and 50 ﬁne grained classes (Table 1).
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19899,"A dataset of almost 6000 labeled question has been created based on this taxonomy1
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19900,"[9]. This dataset which is usually referred to as the TREC (Text REtrieval Conference)
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19901,"dataset, is divided in a training set of 5500 questions and a test set of 500 questions.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19902,"The accuracy of a question classiﬁer is deﬁned as the number of correctly classiﬁed
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19903,"questions divided by total number of questions.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19904,"Table 1. The coarse and ﬁne grained question classes
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19905,"Coarse Fine
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19906,"ABBR abbreviation, expansion
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19907,"DESC deﬁnition, description, manner, reason
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19908,"ENTY animal, body, color, creation, currency, disease, event, food,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19909,"instrument, language, letter, other, plant, product, religion,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19910,"sport, substance, symbol, technique, term, vehicle, word
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19911,"HUM description, gr oup, indivi dual, title
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19912,"LOC city, country, mountain, other, state
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19913,"NUM code, count, date, distance, money, order, other, percent,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19914,"percent, period, speed, temperature, size, weight
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19915,"Li and Roth [9] obtained an accuracy of 84. 0% on the ﬁne grained classes using a
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19916,"SNoW(Sparse Network of Winnows) architecture. Using different semantic features,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19917,"[10] obtained an accuracy of 89.3% for the ﬁn e grained classed. [6] reached accuracies
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19918,"of 89.0% using a Maximum Entropy model and 89.2% using SVMs with linear kernels
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19919,"on the ﬁned grained classes, while they obt ained an accuracy of 93.6% on the coarse
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19920,"grained classes. Zhang et. al. [16] proposed a syntactic tree kernel for SVM-based ques-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19921,"tion classiﬁcation. They obtained an accura cy of 90.0% on the coarse grained classes.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19922,"Pen. et. al. [11] reach an accuracy of 94.0% on the course grained classes with a se-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19923,"mantic tree kernel for SVM classiﬁers. [13] combined rule-based and learning based
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19924,"approaches. They used matched rules as features for a SVM classiﬁer. They reach an
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19925,"accuracy of 90.0% on the ﬁne grained and an accuracy of 94.2% on the coarse grained
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19926,"classes. To our knowledge this is the highest accuracy achieved on the TREC dataset.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19927,"3 Choosing the Classiﬁer
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19928,"The choice of classiﬁer is an important decision in our system. Since in the question
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19929,"classiﬁcation problem the questions are rep resented in a very high dimensional feature
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19930,"space, we decided to choose Support Vector Machines [14] as our classiﬁer. SVMsa r e
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19931,"1http://cogcomp.cs.illinois.edu/Data/QA/QC/Question Classiﬁcation by Weighted Combination of Features 245
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19932,"shown to have good performance on high di mensional data and generally outperform
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19933,"other classiﬁers, such as Nearest Neighbor, Naive Bayes, Decision Trees, SNoWand
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19934,"Maximum Entropy on question classiﬁcation [16,6,5]. We decided to rely on simple
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19935,"linear kernels for the SVMs together with rich features, rather than on other, more com-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19936,"plex kernels. All systems were implemented with LIBSVM [1], a library for support
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19937,"vector machines.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19938,"4 Features in Question Classiﬁcation
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19939,"We used three different types of features: l exical, syntactic and semantic features. We
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19940,"introduce each type of feature and show cla ssiﬁcation results than can be achieved for
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19941,"every feature type.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19942,"4.1 Lexical Features
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19943,"Lexical features of a question are generally based on the context words of the question,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19944,"i.e., the words that appear in a question. In the unigram orbag-of-words approach each
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19945,"word in the vocabulary is treated as a feature. For each question the value of every word
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19946,"feature is set to the frequency count of that word in the question. This can lead to a high
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19947,"dimensional feature space, but that can be d ealt with by using sparse representations
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19948,"that only store non-zero entries. Unigram features are a special case of n-gram features,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19949,"that treat every sequence of nconsecutive words in the question as a feature.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19950,"To obtain insight in the inﬂuence of lexical features on question classiﬁcation, we
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19951,"trained our classiﬁer with different types of l exical features. The classiﬁcation accuracy
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19952,"is listed in Table 2. It shows that, most likely due to data sparseness, unigrams are better
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19953,"features than bigrams.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19954,"In recent studies Huang et. al [5,6] considered question wh-words as a separate fea-
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19955,"ture. They selected 8 types of wh-words, namely what ,which ,when ,where ,who,how,
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19956,"why andrest. For example the wh-word feature of the question “What is the longest
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19957,"river in the world?” is what . We extracted wh-word features from a question with the
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19958,"same approach as [5].
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19959,"Theword shape is a word-level feature that refers to the type of characters used in a
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19960,"word. This can be useful to identify for exam ple numerical values and names. Inspired
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19961,"by [6], we introduced four categories for word shapes: all digit ,lower case ,upper case
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19962,"andother . Not surprisingly, Word shapes alone is not a good feature set for question
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19963,"classiﬁcation (Table 2), but, as will be shown in the next section, combined with other
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19964,"features they can improve cl assiﬁcation accuracy.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19965,"Table 2. The accuracy of SVM classiﬁer based on different lexical features for coarse and ﬁne
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19966,"grained classes
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19967,"Feature Space unigram bigram wh-words word-shapes
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19968,"Coarse 88.2 86.8 45.6 35.5
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19969,"Fine 80.4 75.2 46.8 30.8
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19970,"246 B. Loni et al.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19971,"246 B. Loni et al.
",False,Question Classification by Weighted Combination of Lexical Syntactic and Semantic Features,False,False,True
19972,"Abstract
",True,Recent advances in conversational NLP,False,False,True
19973,"1 Introduction
",True,Recent advances in conversational NLP,False,False,True
19974,"2 Chatbots categories
",True,Recent advances in conversational NLP,False,False,True
19975,"2.1 Social chatbots
",True,Recent advances in conversational NLP,False,False,True
19976,"2.2 Task oriented chatbots
",True,Recent advances in conversational NLP,False,False,True
19977,"Recent advances in conversational NLP :
",False,Recent advances in conversational NLP,False,False,True
19978,"Towards the standardization of Chatbot building
",False,Recent advances in conversational NLP,False,False,True
19979,"Maali Mnasri
",False,Recent advances in conversational NLP,False,False,True
19980,"Opla / Clermont-Ferrand, France
",False,Recent advances in conversational NLP,False,False,True
19981,"maali@opla.ai
",False,Recent advances in conversational NLP,False,False,True
19982,"Dialogue systems have become recently essen-
",False,Recent advances in conversational NLP,False,False,True
19983,"tial in our life. Their use is getting more and
",False,Recent advances in conversational NLP,False,False,True
19984,"more ﬂuid and easy throughout the time. This
",False,Recent advances in conversational NLP,False,False,True
19985,"boils down to the improvements made in NLP
",False,Recent advances in conversational NLP,False,False,True
19986,"and AI ﬁelds. In this paper, we try to provide
",False,Recent advances in conversational NLP,False,False,True
19987,"an overview to the current state of the art of
",False,Recent advances in conversational NLP,False,False,True
19988,"dialogue systems, their categories and the dif-
",False,Recent advances in conversational NLP,False,False,True
19989,"ferent approaches to build them. We end up
",False,Recent advances in conversational NLP,False,False,True
19990,"with a discussion that compares all the tech-
",False,Recent advances in conversational NLP,False,False,True
19991,"niques and analyzes the strengths and weak-
",False,Recent advances in conversational NLP,False,False,True
19992,"nesses of each. Finally, we present an opinion
",False,Recent advances in conversational NLP,False,False,True
19993,"piece suggesting to orientate the research to-
",False,Recent advances in conversational NLP,False,False,True
19994,"wards the standardization of dialogue systems
",False,Recent advances in conversational NLP,False,False,True
19995,"building.
",False,Recent advances in conversational NLP,False,False,True
19996,"1 Introduction
",False,Recent advances in conversational NLP,False,False,True
19997,"Conversational agents or dialogue systems, re-
",False,Recent advances in conversational NLP,False,False,True
19998,"ferred also as chatbots by the media and the in-
",False,Recent advances in conversational NLP,False,False,True
19999,"dustrials, have become very common in our ev-
",False,Recent advances in conversational NLP,False,False,True
20000,"eryday life. We encounter them, for example, in
",False,Recent advances in conversational NLP,False,False,True
20001,"our mobile phones as personal assistants or in e-
",False,Recent advances in conversational NLP,False,False,True
20002,"commerce websites as selling bots. These sys-
",False,Recent advances in conversational NLP,False,False,True
20003,"tems are intended to carry coherent conversations
",False,Recent advances in conversational NLP,False,False,True
20004,"with humans in natural language text or speech or
",False,Recent advances in conversational NLP,False,False,True
20005,"even both. Developing intelligent conversational
",False,Recent advances in conversational NLP,False,False,True
20006,"agents is still an unresolved research problem that
",False,Recent advances in conversational NLP,False,False,True
20007,"raises many challenges in the artiﬁcial intelligence
",False,Recent advances in conversational NLP,False,False,True
20008,"community. We try through this work to identify
",False,Recent advances in conversational NLP,False,False,True
20009,"the different existing algorithms to build chatbots.
",False,Recent advances in conversational NLP,False,False,True
20010,"We also classify the predominant approaches and
",False,Recent advances in conversational NLP,False,False,True
20011,"compare them according to the ﬁnal use case while
",False,Recent advances in conversational NLP,False,False,True
20012,"stating each approach strengths and weaknesses.
",False,Recent advances in conversational NLP,False,False,True
20013,"We aim at uncovering the issues related to this task
",False,Recent advances in conversational NLP,False,False,True
20014,"which can help researchers choose the future di-
",False,Recent advances in conversational NLP,False,False,True
20015,"rections in conversational NLP.
",False,Recent advances in conversational NLP,False,False,True
20016,"Although the ﬁrst chatbots have been developed
",False,Recent advances in conversational NLP,False,False,True
20017,"many years ago, this ﬁeld has never been more fo-
",False,Recent advances in conversational NLP,False,False,True
20018,"cused on than these last years. This can be ex-
",False,Recent advances in conversational NLP,False,False,True
20019,"plained by the recent development on AI and NLP
",False,Recent advances in conversational NLP,False,False,True
20020,"technologies as well as the data availability. Thecurrent scientiﬁc and technological landscape is
",False,Recent advances in conversational NLP,False,False,True
20021,"starting to get crowded with the variety of methods
",False,Recent advances in conversational NLP,False,False,True
20022,"to build chatbots (Gilbert et al., 2019; Haponchyk
",False,Recent advances in conversational NLP,False,False,True
20023,"et al., 2018; Yu et al., 2019; Hwang et al., 2018)
",False,Recent advances in conversational NLP,False,False,True
20024,"while there is a lack of tools that can help re-
",False,Recent advances in conversational NLP,False,False,True
20025,"searchers and industrials focus on improving chat-
",False,Recent advances in conversational NLP,False,False,True
20026,"bots performance. We discuss this problem in sec-
",False,Recent advances in conversational NLP,False,False,True
20027,"tion 6. Our main contributions consist of :
",False,Recent advances in conversational NLP,False,False,True
20028,"analyzing the current scientiﬁc and techno-
",False,Recent advances in conversational NLP,False,False,True
20029,"logical state of the art of conversational sys-
",False,Recent advances in conversational NLP,False,False,True
20030,"tems
",False,Recent advances in conversational NLP,False,False,True
20031,"presenting a new vision towards the standard-
",False,Recent advances in conversational NLP,False,False,True
20032,"ization of conversational NLP
",False,Recent advances in conversational NLP,False,False,True
20033,"2 Chatbots categories
",False,Recent advances in conversational NLP,False,False,True
20034,"We choose to categorize chatbots into two major
",False,Recent advances in conversational NLP,False,False,True
20035,"types: social chatbots and task oriented chatbots.
",False,Recent advances in conversational NLP,False,False,True
20036,"2.1 Social chatbots
",False,Recent advances in conversational NLP,False,False,True
20037,"They are designed to carry unstructured human-
",False,Recent advances in conversational NLP,False,False,True
20038,"like conversations. They are considered as chit-
",False,Recent advances in conversational NLP,False,False,True
20039,"chat bots. Currently, such systems may have an
",False,Recent advances in conversational NLP,False,False,True
20040,"entertainment value but ﬁrstly, they were designed
",False,Recent advances in conversational NLP,False,False,True
20041,"as a testbed for psychological therapy and they
",False,Recent advances in conversational NLP,False,False,True
20042,"are still used, today, for this purpose. These sys-
",False,Recent advances in conversational NLP,False,False,True
20043,"tems (e.g., ELIZA (Weizenbaum, 1966), PARRY
",False,Recent advances in conversational NLP,False,False,True
20044,"(Colby, 1975), ALICE, CLEVER, Microsoft Lit-
",False,Recent advances in conversational NLP,False,False,True
20045,"tle Bing, etc) have taken the ﬁrst steps towards
",False,Recent advances in conversational NLP,False,False,True
20046,"conversational agents.
",False,Recent advances in conversational NLP,False,False,True
20047,"2.2 Task oriented chatbots
",False,Recent advances in conversational NLP,False,False,True
20048,"We also choose to classify them into two cate-
",False,Recent advances in conversational NLP,False,False,True
20049,"gories: generalist task oriented and specialist task
",False,Recent advances in conversational NLP,False,False,True
20050,"oriented chatbots.
",False,Recent advances in conversational NLP,False,False,True
20051,"2.2.1 Generalist task oriented chatbots
",False,Recent advances in conversational NLP,False,False,True
20052,"2.2.1 Generalist task oriented chatbots
",False,Recent advances in conversational NLP,False,False,True
20053,"Abstract
",True,s11042-020-10183-2,False,False,True
20054,"1 Introduction
",True,s11042-020-10183-2,False,False,True
20055,"1.1 Existingapproachesforfakenewsdetection
",True,s11042-020-10183-2,False,False,True
20056,"1.2 Ourcontribution
",True,s11042-020-10183-2,False,False,True
20057,"2 Relatedwork
",True,s11042-020-10183-2,False,False,True
20058,"51% for fake news detection.
",True,s11042-020-10183-2,False,False,True
20059,"3 Methodology
",True,s11042-020-10183-2,False,False,True
20060,"3.1 Wordembedding
",True,s11042-020-10183-2,False,False,True
20061,"3.2 GloVe
",True,s11042-020-10183-2,False,False,True
20062,"3.3 BERT
",True,s11042-020-10183-2,False,False,True
20063,"3.4 Fine-tuningofBERT
",True,s11042-020-10183-2,False,False,True
20064,"3.5 Deeplearningmodelsforfakenewsdetection
",True,s11042-020-10183-2,False,False,True
20065,"3.6 Proposedmodel:FakeBERT
",True,s11042-020-10183-2,False,False,True
20066,"4 Experiments
",True,s11042-020-10183-2,False,False,True
20067,"4.1 Datasetdescription
",True,s11042-020-10183-2,False,False,True
20068,"https://doi.org/10.1007/s11042-020-10183-2
",False,s11042-020-10183-2,False,False,True
20069,"FakeBERT:Fakenewsdetectioninsocialmedia
",False,s11042-020-10183-2,False,False,True
20070,"withaBERT-baseddeeplearningapproach
",False,s11042-020-10183-2,False,False,True
20071,"RohitKumarKaliyar1·AnuragGoswami1·PratikNarang2
",False,s11042-020-10183-2,False,False,True
20072,"Received:1May2020/Revised:24August2020/Accepted:11November2020/
",False,s11042-020-10183-2,False,False,True
20073,"©SpringerScience+BusinessMedia,LLC,partofSpringerNature2021
",False,s11042-020-10183-2,False,False,True
20074,"In the modern era of computing, the news ecosystem has transformed from old traditional
",False,s11042-020-10183-2,False,False,True
20075,"print media to social media outlets. Social media platforms allow us to consume news much
",False,s11042-020-10183-2,False,False,True
20076,"faster, with less restricted editing results in the spread of fake news at an incredible pace and
",False,s11042-020-10183-2,False,False,True
20077,"scale. In recent researches, many useful methods for fake news detection employ sequential
",False,s11042-020-10183-2,False,False,True
20078,"neural networks to encode news content and social context-level information where the text
",False,s11042-020-10183-2,False,False,True
20079,"sequence was analyzed in a unidirectional way. Therefore, a bidirectional training approach
",False,s11042-020-10183-2,False,False,True
20080,"is a priority for modelling the relevant information of fake news that is capable of improv-
",False,s11042-020-10183-2,False,False,True
20081,"ing the classification performance with the ability to capture semantic and long-distance
",False,s11042-020-10183-2,False,False,True
20082,"dependencies in sentences. In this paper, we propose a BERT-based (Bidirectional Encoder
",False,s11042-020-10183-2,False,False,True
20083,"Representations from Transformers) deep learning approach (FakeBERT) by combining dif-
",False,s11042-020-10183-2,False,False,True
20084,"ferent parallel blocks of the single-layer deep Convolutional Neural Network (CNN) having
",False,s11042-020-10183-2,False,False,True
20085,"different kernel sizes and filters with the BERT. Such a combination is useful to handle
",False,s11042-020-10183-2,False,False,True
20086,"ambiguity, which is the greatest challenge to natural language understanding. Classification
",False,s11042-020-10183-2,False,False,True
20087,"results demonstrate that our proposed model (FakeBERT) outperforms the existing models
",False,s11042-020-10183-2,False,False,True
20088,"with an accuracy of 98.90%.
",False,s11042-020-10183-2,False,False,True
20089,"Keywords Fake news ·Neural network ·Social media ·Deep learning ·BERT
",False,s11042-020-10183-2,False,False,True
20090,"1 Introduction
",False,s11042-020-10183-2,False,False,True
20091,"In the past few years, various social media platforms such as Twitter, Facebook, Instagram,
",False,s11042-020-10183-2,False,False,True
20092,"etc. have become very popular since they facilitate the easy acquisition of information and
",False,s11042-020-10183-2,False,False,True
20093,"/envelopebackPratik Narang
",False,s11042-020-10183-2,False,False,True
20094,"pratik.narang@pilani.bits-pilani.ac.in
",False,s11042-020-10183-2,False,False,True
20095,"Rohit Kumar Kaliyar
",False,s11042-020-10183-2,False,False,True
20096,"rk5370@bennett.edu.in
",False,s11042-020-10183-2,False,False,True
20097,"Anurag Goswami
",False,s11042-020-10183-2,False,False,True
20098,"anurag.goswami@bennett.edu.in
",False,s11042-020-10183-2,False,False,True
20099,"1Departement of Computer Science Engineering, Bennett University, Greater Noida, India
",False,s11042-020-10183-2,False,False,True
20100,"2Departement of CSIS, BITS Pilani, Pilani, Rajasthan, IndiaMultimediaToolsandApplications(2021)80: –1178811765
",False,s11042-020-10183-2,False,False,True
20101,"Publishedonline:7January2021Fig.1 Examples of some fake news spread over social media (Source: Facebook®)
",False,s11042-020-10183-2,False,False,True
20102,"provide a quick platform for information sharing [ 10,21]. The availability of unauthentic
",False,s11042-020-10183-2,False,False,True
20103,"data on social media platforms has gained massive attention among researchers and become
",False,s11042-020-10183-2,False,False,True
20104,"a hot-spot for sharing fake news [ 16,46]. Fake news has been an important issue due to
",False,s11042-020-10183-2,False,False,True
20105,"its tremendous negative impact [ 16,46,53], it has increased attention among researchers,
",False,s11042-020-10183-2,False,False,True
20106,"journalists, politicians and the general public. In the context of writing style, fake news is
",False,s11042-020-10183-2,False,False,True
20107,"written or published with the intent to mislead the people and to damage the image of an
",False,s11042-020-10183-2,False,False,True
20108,"agency, entity, person, either for financial or political benefits [ 14,35,39,53]. Few examples
",False,s11042-020-10183-2,False,False,True
20109,"of fake news are shown in Fig. 1. These examples of fake news were in trending during the
",False,s11042-020-10183-2,False,False,True
20110,"COVID-19 pandemic and 2016 U.S. General Presidential Election.
",False,s11042-020-10183-2,False,False,True
20111,"In the research context, related synonyms (keywords) often linked with fake news:
",False,s11042-020-10183-2,False,False,True
20112,"– Rumor: A rumour [ 4,12,16] is an unverified claim about any event, transmitting from
",False,s11042-020-10183-2,False,False,True
20113,"individual to individual in the society. It might imply to an occurrence, article, and
",False,s11042-020-10183-2,False,False,True
20114,"any social issue of open public concern. It might end up being a socially dangerous
",False,s11042-020-10183-2,False,False,True
20115,"phenomenon in any human culture.
",False,s11042-020-10183-2,False,False,True
20116,"– Hoax: A hoax is a falsehood deliberately fabricated to masquerade as the truth [ 43].
",False,s11042-020-10183-2,False,False,True
20117,"Currently, it has been increasing at an alarming rate. Hoax is also known as with similar
",False,s11042-020-10183-2,False,False,True
20118,"names like prank or jape.
",False,s11042-020-10183-2,False,False,True
20119,"1.1 Existingapproachesforfakenewsdetection
",False,s11042-020-10183-2,False,False,True
20120,"Detection of fake news is challenging as it is intentionally written to falsify information.
",False,s11042-020-10183-2,False,False,True
20121,"The former theories [ 1] are valuable in guiding research on fake news detection using dif-
",False,s11042-020-10183-2,False,False,True
20122,"ferent classification models. Existing learnings for fake news detection can be generally
",False,s11042-020-10183-2,False,False,True
20123,"categorized as (i) News Content-based learning and (ii) Social Context-based learning.
",False,s11042-020-10183-2,False,False,True
20124,"News content-based approaches [ 1,14,51,53] deals with different writing style of pub-
",False,s11042-020-10183-2,False,False,True
20125,"lished news articles. In these techniques, our main focus is to extract several features in
",False,s11042-020-10183-2,False,False,True
20126,"fake news article related to both information as well as the writing style. Furthermore, fake
",False,s11042-020-10183-2,False,False,True
20127,"news publishers regularly have malignant plans to spread mutilated and deluding, requiring
",False,s11042-020-10183-2,False,False,True
20128,"specific composition styles to interest and convince a wide extent of consumers that are not
",False,s11042-020-10183-2,False,False,True
20129,"present in true news stories. In these learnings, style-based methodologies [ 12,35,53]a r e
",False,s11042-020-10183-2,False,False,True
20130,"helpful to capture the writing style of manipulators using linguistic features for identifyingMultimediaToolsandApplications(2021)80: –1178811765 11766Fig.2 Approaches for fake news detection
",False,s11042-020-10183-2,False,False,True
20131,"fake articles. Thus, it is difficult to detect fake news more accurately by using only news
",False,s11042-020-10183-2,False,False,True
20132,"content-based features [ 14,33,46]. Thus, we also need to investigate the engagement of
",False,s11042-020-10183-2,False,False,True
20133,"fake news articles with users.
",False,s11042-020-10183-2,False,False,True
20134,"Social context-based approaches [ 14,17,38,51,53] deals with the latent information
",False,s11042-020-10183-2,False,False,True
20135,"between the user and news article.Social engagements (the semantic relationship between
",False,s11042-020-10183-2,False,False,True
20136,"news articles and user) can be used as a significant feature for fake news detection. In
",False,s11042-020-10183-2,False,False,True
20137,"these approaches, instance-based methodologies [ 51] deals with the behaviour of the user
",False,s11042-020-10183-2,False,False,True
20138,"towards any social media post to induce the integrity of unique news stories. Furthermore,
",False,s11042-020-10183-2,False,False,True
20139,"propagation-based methodologies [ 51] deals with the relations of significant social media
",False,s11042-020-10183-2,False,False,True
20140,"posts to guide the learning of validity scores by propagating credibility values between
",False,s11042-020-10183-2,False,False,True
20141,"users, posts, and news. Approaches related to fake news detection show in Fig. 2.I nm o s t
",False,s11042-020-10183-2,False,False,True
20142,"of the existing and useful methods [ 14,38,51] consists of news content and context level
",False,s11042-020-10183-2,False,False,True
20143,"features using unidirectional pre-trained word embedding models (such as GloVe, TF-IDF,
",False,s11042-020-10183-2,False,False,True
20144,"word2Vec, etc.) There is a large scope to use bidirectional pre-trained word embedding
",False,s11042-020-10183-2,False,False,True
20145,"models having powerful feature extraction capability.
",False,s11042-020-10183-2,False,False,True
20146,"1.2 Ourcontribution
",False,s11042-020-10183-2,False,False,True
20147,"In the existing approaches [ 1,33,40], for the detection of fake news, many useful methods
",False,s11042-020-10183-2,False,False,True
20148,"have been presented using traditional machine learning models. The primary advantage of
",False,s11042-020-10183-2,False,False,True
20149,"using deep learning model over existing classical feature-based approaches is that it does
",False,s11042-020-10183-2,False,False,True
20150,"not require any handwritten features; instead, it identifies the best feature set on its own. The
",False,s11042-020-10183-2,False,False,True
20151,"powerful learning ability of deep CNN is primarily due to the use of multiple feature extrac-
",False,s11042-020-10183-2,False,False,True
20152,"tion stages that can automatically learn representations from the dataset. In the existingMultimediaToolsandApplications(2021)80: –1178811765 11767approaches [ 18,19,26], several inspiring ideas have been discussed to bring advancements
",False,s11042-020-10183-2,False,False,True
20153,"in deep Convolutional Neural Networks(CNNs) like exploiting temporal and channel infor-
",False,s11042-020-10183-2,False,False,True
20154,"mation, depth of architecture, and graph-based multi-path information processing. The idea
",False,s11042-020-10183-2,False,False,True
20155,"of using a block of layers as a structural unit is also gaining popularity among researchers.
",False,s11042-020-10183-2,False,False,True
20156,"In this paper, we propose a BERT-based deep learning approach (FakeBERT) by combining
",False,s11042-020-10183-2,False,False,True
20157,"different parallel blocks of the single-layer CNNs with the Bidirectional Encoder Repre-
",False,s11042-020-10183-2,False,False,True
20158,"sentations from Transformers (BERT). We utilize BERT as a sentence encoder, which can
",False,s11042-020-10183-2,False,False,True
20159,"accurately get the context representation of a sentence. This work is in contrast to pre-
",False,s11042-020-10183-2,False,False,True
20160,"vious research works [ 9] where researchers looked at a text sequence in a unidirectional
",False,s11042-020-10183-2,False,False,True
20161,"way (either left to right or right to left for pre-training). Many existing and useful methods
",False,s11042-020-10183-2,False,False,True
20162,"had been [ 9,24] presented with sequential neural networks to encode the relevant infor-
",False,s11042-020-10183-2,False,False,True
20163,"mation. However, a deep neural network with bidirectional training approach can be an
",False,s11042-020-10183-2,False,False,True
20164,"optimal and accurate solution for the detection of fake news. Our proposed method improves
",False,s11042-020-10183-2,False,False,True
20165,"the performance of fake news detection with the powerful ability to capture semantic and
",False,s11042-020-10183-2,False,False,True
20166,"long-distance dependencies in sentences.
",False,s11042-020-10183-2,False,False,True
20167,"To design our proposed architecture, we have added a classification layer on the top
",False,s11042-020-10183-2,False,False,True
20168,"of the encoder output, multiplying the output vector by the embedding matrix, and finally
",False,s11042-020-10183-2,False,False,True
20169,"calculated the probability of each vector with the Softmax function. Our model is a com-
",False,s11042-020-10183-2,False,False,True
20170,"bination of three parallel blocks of 1D-convolutional neural networks with BERT having
",False,s11042-020-10183-2,False,False,True
20171,"different kernel sizes and filters following by a max-pooling layer across each block. With
",False,s11042-020-10183-2,False,False,True
20172,"this combination, the documents were processed using different CNN topologies by vary-
",False,s11042-020-10183-2,False,False,True
20173,"ing kernel size (different n-grams), filters, and several hidden layers or nodes. The design
",False,s11042-020-10183-2,False,False,True
20174,"of FakeBERT consists of five convolution layers, five max-pooling layers followed by two
",False,s11042-020-10183-2,False,False,True
20175,"densely connected layers and one embedding layer (BERT-layer) of input. In each layer,
",False,s11042-020-10183-2,False,False,True
20176,"several filters have been applied to extract the information from the training dataset. Such a
",False,s11042-020-10183-2,False,False,True
20177,"combination of BERT with one-dimensional deep convolutional neural network (1d-CNN)
",False,s11042-020-10183-2,False,False,True
20178,"is useful to handle large-scale structure as well as unstructured text. It effectively addresses
",False,s11042-020-10183-2,False,False,True
20179,"ambiguity, which is the greatest challenge to natural language understanding. Experiments
",False,s11042-020-10183-2,False,False,True
20180,"were conducted to validate the performance of our proposed model. Several performance
",False,s11042-020-10183-2,False,False,True
20181,"evaluation parameters (training accuracy, validation accuracy, False Positive Rate (FPR),
",False,s11042-020-10183-2,False,False,True
20182,"and False Negative Rate (FNR)) have been taken into consideration to validate the classifica-
",False,s11042-020-10183-2,False,False,True
20183,"tion results. Extensive experimentations demonstrate that our proposed model outperforms
",False,s11042-020-10183-2,False,False,True
20184,"as compared to the existing benchmarks for classifying fake news. We illustrate the perfor-
",False,s11042-020-10183-2,False,False,True
20185,"mance of our bidirectional pre-trained model (BERT) achieved an accuracy of 98.90%. Our
",False,s11042-020-10183-2,False,False,True
20186,"proposed approach produces improved results by 4% comparing to the baseline approaches
",False,s11042-020-10183-2,False,False,True
20187,"and is promising for the detection of fake news.
",False,s11042-020-10183-2,False,False,True
20188,"2 Relatedwork
",False,s11042-020-10183-2,False,False,True
20189,"This section briefly summarizes the work in the field of fake news detection. Kumar et al
",False,s11042-020-10183-2,False,False,True
20190,"[21] have explored a comprehensive survey of diverse aspects of fake news. Different cat-
",False,s11042-020-10183-2,False,False,True
20191,"egories of fake news, existing algorithms for counterfeit news detection, and future aspects
",False,s11042-020-10183-2,False,False,True
20192,"have been explored in this research article. In one of the research, Shin et al [ 37]h a v e
",False,s11042-020-10183-2,False,False,True
20193,"investigated about fundamental theories across various disciplines to enhance the interdis-
",False,s11042-020-10183-2,False,False,True
20194,"ciplinary study of fake news. In their study, authors have mainly investigated the problem
",False,s11042-020-10183-2,False,False,True
20195,"of fake news from four prospectives: False knowledge it carries (what type of false message
",False,s11042-020-10183-2,False,False,True
20196,"you get from the content), writing styles(different writing styles for creating fake news),MultimediaToolsandApplications(2021)80: –1178811765 11768propagation patterns (when it is shared in a network, then which trends it follows), and the
",False,s11042-020-10183-2,False,False,True
20197,"credibility of its creators and spreaders (the credibility score of a news creator and spreader).
",False,s11042-020-10183-2,False,False,True
20198,"Bondielli et al [ 4] have presented a hybrid approach for detecting automated spammers
",False,s11042-020-10183-2,False,False,True
20199,"by amalgamating community-based features with other feature categories, namely meta-
",False,s11042-020-10183-2,False,False,True
20200,"content and interaction-based features. In another research, Ahmed et al [ 1] have focused on
",False,s11042-020-10183-2,False,False,True
20201,"automatic detection of fake content using online fake reviews. Authors have also explored
",False,s11042-020-10183-2,False,False,True
20202,"two different feature extraction methods for classifying fake news. They have examined six
",False,s11042-020-10183-2,False,False,True
20203,"different machine learning models and shown improved accomplishments as compared to
",False,s11042-020-10183-2,False,False,True
20204,"existing state-of-the-art benchmarks. In one of the researches, Allcott et al [ 2] have focused
",False,s11042-020-10183-2,False,False,True
20205,"on a quantitative report to understand the impact of fake news on social media in the 2016
",False,s11042-020-10183-2,False,False,True
20206,"U.S. Presidential General Election and its effect upon U.S. voters. Authors have investigated
",False,s11042-020-10183-2,False,False,True
20207,"the authentic and unauthentic URLs related to fake news from the BuzzFeed dataset. In one
",False,s11042-020-10183-2,False,False,True
20208,"of the studies, Shu et al [ 38] have investigated a way for robotization process through hash-
",False,s11042-020-10183-2,False,False,True
20209,"tag recurrence. In this research article, authors have also presented a comprehensive review
",False,s11042-020-10183-2,False,False,True
20210,"of detecting fake news on social media, false news classifications on psychology and social
",False,s11042-020-10183-2,False,False,True
20211,"concepts, and existing algorithms from a data mining perspective. Ghosh et al [ 14]h a v e
",False,s11042-020-10183-2,False,False,True
20212,"investigated the impact of web-based social networking on political decisions. Quantity
",False,s11042-020-10183-2,False,False,True
20213,"research [ 2,53,54] has been done in the context of detecting political-news-based articles.
",False,s11042-020-10183-2,False,False,True
20214,"Authors have investigated the effect of various political gatherings related to the discus-
",False,s11042-020-10183-2,False,False,True
20215,"sion of any fake news as agenda. Authors have also explored the Twitter-based data of six
",False,s11042-020-10183-2,False,False,True
20216,"Venezuelan government officials with a specific end goal to investigate bot collaboration.
",False,s11042-020-10183-2,False,False,True
20217,"Their discoveries recommend that political bots in Venezuela tend to imitate individuals
",False,s11042-020-10183-2,False,False,True
20218,"from political gatherings or basic natives.
",False,s11042-020-10183-2,False,False,True
20219,"In one of the studies, Zhou et al [ 53] have investigated the ability of social media to
",False,s11042-020-10183-2,False,False,True
20220,"aggregate the judgments of a large community of users. In their further investigation, they
",False,s11042-020-10183-2,False,False,True
20221,"have explained machine learning approaches with the end goal to develop a better rumours
",False,s11042-020-10183-2,False,False,True
20222,"detection. They have investigated the difficulties for the spread of rumours, rumours classifi-
",False,s11042-020-10183-2,False,False,True
20223,"cation, and deception for the advancement of such frameworks. They have also investigated
",False,s11042-020-10183-2,False,False,True
20224,"the utilization of such useful strategies towards creating fascinating structures that can help
",False,s11042-020-10183-2,False,False,True
20225,"individuals in settling on choices towards evaluating the integrity of data gathered from var-
",False,s11042-020-10183-2,False,False,True
20226,"ious social media platforms. V osoughi et al [ 46] have recognized salient features of rumours
",False,s11042-020-10183-2,False,False,True
20227,"by investigating three aspects of information spread online: linguistic style, characteristics
",False,s11042-020-10183-2,False,False,True
20228,"of people involved in propagating information, and network propagation subtleties. Authors
",False,s11042-020-10183-2,False,False,True
20229,"have analyzed their proposed algorithm on 209 rumours representing 938,806 tweets col-
",False,s11042-020-10183-2,False,False,True
20230,"lected from real-world events, including the 2013 Boston Marathon bombings, the 2014
",False,s11042-020-10183-2,False,False,True
20231,"Ferguson unrest, and the 2014 Ebola epidemic. They have expressed the effectiveness of
",False,s11042-020-10183-2,False,False,True
20232,"their proposed framework with all existing methods. The primary objective of their study
",False,s11042-020-10183-2,False,False,True
20233,"was to introduce a novel way of assessing style-similarity between different text contents.
",False,s11042-020-10183-2,False,False,True
20234,"They have implemented numerous machine learning models and achieved an accuracy of
",False,s11042-020-10183-2,False,False,True
20235,"51% for fake news detection.
",False,s11042-020-10183-2,False,False,True
20236,"Chen et al [ 7] have proposed an unsupervised learning model combining recurrent neural
",False,s11042-020-10183-2,False,False,True
20237,"networks and auto-encoders to distinguish rumours as anomalies from other credible micro-
",False,s11042-020-10183-2,False,False,True
20238,"blogs based on users’ behaviours. The experimental results show that their proposed model
",False,s11042-020-10183-2,False,False,True
20239,"was able to achieve an accuracy of 92.49% with an F1 score of 89.16%. Further, Yang
",False,s11042-020-10183-2,False,False,True
20240,"et al [ 49] have arrived with comparative resolutions for detecting false rumours. During
",False,s11042-020-10183-2,False,False,True
20241,"the 2011 riots in England, authors have noticed and investigated that any improvement in
",False,s11042-020-10183-2,False,False,True
20242,"the false rumours based stories could produce good results. In their investigation of the
",False,s11042-020-10183-2,False,False,True
20243,"2013 Boston Marathon bombings, they have found some exciting news stories, and mostMultimediaToolsandApplications(2021)80: –1178811765 11769of them were rumours and produced a significant impact on the share market. Shu et al
",False,s11042-020-10183-2,False,False,True
20244,"[39] have explored the connection between fake and real facts available on social media
",False,s11042-020-10183-2,False,False,True
20245,"platforms using an open tweet dataset. This dataset was created by gathering online tweets
",False,s11042-020-10183-2,False,False,True
20246,"from Twitter that contains URLs from reality checking facts. In their investigation, they have
",False,s11042-020-10183-2,False,False,True
20247,"found that URL’s are the most widely recognized strategy to share news articles on various
",False,s11042-020-10183-2,False,False,True
20248,"stages for the measurement of client articulation (for example, Twitter’s limit is with 140
",False,s11042-020-10183-2,False,False,True
20249,"characters constraint). In their further investigation, they have used a Hoax-based dataset
",False,s11042-020-10183-2,False,False,True
20250,"that gives a more accurate prediction for distinguishing fake news stories by conflicting
",False,s11042-020-10183-2,False,False,True
20251,"them against known news sources from renowned inspection sites.
",False,s11042-020-10183-2,False,False,True
20252,"In one of the researches, Monteiro et al [ 25] have collected a fake news dataset in the
",False,s11042-020-10183-2,False,False,True
20253,"Portuguese language and investigated their results based on different linguistic features.
",False,s11042-020-10183-2,False,False,True
20254,"Authors have achieved the highest accuracy of 49% using machine learning techniques.
",False,s11042-020-10183-2,False,False,True
20255,"One of the researches, Karimi et al [ 20] have analyzed 360 satirical news articles includ-
",False,s11042-020-10183-2,False,False,True
20256,"ing civics, science, business, and delicate news. They have also proposed an SVM-based
",False,s11042-020-10183-2,False,False,True
20257,"model. In their investigation, their five highlights are Absurdity, Humor, Grammar, Nega-
",False,s11042-020-10183-2,False,False,True
20258,"tive effect, and punctuation. Their proposed framework achieved an accuracy of 38.81%.
",False,s11042-020-10183-2,False,False,True
20259,"One of the researches, Perez-Rosas et al [ 29] have explained the automatic identification
",False,s11042-020-10183-2,False,False,True
20260,"of fake content in online news articles. They have presented a comprehensive analysis for
",False,s11042-020-10183-2,False,False,True
20261,"the identification of linguistic features in the false news content. In one of the studies,
",False,s11042-020-10183-2,False,False,True
20262,"Castillo et al [ 5] have investigated feature-based methods to assess the credibility of tweets
",False,s11042-020-10183-2,False,False,True
20263,"on Twitter. Roy et al [ 34] have explored the neural embedding approach using the deep
",False,s11042-020-10183-2,False,False,True
20264,"recurrent model. They have used weighted n-gram bag of word model using statistical fea-
",False,s11042-020-10183-2,False,False,True
20265,"tures and other external features with the help of featuring engineering. Subsequently, they
",False,s11042-020-10183-2,False,False,True
20266,"have combined all features and classifying fake news with the accuracy of 43.82%. One of
",False,s11042-020-10183-2,False,False,True
20267,"the researches, Wang et al [ 47] have presented a novel dataset for fake news detection. They
",False,s11042-020-10183-2,False,False,True
20268,"have proposed a hybrid architecture to solve fake news problem. They have created a model
",False,s11042-020-10183-2,False,False,True
20269,"using two main components; one is a Convolutional Neural Network for meta-data represen-
",False,s11042-020-10183-2,False,False,True
20270,"tation learning, followed by a Long Short-Term Memory neural network (LSTM). Although
",False,s11042-020-10183-2,False,False,True
20271,"being complicated with many parameters to be optimized, their proposed model performs
",False,s11042-020-10183-2,False,False,True
20272,"poorly on the test set, with only 27.4% inaccuracy. One of the researches, Peters et al [ 30]
",False,s11042-020-10183-2,False,False,True
20273,"took a different perspective on detecting fake news by looking at its linguistic characteris-
",False,s11042-020-10183-2,False,False,True
20274,"tics. Despite substantial dependence on lexical resources, the performance on political-set
",False,s11042-020-10183-2,False,False,True
20275,"was even slower than [ 47], with an accuracy of 22.0% only.
",False,s11042-020-10183-2,False,False,True
20276,"In many existing studies [ 13,23,28,42], authors have explored the problem of fake news
",False,s11042-020-10183-2,False,False,True
20277,"employing a real-world fake news dataset: Fake-News. In one of the studies, Ahmed et al [ 1]
",False,s11042-020-10183-2,False,False,True
20278,"have utilised TF-IDF (Term Frequency-Inverse Document Frequency) as a feature extrac-
",False,s11042-020-10183-2,False,False,True
20279,"tion method with different machine learning models. Extensive experiments have performed
",False,s11042-020-10183-2,False,False,True
20280,"with LR (Linear-regression model) and obtained an accuracy of 89.00%. Subsequently, they
",False,s11042-020-10183-2,False,False,True
20281,"have shown an accuracy of 92% using their LSVM (Linear Support Vector Machine). Liu
",False,s11042-020-10183-2,False,False,True
20282,"et al [ 23] have investigated the methods for recognizing false tweets. In their investigation,
",False,s11042-020-10183-2,False,False,True
20283,"authors have utilized a corpus of more than 8 million tweets gathered from the supporters
",False,s11042-020-10183-2,False,False,True
20284,"of the presidential candidates in the general election in the U.S. In their investigation, they
",False,s11042-020-10183-2,False,False,True
20285,"have employed deep CNNs for fake news detection. In their approach, they have utilised the
",False,s11042-020-10183-2,False,False,True
20286,"concept of subjectivity analysis and obtained an accuracy of 92.10%. O’Brien et al [ 28]h a v e
",False,s11042-020-10183-2,False,False,True
20287,"applied deep learning strategies for classifying fake news. In their study, they have achieved
",False,s11042-020-10183-2,False,False,True
20288,"an accuracy of 93.50% using the black-box method. Ghanem et al [ 13] have adopted dif-
",False,s11042-020-10183-2,False,False,True
20289,"ferent word embeddings, including n-gram features to detect the stances in fake articles.
",False,s11042-020-10183-2,False,False,True
20290,"They have obtained an accuracy of 48.80%. Ruchansky et al [ 35] have employed a deepMultimediaToolsandApplications(2021)80: –1178811765 11770hybrid model for classifying fake news. They have utilized news-user relationships as an
",False,s11042-020-10183-2,False,False,True
20291,"essential factor and achieved an accuracy of 89.20%. In one of the studies, Singh et al [ 42]
",False,s11042-020-10183-2,False,False,True
20292,"have investigated with LIWC (Linguistic Analysis and Word Count) features using tradi-
",False,s11042-020-10183-2,False,False,True
20293,"tional machine learning methods for classifying fake news. They have explored the problem
",False,s11042-020-10183-2,False,False,True
20294,"of fake news with SVM (support vector machine) as a classifier obtained an accuracy of
",False,s11042-020-10183-2,False,False,True
20295,"87.00%. In one of the studies, Jwa et al [ 18] have explored the approach towards automatic
",False,s11042-020-10183-2,False,False,True
20296,"fake news detection. They have used Bidirectional Encoder Representations from Trans-
",False,s11042-020-10183-2,False,False,True
20297,"formers model (BERT) model to detect fake news by analyzing the relationship between the
",False,s11042-020-10183-2,False,False,True
20298,"headline and the body text of the news story. Their results improve the 0.14 F-score over
",False,s11042-020-10183-2,False,False,True
20299,"existing state-of-the-art models. Weiss et al [ 48] have investigated the origins of the term
",False,s11042-020-10183-2,False,False,True
20300,"“fake news” and the factors contributing to its current prevalence. This lack of consensus
",False,s11042-020-10183-2,False,False,True
20301,"may have future implications for students in particular and higher education. Crestani et al
",False,s11042-020-10183-2,False,False,True
20302,"[8] have proposed a novel model that can classify a user as a potential fact checker or a
",False,s11042-020-10183-2,False,False,True
20303,"potential fake news spreader. Their model was based on a Convolutional Neural Network
",False,s11042-020-10183-2,False,False,True
20304,"(CNN) and combined word embeddings with features that represent users’ personality traits
",False,s11042-020-10183-2,False,False,True
20305,"and linguistic patterns.
",False,s11042-020-10183-2,False,False,True
20306,"3 Methodology
",False,s11042-020-10183-2,False,False,True
20307,"In this section, an overview of word embedding, GloVe word embedding, BERT model,
",False,s11042-020-10183-2,False,False,True
20308,"fine-tuning of BERT, and the selection of hyperparameters discussed. Our proposed model
",False,s11042-020-10183-2,False,False,True
20309,"(FakeBERT) and other deep learning architectures also investigated in this section.
",False,s11042-020-10183-2,False,False,True
20310,"3.1 Wordembedding
",False,s11042-020-10183-2,False,False,True
20311,"Word embeddings [ 30] are widely used in both machine learning as well as deep learning
",False,s11042-020-10183-2,False,False,True
20312,"models. These models perform well in cases such as reduced training time and improved
",False,s11042-020-10183-2,False,False,True
20313,"overall classification performance of the model. Pre-trained representations can also either
",False,s11042-020-10183-2,False,False,True
20314,"be static or contextual (refer Fig. 3for more details). Contextual models generate a rep-
",False,s11042-020-10183-2,False,False,True
20315,"resentation of each word that is based on the other words in the sentence. Word2Vec and
",False,s11042-020-10183-2,False,False,True
20316,"GloVe [ 50] are currently among the most widely used word embedding models that can con-
",False,s11042-020-10183-2,False,False,True
20317,"vert words into meaningful vectors. For using pre-trained embedding models for training,
",False,s11042-020-10183-2,False,False,True
20318,"we displace the parameters of the processing layer with input embedding vectors. Primarily,
",False,s11042-020-10183-2,False,False,True
20319,"we maintain the index and then fix this layer, restricting it from being updated throughout
",False,s11042-020-10183-2,False,False,True
20320,"the method of gradient descent [ 30,31]. Our experiment shows that embedding-based input
",False,s11042-020-10183-2,False,False,True
20321,"vectors perform a valuable role in text classification tasks.
",False,s11042-020-10183-2,False,False,True
20322,"3.2 GloVe
",False,s11042-020-10183-2,False,False,True
20323,"The GloVe is a weighted least square model [ 3] that train the model using co-occurrence
",False,s11042-020-10183-2,False,False,True
20324,"counts of the words in the input vectors. It effectively leverages the benefits of the statistical
",False,s11042-020-10183-2,False,False,True
20325,"information by training on the non-zero elements in a word-to-word co-occurrence matrix.
",False,s11042-020-10183-2,False,False,True
20326,"The GloVe is an unsupervised training model that is useful to find the co-relation between
",False,s11042-020-10183-2,False,False,True
20327,"two words with their distance in a vector space [ 31]. These generated vectors are known as
",False,s11042-020-10183-2,False,False,True
20328,"word embedding vectors. We have used word embedding as semantic features in addition
",False,s11042-020-10183-2,False,False,True
20329,"to n-grams because they represent the semantic distances between the words in the con-
",False,s11042-020-10183-2,False,False,True
20330,"text. The smallest package of embedding is 822Mb, called “glove.6B.zip”. GloVe model
",False,s11042-020-10183-2,False,False,True
20331,"is trained on a dataset having one billion words with a dictionary of 400 thousand words.MultimediaToolsandApplications(2021)80: –1178811765 11771Fig.3 An Overview of existing word-embedding models
",False,s11042-020-10183-2,False,False,True
20332,"There exist different embedding vector sizes, having 50, 100, 200 and 300 dimensions for
",False,s11042-020-10183-2,False,False,True
20333,"processing. In this paper, we have taken the 100-dimensional version.
",False,s11042-020-10183-2,False,False,True
20334,"3.3 BERT
",False,s11042-020-10183-2,False,False,True
20335,"BERT [ 11] is a advanced pre-trained word embedding model based on transformer encoded
",False,s11042-020-10183-2,False,False,True
20336,"architecture [ 44]. We utilize BERT as a sentence encoder, which can accurately get the
",False,s11042-020-10183-2,False,False,True
20337,"context representation of a sentence [ 30]. BERT removes the unidirectional constraint using
",False,s11042-020-10183-2,False,False,True
20338,"a mask language model (MLM) [ 44]. It randomly masks some of the tokens from the input
",False,s11042-020-10183-2,False,False,True
20339,"and predicts the original vocabulary id of the masked word based only. MLM has increased
",False,s11042-020-10183-2,False,False,True
20340,"the capability of BERT to outperforms as compared to previous embedding methods. It
",False,s11042-020-10183-2,False,False,True
20341,"is a deeply bidirectional system that is capable of handling the unlabelled text by jointly
",False,s11042-020-10183-2,False,False,True
20342,"conditioning on both left and right context in all layers. In this research, we have extracted
",False,s11042-020-10183-2,False,False,True
20343,"embeddings for a sentence or a set of words or pooling the sequence of hidden-states for the
",False,s11042-020-10183-2,False,False,True
20344,"whole input sequence. A deep bidirectional model is more powerful than a shallow left-to-
",False,s11042-020-10183-2,False,False,True
20345,"right and right-to-left model. In the existing research [ 11], two types of BERT models have
",False,s11042-020-10183-2,False,False,True
20346,"been investigated for context-specific tasks, are:
",False,s11042-020-10183-2,False,False,True
20347,"– BERT Base (refer Table 1for more information about parameters setting): Smaller in
",False,s11042-020-10183-2,False,False,True
20348,"size, computationally affordable and not applicable to complex text mining operations.
",False,s11042-020-10183-2,False,False,True
20349,"– BERT Large (refer Table 2for more information about parameters setting): Larger in
",False,s11042-020-10183-2,False,False,True
20350,"size, computationally expensive and crunches large text data to deliver the best results.
",False,s11042-020-10183-2,False,False,True
20351,"3.4 Fine-tuningofBERT
",False,s11042-020-10183-2,False,False,True
20352,"Fine-tuning of BERT [ 11] is a process that allows it to model many downstream tasks, irre-
",False,s11042-020-10183-2,False,False,True
20353,"spective of the text form (single text or text pairs). A limited exploration is available to
",False,s11042-020-10183-2,False,False,True
20354,"enhance the computing power of BERT to improve the performance on target tasks. BERT
",False,s11042-020-10183-2,False,False,True
20355,"model uses a self-attention mechanism to unify the word vectors as inputs that include
",False,s11042-020-10183-2,False,False,True
20356,"bidirectional cross attention between two sentences. Mainly, there exist a few fine-tuningMultimediaToolsandApplications(2021)80: –1178811765 11772Table1 Parameters for BERT-Base
",False,s11042-020-10183-2,False,False,True
20357,"Parameter Name Value of Parameter
",False,s11042-020-10183-2,False,False,True
20358,"Number of Layers 12
",False,s11042-020-10183-2,False,False,True
20359,"Hidden Size 768
",False,s11042-020-10183-2,False,False,True
20360,"Attention Heads 12
",False,s11042-020-10183-2,False,False,True
20361,"Number of Parameters 110M
",False,s11042-020-10183-2,False,False,True
20362,"strategies that we need to consider: 1) The first factor is the pre-processing of long text since
",False,s11042-020-10183-2,False,False,True
20363,"the maximum sequence length of BERT is 512. In our research, we have taken the sequence
",False,s11042-020-10183-2,False,False,True
20364,"length of 512. 2) The second factor is layer selection. The official BERT-base model con-
",False,s11042-020-10183-2,False,False,True
20365,"sists of an embedding layer, a 12-layer encoder, and a pooling layer. 3) The third factor
",False,s11042-020-10183-2,False,False,True
20366,"is the over-fitting problem. BERT can be fine-tuned with different learning parameters for
",False,s11042-020-10183-2,False,False,True
20367,"different context-specific tasks [ 44] (refer Table 2for more information).
",False,s11042-020-10183-2,False,False,True
20368,"3.5 Deeplearningmodelsforfakenewsdetection
",False,s11042-020-10183-2,False,False,True
20369,"Deep learning models are well-known for achieving state-of-the-art results in a wide range
",False,s11042-020-10183-2,False,False,True
20370,"of artificial intelligence applications [ 31]. This section provides an overview of the deep
",False,s11042-020-10183-2,False,False,True
20371,"learning models used in our research with their architectures to achieve the end goal. Exper-
",False,s11042-020-10183-2,False,False,True
20372,"iments have been conducted using deep learning-based models (CNN and LSTM [ 15]) and
",False,s11042-020-10183-2,False,False,True
20373,"our proposed model (FakeBERT) with different pre-trained word embeddings.
",False,s11042-020-10183-2,False,False,True
20374,"a) Convolutional Neural Network (CNN): In Fig. 4, the computational graph of our
",False,s11042-020-10183-2,False,False,True
20375,"designed Convolutional Neural Network (CNN) model is shown. This CNN model (Fig. 4)
",False,s11042-020-10183-2,False,False,True
20376,"truncates, zero-pads, and tokenizes the fake news article separately and passes each into an
",False,s11042-020-10183-2,False,False,True
20377,"embedding layer. In this architecture (refer Table 3and Fig. 4), first convolution layer holds
",False,s11042-020-10183-2,False,False,True
20378,"128 filters with kernels size=5, which decreases the input embedding vector from 1000 to
",False,s11042-020-10183-2,False,False,True
20379,"996 after convolution process. In the network, after each convolution layer, a max-pooling
",False,s11042-020-10183-2,False,False,True
20380,"layer is also present to reduce the input vector dimension. Subsequently, a max-pooling
",False,s11042-020-10183-2,False,False,True
20381,"layer with filter size=5; that further minimises the embedding vector to 1/5th of 996, i.e.
",False,s11042-020-10183-2,False,False,True
20382,"199. The second convolution layer holds 128 filters with kernels size=5, which decreases
",False,s11042-020-10183-2,False,False,True
20383,"the input embedding vector from 199 to 195. Subsequently, this is the max-pooling layer
",False,s11042-020-10183-2,False,False,True
20384,"with filter size 5; that further reduces the input vector to 1/5th of 199, i.e. 39. After three
",False,s11042-020-10183-2,False,False,True
20385,"convolution layers, a flatten layer is added to convert 2-D input to 1-D. Subsequently, there
",False,s11042-020-10183-2,False,False,True
20386,"are two hidden layers having 128 neurons in each one. The outputs of the CNNs are passed
",False,s11042-020-10183-2,False,False,True
20387,"through a dense layer with dropout and then passed through a softmax layer to yield a stance
",False,s11042-020-10183-2,False,False,True
20388,"classification. Number of trainable parameters are also shown in Table 3.
",False,s11042-020-10183-2,False,False,True
20389,"Table2 Parameters for BERT-Large
",False,s11042-020-10183-2,False,False,True
20390,"Parameter Name Value of Parameter
",False,s11042-020-10183-2,False,False,True
20391,"Number of Layers 24
",False,s11042-020-10183-2,False,False,True
20392,"Hidden Size 1024
",False,s11042-020-10183-2,False,False,True
20393,"Attention Heads 16
",False,s11042-020-10183-2,False,False,True
20394,"Number of Parameters 340MMultimediaToolsandApplications(2021)80: –1178811765 11773Fig.4 CNN model
",False,s11042-020-10183-2,False,False,True
20395,"Table3 CNN layered architecture
",False,s11042-020-10183-2,False,False,True
20396,"Layer Input size Output size Param number
",False,s11042-020-10183-2,False,False,True
20397,"Embedding 1000 1000 ×100 25187700
",False,s11042-020-10183-2,False,False,True
20398,"Conv1D 1000 ×100 996 ×128 64128
",False,s11042-020-10183-2,False,False,True
20399,"Maxpool 996 ×128 199 ×128 0
",False,s11042-020-10183-2,False,False,True
20400,"Conv1D 199 ×128 195 ×128 82048
",False,s11042-020-10183-2,False,False,True
20401,"Maxpool 195 ×128 39 ×128 0
",False,s11042-020-10183-2,False,False,True
20402,"Conv1D 39 ×128 35 ×128 82048
",False,s11042-020-10183-2,False,False,True
20403,"Maxpool 35 ×128 1 ×128 0
",False,s11042-020-10183-2,False,False,True
20404,"Flatten 1 ×128 128 0
",False,s11042-020-10183-2,False,False,True
20405,"Dense 128 128 16512
",False,s11042-020-10183-2,False,False,True
20406,"Dense 128 2 258
",False,s11042-020-10183-2,False,False,True
20407,"Table4 LSTM layered architecture
",False,s11042-020-10183-2,False,False,True
20408,"Layer Input size Output size Param number
",False,s11042-020-10183-2,False,False,True
20409,"Embedding 1000 ×100 1000 ×100 25187700
",False,s11042-020-10183-2,False,False,True
20410,"Dropout 1000 ×100 1000 ×100 0
",False,s11042-020-10183-2,False,False,True
20411,"Conv1D 1000 ×100 1000 ×32 16032
",False,s11042-020-10183-2,False,False,True
20412,"Maxpool 1000 ×32 500 ×32 0
",False,s11042-020-10183-2,False,False,True
20413,"Conv1D 500 ×32 500 ×64 6208
",False,s11042-020-10183-2,False,False,True
20414,"Maxpool 500 ×64 250 ×64 0
",False,s11042-020-10183-2,False,False,True
20415,"LSTM 250 ×64 100 66000
",False,s11042-020-10183-2,False,False,True
20416,"Batch-Normalization 100 100 400
",False,s11042-020-10183-2,False,False,True
20417,"Dense 100 256 25856
",False,s11042-020-10183-2,False,False,True
20418,"Dense 256 128 32896
",False,s11042-020-10183-2,False,False,True
20419,"Dense 128 64 8256
",False,s11042-020-10183-2,False,False,True
20420,"Dense 64 2 130MultimediaToolsandApplications(2021)80: –1178811765 11774Fig.5 FakeBERT model
",False,s11042-020-10183-2,False,False,True
20421,"b) Long Short Term Memory Network (LSTM): In this paper, we have implemented the
",False,s11042-020-10183-2,False,False,True
20422,"LSTM model having four dense layers with a batch normalization process for the classi-
",False,s11042-020-10183-2,False,False,True
20423,"fication of fake news. The selection of optimal hyperparameters is also made for accurate
",False,s11042-020-10183-2,False,False,True
20424,"results. From Table 4, we can observe the layered architecture of the LSTM model.
",False,s11042-020-10183-2,False,False,True
20425,"3.6 Proposedmodel:FakeBERT
",False,s11042-020-10183-2,False,False,True
20426,"In this paper, the most fundamental advantage of selecting a deep convolutional neural net-
",False,s11042-020-10183-2,False,False,True
20427,"work is the automatic feature extraction. In our proposed model, we pass the input in the
",False,s11042-020-10183-2,False,False,True
20428,"form of a tensor in which local elements correlates with one another. More concrete results
",False,s11042-020-10183-2,False,False,True
20429,"can be achieved with a deep architecture which develops hierarchical representations of
",False,s11042-020-10183-2,False,False,True
20430,"learning. From Fig. 5, we can perceive the computational graph of our proposed approach
",False,s11042-020-10183-2,False,False,True
20431,"(FakeBERT). In many existing and useful studies [ 6,52], the problem of fake news has
",False,s11042-020-10183-2,False,False,True
20432,"examined utilising a unidirectional pre-trained word embedding model followed by a 1D-
",False,s11042-020-10183-2,False,False,True
20433,"convolutional-pooling layer network [ 52]. Our suggested model obtains the advantages of
",False,s11042-020-10183-2,False,False,True
20434,"automated feature engineering approach [ 36]. In our model, inputs are the vectors gener-
",False,s11042-020-10183-2,False,False,True
20435,"ated after word-embedding from BERT. We give the equal dimensional input vectors to all
",False,s11042-020-10183-2,False,False,True
20436,"three convolutional layers present in parallel blocks [ 26] followed by a pooling layer in each
",False,s11042-020-10183-2,False,False,True
20437,"block. In our proposed model, the decision of chosen number of convolutional layers, ker-
",False,s11042-020-10183-2,False,False,True
20438,"nels sizes, no. of filters, and optimal hyperparameters etc.[ 19,26] to make our model more
",False,s11042-020-10183-2,False,False,True
20439,"accurate as follows:
",False,s11042-020-10183-2,False,False,True
20440,"Convolutionallayer The convolutional layer consists of a set of filters and kernels [ 52]f o r
",False,s11042-020-10183-2,False,False,True
20441,"better semantic representations of words having a different length. The significant actions
",False,s11042-020-10183-2,False,False,True
20442,"performed are matrix multiplications (non-linear operation) passes through an activation
",False,s11042-020-10183-2,False,False,True
20443,"function to produce the final output. In our proposed model, we have used three parallel
",False,s11042-020-10183-2,False,False,True
20444,"blocks of 1D-CNN having one layer in each block and two straight forward layers after the
",False,s11042-020-10183-2,False,False,True
20445,"concatenation process with different kernel sizes and filters.
",False,s11042-020-10183-2,False,False,True
20446,"Max-pooling layer Max-pooling layer effectively down-samples [ 27,36] the output
",False,s11042-020-10183-2,False,False,True
20447,"obtained from the convolutional layer and reduce the number of computation operationsMultimediaToolsandApplications(2021)80: –1178811765 11775needed in the system. Its function is to progressively reduce the spatial size of the represen-
",False,s11042-020-10183-2,False,False,True
20448,"tation to reduce the amount of parameters and computation in the network. In our proposed
",False,s11042-020-10183-2,False,False,True
20449,"model, we have used five max-pooling layers (three using parallel blocks of 1D-CNN and
",False,s11042-020-10183-2,False,False,True
20450,"two with straight forward convolutional layers).
",False,s11042-020-10183-2,False,False,True
20451,"Flattenlayer In between the convolutional layer and the fully connected layer, there is a
",False,s11042-020-10183-2,False,False,True
20452,"Flatten layer. Flattening transforms a two-dimensional matrix of features into a vector that
",False,s11042-020-10183-2,False,False,True
20453,"can be fed into a fully connected neural network classifier.
",False,s11042-020-10183-2,False,False,True
20454,"Dense layer A dense layer is just a regular layer of neurons in a neural network. Each
",False,s11042-020-10183-2,False,False,True
20455,"neuron receives input from all the neurons in the previous layer, thus densely connected.
",False,s11042-020-10183-2,False,False,True
20456,"The layer has a weight matrix W, a bias vector b, and the activations of previous layer a.
",False,s11042-020-10183-2,False,False,True
20457,"In many existing and useful methods [ 36,45], authors have mostly used one or two dense
",False,s11042-020-10183-2,False,False,True
20458,"layers in their proposed networks to prevent over-fitting. In our proposed model, we have
",False,s11042-020-10183-2,False,False,True
20459,"also taken two dense layers with a diverse number of filters.
",False,s11042-020-10183-2,False,False,True
20460,"Dropout Dropout is a regularization technique [ 36,45] where randomly selected neurons
",False,s11042-020-10183-2,False,False,True
20461,"are ignored during training. Its main contribution to the activation of downstream neurons
",False,s11042-020-10183-2,False,False,True
20462,"is temporally removed on the forward pass and any weight updates are not applied to the
",False,s11042-020-10183-2,False,False,True
20463,"neuron on the backward pass. We have applied dropout to dense layers in the network.
",False,s11042-020-10183-2,False,False,True
20464,"Dropout works by randomly setting the outgoing edges of hidden units to 0 at each update
",False,s11042-020-10183-2,False,False,True
20465,"of the training phase. We have used the value of dropout is 0.2 in our investigations.
",False,s11042-020-10183-2,False,False,True
20466,"ActivationFunction ReLu refers to the Rectifier Unit, the most commonly deployed acti-
",False,s11042-020-10183-2,False,False,True
20467,"vation function [ 22,41] for the outputs of the CNN neurons. The main advantage of using
",False,s11042-020-10183-2,False,False,True
20468,"the ReLU function over other activation functions is that it does not activate all the neurons
",False,s11042-020-10183-2,False,False,True
20469,"at the same time. ReLU is computed after the convolution and is a non-linear activation
",False,s11042-020-10183-2,False,False,True
20470,"function like tanh or sigmoid. The equation of ReLU can be written as:
",False,s11042-020-10183-2,False,False,True
20471,"σ=max( 0,z) (1)
",False,s11042-020-10183-2,False,False,True
20472,"here z =input
",False,s11042-020-10183-2,False,False,True
20473,"LossFunction( L)The cross-entropy compares the model’s prediction with the label which
",False,s11042-020-10183-2,False,False,True
20474,"is the true probability distribution. The cross-entropy goes down as the prediction gets more
",False,s11042-020-10183-2,False,False,True
20475,"and more accurate. It becomes zero if the prediction is perfect. As such, the cross-entropy
",False,s11042-020-10183-2,False,False,True
20476,"can be a loss function to train a classification model. So predicting a probability of .014
",False,s11042-020-10183-2,False,False,True
20477,"when the actual observation label is 1 would be bad and result in a high loss value. In binary
",False,s11042-020-10183-2,False,False,True
20478,"classification, where the number of classes ( M) equals 2, cross-entropy can be calculated as:
",False,s11042-020-10183-2,False,False,True
20479,"L=−(ylog(p) +(1−y)log( 1−p)) (2)
",False,s11042-020-10183-2,False,False,True
20480,"IfM>2 (i.e. multi-class classification), we calculate a separate loss for each class label per
",False,s11042-020-10183-2,False,False,True
20481,"observation and sum the result.
",False,s11042-020-10183-2,False,False,True
20482,"−M/summationdisplay
",False,s11042-020-10183-2,False,False,True
20483,"c=1yo,clog/parenleftbig
",False,s11042-020-10183-2,False,False,True
20484,"po,c/parenrightbig
",False,s11042-020-10183-2,False,False,True
20485,"(3)MultimediaToolsandApplications(2021)80: –1178811765 11776Table5 FakeBERT layered architecture
",False,s11042-020-10183-2,False,False,True
20486,"Layer Input size Output size Param number
",False,s11042-020-10183-2,False,False,True
20487,"Embedding 1000 1000 ×100 25187700
",False,s11042-020-10183-2,False,False,True
20488,"Conv1D 1000 ×100 998 ×128 38528
",False,s11042-020-10183-2,False,False,True
20489,"Conv1D 1000 ×100 997 ×128 51328
",False,s11042-020-10183-2,False,False,True
20490,"Conv1D 1000 ×100 996 ×128 64128
",False,s11042-020-10183-2,False,False,True
20491,"Maxpool 998 ×128 199 ×128 0
",False,s11042-020-10183-2,False,False,True
20492,"Maxpool 997 ×128 199 ×128 0
",False,s11042-020-10183-2,False,False,True
20493,"Maxpool 996 ×128 199 ×128 0
",False,s11042-020-10183-2,False,False,True
20494,"Concatenate 199 ×128, 199 ×128, 199 ×128 597 ×128 0
",False,s11042-020-10183-2,False,False,True
20495,"Conv1D 597 ×128 593 ×128 82048
",False,s11042-020-10183-2,False,False,True
20496,"Maxpool 593 ×128 118 ×128 0
",False,s11042-020-10183-2,False,False,True
20497,"Conv1D 118 ×128 114 ×128 82048
",False,s11042-020-10183-2,False,False,True
20498,"Maxpool 114 ×128 3 ×128 0
",False,s11042-020-10183-2,False,False,True
20499,"Flatten 3 ×128 384 0
",False,s11042-020-10183-2,False,False,True
20500,"Dense 384 128 49280
",False,s11042-020-10183-2,False,False,True
20501,"Dense 128 2 258
",False,s11042-020-10183-2,False,False,True
20502,"Here y- binary indicator (0 or 1) if class label cis the correct classification for observation
",False,s11042-020-10183-2,False,False,True
20503,"o,p- predicted probability observation ois of class c
",False,s11042-020-10183-2,False,False,True
20504,"We can observe the computational graph and layered architecture of our proposed Fake-
",False,s11042-020-10183-2,False,False,True
20505,"BERT model using Table 5and Fig. 5. In this design, the input is scattered into three parallel
",False,s11042-020-10183-2,False,False,True
20506,"blocks of 1D-CNN having 128 filters and one convolutional layer across each block. First
",False,s11042-020-10183-2,False,False,True
20507,"convolution layer consists of 128 filters and kernel size=3, which reduces input embedding
",False,s11042-020-10183-2,False,False,True
20508,"vector from 1000 to 998, second layer has 128 filters and kernel size=4, which reduces
",False,s11042-020-10183-2,False,False,True
20509,"input vector from 1000 to 997, and third layer has 128 filters and kernel size=5, which
",False,s11042-020-10183-2,False,False,True
20510,"decreases input vector from 1000 to 996. After a particular convolution layer, a max-pooling
",False,s11042-020-10183-2,False,False,True
20511,"layer is also present to decrease the dimension. Subsequently, a max-pooling layer with ker-
",False,s11042-020-10183-2,False,False,True
20512,"nelsize=5 further reduces the vector to 1/5th of 996, i.e. 199. After concatenation of three
",False,s11042-020-10183-2,False,False,True
20513,"above conv-layers, a convolution layer is applied having kernel size=5 including 128 filters.
",False,s11042-020-10183-2,False,False,True
20514,"Table6 Optimal hyperparameters with CNN
",False,s11042-020-10183-2,False,False,True
20515,"Hyperparameter Value
",False,s11042-020-10183-2,False,False,True
20516,"Number of convolution layers 3
",False,s11042-020-10183-2,False,False,True
20517,"Number of max pooling layers 3
",False,s11042-020-10183-2,False,False,True
20518,"Number of dense layers 2
",False,s11042-020-10183-2,False,False,True
20519,"Number of Flatten layers 1
",False,s11042-020-10183-2,False,False,True
20520,"Loss function Categorical-crossentropy
",False,s11042-020-10183-2,False,False,True
20521,"Activation function Relu
",False,s11042-020-10183-2,False,False,True
20522,"Learning rate 0.001
",False,s11042-020-10183-2,False,False,True
20523,"Optimizer Ada-delta
",False,s11042-020-10183-2,False,False,True
20524,"Number of epochs 10
",False,s11042-020-10183-2,False,False,True
20525,"Batch size 128MultimediaToolsandApplications(2021)80: –1178811765 11777Table7 Optimal hyperparameters with LSTM
",False,s11042-020-10183-2,False,False,True
20526,"Hyperparameter Value
",False,s11042-020-10183-2,False,False,True
20527,"Number of convolution layers 2
",False,s11042-020-10183-2,False,False,True
20528,"Number of max pooling layers 2
",False,s11042-020-10183-2,False,False,True
20529,"Number of dense layers 4
",False,s11042-020-10183-2,False,False,True
20530,"Dropout rate .2
",False,s11042-020-10183-2,False,False,True
20531,"Optimizer Adam
",False,s11042-020-10183-2,False,False,True
20532,"Activation function Relu
",False,s11042-020-10183-2,False,False,True
20533,"Loss function Binary-crossentropy
",False,s11042-020-10183-2,False,False,True
20534,"Number of epochs 10
",False,s11042-020-10183-2,False,False,True
20535,"Batch size 64
",False,s11042-020-10183-2,False,False,True
20536,"Table8 Optimal hyperparameters with FakeBERT
",False,s11042-020-10183-2,False,False,True
20537,"Hyperparameter Value
",False,s11042-020-10183-2,False,False,True
20538,"Number of convolution layers 5
",False,s11042-020-10183-2,False,False,True
20539,"Number of max pooling layers 5
",False,s11042-020-10183-2,False,False,True
20540,"Number of dense layers 2
",False,s11042-020-10183-2,False,False,True
20541,"Number of Flatten layers 1
",False,s11042-020-10183-2,False,False,True
20542,"Dropout rate .2
",False,s11042-020-10183-2,False,False,True
20543,"Optimizer Adadelta
",False,s11042-020-10183-2,False,False,True
20544,"Activation function Relu
",False,s11042-020-10183-2,False,False,True
20545,"Loss function Categorical-crossentropy
",False,s11042-020-10183-2,False,False,True
20546,"Number of epochs 10
",False,s11042-020-10183-2,False,False,True
20547,"Batch size 128
",False,s11042-020-10183-2,False,False,True
20548,"Table9 Attributes in the fake news dataset
",False,s11042-020-10183-2,False,False,True
20549,"Attribute Number of Instances
",False,s11042-020-10183-2,False,False,True
20550,"ID (unique value to the news article) 20800
",False,s11042-020-10183-2,False,False,True
20551,"title (main heading related to particular news) 20242
",False,s11042-020-10183-2,False,False,True
20552,"author (name of the creator of that news) 18843
",False,s11042-020-10183-2,False,False,True
20553,"text (complete news article) 20761
",False,s11042-020-10183-2,False,False,True
20554,"label (information about that the article as fake or real) 20800MultimediaToolsandApplications(2021)80: –1178811765 11778Table10 Fake news dataset with the class labels
",False,s11042-020-10183-2,False,False,True
20555,"Class label Number of Instances
",False,s11042-020-10183-2,False,False,True
20556,"True 10540
",False,s11042-020-10183-2,False,False,True
20557,"False 10260
",False,s11042-020-10183-2,False,False,True
20558,"Subsequently, there are two hidden layers having 384 and 128 nodes respectively. The num-
",False,s11042-020-10183-2,False,False,True
20559,"ber of trainable parameters across each layer is also presented (for more details refer column
",False,s11042-020-10183-2,False,False,True
20560,"Param number) in Table 5. This model is not both computationally complex for training at
",False,s11042-020-10183-2,False,False,True
20561,"any real-world fake news dataset. The work was carried using the NVIDIA DGX-1 V100
",False,s11042-020-10183-2,False,False,True
20562,"machine. The machine is equipped with 40600 CUDA cores, 5120 tensor cores, 128 GB
",False,s11042-020-10183-2,False,False,True
20563,"RAM and 1000 TFLOPS speed.
",False,s11042-020-10183-2,False,False,True
20564,"4 Experiments
",False,s11042-020-10183-2,False,False,True
20565,"Experiments have been conducted using deep learning models (CNN and LSTM) and our
",False,s11042-020-10183-2,False,False,True
20566,"proposed model (FakeBERT) using pre-trained word embedding techniques (BERT and
",False,s11042-020-10183-2,False,False,True
20567,"GloVe). Performances are recorded of different classification models and analyzed with the
",False,s11042-020-10183-2,False,False,True
20568,"benchmark results.
",False,s11042-020-10183-2,False,False,True
20569,"4.1 Datasetdescription
",False,s11042-020-10183-2,False,False,True
20570,"In this paper, we have done extensive experiments using the real-world fake news dataset.1
",False,s11042-020-10183-2,False,False,True
20571,"It (refer Table 9) consists of two files (i) train.csv, and (ii) test.csv: A testing dataset without
",False,s11042-020-10183-2,False,False,True
20572,"the label. It is a collection of the fake and real news of propagated during the time of the
",False,s11042-020-10183-2,False,False,True
20573,"U.S. General Presidential Election-2016. In Table 10, we can see the instances with the class
",False,s11042-020-10183-2,False,False,True
20574,"labels in the respective fake news dataset.
",False,s11042-020-10183-2,False,False,True
20575,"4.1.1 Hyperparametersetting
",False,s11042-020-10183-2,False,False,True
20576,"4.1.1 Hyperparametersetting
",False,s11042-020-10183-2,False,False,True
20577,"Abstract
",True,S16-1085,False,False,True
20578,"1 Introduction
",True,S16-1085,False,False,True
20579,"2 Task Description
",True,S16-1085,False,False,True
20580,"1. To learn which words challenge non-native En-
",True,S16-1085,False,False,True
20581,"2. To investigate how well one’s individual vo-
",True,S16-1085,False,False,True
20582,"3. To introduce a new corpus to be used in Text
",True,S16-1085,False,False,True
20583,"3 User Study
",True,S16-1085,False,False,True
20584,"3.1 Data Sources
",True,S16-1085,False,False,True
20585,"Proceedings of SemEval-2016 , pages 560–569,
",False,S16-1085,False,False,True
20586,"San Diego, California, June 16-17, 2016. c
",False,S16-1085,False,False,True
20587,"2016 Association for Computational Linguistics
",False,S16-1085,False,False,True
20588,"SemEval 2016 Task 11: Complex Word Identiﬁcation
",False,S16-1085,False,False,True
20589,"Gustavo Henrique Paetzold and Lucia Specia
",False,S16-1085,False,False,True
20590,"Department of Computer Science
",False,S16-1085,False,False,True
20591,"University of Shefﬁeld, UK
",False,S16-1085,False,False,True
20592,"{ghpaetzold1,l.specia }@sheffield.ac.uk
",False,S16-1085,False,False,True
20593,"We report the ﬁndings of the Complex Word
",False,S16-1085,False,False,True
20594,"Identiﬁcation task of SemEval 2016. To cre-
",False,S16-1085,False,False,True
20595,"ate a dataset, we conduct a user study with
",False,S16-1085,False,False,True
20596,"400 non-native English speakers, and ﬁnd that
",False,S16-1085,False,False,True
20597,"complex words tend to be rarer, less ambigu-
",False,S16-1085,False,False,True
20598,"ous and shorter. A total of 42 systems were
",False,S16-1085,False,False,True
20599,"submitted from 21 distinct teams, and nine
",False,S16-1085,False,False,True
20600,"baselines were provided. The results high-
",False,S16-1085,False,False,True
20601,"light the effectiveness of Decision Trees and
",False,S16-1085,False,False,True
20602,"Ensemble methods for the task, but ultimately
",False,S16-1085,False,False,True
20603,"reveal that word frequencies remain the most
",False,S16-1085,False,False,True
20604,"reliable predictor of word complexity.
",False,S16-1085,False,False,True
20605,"1 Introduction
",False,S16-1085,False,False,True
20606,"Complex Word Identiﬁcation (CWI) is the task of
",False,S16-1085,False,False,True
20607,"deciding which words should be simpliﬁed in a
",False,S16-1085,False,False,True
20608,"given text. It is commonly connected with the task
",False,S16-1085,False,False,True
20609,"of Lexical Simpliﬁcation (LS), which has as goal to
",False,S16-1085,False,False,True
20610,"replace complex words and expressions with sim-
",False,S16-1085,False,False,True
20611,"pler alternatives. In the usual LS pipeline, which
",False,S16-1085,False,False,True
20612,"was ﬁrst introduced by (Shardlow, 2014), CWI is
",False,S16-1085,False,False,True
20613,"the ﬁrst step. An effective CWI strategy can pre-
",False,S16-1085,False,False,True
20614,"vent LS approaches from replacing simple words,
",False,S16-1085,False,False,True
20615,"and hence prevent them from making grammatical
",False,S16-1085,False,False,True
20616,"and/or semantic errors. Early LS approaches (De-
",False,S16-1085,False,False,True
20617,"vlin and Tait, 1998; Carroll et al., 1999) do not in-
",False,S16-1085,False,False,True
20618,"clude CWI. As shown in (Paetzold and Specia, 2013;
",False,S16-1085,False,False,True
20619,"Shardlow, 2014), ignoring this step can considerably
",False,S16-1085,False,False,True
20620,"decrease the quality of the output produced by a sim-
",False,S16-1085,False,False,True
20621,"pliﬁer.
",False,S16-1085,False,False,True
20622,"CWI has been gaining popularity in recent re-
",False,S16-1085,False,False,True
20623,"search. The LS approach in (Horn et al., 2014)
",False,S16-1085,False,False,True
20624,"employs an implicit CWI strategy in which a targetword is only deemed complex if the LS model can
",False,S16-1085,False,False,True
20625,"ﬁnd a candidate substitution which is simpler. Their
",False,S16-1085,False,False,True
20626,"results, however, show that the approach is unable
",False,S16-1085,False,False,True
20627,"to ﬁnd simpliﬁcations for one third of the complex
",False,S16-1085,False,False,True
20628,"words in the dataset. (Shardlow, 2013b) presents
",False,S16-1085,False,False,True
20629,"the CW corpus: the ﬁrst dataset for CWI. Although
",False,S16-1085,False,False,True
20630,"a relevant contribution, this dataset contains only
",False,S16-1085,False,False,True
20631,"731 instances extracted automatically from the Sim-
",False,S16-1085,False,False,True
20632,"ple English Wikipedia edits, which raises concerns
",False,S16-1085,False,False,True
20633,"about its reliability and applicability.
",False,S16-1085,False,False,True
20634,"The results obtained by Shardlow (2013a) high-
",False,S16-1085,False,False,True
20635,"light some of the issues of the dataset. They use the
",False,S16-1085,False,False,True
20636,"CW corpus to compare the performance of three so-
",False,S16-1085,False,False,True
20637,"lutions to CWI: a Threshold-Based approach, a Sup-
",False,S16-1085,False,False,True
20638,"port Vector Machine (SVM), and a “Simplify Ev-
",False,S16-1085,False,False,True
20639,"erything” approach. In their experiments, the “Sim-
",False,S16-1085,False,False,True
20640,"plify Everything” approach achieves higher Accu-
",False,S16-1085,False,False,True
20641,"racy, Recall and F-scores than all other systems, sug-
",False,S16-1085,False,False,True
20642,"gesting that simplifying all words in a sentence is
",False,S16-1085,False,False,True
20643,"the most effective approach for CWI. These results
",False,S16-1085,False,False,True
20644,"are clearly counter intuitive and conﬂicting with the
",False,S16-1085,False,False,True
20645,"conclusions drawn in (Paetzold and Specia, 2013;
",False,S16-1085,False,False,True
20646,"Paetzold, 2013; Shardlow, 2014).
",False,S16-1085,False,False,True
20647,"In this paper we describe the ﬁrst edition of the
",False,S16-1085,False,False,True
20648,"Complex Word Identiﬁcation task, organized at Se-
",False,S16-1085,False,False,True
20649,"mEval 2016. This is an initiative that aims to pro-
",False,S16-1085,False,False,True
20650,"vide reliable resources and new insights for CWI, as
",False,S16-1085,False,False,True
20651,"well as to establish the state of the art performance
",False,S16-1085,False,False,True
20652,"in CWI for English texts, and bring more visibility
",False,S16-1085,False,False,True
20653,"to the area of Text Simpliﬁcation.
",False,S16-1085,False,False,True
20654,"2 Task Description
",False,S16-1085,False,False,True
20655,"The Complex Word Identiﬁcation task of SemEval
",False,S16-1085,False,False,True
20656,"2016 invited participants to create systems that,560given a sentence and a target word within it, can
",False,S16-1085,False,False,True
20657,"predict whether or not a non-native English speaker
",False,S16-1085,False,False,True
20658,"would be able to understand the meaning of the tar-
",False,S16-1085,False,False,True
20659,"get word. We chose non-native speakers as a target
",False,S16-1085,False,False,True
20660,"audience because, unlike second language learners
",False,S16-1085,False,False,True
20661,"and those with low literacy levels or conditions such
",False,S16-1085,False,False,True
20662,"as Aphasia and Dyslexia, non-native speakers of En-
",False,S16-1085,False,False,True
20663,"glish have not yet been explicitly assessed with re-
",False,S16-1085,False,False,True
20664,"spect to their simpliﬁcation needs. In addition, the
",False,S16-1085,False,False,True
20665,"broad availability of such an audience makes data
",False,S16-1085,False,False,True
20666,"collection more feasible.
",False,S16-1085,False,False,True
20667,"We have established main goals for the task:
",False,S16-1085,False,False,True
20668,"1. To learn which words challenge non-native En-
",False,S16-1085,False,False,True
20669,"glish speakers and to understand what their
",False,S16-1085,False,False,True
20670,"traits are.
",False,S16-1085,False,False,True
20671,"2. To investigate how well one’s individual vo-
",False,S16-1085,False,False,True
20672,"cabulary limitations can be predicted from the
",False,S16-1085,False,False,True
20673,"overall vocabulary limitations of others in the
",False,S16-1085,False,False,True
20674,"same category.
",False,S16-1085,False,False,True
20675,"3. To introduce a new corpus to be used in Text
",False,S16-1085,False,False,True
20676,"Simpliﬁcation and other tasks related to Topic
",False,S16-1085,False,False,True
20677,"Modelling and Semantics.
",False,S16-1085,False,False,True
20678,"4. To evaluate the reliability of various resources
",False,S16-1085,False,False,True
20679,"commonly used in the creation of Lexical Sim-
",False,S16-1085,False,False,True
20680,"pliﬁcation approaches.
",False,S16-1085,False,False,True
20681,"5. To establish the state of the art performance in
",False,S16-1085,False,False,True
20682,"CWI for English texts.
",False,S16-1085,False,False,True
20683,"6. To investigate and establish evaluation metrics
",False,S16-1085,False,False,True
20684,"for the task of CWI.
",False,S16-1085,False,False,True
20685,"In order to achieve these objectives for the shared
",False,S16-1085,False,False,True
20686,"task, we started by creating a manually annotated
",False,S16-1085,False,False,True
20687,"dataset through a user study.
",False,S16-1085,False,False,True
20688,"3 User Study
",False,S16-1085,False,False,True
20689,"In the study, volunteers were asked to judge whether
",False,S16-1085,False,False,True
20690,"or not they could understand the meaning of each
",False,S16-1085,False,False,True
20691,"word in a given sentence. In the following we pro-
",False,S16-1085,False,False,True
20692,"vide more details on the sentences used and the an-
",False,S16-1085,False,False,True
20693,"notation process.
",False,S16-1085,False,False,True
20694,"3.1 Data Sources
",False,S16-1085,False,False,True
20695,"We selected 9,200 sentences to be annotated, after
",False,S16-1085,False,False,True
20696,"ﬁltering out cases with spurious characters, HTMLor CSS markup, or outside the 20-40 word-length
",False,S16-1085,False,False,True
20697,"range. These sentences were taken from three
",False,S16-1085,False,False,True
20698,"sources:
",False,S16-1085,False,False,True
20699,"CW Corpus (Shardlow, 2013b): composed of
",False,S16-1085,False,False,True
20700,"731 sentences from the Simple English Wikipedia
",False,S16-1085,False,False,True
20701,"731 sentences from the Simple English Wikipedia
",False,S16-1085,False,False,True
20702,"Abstract. Changes in relational database schemas are part of the continuous improvement process of information
",True,sbbd_shp_07,False,False,True
20703,"1. INTRODUCTION
",True,sbbd_shp_07,False,False,True
20704,"2.2 MERGING A SET OF DELTA VERSIONS
",True,sbbd_shp_07,False,False,True
20705,"3<version id=""3"">
",True,sbbd_shp_07,False,False,True
20706,"1:function schemaMerge
",True,sbbd_shp_07,False,False,True
20707,"3:M empty set {Set of merged changes}
",True,sbbd_shp_07,False,False,True
20708,"4:for each delta di2Ddo
",True,sbbd_shp_07,False,False,True
20709,"5: for each change c2dido
",True,sbbd_shp_07,False,False,True
20710,"6: addcinMap[c:object name]:values
",True,sbbd_shp_07,False,False,True
20711,"7: end for
",True,sbbd_shp_07,False,False,True
20712,"8:end for
",True,sbbd_shp_07,False,False,True
20713,"9:for each object name ni2Map:keys do
",True,sbbd_shp_07,False,False,True
20714,"10: current change null
",True,sbbd_shp_07,False,False,True
20715,"A Redundancy-Free Approach to Schema Evolution
",False,sbbd_shp_07,False,False,True
20716,"Claudiomar Desanti1, Marcos Bernardelli1, Márcio Fuckner1, Raquel Kolitski Stasiu12
",False,sbbd_shp_07,False,False,True
20717,"1Pontifícia Universidade Católica do Paraná, Brazil
",False,sbbd_shp_07,False,False,True
20718,"2Universidade Tecnológica Federal do Paraná, Brazil
",False,sbbd_shp_07,False,False,True
20719,"{c.desanti, m.bernardelli, marcio.fuckner, raquel.stasiu}@pucpr.br, raquel@utfpr.edu.br
",False,sbbd_shp_07,False,False,True
20720,"systems. In general, iterative and incremental approaches for software development produce frequent releases in order
",False,sbbd_shp_07,False,False,True
20721,"to improve the operational environment and also attend to business or legal requirements. As an eﬀect, such dynamic
",False,sbbd_shp_07,False,False,True
20722,"produces signiﬁcant amounts of work to database administrators when applying those changes in production. The risk
",False,sbbd_shp_07,False,False,True
20723,"also increases as cumulative changes need to be manually executed in the environment. Having those issues in mind
",False,sbbd_shp_07,False,False,True
20724,"we propose an approach to automate the schema evolution, using a redundancy-free algorithm to merge cumulative
",False,sbbd_shp_07,False,False,True
20725,"changes, reducing downtimes and improving the software availability. When executed in a production environment,
",False,sbbd_shp_07,False,False,True
20726,"the proof-of-concept demonstrated a signiﬁcant reduction of the amount of time to process the update, resulting in the
",False,sbbd_shp_07,False,False,True
20727,"same schema as applied by known tools.
",False,sbbd_shp_07,False,False,True
20728,"Categories and Subject Descriptors: H.2.1 [ Database Management ]: Logical Design—Schema and subschema
",False,sbbd_shp_07,False,False,True
20729,"Keywords: Diﬀ Algorithms, Evolving Database Systems, Schema Evolution
",False,sbbd_shp_07,False,False,True
20730,"1. INTRODUCTION
",False,sbbd_shp_07,False,False,True
20731,"Scopechangesarepartofthecontinuousimprovementprocessofinformationsystems. Severalsoftware
",False,sbbd_shp_07,False,False,True
20732,"development teams use iterative and incremental development approaches in order to accelerate the
",False,sbbd_shp_07,False,False,True
20733,"requirements gathering process, in contrast to traditional waterfall approaches. The agile approach
",False,sbbd_shp_07,False,False,True
20734,"leads to frequent changes of software artifacts and requires an eﬀective change management process.
",False,sbbd_shp_07,False,False,True
20735,"As a consequence, those changes aﬀect database structures and their current data [Batini et al.
",False,sbbd_shp_07,False,False,True
20736,"2009] [Roddick 2009]. Also, several companies acquire oﬀ-the-shelf solutions, which aim at attending
",False,sbbd_shp_07,False,False,True
20737,"diﬀerent conﬁgurations and customer needs. It leads companies to update their environment with
",False,sbbd_shp_07,False,False,True
20738,"small, but frequent releases generated from their suppliers. Risks and downtime rates increase when
",False,sbbd_shp_07,False,False,True
20739,"cumulative and interdependent scripts are applied. As a result, customers decrease the frequency of
",False,sbbd_shp_07,False,False,True
20740,"updates, adopting cumulative “big bang” update strategies, which demands complex, interdependent
",False,sbbd_shp_07,False,False,True
20741,"and error-prone tasks.
",False,sbbd_shp_07,False,False,True
20742,"According to [Fowler 2002], refactoring is a quality improvement process to rewrite source-code
",False,sbbd_shp_07,False,False,True
20743,"without aﬀecting the resulting product. Changes could improve readability, which has impact on
",False,sbbd_shp_07,False,False,True
20744,"futuremaintenance. Moreover, architecturalchangescouldallowsoftwaremodularizationandimprove
",False,sbbd_shp_07,False,False,True
20745,"testability. [Ambler and Sadalage 2006] have adapted Fowler’s concept to ﬁt within the database
",False,sbbd_shp_07,False,False,True
20746,"reality. Such changes aim at attending an indirect business requirement or improve the integration
",False,sbbd_shp_07,False,False,True
20747,"between the software and the database. These improvements could generate new database objects
",False,sbbd_shp_07,False,False,True
20748,"such as indexes, columns, relationships and constraints.
",False,sbbd_shp_07,False,False,True
20749,"Schema evolution is the ability for a database schema to evolve without the loss of existing infor-
",False,sbbd_shp_07,False,False,True
20750,"mation. Due to its great signiﬁcance and practical importance, many research eﬀorts have been made
",False,sbbd_shp_07,False,False,True
20751,"to propose approaches for schema evolution. The work of [Papastefanatos et al. 2008] proposes a
",False,sbbd_shp_07,False,False,True
20752,"Simpósio Brasileiro de Banco de Dados - SBBD 2013
",False,sbbd_shp_07,False,False,True
20753,"Short Papers
",False,sbbd_shp_07,False,False,True
20754,"1graph-based framework for capturing changes in database schemas and generating annotation based
",False,sbbd_shp_07,False,False,True
20755,"on a proposed extension to the SQL language, speciﬁcally tailored for the management of evolution.
",False,sbbd_shp_07,False,False,True
20756,"Using a similar approach, the work of [Curino et al. 2009] presents a web-based framework, which
",False,sbbd_shp_07,False,False,True
20757,"uses several semantic techniques for query rewriting. Techniques for mapping the diﬀerences between
",False,sbbd_shp_07,False,False,True
20758,"models can also be used as a key element for the schema evolution problem, even if they address
",False,sbbd_shp_07,False,False,True
20759,"diﬀerent contexts such as ontologies and object-oriented models. For example, [Fagin et al. 2011]
",False,sbbd_shp_07,False,False,True
20760,"presents two fundamental operators on schema mappings, namely composition and inversion to ad-
",False,sbbd_shp_07,False,False,True
20761,"dress the mapping adaptation problem in the context of schema evolution. [Carvalho et al. 2013]
",False,sbbd_shp_07,False,False,True
20762,"proposes an evolutionary approach to ﬁnd complex matches between schemas of semantically related
",False,sbbd_shp_07,False,False,True
20763,"data repositories using only the data instances. The work of [dos Santos Mello et al. 2002] describes
",False,sbbd_shp_07,False,False,True
20764,"a uniﬁcation method for heterogeneous XML schemata.
",False,sbbd_shp_07,False,False,True
20765,"Diﬀerently from other works, this approach proposes a schema evolution mechanism that minimizes
",False,sbbd_shp_07,False,False,True
20766,"possible redundancies that exists in a set of schema updates. A schema update in this particular
",False,sbbd_shp_07,False,False,True
20767,"scenario could be interpreted as a group of SQL sentences, arranged in a labeled version. The proposed
",False,sbbd_shp_07,False,False,True
20768,"mechanism allows the user to apply the set of updates in only one step. For the sake of clarity, the
",False,sbbd_shp_07,False,False,True
20769,"solution could be decomposed in two main processes: (i) Generation of a delta version with the
",False,sbbd_shp_07,False,False,True
20770,"diﬀerence between two schemas and (ii) merging a set of delta versions. In (i) the inputs come from
",False,sbbd_shp_07,False,False,True
20771,"the development team, whereas in (ii), the tasks are conducted by database administrators who want
",False,sbbd_shp_07,False,False,True
20772,"to refresh their environments with the last stable version of the software. The results show that our
",False,sbbd_shp_07,False,False,True
20773,"approach reduced the amount of time to process the update, resulting in the same target schema as
",False,sbbd_shp_07,False,False,True
20774,"applied by known tools.
",False,sbbd_shp_07,False,False,True
20775,"2. A REDUNDANCY-FREE APPROACH TO SCHEMA EVOLUTION
",False,sbbd_shp_07,False,False,True
20776,"A method was developed to address the problem of ﬁnding diﬀerences between two schemas using
",False,sbbd_shp_07,False,False,True
20777,"a diﬀ algorithm. This type of algorithm aims at ﬁnding the shortest distance between two scripts,
",False,sbbd_shp_07,False,False,True
20778,"which can be represented by tiandti 1, where trepresents the schema and iis its version number.
",False,sbbd_shp_07,False,False,True
20779,"The result is a delta version, represented by d, which has directives to allow the construction of one
",False,sbbd_shp_07,False,False,True
20780,"of the tversions using the previous or a subsequent version [Cobena et al. 2001]. In this particular
",False,sbbd_shp_07,False,False,True
20781,"case, both scripts and deltas are sets of SQL commands. The generated prototype was developed
",False,sbbd_shp_07,False,False,True
20782,"to recognize PostgreSQL objects [PostgreSQL Global Development Group 2013]. However, a clear
",False,sbbd_shp_07,False,False,True
20783,"separation between core and interface layers demands few changes to adapt the prototype to work
",False,sbbd_shp_07,False,False,True
20784,"with other relational databases. The next section describes the process to obtain the delta version.
",False,sbbd_shp_07,False,False,True
20785,"2.1 FINDING THE DELTA VERSION BETWEEN TWO SCHEMAS
",False,sbbd_shp_07,False,False,True
20786,"A database schema is a hierarchical structure of objects. Hence, a tree could be used as its represen-
",False,sbbd_shp_07,False,False,True
20787,"tation. Figure 1 presents an illustrative and non-exhaustive tree that ﬁts to the natural taxonomy of
",False,sbbd_shp_07,False,False,True
20788,"a schema. To generate the delta d, we transform schemas tiandti 1into trees and use them as an
",False,sbbd_shp_07,False,False,True
20789,"input to the diﬀ algorithm. To discover the diﬀerences, the algorithm needs to identify which node
",False,sbbd_shp_07,False,False,True
20790,"was changed (e.g., attribute, table or index). To provide such functionalities the algorithm traverses
",False,sbbd_shp_07,False,False,True
20791,"both trees from the upper nodes to the leaf, matching each one with their corresponding node in the
",False,sbbd_shp_07,False,False,True
20792,"opposite tree.
",False,sbbd_shp_07,False,False,True
20793,"In order to avoid visiting all nodes during the comparison, the MD5 (Message-Digest Algorithm 5)
",False,sbbd_shp_07,False,False,True
20794,"1hash function is executed for each node, using the associated SQL sentence as an input parameter.
",False,sbbd_shp_07,False,False,True
20795,"Figure2showstwotreeswiththeircalculatedhashvalues. Whentraversingthetree (a), thealgorithm
",False,sbbd_shp_07,False,False,True
20796,"ignores the node 1because both hashes from trees (a)and (b)are the same. By the other hand, the
",False,sbbd_shp_07,False,False,True
20797,"same example shows node2with diﬀerent hash values, which justiﬁes further veriﬁcation.
",False,sbbd_shp_07,False,False,True
20798,"1A complete speciﬁcation of the MD5 algorithm can be found at http://tools.ietf.org/html/rfc1321
",False,sbbd_shp_07,False,False,True
20799,"Simpósio Brasileiro de Banco de Dados - SBBD 2013 
",False,sbbd_shp_07,False,False,True
20800,"Short Papers
",False,sbbd_shp_07,False,False,True
20801,"2schema
",False,sbbd_shp_07,False,False,True
20802,"views
",False,sbbd_shp_07,False,False,True
20803,"...tables
",False,sbbd_shp_07,False,False,True
20804,"table 1
",False,sbbd_shp_07,False,False,True
20805,"name
",False,sbbd_shp_07,False,False,True
20806,"ownerattributes
",False,sbbd_shp_07,False,False,True
20807,"attrib 1
",False,sbbd_shp_07,False,False,True
20808,"name
",False,sbbd_shp_07,False,False,True
20809,"type
",False,sbbd_shp_07,False,False,True
20810,"precision
",False,sbbd_shp_07,False,False,True
20811,"null
",False,sbbd_shp_07,False,False,True
20812,"defaultattrib n
",False,sbbd_shp_07,False,False,True
20813,"name
",False,sbbd_shp_07,False,False,True
20814,"type
",False,sbbd_shp_07,False,False,True
20815,"precision
",False,sbbd_shp_07,False,False,True
20816,"null
",False,sbbd_shp_07,False,False,True
20817,"defaultindexes
",False,sbbd_shp_07,False,False,True
20818,"...table n
",False,sbbd_shp_07,False,False,True
20819,"name
",False,sbbd_shp_07,False,False,True
20820,"ownerattributes
",False,sbbd_shp_07,False,False,True
20821,"...indexes
",False,sbbd_shp_07,False,False,True
20822,"Fig. 1. A tree representing the schema
",False,sbbd_shp_07,False,False,True
20823,"(a) t i 1hash=100
",False,sbbd_shp_07,False,False,True
20824,"node1 hash=30
",False,sbbd_shp_07,False,False,True
20825,"...node2 hash=70
",False,sbbd_shp_07,False,False,True
20826,"node3 hash=42 node4 hash=37(b) t i hash=188
",False,sbbd_shp_07,False,False,True
20827,"node1 hash=30
",False,sbbd_shp_07,False,False,True
20828,"...node2 hash=150
",False,sbbd_shp_07,False,False,True
20829,"node3 hash=40 node4 hash=25
",False,sbbd_shp_07,False,False,True
20830,"Fig. 2. A tree representing the schema
",False,sbbd_shp_07,False,False,True
20831,"The algorithm evaluates each node from the versions tiandti 1using three possible patterns:
",False,sbbd_shp_07,False,False,True
20832,"dropoperations indicating that one or more objects in tiwere removed from ti 1,createoperations
",False,sbbd_shp_07,False,False,True
20833,"indicating that one or more new objects in tiwere identiﬁed, and modifyoperations selecting objects
",False,sbbd_shp_07,False,False,True
20834,"belonging to both trees, with diﬀerent values on at least one common attribute in the node.
",False,sbbd_shp_07,False,False,True
20835,"At the end of the comparison process, a delta version is generated in XML format (see Figure 3).
",False,sbbd_shp_07,False,False,True
20836,"A root node called versioncontains the attribute that indicates the version number. The subsequent
",False,sbbd_shp_07,False,False,True
20837,"nodes represent structural changes by object type. The implemented prototype recognizes structural
",False,sbbd_shp_07,False,False,True
20838,"changes in tables, views, sequences and indexes. Each one has speciﬁc metadata, providing suﬃcient
",False,sbbd_shp_07,False,False,True
20839,"information to change the schema. There is a common attribute called op, which identiﬁes the type of
",False,sbbd_shp_07,False,False,True
20840,"diﬀerence ( drop,createormodify). One special node called scriptscan be used to allow administrators
",False,sbbd_shp_07,False,False,True
20841,"to insert user-deﬁned scripts, commonly used for conversion process purposes.
",False,sbbd_shp_07,False,False,True
20842,"2.2 MERGING A SET OF DELTA VERSIONS
",False,sbbd_shp_07,False,False,True
20843,"This section presents an approach to evaluate a set of delta versions, identify and remove their redun-
",False,sbbd_shp_07,False,False,True
20844,"dancies, and ﬁnally apply those changes in the target schema. The approach is organized as follows:
",False,sbbd_shp_07,False,False,True
20845,"(i) recovery of delta versions from the repository, (ii) merging the delta versions, removing redundan-
",False,sbbd_shp_07,False,False,True
20846,"cies, and applying the changes in the target schema. In (i) the current schema and the set of deltas
",False,sbbd_shp_07,False,False,True
20847,"are selected from one repository. Schemas should be located in a conﬁguration control infrastructure
",False,sbbd_shp_07,False,False,True
20848,"with support to artifact versioning and tracking, which is out of the scope of this work.
",False,sbbd_shp_07,False,False,True
20849,"Simpósio Brasileiro de Banco de Dados - SBBD 2013 
",False,sbbd_shp_07,False,False,True
20850,"Short Papers
",False,sbbd_shp_07,False,False,True
20851,"3<version id=""3"">
",False,sbbd_shp_07,False,False,True
20852,"<tables>
",False,sbbd_shp_07,False,False,True
20853,"<table id=""invoice"" operation=""modify"">
",False,sbbd_shp_07,False,False,True
20854,"<attributes>
",False,sbbd_shp_07,False,False,True
20855,"<attr id=""number"" op=""create""><name>number</name><type>integer</type> ...
",False,sbbd_shp_07,False,False,True
20856,"<attr id=""comments"" op=""modify""><name>remarks</name> <type>varchar</type> ...
",False,sbbd_shp_07,False,False,True
20857,"</atributes>
",False,sbbd_shp_07,False,False,True
20858,"</table>
",False,sbbd_shp_07,False,False,True
20859,"<tables>
",False,sbbd_shp_07,False,False,True
20860,"<scripts>...</scripts>
",False,sbbd_shp_07,False,False,True
20861,"</version>
",False,sbbd_shp_07,False,False,True
20862,"Fig. 3. Generated delta example
",False,sbbd_shp_07,False,False,True
20863,"In (ii), the algorithm receives a list of delta scripts, identify and merge those changes in one
",False,sbbd_shp_07,False,False,True
20864,"redundancy-free script. Three types of changes could occur in one database object: dropopera-
",False,sbbd_shp_07,False,False,True
20865,"tion,createoperation, and modifyoperation. For illustration purposes, the following scenario with a
",False,sbbd_shp_07,False,False,True
20866,"delta script in the version v1can be considered. This delta script from version 1 creates a new ﬁeld
",False,sbbd_shp_07,False,False,True
20867,"in the table invoicecalledtrackingNumber . The ﬁeld trackingNumber is an integer ﬁeld that accepts
",False,sbbd_shp_07,False,False,True
20868,"null values and has a precision of 8 digits. In version v2the ﬁeld is changed in order to forbid null
",False,sbbd_shp_07,False,False,True
20869,"values. Finally, in the version v3, the precision is changed to support 10 digits instead of 8.
",False,sbbd_shp_07,False,False,True
20870,"The Algorithm 1 was created in order to mimic the decisions typically made by one database ad-
",False,sbbd_shp_07,False,False,True
20871,"ministrator when merging two scripts. The function called schemaMerge requires an input parameter
",False,sbbd_shp_07,False,False,True
20872,"called D, which has the set of deltas to merge. Lines from 12 to 20 are responsible for evaluate each
",False,sbbd_shp_07,False,False,True
20873,"sentence from the delta, using a rule-based algorithm detailed in Algorithm 2 called mergeSentences .
",False,sbbd_shp_07,False,False,True
20874,"The rules guide the merging process, deciding what is the most economic operation. Redundancies
",False,sbbd_shp_07,False,False,True
20875,"are removed as a consequence of a series of override operations in the underlying object. For example,
",False,sbbd_shp_07,False,False,True
20876,"taking two SQL sentences s1=“create table foo (ﬁeld1 integer)” ands2=“alter table foo add ﬁeld2
",False,sbbd_shp_07,False,False,True
20877,"char(1)”. The ﬁrst sentence ( s1) is acreate operation , while the second ( s2) is amodify operation .
",False,sbbd_shp_07,False,False,True
20878,"According to the rule 3 described in the algorithm 2, the merging process will generate only one
",False,sbbd_shp_07,False,False,True
20879,"merged sentence Sr=“create table foo (ﬁeld1 integer, ﬁeld2 char(1))” .
",False,sbbd_shp_07,False,False,True
20880,"Algorithm 1 Schema merging algorithm
",False,sbbd_shp_07,False,False,True
20881,"Require: D=d1; d2; :::; d n: delta bundle
",False,sbbd_shp_07,False,False,True
20882,"1:function schemaMerge
",False,sbbd_shp_07,False,False,True
20883,"2:Map empty map {Map of schema objects with their changes}
",False,sbbd_shp_07,False,False,True
20884,"3:M empty set {Set of merged changes}
",False,sbbd_shp_07,False,False,True
20885,"4:for each delta di2Ddo
",False,sbbd_shp_07,False,False,True
20886,"5: for each change c2dido
",False,sbbd_shp_07,False,False,True
20887,"6: addcinMap[c:object name]:values
",False,sbbd_shp_07,False,False,True
20888,"7: end for
",False,sbbd_shp_07,False,False,True
20889,"8:end for
",False,sbbd_shp_07,False,False,True
20890,"9:for each object name ni2Map:keys do
",False,sbbd_shp_07,False,False,True
20891,"10: current change null
",False,sbbd_shp_07,False,False,True
20892,"11: for each change c2Map[ ni]:values do
",False,sbbd_shp_07,False,False,True
20893,"11: for each change c2Map[ ni]:values do
",False,sbbd_shp_07,False,False,True
20894,"Abstract
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20895,"1 Introduction
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20896,"2 Task Description
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20897,"1. To learn which words challenge non-native En-
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20898,"2. To investigate how well one’s individual vo-
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20899,"3. To introduce a new corpus to be used in Text
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20900,"3 User Study
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20901,"3.1 Data Sources
",True,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20902,"Proceedings of SemEval-2016 , pages 560–569,
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20903,"San Diego, California, June 16-17, 2016. c
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20904,"2016 Association for Computational Linguistics
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20905,"SemEval 2016 Task 11: Complex Word Identiﬁcation
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20906,"Gustavo Henrique Paetzold and Lucia Specia
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20907,"Department of Computer Science
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20908,"University of Shefﬁeld, UK
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20909,"{ghpaetzold1,l.specia }@sheffield.ac.uk
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20910,"We report the ﬁndings of the Complex Word
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20911,"Identiﬁcation task of SemEval 2016. To cre-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20912,"ate a dataset, we conduct a user study with
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20913,"400 non-native English speakers, and ﬁnd that
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20914,"complex words tend to be rarer, less ambigu-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20915,"ous and shorter. A total of 42 systems were
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20916,"submitted from 21 distinct teams, and nine
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20917,"baselines were provided. The results high-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20918,"light the effectiveness of Decision Trees and
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20919,"Ensemble methods for the task, but ultimately
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20920,"reveal that word frequencies remain the most
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20921,"reliable predictor of word complexity.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20922,"1 Introduction
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20923,"Complex Word Identiﬁcation (CWI) is the task of
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20924,"deciding which words should be simpliﬁed in a
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20925,"given text. It is commonly connected with the task
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20926,"of Lexical Simpliﬁcation (LS), which has as goal to
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20927,"replace complex words and expressions with sim-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20928,"pler alternatives. In the usual LS pipeline, which
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20929,"was ﬁrst introduced by (Shardlow, 2014), CWI is
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20930,"the ﬁrst step. An effective CWI strategy can pre-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20931,"vent LS approaches from replacing simple words,
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20932,"and hence prevent them from making grammatical
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20933,"and/or semantic errors. Early LS approaches (De-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20934,"vlin and Tait, 1998; Carroll et al., 1999) do not in-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20935,"clude CWI. As shown in (Paetzold and Specia, 2013;
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20936,"Shardlow, 2014), ignoring this step can considerably
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20937,"decrease the quality of the output produced by a sim-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20938,"pliﬁer.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20939,"CWI has been gaining popularity in recent re-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20940,"search. The LS approach in (Horn et al., 2014)
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20941,"employs an implicit CWI strategy in which a targetword is only deemed complex if the LS model can
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20942,"ﬁnd a candidate substitution which is simpler. Their
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20943,"results, however, show that the approach is unable
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20944,"to ﬁnd simpliﬁcations for one third of the complex
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20945,"words in the dataset. (Shardlow, 2013b) presents
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20946,"the CW corpus: the ﬁrst dataset for CWI. Although
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20947,"a relevant contribution, this dataset contains only
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20948,"731 instances extracted automatically from the Sim-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20949,"ple English Wikipedia edits, which raises concerns
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20950,"about its reliability and applicability.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20951,"The results obtained by Shardlow (2013a) high-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20952,"light some of the issues of the dataset. They use the
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20953,"CW corpus to compare the performance of three so-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20954,"lutions to CWI: a Threshold-Based approach, a Sup-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20955,"port Vector Machine (SVM), and a “Simplify Ev-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20956,"erything” approach. In their experiments, the “Sim-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20957,"plify Everything” approach achieves higher Accu-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20958,"racy, Recall and F-scores than all other systems, sug-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20959,"gesting that simplifying all words in a sentence is
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20960,"the most effective approach for CWI. These results
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20961,"are clearly counter intuitive and conﬂicting with the
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20962,"conclusions drawn in (Paetzold and Specia, 2013;
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20963,"Paetzold, 2013; Shardlow, 2014).
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20964,"In this paper we describe the ﬁrst edition of the
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20965,"Complex Word Identiﬁcation task, organized at Se-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20966,"mEval 2016. This is an initiative that aims to pro-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20967,"vide reliable resources and new insights for CWI, as
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20968,"well as to establish the state of the art performance
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20969,"in CWI for English texts, and bring more visibility
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20970,"to the area of Text Simpliﬁcation.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20971,"2 Task Description
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20972,"The Complex Word Identiﬁcation task of SemEval
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20973,"2016 invited participants to create systems that,560given a sentence and a target word within it, can
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20974,"predict whether or not a non-native English speaker
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20975,"would be able to understand the meaning of the tar-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20976,"get word. We chose non-native speakers as a target
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20977,"audience because, unlike second language learners
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20978,"and those with low literacy levels or conditions such
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20979,"as Aphasia and Dyslexia, non-native speakers of En-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20980,"glish have not yet been explicitly assessed with re-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20981,"spect to their simpliﬁcation needs. In addition, the
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20982,"broad availability of such an audience makes data
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20983,"collection more feasible.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20984,"We have established main goals for the task:
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20985,"1. To learn which words challenge non-native En-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20986,"glish speakers and to understand what their
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20987,"traits are.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20988,"2. To investigate how well one’s individual vo-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20989,"cabulary limitations can be predicted from the
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20990,"overall vocabulary limitations of others in the
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20991,"same category.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20992,"3. To introduce a new corpus to be used in Text
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20993,"Simpliﬁcation and other tasks related to Topic
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20994,"Modelling and Semantics.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20995,"4. To evaluate the reliability of various resources
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20996,"commonly used in the creation of Lexical Sim-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20997,"pliﬁcation approaches.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20998,"5. To establish the state of the art performance in
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
20999,"CWI for English texts.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21000,"6. To investigate and establish evaluation metrics
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21001,"for the task of CWI.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21002,"In order to achieve these objectives for the shared
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21003,"task, we started by creating a manually annotated
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21004,"dataset through a user study.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21005,"3 User Study
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21006,"In the study, volunteers were asked to judge whether
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21007,"or not they could understand the meaning of each
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21008,"word in a given sentence. In the following we pro-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21009,"vide more details on the sentences used and the an-
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21010,"notation process.
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21011,"3.1 Data Sources
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21012,"We selected 9,200 sentences to be annotated, after
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21013,"ﬁltering out cases with spurious characters, HTMLor CSS markup, or outside the 20-40 word-length
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21014,"range. These sentences were taken from three
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21015,"sources:
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21016,"CW Corpus (Shardlow, 2013b): composed of
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21017,"731 sentences from the Simple English Wikipedia
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21018,"731 sentences from the Simple English Wikipedia
",False,SemEval 2016 Task 11 Complex Word Identification,False,False,True
21019,Abstract,True,Systematic Design of a Transimpedance Amplifier,False,False,False
21020,"I. I NTRODUCTION
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21021,"II. S
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21022,"III. D
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21023,"V. D ESIGN METHOD
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21024,"VI. M EASUREMENTS
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21025,"VII. D
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21026,"VIII. C
",True,Systematic Design of a Transimpedance Amplifier,False,False,False
21027,"530 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 3, MARCH 2010
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21028,"Systematic Design of a Transimpedance Ampliﬁer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21029,"With Speciﬁed Electromagnetic Out-of-Band
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21030,"Interference Behavior
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21031,"Marcel J. van der Horst, André C. Linnenbank , Member , IEEE , Wouter A. Serdijn , Senior Member , IEEE , and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21032,"John R. Long , Member , IEEE
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21033,"magnetic interference (EMI) behavior is usually completelydisregarded. EMI can, e.g., result in detection of low-frequencyenvelope variations of the usually high-frequency interferingsignals. If the detected signals end up in the pass band of thenegative-feedback ampliﬁer, they cannot be distinguished fromthe intended signal any longer, so the signal-to-error ratio (SER)is reduced. Several measures can be taken to prevent unaccept-able reduction of the SER, like applying ﬁlters, chokes, etc. Inthis paper, however, circuit design aspects are investigated. It isassumed that interference reaches the ampliﬁer input and that theSER has to be assured by a proper design of the negative-feedbackampliﬁer. Since EMI is related to nonlinear distortion, it is afunction of the loop gain of the negative-feedback ampliﬁer. Fora given electromagnetic (EM) environment it is therefore possibleto calculate the minimum loop gain required to reduce EMI to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21034,"acceptable levels without ﬁltering. To illustrate this systematic
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21035,"design method a transimpedance ampliﬁer is designed and builtto properly function in interfering ﬁeld strengths up to 30 V/m.Experimental results are in good agreement with theory.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21036,"Index Terms— Envelope detection, electromagnetic interference
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21037,"(EMI ), negative-feedback ampliﬁer, nonlinear distortion, signal-to-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21038,"error ratio (SER), susceptibility, transimpedance ampliﬁer.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21039,"I. I NTRODUCTION
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21040,"DESIGNING a negative-feedback ampliﬁer usually starts
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21041,"with determining the speciﬁcations of the ampliﬁer to be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21042,"designed. Electromagnetic compatibility (EMC) is a require-ment that is often disregarded in the design process. Yet, with thecurrent congestion of the electromagnetic (EM) spectrum due toelectronic systems that intentionally, or unintentionally, pollutethe EM environment, the interference burden for negative-feed-back ampliﬁers increases. In order not to degrade the ampliﬁerperformance due to electromagnetic interference (EMI), EMCshould be considered and incorporated in the design process.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21043,"Manuscript received July 08, 2008; revised November 06, 2008, March 12,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21044,"2009. First published June 10, 2009; current version published March 05, 2010.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21045,"This paper was recommended by Associate Editor A. J. Lopez Martin.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21046,"M. J. van der Horst is with the Department of Electrical Engineering,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21047,"Hogeschool van Amsterdam, Amsterdam 1000 BA, The Netherlands (e-mail:
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21048,"m.j.van.der.horst@hva.nl).
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21049,"A. C. Linnenbank is with the Heart Failure Research Center, Academic Med-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21050,"ical Center, University of Amsterdam, Amsterdam 1105 AZ, The Netherlands(e-mail: a.c.linnenbank@amc.uva.nl).
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21051,"W. A. Serdijn and J. R. Long are with the Electronics Research Labora-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21052,"tory/DIMES, Delft University of Technology, Delft 2628 CD, The Netherlands
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21053,"(e-mail: w.a.serdijn@tudelft.nl; resp.j.r.long@tudelft.nl).
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21054,"Digital Object Identiﬁer 10.1109/TCSI.2009.2025003In practical situations, the negative-feedback ampliﬁer itself
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21055,"seldomly is a severe source of interference, but its susceptibilitymay be large. Therefore, this paper concentrates on reducingthe ampliﬁer susceptibility to an acceptable level. Here, EMIinduced errors, by out-of-band interference, comparable to the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21056,"total white noise generated by the ampliﬁer are considered to be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21057,"acceptable.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21058,"Usually, EMI is prevented by screening of wires, using
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21059,"chokes and applying ﬁlters reducing the disturbing out-of-bandsignal at the input of the ampliﬁer. Here, as an additional, al-ternative, measure, it is investigated how the negative-feedbackampliﬁer itself can be designed to have a low EMI susceptibilityto the unﬁltered out-of-band disturbing signal at its input, byusing circuit design strategies only.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21060,"The effects of EMI on active devices like bipolar junction
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21061,"transistors (BJT) or FETs are closely related to second-orderharmonic distortion, i.e., it is determined by the second-ordernonlinearity of the device [1]. Disturbance results in dc-shifts ofthe bias currents and detection of the envelope of the disturbingsignal. Although slight shifts in biasing may be tolerated in some
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21062,"cases, envelope detection may result in signals within the am-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21063,"pliﬁer bandwidth that cannot be distinguished from the intendedsignals. This adversely affects the ﬁdelity of the intended signaltransfer, and thus the signal-to-error ratio (SER). The adverseeffect on the SER due to envelope detection is meant when theterm EMI is used in the remainder of this paper. Note that bothnoise and EMI determine the total SER.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21064,"Methods to model and analyze distortion and EMI in BJTs,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21065,"FETs, negative-feedback ampliﬁers, and also some designsof low EMI susceptible opamps have been published, e.g.,[2]–[13]. A simple, systematic method to design an applica-tion speciﬁc negative-feedback ampliﬁer with speciﬁed SER,output-, and bandwidth requirements has not been presented asyet.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21066,"Since EMI is related to nonlinear distortion, it is a function
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21067,"of the loop gain of the negative-feedback ampliﬁer [11]. For a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21068,"given EM environment and active device, it is therefore possibleto calculate the minimum loop gain required to reduce EMI toacceptable levels without ﬁltering. This is demonstrated by sys-tematically designing a transimpedance ampliﬁer for a speciﬁcEM environment. Section II presents the design speciﬁcations,including the minimally allowed SER.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21069,"In order to ensure a particular SER, the amount of disturbance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21070,"the electromagnetic ﬁeld generates has to be determined. There-fore, a simple method for approximating the disturbance will bepresented in Section III.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21071,"1549-8328/$26.00 © 2010 IEEE
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21072,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. V AN DER HORST et al. : SYSTEMATIC DESIGN OF A TIA WITH SPECIFIED ELECTROMAGNETIC OUT-OF-BAND INTERFERENCE BEHA VIOR 531
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21073,"TABLE I
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21074,"SPECIFICATIONS
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21075,"Preliminary computations showed that the bandwidth spec-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21076,"iﬁcations could not be met with a single BJT stage due to theMiller effect. Cascode stages neutralize the adverse effect ofthe base–collector or gate–drain capacitance and the Miller ef-fect does not occur. A hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21077,"model valid for both linear and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21078,"second-order nonlinearity analysis of a BJT–BJT cascode stageis shown in Section IV. With the necessary changes, it can alsobe used to model these effects in cascode stages consisting ofother combinations of active devices.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21079,"Section V demonstrates the design of a transimpedance am-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21080,"pliﬁer that meets all speciﬁcations, with this BJT–BJT cascodeas active part. Susceptibility measurements on the realized tran-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21081,"simpedance ampliﬁer are presented in Section VI, and are in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21082,"close agreement with theory. Section VII presents a short dis-cussion. Finally, Section VIII presents the concluding remarks.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21083,"II. S
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21084,"PECIFICATIONS
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21085,"The method to design a transimpedance ampliﬁer with a spec-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21086,"iﬁed SER in high ﬁeld strengths is demonstrated by designingsuch an ampliﬁer for a given signal source and load. Table I sum-marizes the speciﬁcations of the source, load, signal transfer,and EM environment, respectively. All speciﬁcations chosen arerealistic and may occur in practice.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21087,"The envelope of the plane waves shows low-frequency
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21088,"variations that are in the ampliﬁers’ pass band. The maximumvariation in the envelope corresponds to an amplitude modula-tion with a modulation index of 1, as stated in Table I. Such anEM environment may very well occur in practice. Industrial,scientiﬁc, and medical equipment (ISM), e.g., used for heating,diathermy, or electrosurgery, but also radio transmitters, mayradiate EM ﬁelds with high ﬁeld strengths and low-frequencyenvelope variations. In some ISM bands (13.5, 27.0, and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21089,"40.7 MHz), the amount of power that may be radiated is even
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21090,"unrestricted in some countries [14]. EM-ﬁeld strength levelsbetween 10 and 30 V/m can readily occur in the vicinity ofthese radiating equipment. Therefore, equipment in an indus-trial environment and life-supporting medical systems shouldbe immune to EM ﬁelds up to at least 10 V/m [14], [15].
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21091,"The consequences of a too large susceptibility in a harsh EM
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21092,"environment, i.e., a too low SER, of the negative-feedback am-pliﬁer (as part of a lager system) may vary from life-threateningsituations in medical environments [16], [17] and in aviation,to inconvenience when telephones can be used to listen to AMradio broadcasts [1]. Maintaining a high enough SER in highEM-ﬁeld strength levels is thus important, especially in possiblelife-threatening situations. Here, a minimal SER equal to 70 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21093,"is expected to prevent these detrimental effects.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21094,"The relatively low upper frequency of 100 MHz to design for,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21095,"has been speciﬁcally chosen. This relieves the measurement dif-ﬁculties that can be expected at higher frequencies where board
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21096,"lay-out properties do play a signiﬁcant role. The method pre-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21097,"sented in this work is, nonetheless, also applicable to muchhigher frequencies in the GHz range such as used by cellularphones.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21098,"We will assume that both the negative-feedback ampliﬁer and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21099,"the load are shielded from interfering ﬁelds, but the interconnectbetween source and ampliﬁer will not be shielded. To ease thecalculation of EM coupling, the interconnect is assumed to con-sist of two wires that have a ﬁxed distance to each other, com-parable with two wires of ribbon cable, and that EM interactionwith the shield does not occur. We will use a so called two-wireline [18] with a length of 0.1 m, a distance between the wires of1.27 mm, an inductance of 92.7 nH, and a measured capacitanceof 2.36 pF.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21100,"The intended signal is smallest at the input of the ampliﬁer,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21101,"this is thus the place where noise and EMI have the largest detri-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21102,"mental effect. Both noise and EMI effects are therefore trans-ferred to an equivalent source at the input of the ampliﬁer.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21103,"If it is assumed that the equivalent noise power and equivalent
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21104,"EMI power are uncorrelated, the SER is simply the ratio of thesignal power and the sum of both.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21105,"III. D
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21106,"ETERMINING THE DISTURBING CURRENT
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21107,"Using transmission line equations and by integrating the total
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21108,"ﬁeld along the two-wire, voltages and currents at the termi-nals of the wire can be determined accurately [19]. These volt-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21109,"ages and currents can also be determined for other types of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21110,"interconnects, e.g., microstrip lines, with this generally validmethod. For the speciﬁed frequency range, the two-wire is elec-trically short. Its electrical behavior may be described by meansof lumped-circuit models instead of using the transmission lineequations, while the induced voltage and current can be mod-eled by a current source in parallel with the admittances and avoltage source in series with the impedances respectively, e.g.,[18].
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21111,"The magnitude of the disturbing current at the input of current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21112,"processing ampliﬁers, can easily be determined by assuming theinput impedance of the ampliﬁer,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21113,", to be zero.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21114,"The practical transimpedance ampliﬁer will not have a zero
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21115,"input impedance. To simplify the design process, an ideal ampli-ﬁer can be considered nevertheless. Deviations in the calculated
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21116,"disturbing signal, due to deviations of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21117,"from the ideal value,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21118,"presented to the input of the ampliﬁer can be evaluated later. Ifthe practical transimpedance ampliﬁer is designed properly, theconstraint for the source impedance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21119,"still holds. The
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21120,"deviations between the “ideal” and “practical” values of the dis-turbing signal are thus expected to be small.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21121,"Under the speciﬁed conditions, the disturbing current at the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21122,"input of the transimpedance ampliﬁer can now be approximatedby
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21123,"[18], where
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21124," is the angular fre-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21125,"quency,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21126," is the capacitance of the two-wire,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21127," the distance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21128,"between the conductors,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21129," is the electric ﬁeld strength,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21130," is the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21131,"angle between the E-ﬁeld and the two-wire, and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21132," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21133,"When the orientation of the E-ﬁeld is perpendicular to the two-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21134,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. 532 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 3, MARCH 2010
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21135,"Fig. 1. Total disturbing current at the input of the transimpedance ampliﬁer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21136,"as a function of frequency calculated with a lumped-circuit model (solid line)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21137,"and as predicted by transmission line theory (dashed line). At a frequency of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21138,"about 240 MHz, the lumped-circuit model becomes less accurate because the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21139,"two-wire is not electrically short anymore. Resonance and antiresonance pointscan be identiﬁed in the transmission line model based
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21140,"/105
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21141," calculation that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21142,"do not occur in the simple model. The lumped-circuit model completely looses
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21143,"validity at approximately 1 GHz.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21144,"wire
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21145," is maximal. Note that for frequencies
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21146,"higher than 100 MHz, the contributions of the magnetic ﬁeldcomponent and inductance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21147,"have to be taken into account as
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21148,"well.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21149,"In EMC engineering, it is customary to assume the worst case
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21150,"scenario, i.e., maximal ﬁeld coupling. This also makes sensefrom a design point of view; therefore, in this paper, maximal
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21151,"is assumed
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21152," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21153,"To demonstrate the validity of the lumped-circuit model
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21154,"for the speciﬁed maximum frequency of the interfering planewaves, Fig. 1 shows both the graph of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21155,"determined by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21156,"the lumped-circuit model, including the magnetic ﬁeld com-ponent, [18] and determined by the transmission line method
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21157,"[19]. Both graphs are in good agreement up to the frequency at
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21158,"which the lumped-circuit model becomes less accurate (
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21159,"MHz) because the two-wire is not electrically short anymore.When designing for low EMI for frequencies higher than ap-proximately 1 GHz the transmission line method to determine
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21160,"should be used.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21161,"IV . M ODIFIED CASCODE MODEL
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21162,"A cascode stage consists of a cascade of two transistors, a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21163,"common-emitter (CE) and a common-base (CB) stage, wherethe CB stage is used to ensure a unilateral behavior of the CEstage.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21164,"The CB stage is a single-stage current ampliﬁer with a current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21165,"gain very close to one. Using the method presented in [11] it
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21166,"can be concluded that the envelope detection of the CB stage is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21167,"negligible compared to that of the CE stage, even at frequencieshigher than transit frequency,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21168,", of the CB stage, as long as the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21169,"loop gain around the CB stage is large. This large loop gain isusually easily obtained by assuring that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21170,", and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21171," , where
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21172," , and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21173," have
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21174,"their usual meaning as used in the hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21175," model and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21176," is the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21177,"impedance that loads the output of the CB stage. The (linear)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21178,"Fig. 2. Modiﬁed hybrid- /25model of the BJT–BJT cascode, valid when /114
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21179," /29
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21180,"/114
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21181," /59/114
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21182," /29 /90
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21183," /59/103
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21184," /2 /114
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21185," /29 /12
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21186," , and valid for linear and second-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21187,"order nonlinear transfer analysis.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21188,"transconductance is denoted by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21189," and will be discussed in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21190,"more detail later.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21191,"The hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21192," model capable of describing both linear and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21193,"second-order nonlinear behavior of the cascode is shown inFig. 2. The cascode shows an input impedance formed by baseresistance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21194,", base–emitter resistance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21195," , and base–emitter
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21196,"capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21197," of the CE stage. In parallel with
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21198," there
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21199,"is a capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21200," (Miller
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21201,"approximation).
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21202," is the input admittance of the CB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21203,"stage and equals
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21204," . For frequencies lower
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21205,"than the pole in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21206," , which is approximately equal to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21207,"and for frequencies
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21208,"higher than this pole its value reduces to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21209," . The
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21210,"output is formed by a resistance,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21211," , approximately equal to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21212,", shunted by a capacitance equal to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21213," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21214,"At the input, a voltage controlled current source of value
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21215,"represents the second-order nonlinearity of the base
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21216,"current due to the nonlinear input conductance. Finally, at theoutput there is also a voltage controlled current source of value
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21217,"that represents the linear and second-order
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21218,"nonlinear term of the collector current due to the exponentialinput voltage to output current relation of the BJT.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21219,",
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21220,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21221," can be determined by a Taylor expansion in the BJT
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21222,"operating point [2], [6], [7], [11].
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21223,"The expressions for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21224," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21225," are thus given by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21226,"in amperes per volt (1)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21227,"in amperes per volt squared (2)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21228,"where
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21229," is the bias current,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21230," is Boltzmann’s constant,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21231," is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21232,"the absolute temperature in Kelvin,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21233," is the electron charge, and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21234,"is the forward emission coefﬁcient, usually close to 1.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21235,"In the so-called midcurrent region the current gain
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21236," is vir-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21237,"tually constant, because secondary nonlinear effects due to bothlow (recombination in the base–emitter depletion layer) andhigh bias current (nonnegligible injection of minority carriersin the base) operation are negligible [20], [21]. The distortionbehavior of the BJT is minimal in this case.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21238,"A Taylor expansion of the nonlinear relation of the base cur-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21239,"rent and the base–emitter voltage in the midcurrent region re-sults in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21240,"in amperes per volt square
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21241," (3)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21242,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. V AN DER HORST et al. : SYSTEMATIC DESIGN OF A TIA WITH SPECIFIED ELECTROMAGNETIC OUT-OF-BAND INTERFERENCE BEHA VIOR 533
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21243,"After some straightforward mathematical manipulation of the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21244,"Gummel–Poon equations [20], the lower and upper boundariesof the midcurrent region can be determined from
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21245,"(4)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21246,"(5)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21247,"where
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21248," is the saturation current,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21249," is the base–emitter leakage
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21250,"coefﬁcient,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21251," is the forward current ampliﬁcation factor,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21252," is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21253,"base–emitter leakage current, and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21254," is the forward current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21255,"value where the current gain drops to half its value [20]. Coefﬁ-cient
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21256,"presents a boundary value for the maximal contribution
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21257,"of the base–emitter leakage current to the base current, whilecoefﬁcient
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21258,"presents the maximal contribution of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21259," to the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21260,"collector current. Values of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21261," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21262," usually result
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21263,"in negligible secondary nonlinear effects and therefore seem tobe reasonable. The results of (4) and (5) can be used to easilyobtain the boundary values of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21264,".
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21265,"Truncating a Taylor expansion after the second term intro-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21266,"duces inaccuracy. However, for values of signal voltage
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21267," up
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21268,"to 10 mV the inaccuracy of the resulting signal current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21269," is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21270,"smaller than 1% [11].
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21271," depends on
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21272," and is therefore also
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21273,"nonlinear. The resulting nonlinear current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21274," is, however, pro-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21275,"portional to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21276," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21277," represents the second-order non-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21278,"linear value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21279," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21280," is the low-frequency variation of the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21281,"envelope. Current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21282," is often negligibly small compared to the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21283,"currents at
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21284," due to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21285," , respectively,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21286," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21287," may therefore be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21288,"regarded as being constant.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21289," is a constant because its value
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21290,"is determined by a junction capacitance which is only slightlydependent on
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21291,". The junction capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21292," may have
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21293,"considerable signal voltage across it, modulating its capacitancevalue. It is, however, usually loaded by the input impedance of asubsequent stage or load, that is much smaller than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21294,",
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21295,"making its (nonlinear) effect negligible.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21296,"In properly designed negative-feedback ampliﬁers
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21297," will be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21298,"most probably below 10 mV. Therefore, the modiﬁed hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21299,"model of Fig. 2 can be used to analyze linear and second-ordernonlinear, EMI, behavior.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21300,"The combined action of the voltage controlled current sources
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21301,"tend to reduce the second-order nonlinear behavior. This lin-earizing action is especially effective when signal source resis-tance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21302,"is much larger than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21303," [3], see Section V, (13).
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21304,"V. D ESIGN METHOD
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21305,"The design of an application-speciﬁc negative-feedback am-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21306,"pliﬁer can be divided in several orthogonal steps [22]. From thesource and load speciﬁcations the required transfer and feed-back conﬁguration is determined ﬁrst. Second, the noise proper-ties are investigated, which result in optimal bias requirements.Third, the output requirements to the load are determined. Asfourth step, the required loop gain and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21307,"are determined to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21308,"meet the EMI speciﬁcation. The ﬁfth step is estimating the band-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21309,"width and applying frequency compensation to achieve a Butter-worth (maximally ﬂat) characteristic. Finally, as sixth step thebiasing arrangement is designed.From the source and load speciﬁcations (see Table I), it fol-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21310,"lows that the transimpedance should have a 100 k
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21311,"value to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21312,"realize 1 V across the load for a source current of 10
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21313," A. This
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21314,"transimpedance can, e.g., be implemented by a negative-feed-back ampliﬁer with a feedback resistor of 100 k
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21315,".
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21316,"Since noise will always be generated, thus also when there is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21317,"no interference, it will determine the maximal obtainable SERand is therefore considered before EMI. The maximal allowableEMI now follows from the required SER and the noise behavior.Noise and EMI are extensively dealt with in the next sections.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21318,"For elaborate treatment of the other steps we refer to [22].
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21319,"They are only brieﬂy discussed here.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21320,"A. Noise
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21321,"Under the assumption that the signal source generates noise
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21322,"equivalent to the thermal noise of the real part of its admittance,for the transimpedance ampliﬁer with a bipolar input stage theequivalent input noise power is given by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21323,"(6)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21324,"In this equation,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21325," is the upper frequency of the bandwidth.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21326,"Because of the required large bandwidth of 1 MHz the inﬂuenceof the lower frequency corner of the bandwidth and the inﬂuenceof ﬂicker noise can be neglected, since for modern BJTs thefrequency at which the ﬂicker noise equals the white noise isusually a few Hz [22]. Further,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21327,"is the source resistance,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21328,"is formed by the source capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21329," , and the capacitance of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21330,"the two-wire
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21331," , in parallel.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21332," is the feedback resistor,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21333," is the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21334,"base resistance, and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21335," is the dc current gain of the transistor.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21336,"For low noise power, it immediately follows from (6) that a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21337,"BJT should be used having a high value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21338," and, preferably,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21339,"a low value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21340," . Also, (6) is valid under the assumption that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21341,"current noise contribution of the BJT is dominated by the basecurrent. This is allowed when
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21342,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21343,"[22],
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21344," being the transit frequency; conditions that usually can
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21345,"be met easily.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21346,"B. Calculation of the Required Transconductance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21347,"When we design for equal contributions of both noise power
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21348,"and EMI power to the SER, the most optimal design regardingthe SER results. After all, when a lot design effort is put in lownoise design while EMI dominates the SER, this effort is wasted.Similarly, design effort and power is wasted when EMI is de-signed to be much lower than the noise. Therefore, EMI shouldhave at most the same order of magnitude as the noise. From this
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21349,"maximum EMI power, the minimal required transconductance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21350,"of the active part is determined.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21351,"The disturbing current generated by the nonconstant enve-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21352,"lope interfering EM ﬁeld, shows the same amplitude variations
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21353,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. 534 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 3, MARCH 2010
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21354,"Fig. 3. Hybrid- /25signal diagram of the transimpedance ampliﬁer.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21355,"as the EM ﬁeld. Like noise, the resulting envelope detection
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21356,"(EMI) caused by the disturbing current can be represented byone equivalent current source at the input of the negative-feed-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21357,"back ampliﬁer. This source is given by [11]:
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21358,"(7)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21359,"The angular frequency of the disturbing current is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21360," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21361,"is the transfer from the disturbing current to the input of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21362,"the BJT, i.e., the transfer from
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21363," to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21364," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21365," is the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21366,"low-frequency term of the transfer from signal source
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21367," to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21368,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21369," is the modulation depth. For a transimpedance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21370,"ampliﬁer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21371," also determines the transfer from source
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21372,"to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21373," , when
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21374," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21375," is the second-order
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21376,"nonlinearity term, which is a measure for the second-order
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21377,"nonlinear behavior of the negative-feedback ampliﬁer. Here,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21378,".
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21379,"Fig. 3 depicts the hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21380," diagram of the transimpedance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21381,"ampliﬁer. Although the exact values of the circuit elements ofthe cascode are not yet known, using Table I some conclusionscan be drawn: load capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21382,"will most probably be much
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21383,"larger than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21384," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21385," . There will be two poles
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21386,"determining the bandwidth, with pole
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21387," affected by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21388," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21389,"located at a lower frequency than pole
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21390," affected by the input
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21391,"capacitance formed by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21392," , and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21393," shunted by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21394,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21395," . Expressions for the poles will be given later.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21396,"Transfer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21397," will have a zero located at approximately
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21398,"the same frequency as
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21399," , and therefore, shows a single pole
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21400,"transfer [11]. The loop gain often shows two dominant poles
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21401,"in case of application speciﬁc ampliﬁers.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21402," is given by the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21403,"ratio of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21404," and the loop gain and, when
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21405," , it will
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21406,"increase with increasing frequency up to some maximum valueat
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21407,"after which it will decrease again [11], as Fig. 4 shows.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21408,"As a result,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21409," will show the same behavior.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21410,"Fig. 4 shows the transfer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21411," , current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21412," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21413,"the resulting equivalent envelope detection source
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21414,"in one ﬁgure. It shows how the slopes of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21415," depend
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21416,"on
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21417," and the slopes of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21418," . Between
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21419," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21420,"increases with a slope of 80 dB/dec and it sta-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21421,"bilizes at a constant value for frequencies higher than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21422," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21423,"Further,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21424," will occur near the upper corner frequency of the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21425,"bandwidth
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21426," given by the square root of the loop gain poles
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21427,"product (LP). LP equals
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21428," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21429," is the loop
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21430,"gain. Note that the ﬂat region of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21431," is due to out-of-band
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21432,"disturbance and thus the region of interest.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21433,"Fig. 4. Transfers /106 /31 /106(solid line), /106 /105
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21434," /106(dotted line), and /106 /105
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21435," /106(dashed
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21436,"line) as a function of frequency. The maximum frequency used in this ﬁgure is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21437,"in the “differentiating” region of /105
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21438," . Note that the ﬂat region of /106 /105
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21439," /106is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21440,"due to out-of-band disturbance and thus the region of interest.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21441,"Transfer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21442," is given by [11]
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21443,"(8)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21444,"where
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21445," is the zero-frequency value of the load that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21446,"current source
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21447," experiences. Further, there is a term
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21448,"that increases the damping factor
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21449," ,g i v e n
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21450,"by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21451," , with
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21452," being
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21453,"a phantom zero introduced by the designer to manipulate thepoles into the desired positions for the required frequency re-sponse of the source to load transfer, the transimpedance in thiscase.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21454,"is chosen to obtain a Butterworth transfer.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21455,"For values of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21456," smaller than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21457," overshoot will occur
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21458,"at the upper edge of the bandwidth in both the source to loadtransfer and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21459,". Proper frequency compensation thus reduces
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21460,"EMI susceptibility near the upper edge of the bandwidth.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21461,"The maximum value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21462," occurs at a frequency approxi-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21463,"mately equal to the upper limit of the bandwidth,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21464," . Using this
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21465,"approximation and (8) (disregarding the phantom zero term forthe time being), the maximum value is derived to be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21466,"(9)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21467,"Still,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21468," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21469," are unknown. To come to an equation that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21470,"can be solved, Fig. 3 has to be considered.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21471,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. V AN DER HORST et al. : SYSTEMATIC DESIGN OF A TIA WITH SPECIFIED ELECTROMAGNETIC OUT-OF-BAND INTERFERENCE BEHA VIOR 535
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21472,"Output resistance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21473," can be expected to have a value much
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21474,"greater than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21475," and can therefore be neglected. For now, it is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21476,"assumed that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21477," can be neglected because it is much smaller
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21478,"than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21479," . When this is not the case, its effect can be evaluated in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21480,"a later stadium. Feedback factor
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21481," is determined from Fig. 3
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21482,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21483," . With
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21484," it follows for the loop
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21485,"gain
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21486,"(10)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21487,"Also, from Fig. 3 follows directly for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21488,"(11)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21489,"Equation (8) shows that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21490," when
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21491," .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21492,"It is now possible to simplify this expression to a form where,apart from
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21493,", no hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21494," parameters appear.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21495,"(12)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21496,"Envelope variations usually occur at a low frequency. As an
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21497,"approximation,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21498," in (7) can be used. For
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21499," can now be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21500,"derived to hold
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21501,"(13)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21502,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21503," . Note that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21504," may become small
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21505,"when the BJT is current driven, i.e.,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21506," , and reaches
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21507,"a maximal value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21508," under voltage drive condition:
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21509,".
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21510,"Combining the expression for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21511," , with the phantom zero dis-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21512,"regarded, (7), (9), (12), and (13) and solving for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21513," results in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21514,"(14)
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21515,"The required transconductance can be calculated from this equa-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21516,"tion if one uses the desired bandwidth as the value for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21517," and a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21518,"ﬁrst-order approximation for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21519," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21520," . This ﬁrst-order approx-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21521,"imation for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21522," follows from Fig. 3 by neglecting the inﬂuence of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21523,", which is allowed because the shunt
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21524," is in series with
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21525,"the large-valued resistor
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21526," , and is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21527,". Under the condition that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21528," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21529," are much larger
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21530,"than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21531," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21532," is larger than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21533," . Pole
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21534," thus
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21535,"follows from the transistor properties. As a ﬁrst-order approxi-mation of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21536,", the maximal forward current gain
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21537," , as speciﬁed
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21538,"in Spice models, can be used.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21539,"C. Design of the BJT Transimpedance Ampliﬁer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21540,"To quickly realize a prototype transimpedance ampliﬁer, it
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21541,"was chosen to design and build it using discrete BJTs. Fromthe large amount of BJTs that satisfy the design constraints, theBC548B, an npn transistor, was chosen.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21542,"1) Noise Calculation: From the noise equation derived ear-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21543,"lier (6), the optimal bias current for the transistor is determinedTABLE II
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21544,"HYBRID - /25PARAMETERS , /73
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21545," /61/49 /58 /51mA, /85
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21546," /61/50 V
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21547,"to be approximately 10
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21548," A for the BC548B. The noise contri-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21549,"bution of the BJT is virtually negligible compared to the noisecontribution of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21550,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21551," . The resulting equivalent noise power
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21552,"is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21553," . The resulting SNR is 84.5 dB, which
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21554,"is the maximal obtainable SER.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21555,"2) Output Capability: In order to deliver a signal of 1 V
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21556,"peak to the load, a current of 637
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21557," A is required. When this
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21558,"current has to be delivered to the load, one has to make sure thatenough current keeps ﬂowing through the output stage to avoidunacceptable decrease in transit frequency
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21559,". Biasing the output
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21560,"stage at approximately 1.5 times the current to be delivered is agood strategy [22], resulting in a minimal bias current of 1 mA.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21561,"3) EMI: To be on the safe side, to compensate for component
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21562,"spread and uncertainties in the exact value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21563,", the SER
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21564,"to design for can be taken 73 dB, so there is a margin of 3 dB.With a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21565,"dB the total equivalent input error power
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21566,"equals
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21567," A
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21568,". If it is assumed that both
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21569,"components equally contribute to the “error power”, a value for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21570,"of 1.58 nA is obtained.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21571,"It follows from
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21572," that at
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21573," the allowed
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21574,"maximal value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21575," is 567 nA. However, to be on the safe
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21576,"side again, a bandwidth of 1.1 MHz is designed for. Using (14)with the corresponding
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21577,"of 621 nA, it is now found that
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21578,"should have a value of 48.5 mA/V , which corresponds to an
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21579,"of 1.3 mA, to satisfy the EMI demands. This value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21580," is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21581,"located in the midcurrent region. A
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21582," of 294 and an
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21583," of 628
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21584,"M rad/s followed for the BC548B.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21585,"4) Discussion: The
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21586," determined from the above EMI con-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21587,"siderations is only a little higher than required for the outputcapability. The power consumption thus does not excessivelyincrease. There is, however, a large discrepancy between thevalues of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21588,"for minimal noise performance and required for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21589,"EMI performance.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21590,"Biasing the cascode at a current of 1.3 mA instead of 10
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21591," A
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21592,"changes the noise behavior of the BJT. The contribution of theBJT to the equivalent noise power increases and will now beof the same order of magnitude as the noise from
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21593,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21594," ,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21595,"which is still acceptable. The equivalent noise current power is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21596,", resulting in a SER of 73.5 dB.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21597,"5) SER Analysis and Ampliﬁer Implementation: Biasing the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21598,"BJT cascode at a collector current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21599," of 1.3 mA and a col-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21600,"lector–emitter voltage of 2 V results in the values of the modi-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21601,"ﬁed hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21602," parameters tabulated in Table II. They were deter-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21603,"mined using Pspice to determine the linear values and (2) and (3)to determine the second-order values of the modiﬁed hybrid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21604,"model. The simulated value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21605," is slightly larger than the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21606,"value obtained with (1). This is because the calculated
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21607," of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21608,"1.3 mA is a rounded up value.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21609,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. 536 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 3, MARCH 2010
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21610,"Fig. 5. Schematic of the transimpedance ampliﬁer.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21611,"The loop gain is equal to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21612," , which results in an accuracy
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21613,"of the transimpedance of 99.6 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21614," ; just 0.4 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21615," less than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21616,"in the ideal case. Poles
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21617," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21618," can be found at
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21619,"rad/s, respectively,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21620," rad/s. The bandwidth
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21621," ,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21622,"as predicted by the LP, is
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21623," rad/s (1.29 MHz) and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21624,". Note that the bandwidth speciﬁcations can not be
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21625,"met with a noncascode single stage. Due to the Miller effect thebandwidth is in this case limited to approximately 460 kHz.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21626,"reaches a high value at approximately
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21627," due to the low
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21628,"value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21629," . The minimal SER to be expected near
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21630," amounts
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21631,"70 dB, which is just within speciﬁcations. After frequency com-pensation to obtain a Butterworth characteristic, however, noovershoot will occur and therefore
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21632,"will decrease, resulting
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21633,"in a larger SER.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21634,"For a Butterworth characteristic a phantom zero [22] was in-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21635,"troduced by shunting
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21636," with a capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21637," of 1.18 pF.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21638,"Now, we have:
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21639," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21640," rad/s, which
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21641,"is indeed very close to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21642," rad/s.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21643,"is determined to be 1211
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21644," . This results in a corre-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21645,"sponding value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21646," pA at
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21647," . For frequencies just
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21648,"above
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21649," , the slope of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21650," (see Fig. 4) has not reached
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21651,"dB/dec yet. This slope is reached after approximately an octave.In the frequency band
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21652,"decreases about 3 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21653,"and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21654," increases 6 dB, resulting in an increase of 6 dB in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21655,". As a result,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21656," pA for frequencies larger than
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21657,". The SER to be expected thus equals 76.1 dB.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21658,"The required SER is easily reached after frequency compen-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21659,"sation. The designer could consider to reduce the
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21660," a little
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21661,"in order to reduce power consumption. As a consequence
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21662,"will increase and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21663," will decrease, but the required SER can
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21664,"still be reached. A trade-off between power consumption andSER is thus possible. We will not elaborate on this here.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21665,"Fig. 5 shows the ﬁnal schematic of the designed tran-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21666,"simpedance ampliﬁer. A current source realized with a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21667,"BC556A pnp BJT biases the cascode at a collector currentof 1.3 mA. The resistors required for establishing the desired
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21668,"base–emitter and collector–emitter voltages are chosen suchthat LP of the transimpedance is virtually not reduced. Spicesimulations show a transimpedance of 99.6 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21669,", a bandwidth
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21670,"of 1.29 MHz (
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21671," rad/s) and an
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21672," of 1.55 nA. These
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21673,"ﬁgures are very close to the calculated values.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21674,"The effects of the nonzero input impedance of the tran-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21675,"simpedance ampliﬁer on
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21676," can now be evaluated. As
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21677,"stated before, the effect is expected to be minor; the inaccuracyof the ampliﬁer transfer function is just 0.4 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21678,". Such a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21679,"low value of the inaccuracy implies an input impedance muchsmaller than the impedance formed by
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21680,"and the two-wire.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21681,"Using the accurate transmission line equations, the effect of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21682,"the nonzero input impedance has been evaluated. It was foundthat over the frequency range of interest the accuracy of the ap-proximated value of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21683,"is within 90%, which is acceptable.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21684,"VI. M EASUREMENTS
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21685,"The transimpedance ampliﬁer as depicted in Fig. 5 has
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21686,"been built and tested. The transimpedance was measured to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21687,"be 99.6 dB
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21688," and the bandwidth 1.1 MHz. Compensation
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21689,"is realized by a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21690," of 1 pF. It should be noted that due to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21691,"component spread in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21692," and the parasitic capacitance of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21693," ,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21694,"the actual total compensation capacitance was approximately1.6 pF. This was accounted for in the calculation of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21695,"in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21696,"Fig. 6.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21697,"Generating an EM plane wave of 30 V/m and to make also
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21698,"sure that just this plane wave is picked up by the transimpedanceampliﬁer, may be a tedious task. As shown in Section III, the dis-turbance current is dominated by a capacitance and the electricﬁeld component. Therefore, it was chosen to capacitively couplethe disturbing signal to the ampliﬁer. Besides, simulating ﬁeldto wire coupling by coupling an equivalent signal via a conduc-tance to the ampliﬁer, is a valid and generally used method for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21699,"frequencies at which transmission line effects are minimal [23].
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21700,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. V AN DER HORST et al. : SYSTEMATIC DESIGN OF A TIA WITH SPECIFIED ELECTROMAGNETIC OUT-OF-BAND INTERFERENCE BEHA VIOR 537
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21701,"Fig. 6. Amplitude of the equivalent envelope detection source at the input of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21702,"the ampliﬁer as a function of frequency. The line is calculated. The crosses are
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21703,"actual measurements. The ampliﬁer is frequency compensated to obtain a But-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21704,"terworth characteristic. Note that the out-of-band measurements of interest are
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21705,"located above 1 MHz. The in-band detection components are only shown for
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21706,"completeness.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21707,"The electric ﬁeld component has been replaced by a voltage
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21708,"from a signal generator and the capacitance by a coupling ca-pacitor equal to the capacitance of the, removed, two-wire. Thevoltage was chosen such that at 1 MHz
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21709,"amounted to the re-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21710,"quired 567 nA. Due to the differentiating character of the cou-pling capacitance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21711,"will increase with increasing frequency.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21712,"The measured and calculated values of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21713," are shown in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21714,"Fig. 6. The measured values are in close agreement with thecalculations and, as expected, no overshoot appears both in cal-culation and in measurement. Note that the out-of-band mea-surements of interest are located above 1 MHz. The in-band de-tection components are only shown for completeness.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21715,"The calculated equivalent current
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21716,"ﬂattens out at a max-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21717,"imum value of approximately 428 pA. This is close to the ap-proximated value of 450 pA.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21718,"VII. D
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21719,"ISCUSSION
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21720,"This ampliﬁer was designed to meet a certain SER speciﬁca-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21721,"tion for interfering ﬁelds up to 100 MHz. That does not implythat there are no other design solutions and not even that this isthe best one possible. It can, e.g., be expected that both noiseand EMI requirements can be met by a (CMOS) FET imple-mentation of the ampliﬁer also. Probably, however, a more com-plicated, multistage solution will be required to meet the band-width speciﬁcation due to the often too low LP of a single stageFET (cascode) implementation.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21722,"Here, we have chosen to demonstrate that with enough loop
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21723,"gain severe EMI requirements can be met with a BJT cascode,that is stronger nonlinear than a FET (cascode).
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21724,"Extending the speciﬁcation for the interference from 100
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21725,"MHz to 1 GHz or higher, two extra effects have to be taken intoaccount in the design process. First, transmission line theoryshows resonances in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21726,"at frequencies higher than 1 GHz.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21727,"Second,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21728," and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21729," (in series with
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21730," ) will
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21731,"introduce a nondominant pole in
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21732," at approximately 1.4
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21733,"GHz. Its effect on the disturbance is that of a ﬁrst-order low-passﬁlter. High-frequency maxima will thus be attenuated, leavingthe maximum at ca. 340 MHz as the EMI determining valueof
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21734,". A new, higher value, of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21735," , and hence,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21736," , willbe needed. It could be that now our simple cascode ampliﬁer
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21737,"can not meet the speciﬁcations and a different implementationis required.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21738,"VIII. C
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21739,"ONCLUSION
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21740,"In this paper, a systematic method to design a transimpedance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21741,"ampliﬁer with speciﬁed EMI behavior has been presented. It
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21742,"makes use of a simple method to approximate the amount of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21743,"disturbing signal at the input of a negative-feedback ampliﬁerdue to an interfering EM plane wave, and also of a modiﬁed hy-brid-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21744,"model that can be used for calculation of both linear and
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21745,"second-order transfers of a bipolar junction transistor cascode.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21746,"Using the design method, the required transconductance to
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21747,"reach the speciﬁcations can be calculated, from which the bi-asing of the cascode follows.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21748,"The transimpedance ampliﬁer has been designed to have a
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21749,"transimpedance of 100 k
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21750,", a bandwidth of 1 MHz, and a min-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21751,"imal signal-to-error ratio of 70 dB, resulting from both noise andinterference, while being subjected to a plane wave of 30 V/m.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21752,"The transimpedance ampliﬁer has been built. Measurements
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21753,"are in good agreement with calculations and simulations, andthus, support the method presented in this paper.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21754,"R
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21755,"EFERENCES
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21756,"[1] J. J. Goedbloed, “Kluwer technische boeken,” in Elektromagnetische
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21757,"Compatibiliteit , 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1993.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21758,"[2] D. D. Weiner and J. F. Spina , Sinusoidal Analysis and Modeling of
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21759,"Weakly Nonlinear Circuits, With Applications to Nonlinear Effects .
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21760,"New York: Van Nostrand/Reinhold, 1980.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21761,"[3] E. K. de Lange, A. van Staveren, O. De Feo, F. L. Neerhoff, M. Hasler,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21762,"and J. R. Long, “Predicting nonlinear distortion in common-emitter
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21763,"stages for ampliﬁer design using volterra series,” in Proc. NDES , 2002,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21764,"pp. 41–44.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21765,"[4] R. van Langevelde and F. M. Klaassen, “Effect of gate-ﬁeld dependent
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21766,"mobility degradation on distortion analysis in mosfets,” IEEE Trans.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21767,"Electron Devices , vol. 44, no. 11, pp. 2044–2052, Nov. 1997.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21768,"[5] R. van Langevelde and F. M. Klaassen, “Accurate drain conductance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21769,"modeling for distortion analysis in mosfets,” in Proc. Int. Electron De-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21770,"vices Meet. , 1997, pp. 313–316.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21771,"[6] W. Sansen, “Distortion in elementary transistor circuits,” IEEE Trans.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21772,"Circuits Syst. II, Analog Digit. Signal Process. , vol. 46, no. 3, pp.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21773,"315–325, Mar. 1999.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21774,"[7] P. Wambacq, G. G. E. Gielen, P. R. Kinget, and W. Sansen, “High-fre-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21775,"quency distortion analysis of analog integrated circuits,” IEEE Trans.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21776,"Circuits Syst. II, Analog Digit. Signal Process. , vol. 46, no. 3, pp.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21777,"335–345, Mar. 1999.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21778,"[8] F. Fiori and P. S. Crovetti, “Nonlinear effects of radio-frequency
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21779,"interference in operational ampliﬁers,” IEEE Trans. Circuits Syst. I,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21780,"Fundam. Theory Appl. , vol. 49, no. 3, pp. 367–372, Mar. 2002.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21781,"[9] S. Grafﬁ, G. Masetti, and D. Golzio, “New macromodels and measure-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21782,"ments for the analysis of EMI effects in 741 op-amp circuits,” IEEE
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21783,"Trans. Electomagn. Compat. , vol. 33, no. 1, pp. 25–33, Feb. 1991.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21784,"[10] E. D. Totev and C. J. M. Verhoeven, “Design considerations for low-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21785,"ering sensitivity to out of band interference of negative-feedback am-pliﬁers,” in Proc. ISCAS , 2005, vol. 2, pp. 1597–1600.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21786,"[11] M. J. van der Horst, A. C. Linnenbank, and A. van Staveren, “Am-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21787,"plitude-modulation detection in single-stage negative-feedback ampli-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21788,"ﬁers due to interfering out-of-band signals,” IEEE Trans. Electromagn.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21789,"Compat. , vol. 47, no. 1, pp. 34–44, Feb. 2005.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21790,"[12] J. M. Redouté and M. Steyaert, “EMI resistant CMOS differential input
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21791,"stages,” IEEE Trans. Circuits Syst. I, Reg. Papers , vol. 57, no. 2, pp.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21792,"323–331, Feb. 2010.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21793,"[13] A. Richelli, L. Colalongo, M. Quarantelli, and Z. M. Kovacs-Vajna,
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21794,"“Robust design of low EMI susceptibility CMOS opamp,” IEEE Trans.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21795,"Electromagn. Compat. , vol. 46, no. 2, pp. 291–298, May 2004.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21796,"[14] Cenelec, Limits and methods of measurement of radio disturbance
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21797,"characteristics of industrial, scientiﬁc and medical (ISM) radio-fre-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21798,"quency equipment Cenelec, Brussels, Belgium, NEN-EN 55011, 1992.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21799,"[15] “Medical Electrical Equipment, Part 1,” IEC 601-1-2, 1993.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21800,"Authorized licensed use limited to: IEEE Editors in Chief. Downloaded on May 10,2010 at 09:05:40 UTC from IEEE Xplore.  Restrictions apply. 538 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS, VOL. 57, NO. 3, MARCH 2010
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21801,"[16] “Radiofrequency interference with medical devices,” IEEE Eng. Med.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21802,"Biol. Mag. , vol. 17, no. 3, pp. 111–114, May/Jun. 1998, IEEE.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21803,"[17] P. S. Ruggera and E. R. O’Bryan, “Studies of apnea monitor radiofre-
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21804,"quency electromagnetic interference,” in Proc. Annu. Int. Conf. IEEE
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21805,"Eng. Med. Biol. Soc. , 1991, pp. 1641–1643.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21806,"[18] C. R. Paul , Introduction to Electromagnetic Compatibility , 1st ed.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21807,"[18] C. R. Paul , Introduction to Electromagnetic Compatibility , 1st ed.
",False,Systematic Design of a Transimpedance Amplifier,False,False,False
21808,Abstract ,True,targeted_phishing,False,False,False
21809,"I. I NTRODUCTION
",True,targeted_phishing,False,False,False
21810,"II. R ELATED WORK
",True,targeted_phishing,False,False,False
21811,"III. E XPERIMENTAL METHODOLOGY
",True,targeted_phishing,False,False,False
21812,"IV. A NALYSIS AND RESULTS
",True,targeted_phishing,False,False,False
21813,"V. F UTURE WORK
",True,targeted_phishing,False,False,False
21814,"VI. C ONCLUSION
",True,targeted_phishing,False,False,False
21815,"arXiv:2301.00665v1  [cs.CL]  30 Dec 2022Targeted Phishing Campaigns using Large Scale
",False,targeted_phishing,False,False,False
21816,"Language Models
",False,targeted_phishing,False,False,False
21817,"Rabimba Karanjai
",False,targeted_phishing,False,False,False
21818,"Department of Computer Science
",False,targeted_phishing,False,False,False
21819,"University Of Houston
",False,targeted_phishing,False,False,False
21820,"Houston, United States
",False,targeted_phishing,False,False,False
21821,"rkaranjai@uh.edu
",False,targeted_phishing,False,False,False
21822,"GPT-2, and other large language models have achieved impres sive
",False,targeted_phishing,False,False,False
21823,"results in various natural language processing tasks, incl uding
",False,targeted_phishing,False,False,False
21824,"language translation, summarization, and text generation . In
",False,targeted_phishing,False,False,False
21825,"recent years, there has been a growing concern about the
",False,targeted_phishing,False,False,False
21826,"potential use of NLMs to generate phishing emails, which are
",False,targeted_phishing,False,False,False
21827,"fraudulent emails that trick individuals into revealing se nsitive
",False,targeted_phishing,False,False,False
21828,"information or performing actions that beneﬁt the attacker s.
",False,targeted_phishing,False,False,False
21829,"This research paper aims to investigate the feasibility and
",False,targeted_phishing,False,False,False
21830,"effectiveness of NLMs in generating phishing emails. To thi s end,
",False,targeted_phishing,False,False,False
21831,"we propose a framework for evaluating the performance of NLM s
",False,targeted_phishing,False,False,False
21832,"in generating phishing emails based on various metrics, inc luding
",False,targeted_phishing,False,False,False
21833,"the quality of the generated text, the ability to bypass spam ﬁlters,
",False,targeted_phishing,False,False,False
21834,"and the success rate of tricking individuals into falling fo r the
",False,targeted_phishing,False,False,False
21835,"phishing attack.
",False,targeted_phishing,False,False,False
21836,"We evaluate the performance of several NLMs on a dataset
",False,targeted_phishing,False,False,False
21837,"of phishing emails and compare their results with those of a
",False,targeted_phishing,False,False,False
21838,"baseline model. Our results show that NLMs can indeed genera te
",False,targeted_phishing,False,False,False
21839,"phishing emails that are difﬁcult to detect and that have a hi gh
",False,targeted_phishing,False,False,False
21840,"success rate in tricking individuals. However, we also ﬁnd t hat
",False,targeted_phishing,False,False,False
21841,"the performance of NLMs in generating phishing emails depen ds
",False,targeted_phishing,False,False,False
21842,"on the speciﬁc NLM and the training data used, and that there
",False,targeted_phishing,False,False,False
21843,"are limitations to their effectiveness.
",False,targeted_phishing,False,False,False
21844,"Overall, our research suggests that NLMs have the potential
",False,targeted_phishing,False,False,False
21845,"to signiﬁcantly impact the landscape of phishing attacks an d
",False,targeted_phishing,False,False,False
21846,"highlights the need for further research on the ethical and
",False,targeted_phishing,False,False,False
21847,"security implications of using NLMs for malicious purposes .
",False,targeted_phishing,False,False,False
21848,"I. I NTRODUCTION
",False,targeted_phishing,False,False,False
21849,"Recent advances in natural language generation (NLG) have
",False,targeted_phishing,False,False,False
21850,"greatly improved the diversity, control, and quality of mac hine-
",False,targeted_phishing,False,False,False
21851,"generated text. However, this increased ability to quickly
",False,targeted_phishing,False,False,False
21852,"and efﬁciently create unique, manipulable, human-like tex t
",False,targeted_phishing,False,False,False
21853,"also presents new challenges for detecting the abuse of NLG
",False,targeted_phishing,False,False,False
21854,"models in phishing attacks.
",False,targeted_phishing,False,False,False
21855,"Machine-generated texts can pose various risks depending
",False,targeted_phishing,False,False,False
21856,"on the context and how they are used. For example, in the case
",False,targeted_phishing,False,False,False
21857,"of NLG models, the ability to generate legitimate texts atht
",False,targeted_phishing,False,False,False
21858,"looks like emails can lead to attacks like phishing, where th e
",False,targeted_phishing,False,False,False
21859,"attacker tricks the victim into disclosing sensitive infor mation
",False,targeted_phishing,False,False,False
21860,"by impersonating someone else.
",False,targeted_phishing,False,False,False
21861,"Another effect of machine generated text is mass dis-
",False,targeted_phishing,False,False,False
21862,"information campaigns. With the ability to generate large
",False,targeted_phishing,False,False,False
21863,"amounts of text automatically and quickly, it is possible fo r
",False,targeted_phishing,False,False,False
21864,"malicious actors to create fake news, hoaxes, and other form s
",False,targeted_phishing,False,False,False
21865,"of false or misleading information that can harm individual s,
",False,targeted_phishing,False,False,False
21866,"organizations, and even entire societies.Moreover, machine-generated texts can also raise ethical
",False,targeted_phishing,False,False,False
21867,"concerns, such as the impact on employment and the potential
",False,targeted_phishing,False,False,False
21868,"for bias and discrimination. For example, the use of NLG
",False,targeted_phishing,False,False,False
21869,"models to automate certain writing tasks may lead to job loss es
",False,targeted_phishing,False,False,False
21870,"for human writers, and the algorithms used in NLG may reﬂect
",False,targeted_phishing,False,False,False
21871,"and amplify the biases and stereotypes present in the data th ey
",False,targeted_phishing,False,False,False
21872,"are trained on.
",False,targeted_phishing,False,False,False
21873,"Abuses of NLG models, such as phishing [1],
",False,targeted_phishing,False,False,False
21874,"[2],disinformation[3], [4], [5] has been on the rise.
",False,targeted_phishing,False,False,False
21875,"Email is a common method used by phishers to deliver
",False,targeted_phishing,False,False,False
21876,"malicious links and attachments to victims. Anti-Phishing
",False,targeted_phishing,False,False,False
21877,"Working Group found over 121860 phishing email incidents
",False,targeted_phishing,False,False,False
21878,"in march 2017 and in 2016, the APWG received more than
",False,targeted_phishing,False,False,False
21879,"1313771 unique phishing reports. In the ﬁrst quarter of 2017 ,
",False,targeted_phishing,False,False,False
21880,"around 870 organizations were targeted by W2-based phishin g
",False,targeted_phishing,False,False,False
21881,"scams, a signiﬁcant increase from the 100 organizations in
",False,targeted_phishing,False,False,False
21882,"2016. These attacks are becoming more sophisticated and
",False,targeted_phishing,False,False,False
21883,"difﬁcult to detect.
",False,targeted_phishing,False,False,False
21884,"Phishers often use techniques such as bulk mailing, spam-
",False,targeted_phishing,False,False,False
21885,"ming, and including action words and links in phishing email s
",False,targeted_phishing,False,False,False
21886,"to increase their chances of success. However, these techni ques
",False,targeted_phishing,False,False,False
21887,"can be easily detected by improved statistical detection mo d-
",False,targeted_phishing,False,False,False
21888,"els. Another popular method is email masquerading, where th e
",False,targeted_phishing,False,False,False
21889,"attacker gains access to the victim’s email inbox or outbox
",False,targeted_phishing,False,False,False
21890,"and studies the content and nature of the emails to create a
",False,targeted_phishing,False,False,False
21891,"synthetic malicious email that resembles a benign one. This
",False,targeted_phishing,False,False,False
21892,"reduces the chances of detection by automated classiﬁers an d
",False,targeted_phishing,False,False,False
21893,"increases the likelihood of a successful attack. Modern lar ge
",False,targeted_phishing,False,False,False
21894,"language models have enabled users to generate text based
",False,targeted_phishing,False,False,False
21895,"on context. These models can be trained to generate text
",False,targeted_phishing,False,False,False
21896,"using predeﬁned grammars, such as the Dada Engine[1], or
",False,targeted_phishing,False,False,False
21897,"by leveraging deep learning neural networks, such as recurr ent
",False,targeted_phishing,False,False,False
21898,"neural networks (RNNs)[6], to learn and emulate the input to
",False,targeted_phishing,False,False,False
21899,"the system.
",False,targeted_phishing,False,False,False
21900,"NLG systems that use advanced deep learning neural net-
",False,targeted_phishing,False,False,False
21901,"works (DNNs) can be used by phishers to generate coherent
",False,targeted_phishing,False,False,False
21902,"and convincing sequences of text. These systems have been
",False,targeted_phishing,False,False,False
21903,"shown to be effective for generating text in various genres,
",False,targeted_phishing,False,False,False
21904,"from tweets[7] to poetry[8]. It is likely that phishers and
",False,targeted_phishing,False,False,False
21905,"spammers will soon start using email datasets, both legitim ate
",False,targeted_phishing,False,False,False
21906,"and malicious, in conjunction with DNNs to create decep-
",False,targeted_phishing,False,False,False
21907,"tive malicious emails that mimic the properties of legitima te
",False,targeted_phishing,False,False,False
21908,"emails. This makes it harder for pre-trained email detector s toidentify and block these attacks.
",False,targeted_phishing,False,False,False
21909,"In this report, we try to show a class of attacks where
",False,targeted_phishing,False,False,False
21910,"existing large-scale language models have been trained on b oth
",False,targeted_phishing,False,False,False
21911,"legitimate and malicious (phishing and spam) email data. We
",False,targeted_phishing,False,False,False
21912,"also aim to show how the generated emails can bypass existing
",False,targeted_phishing,False,False,False
21913,"production-level email protection mechanisms and propose a
",False,targeted_phishing,False,False,False
21914,"future work to detect such attacks.
",False,targeted_phishing,False,False,False
21915,"II. R ELATED WORK
",False,targeted_phishing,False,False,False
21916,"Phishing detection is a well-studied area in cybersecurity ,
",False,targeted_phishing,False,False,False
21917,"but many victims still fall for these attacks. In their work,
",False,targeted_phishing,False,False,False
21918,"Drake et al [9] provide a detailed analysis of the structure
",False,targeted_phishing,False,False,False
21919,"and tactics used in phishing emails. In this section, we
",False,targeted_phishing,False,False,False
21920,"review previous research on natural language generation, d eep
",False,targeted_phishing,False,False,False
21921,"learning, and their applications in generating and detecti ng
",False,targeted_phishing,False,False,False
21922,"phishing attacks.
",False,targeted_phishing,False,False,False
21923,"Natural language generation techniques have been widely
",False,targeted_phishing,False,False,False
21924,"used to synthesize unique pieces of text. Previous work by
",False,targeted_phishing,False,False,False
21925,"Reiter and Dale et al [10] relied on pre-constructed templat es
",False,targeted_phishing,False,False,False
21926,"for speciﬁc purposes, while the fake email generation syste m
",False,targeted_phishing,False,False,False
21927,"in Baki et al[1] used manually constructed rules to deﬁne the
",False,targeted_phishing,False,False,False
21928,"structure of fake emails. Recent advances in deep learning
",False,targeted_phishing,False,False,False
21929,"have enabled the generation of creative and equitable text
",False,targeted_phishing,False,False,False
21930,"with enough training data. RNN(Recurrent Neural Networks)
",False,targeted_phishing,False,False,False
21931,"language models are used to generate a range of genres,
",False,targeted_phishing,False,False,False
21932,"including poetry by Ghazvininejad et al [8], fake reviews by
",False,targeted_phishing,False,False,False
21933,"Yao et al [6], tweets [7], and geographical information by
",False,targeted_phishing,False,False,False
21934,"Turner et al [11], among others.
",False,targeted_phishing,False,False,False
21935,"III. E XPERIMENTAL METHODOLOGY
",False,targeted_phishing,False,False,False
21936,"The section is divided into four subsections. The ﬁrst
",False,targeted_phishing,False,False,False
21937,"subsection (Section 3.1) describes the nature and source of the
",False,targeted_phishing,False,False,False
21938,"training and evaluation data. The second subsection (Secti on
",False,targeted_phishing,False,False,False
21939,"3.2) discusses the pre-processing steps applied to the data . The
",False,targeted_phishing,False,False,False
21940,"third subsection (Section 3.3) presents the system setup an d
",False,targeted_phishing,False,False,False
21941,"experimental settings used in the study.
",False,targeted_phishing,False,False,False
21942,"A. Data Description
",False,targeted_phishing,False,False,False
21943,"To create a legitimate looking phishing email we ﬁrst need
",False,targeted_phishing,False,False,False
21944,"to start from actually benign and legitimate emails. The tex t
",False,targeted_phishing,False,False,False
21945,"generation algorithms must be trained in legitimate emails .
",False,targeted_phishing,False,False,False
21946,"Hence it was imperative to have valid benign emails in the
",False,targeted_phishing,False,False,False
21947,"dataset used for training. However, since the goal here is to
",False,targeted_phishing,False,False,False
21948,"create emails that even though can serve as a phishing email,
",False,targeted_phishing,False,False,False
21949,"should still look like legitimate emails, a mix of legitimat e and
",False,targeted_phishing,False,False,False
21950,"bad emails was used as a dataset for training and augmenting
",False,targeted_phishing,False,False,False
21951,"the models.
",False,targeted_phishing,False,False,False
21952,"For legitimate datasets, instead of using one dataset on our
",False,targeted_phishing,False,False,False
21953,"own, we use pre-trained models from Meta and Google to cre-
",False,targeted_phishing,False,False,False
21954,"ate benign emails. The pre-trained models utilized are Robe rta,
",False,targeted_phishing,False,False,False
21955,"The Pile, and PushShift.io Reddit. Since training these lar ge
",False,targeted_phishing,False,False,False
21956,"language models is almost impossible in normal infrastruct ure,
",False,targeted_phishing,False,False,False
21957,"we utilize [12] to generate the texts. This has been augmente d
",False,targeted_phishing,False,False,False
21958,"with [13] to have email generation capabilities. Python cle an
",False,targeted_phishing,False,False,False
21959,"text [14] has been used to remove email, and phone numbers
",False,targeted_phishing,False,False,False
21960,"from the dataset.For malicious datasets, we primarily use two datasets to
",False,targeted_phishing,False,False,False
21961,"augment the benign email data. Notably, the Phishing emails
",False,targeted_phishing,False,False,False
21962,"from Jose Nazario’s Phishing corpus [15] and [16] along with
",False,targeted_phishing,False,False,False
21963,"the Enron email dataset [17].
",False,targeted_phishing,False,False,False
21964,"B. Data Processing
",False,targeted_phishing,False,False,False
21965,"Most of the pre-processing was done by trying to remove
",False,targeted_phishing,False,False,False
21966,"personal information using Python clean text [14]. As well a s
",False,targeted_phishing,False,False,False
21967,"Removal of special characters like , #, $, % as well as common
",False,targeted_phishing,False,False,False
21968,"punctuations from the email body.
",False,targeted_phishing,False,False,False
21969,"However, as we have realized later generating emails was
",False,targeted_phishing,False,False,False
21970,"not perfect.
",False,targeted_phishing,False,False,False
21971,"C. Experimental Setup
",False,targeted_phishing,False,False,False
21972,"The experimental setup has been designed with certain
",False,targeted_phishing,False,False,False
21973,"different methods in mind. We primarily focused on
",False,targeted_phishing,False,False,False
21974,"•Using GPT-2 to generate emails. Augmented with email
",False,targeted_phishing,False,False,False
21975,"dataset [18]
",False,targeted_phishing,False,False,False
21976,"•GPT-3 to generate emails without any training
",False,targeted_phishing,False,False,False
21977,"•Contextual support for GPT-3 with da-vinci-beta which
",False,targeted_phishing,False,False,False
21978,"has been trained in email by openai
",False,targeted_phishing,False,False,False
21979,"•The DADA engine [1]
",False,targeted_phishing,False,False,False
21980,"•Word based RNN’s proposed by Xie et al [19], Das et al
",False,targeted_phishing,False,False,False
21981,"[20]
",False,targeted_phishing,False,False,False
21982,"•Augmenting Open Pre-trained Transformer Language
",False,targeted_phishing,False,False,False
21983,"Models[12] on [13]
",False,targeted_phishing,False,False,False
21984,"While using the general large language models were
",False,targeted_phishing,False,False,False
21985,"interesting in trying to produce emails.
",False,targeted_phishing,False,False,False
21986,"The spam and phishing email datasets used for train-
",False,targeted_phishing,False,False,False
21987,"ing the models to produce malicious looking email
",False,targeted_phishing,False,False,False
21988,"produced better results. The Jose Nazario dataset has
",False,targeted_phishing,False,False,False
21989,"32,000 spams and 415 phishing email. These are all
",False,targeted_phishing,False,False,False
21990,"in Unix mbox formatted dataset which were cleaned
",False,targeted_phishing,False,False,False
21991,"using clean-text.
",False,targeted_phishing,False,False,False
21992,"The Enron corpus was email dataset from Enron
",False,targeted_phishing,False,False,False
21993,"Corporation. It has been used in email research for
",False,targeted_phishing,False,False,False
21994,"quite some time and was made public during their legal
",False,targeted_phishing,False,False,False
21995,"investigation. This however gives us a rare glimpse
",False,targeted_phishing,False,False,False
21996,"in the working of legitimate email data of a big
",False,targeted_phishing,False,False,False
21997,"corporation. It consists of 619446 emails from 158
",False,targeted_phishing,False,False,False
21998,"users. It has folders like ”discussion threads, ”notes
",False,targeted_phishing,False,False,False
21999,"inbox” and more in its outlook inboxes. These were
",False,targeted_phishing,False,False,False
22000,"created by default for most of the users but were nota
",False,targeted_phishing,False,False,False
22001,"ctively used for tagging. The ”All Documents” folder
",False,targeted_phishing,False,False,False
22002,"present also had large number of duplicates present
",False,targeted_phishing,False,False,False
22003,"from inbox. To further clean our dataset, these all were
",False,targeted_phishing,False,False,False
22004,"removed. The cleaned corpus has 200399 emails from
",False,targeted_phishing,False,False,False
22005,"158 users.
",False,targeted_phishing,False,False,False
22006,"We have tried to validate our ways with existing ﬁnd-
",False,targeted_phishing,False,False,False
22007,"ings available here https://github.com/egmp777/basic
",False,targeted_phishing,False,False,False
22008,"data cleaning enron case study. But our purpose
",False,targeted_phishing,False,False,False
22009,"was different than exploratory analysis.D. Experiment
",False,targeted_phishing,False,False,False
22010,"A PoC was done to evaluate if we can auto-detect any emails
",False,targeted_phishing,False,False,False
22011,"from the generated text. GPT3 API, GPT2, OPT, Chatbot
",False,targeted_phishing,False,False,False
22012,"and our trained models were all used to generate the texts.
",False,targeted_phishing,False,False,False
22013,"Some samples have been provided below. In our test we had
",False,targeted_phishing,False,False,False
22014,"generated 100 samples using each. However some of these
",False,targeted_phishing,False,False,False
22015,"gave us inconsistent result for a same prompt. We also had
",False,targeted_phishing,False,False,False
22016,"sample fo real phishing email (only one for that topic) to
",False,targeted_phishing,False,False,False
22017,"compare it with.
",False,targeted_phishing,False,False,False
22018,"A variety fo prompts were used to see what kind of emails
",False,targeted_phishing,False,False,False
22019,"the AI’s are more comfortable generating.
",False,targeted_phishing,False,False,False
22020,"We applied the GPT-3 davinci model with default sampling
",False,targeted_phishing,False,False,False
22021,"parameters (temperature=1, top p=1, presence penalty=0, fre-
",False,targeted_phishing,False,False,False
22022,"quency penalty=0) to predict the log probabilities for the ﬁrst
",False,targeted_phishing,False,False,False
22023,"100 tokens in each sample using all the preceding tokens as
",False,targeted_phishing,False,False,False
22024,"the prompt. We then calculated the actual token’s probabili ty,
",False,targeted_phishing,False,False,False
22025,"absolute rank, contextual entropy, and whether it matched
",False,targeted_phishing,False,False,False
22026,"the predicted token by the GPT-3 API. To account for the
",False,targeted_phishing,False,False,False
22027,"maximum of 100 log probabilities returned by the API, we
",False,targeted_phishing,False,False,False
22028,"masked out any outliers with an absolute rank greater than
",False,targeted_phishing,False,False,False
22029,"100. We also normalized the results if there were less than
",False,targeted_phishing,False,False,False
22030,"100 tokens in a sample.
",False,targeted_phishing,False,False,False
22031,"IV. A NALYSIS AND RESULTS
",False,targeted_phishing,False,False,False
22032,"1) Example Results generated: ChatGPT : The following
",False,targeted_phishing,False,False,False
22033,"was generated with the prompt Can you generate an email that
",False,targeted_phishing,False,False,False
22034,"will be from The President of a University asking its student s
",False,targeted_phishing,False,False,False
22035,"to ﬁll up course completion survey form
",False,targeted_phishing,False,False,False
22036,"Dear Students,
",False,targeted_phishing,False,False,False
22037,"As the academic year comes to a close, we would
",False,targeted_phishing,False,False,False
22038,"like to gather your feedback on your experiences and
",False,targeted_phishing,False,False,False
22039,"courses at the University. To help us improve our
",False,targeted_phishing,False,False,False
22040,"programs and support your learning, we encourage
",False,targeted_phishing,False,False,False
22041,"you to take a few minutes to complete the course
",False,targeted_phishing,False,False,False
22042,"completion survey form.
",False,targeted_phishing,False,False,False
22043,"Your honest and thoughtful responses will be greatly
",False,targeted_phishing,False,False,False
22044,"appreciated and will help us better understand your
",False,targeted_phishing,False,False,False
22045,"needs and expectations. The survey form is available
",False,targeted_phishing,False,False,False
22046,"online and can be accessed through your student portal.
",False,targeted_phishing,False,False,False
22047,"We thank you in advance for your participation and
",False,targeted_phishing,False,False,False
22048,"look forward to hearing from you.
",False,targeted_phishing,False,False,False
22049,"Best regards,
",False,targeted_phishing,False,False,False
22050,"[Name], President [University]
",False,targeted_phishing,False,False,False
22051,"GPT3 da-vinci : The following was generated with the
",False,targeted_phishing,False,False,False
22052,"prompt Can you generate an email that will be from The
",False,targeted_phishing,False,False,False
22053,"President of a University asking its students to ﬁll up cours e
",False,targeted_phishing,False,False,False
22054,"completion survey formHello,
",False,targeted_phishing,False,False,False
22055,"As the President of the University, I am writing to
",False,targeted_phishing,False,False,False
22056,"request your help in completing a course completion
",False,targeted_phishing,False,False,False
22057,"survey form. This form will help us to understand how
",False,targeted_phishing,False,False,False
22058,"well our students are doing in their courses and how
",False,targeted_phishing,False,False,False
22059,"we can improve our offerings.
",False,targeted_phishing,False,False,False
22060,"I would greatly appreciate it if you could take a few
",False,targeted_phishing,False,False,False
22061,"minutes to ﬁll out the form. You can access the form
",False,targeted_phishing,False,False,False
22062,"at the following link:
",False,targeted_phishing,False,False,False
22063,"[insert form URL here]
",False,targeted_phishing,False,False,False
22064,"Thank you for your time and assistance.
",False,targeted_phishing,False,False,False
22065,"Sincerely,
",False,targeted_phishing,False,False,False
22066,"[Your Name]
",False,targeted_phishing,False,False,False
22067,"GPT2 : The following was generated with the prompt of
",False,targeted_phishing,False,False,False
22068,"Hello, As the President of the University, I am writing to
",False,targeted_phishing,False,False,False
22069,"request your help in completing a course completion survey
",False,targeted_phishing,False,False,False
22070,"form.
",False,targeted_phishing,False,False,False
22071,"Hello,
",False,targeted_phishing,False,False,False
22072,"As the President of the University, I am writing to
",False,targeted_phishing,False,False,False
22073,"request your help in completing a course completion
",False,targeted_phishing,False,False,False
22074,"survey form. As requested, we would like your answer
",False,targeted_phishing,False,False,False
22075,"by May NUMBER, NUMBER. The survey should
",False,targeted_phishing,False,False,False
22076,"only take a few minutes of your time (it takes only
",False,targeted_phishing,False,False,False
22077,"about NUMBER minutes to complete the survey). All
",False,targeted_phishing,False,False,False
22078,"survey responses are anonymous and conﬁdential, and
",False,targeted_phishing,False,False,False
22079,"will not be shared outside of our University commu-
",False,targeted_phishing,False,False,False
22080,"nity. If you have
",False,targeted_phishing,False,False,False
22081,"OPT : The following was generated with the prompt of
",False,targeted_phishing,False,False,False
22082,"Hello, As the President of the University, I am writing to
",False,targeted_phishing,False,False,False
22083,"request your help in completing a course completion survey
",False,targeted_phishing,False,False,False
22084,"form.
",False,targeted_phishing,False,False,False
22085,"(¡PROMPT¿ Hello, As the President of the University,
",False,targeted_phishing,False,False,False
22086,"I am writing to request your help in completing a
",False,targeted_phishing,False,False,False
22087,"course completion survey form.¡END-OF-PROMPT¿
",False,targeted_phishing,False,False,False
22088,"The purpose of this survey is to gather feedback
",False,targeted_phishing,False,False,False
22089,"from ¡NUMBER¿ current and former students on their
",False,targeted_phishing,False,False,False
22090,"experiences at UofH. Please complete the survey no
",False,targeted_phishing,False,False,False
22091,"later than Friday, October 00th by sending it back to
",False,targeted_phishing,False,False,False
22092,"me via fax no. ¡PHONE¿. Thank you very much for
",False,targeted_phishing,False,False,False
22093,"your time. Best Regards,
",False,targeted_phishing,False,False,False
22094,"UofH Ofﬁce of Admissions ¡EMAIL¿ (See attached
",False,targeted_phishing,False,False,False
22095,"ﬁle: uofh0000 sample.pdf))
",False,targeted_phishing,False,False,False
22096,"The most interesting part of this prompt was the ending
",False,targeted_phishing,False,False,False
22097,"where it signed off as UofH Ofﬁce of admissions and a sample
",False,targeted_phishing,False,False,False
22098,"pdf. Whereas our prompt never indicates those details[21].
",False,targeted_phishing,False,False,False
22099,"Further testing with the GPT2 dataset produced the
",False,targeted_phishing,False,False,False
22100,"following output from the same prompt [21]Hello,
",False,targeted_phishing,False,False,False
22101,"As the President of the University, I am writing to
",False,targeted_phishing,False,False,False
22102,"request your help in completing a course completion
",False,targeted_phishing,False,False,False
22103,"survey form. By completing this form, you are wel-
",False,targeted_phishing,False,False,False
22104,"come to access the COMPANY Club website and ask
",False,targeted_phishing,False,False,False
22105,"additional questions about the UCLA Club and our
",False,targeted_phishing,False,False,False
22106,"events. To access the website you may click the link
",False,targeted_phishing,False,False,False
22107,"at the top of this page.If you prefer to not complete
",False,targeted_phishing,False,False,False
22108,"this form at this time, please let me know and I will
",False,targeted_phishing,False,False,False
22109,"Notably, UCLA was not present in the prompt. This shows
",False,targeted_phishing,False,False,False
22110,"us that with enough clever prompt discovery it is probably
",False,targeted_phishing,False,False,False
22111,"possible to extract meaningful information from the traine d
",False,targeted_phishing,False,False,False
22112,"dataset even with safeguards in place.
",False,targeted_phishing,False,False,False
22113,"2) Training Parameters: The training parameters used for
",False,targeted_phishing,False,False,False
22114,"the HF opt model was
",False,targeted_phishing,False,False,False
22115,"•learning rate:6e−5
",False,targeted_phishing,False,False,False
22116,"•train batch size: 8
",False,targeted_phishing,False,False,False
22117,"•eval batch size: 8
",False,targeted_phishing,False,False,False
22118,"•seed: 42
",False,targeted_phishing,False,False,False
22119,"•distributed type: GPU
",False,targeted_phishing,False,False,False
22120,"•gradient accumulation steps: 16
",False,targeted_phishing,False,False,False
22121,"•total train batch size: 128
",False,targeted_phishing,False,False,False
22122,"•optimizer: Adam with betas = (0.9, 0.999) and epsilon =
",False,targeted_phishing,False,False,False
22123,"1e−8
",False,targeted_phishing,False,False,False
22124,"•lrscheduler type: cosine
",False,targeted_phishing,False,False,False
22125,"•lrscheduler warmup ratio: 0.03
",False,targeted_phishing,False,False,False
22126,"•num epochs: 8
",False,targeted_phishing,False,False,False
22127,"And the training parameters used for HF postbot GPT2
",False,targeted_phishing,False,False,False
22128,"•learning rate: 0.001
",False,targeted_phishing,False,False,False
22129,"•train batch size: 16
",False,targeted_phishing,False,False,False
22130,"•eval batch size: 16
",False,targeted_phishing,False,False,False
22131,"•seed: 42
",False,targeted_phishing,False,False,False
22132,"•distributed type: multi-GPU
",False,targeted_phishing,False,False,False
22133,"•gradient accumulation steps: 8
",False,targeted_phishing,False,False,False
22134,"•total train batch size: 128
",False,targeted_phishing,False,False,False
22135,"•optimizer: Adam with betas = (0.9, 0.999) and epsilon =
",False,targeted_phishing,False,False,False
22136,"1e−8
",False,targeted_phishing,False,False,False
22137,"•lrscheduler type: cosine
",False,targeted_phishing,False,False,False
22138,"•lrscheduler warmup ratio: 0.02
",False,targeted_phishing,False,False,False
22139,"•num epochs: 3
",False,targeted_phishing,False,False,False
22140,"V. F UTURE WORK
",False,targeted_phishing,False,False,False
22141,"Research on the risks of using natural language generation
",False,targeted_phishing,False,False,False
22142,"(NLG) models suggests that being able to detect machine-
",False,targeted_phishing,False,False,False
22143,"generated text is useful for reducing the harm caused by abus e
",False,targeted_phishing,False,False,False
22144,"of these models. When we want to detect machine-generated
",False,targeted_phishing,False,False,False
22145,"text, it can be treated as a binary classiﬁcation problem. We
",False,targeted_phishing,False,False,False
22146,"train a classiﬁer to differentiate between machine-genera ted
",False,targeted_phishing,False,False,False
22147,"and human-generated text [22].
",False,targeted_phishing,False,False,False
22148,"We can use generative models without ﬁne-tuning to detect
",False,targeted_phishing,False,False,False
22149,"their own outputs or the outputs of other similar models.
",False,targeted_phishing,False,False,False
22150,"Autoregressive generative models like GPT-2, GPT-3 are uni -
",False,targeted_phishing,False,False,False
22151,"directional, where each token has an embedding that depends
",False,targeted_phishing,False,False,False
22152,"on the embeddings of the tokens that come before it. Thisshows us that an embedding can be created if we add a token
",False,targeted_phishing,False,False,False
22153,"at the end of an input sequence, thus creating a sequence
",False,targeted_phishing,False,False,False
22154,"of tokens. This now can be used as a new feature vector.
",False,targeted_phishing,False,False,False
22155,"Now once we have these newly created features, they can be
",False,targeted_phishing,False,False,False
22156,"utilized along with human data to train a layer of neurons for
",False,targeted_phishing,False,False,False
22157,"classiﬁcation.
",False,targeted_phishing,False,False,False
22158,"Research on how to detect machine-generated text has
",False,targeted_phishing,False,False,False
22159,"looked at the problem of detecting text when a different
",False,targeted_phishing,False,False,False
22160,"dataset was used to train RoBERTa than GPT-2. But here,
",False,targeted_phishing,False,False,False
22161,"it was observed that just tuning the detection model with
",False,targeted_phishing,False,False,False
22162,"couple of hundred different attack samples provided by doma in
",False,targeted_phishing,False,False,False
22163,"esperts had a signiﬁcant effect on the detector’s performan ce
",False,targeted_phishing,False,False,False
22164,"on different domains[23].
",False,targeted_phishing,False,False,False
22165,"One another possibility is when an attacker decides to gener -
",False,targeted_phishing,False,False,False
22166,"ate the attack from an existing hand-written content. Much l ike
",False,targeted_phishing,False,False,False
22167,"how we have started in this email generation problem. Using
",False,targeted_phishing,False,False,False
22168,"human like sample but tweaking the generating parameters to
",False,targeted_phishing,False,False,False
22169,"closely meet his goals. Analysis showed that making these
",False,targeted_phishing,False,False,False
22170,"targeted changes to texts reduces the effectiveness of GPT- 2
",False,targeted_phishing,False,False,False
22171,"or RoBerta-based detectors [24].
",False,targeted_phishing,False,False,False
22172,"A generalized solution to this is trying to differentiate
",False,targeted_phishing,False,False,False
22173,"between human and machine generated text. Giant Language
",False,targeted_phishing,False,False,False
22174,"Model Test Room is a software developed to improve the
",False,targeted_phishing,False,False,False
22175,"detection of machine-generated text by adding human review
",False,targeted_phishing,False,False,False
22176,"in the pipeline. The tool helps humans classify text by high-
",False,targeted_phishing,False,False,False
22177,"lighting texts based on how likely of them being chosen by the
",False,targeted_phishing,False,False,False
22178,"Transformer model. However, this tool was designed to targe t
",False,targeted_phishing,False,False,False
22179,"GPT-2, which was found to be easier for untrained human
",False,targeted_phishing,False,False,False
22180,"evaluators to detect. In addition, GLTR uses ”top-k” sampli ng
",False,targeted_phishing,False,False,False
22181,"to determine the likelihood of a word being selected, but thi s
",False,targeted_phishing,False,False,False
22182,"method has been largely replaced by nucleus sampling, which
",False,targeted_phishing,False,False,False
22183,"is used in GPT-3 and other works that build on the GPT-
",False,targeted_phishing,False,False,False
22184,"2 architecture. While highlighting words based on sampling
",False,targeted_phishing,False,False,False
22185,"likelihood may improve human classiﬁcation ability, it is c lear
",False,targeted_phishing,False,False,False
22186,"that it still will pose a problem when they have to detect the
",False,targeted_phishing,False,False,False
22187,"more advanced models and sampling methods of today.
",False,targeted_phishing,False,False,False
22188,"In long term, we want to propose a framework that can
",False,targeted_phishing,False,False,False
22189,"differentiate NLG-generated emails from human-generated
",False,targeted_phishing,False,False,False
22190,"emails. Prior work has already been done trying to determine
",False,targeted_phishing,False,False,False
22191,"machine-generated text, however speciﬁcally for email and
",False,targeted_phishing,False,False,False
22192,"malicious emails, there are distinct characteristics we ha ve
",False,targeted_phishing,False,False,False
22193,"observed that can be exploited to augment prior works to be
",False,targeted_phishing,False,False,False
22194,"more effective. Few of these are homogeneous to what we
",False,targeted_phishing,False,False,False
22195,"have seen in language models [25], but some are signiﬁcantly
",False,targeted_phishing,False,False,False
22196,"distinct and should be explored more.
",False,targeted_phishing,False,False,False
22197,"VI. C ONCLUSION
",False,targeted_phishing,False,False,False
22198,"The more we experimented with large language models
",False,targeted_phishing,False,False,False
22199,"and prior works by Das et al [20], Baki et al [1] it became
",False,targeted_phishing,False,False,False
22200,"clear that prior RNN-based models and DIDA engines, even
",False,targeted_phishing,False,False,False
22201,"though show some malicious intent in their generation, don’ t
",False,targeted_phishing,False,False,False
22202,"actually pose threat to be understood as real malicious emai l.
",False,targeted_phishing,False,False,False
22203,"All of them went past Gmail and outlook when sent from
",False,targeted_phishing,False,False,False
22204,"a legitimate email id. The emails generated by GPT3 and
",False,targeted_phishing,False,False,False
22205,"OPT signiﬁcantly pose a larger threat to be believed as real
",False,targeted_phishing,False,False,False
22206,"emails when generated in mass using tools and bulk emailedwith targeted intent. Especially with targeted email datas et
",False,targeted_phishing,False,False,False
22207,"training and keywords in prompts, the models generated very
",False,targeted_phishing,False,False,False
22208,"convincing-looking emails. Even with safeguards in place f or
",False,targeted_phishing,False,False,False
22209,"GPT3, we were able to generate these emails and chatGPT
",False,targeted_phishing,False,False,False
22210,"was a very interesting contender in the tests. Even though
",False,targeted_phishing,False,False,False
22211,"chatgpt didn’t let us generate the email directly in one go, w e
",False,targeted_phishing,False,False,False
22212,"were able to ﬁnd creative ways by ’conversing’ with it and
",False,targeted_phishing,False,False,False
22213,"giving it a plausible context to overcome its barriers. Here we
",False,targeted_phishing,False,False,False
22214,"identify how these new language models can be weaponized
",False,targeted_phishing,False,False,False
22215,"to be used as phishing and scamming tools which gets past our
",False,targeted_phishing,False,False,False
22216,"present email systems like Gmail and Outlook. However, that ’s
",False,targeted_phishing,False,False,False
22217,"hardly surprising considering they look legitimate. We wan t to
",False,targeted_phishing,False,False,False
22218,"further this work by integrating it with tools like PhEmail[ 26]
",False,targeted_phishing,False,False,False
22219,"which makes sending NLG generated emails to targeted bulk
",False,targeted_phishing,False,False,False
22220,"userbase a keypress away.
",False,targeted_phishing,False,False,False
22221,"REFERENCES
",False,targeted_phishing,False,False,False
22222,"[1] S. Baki, R. Verma, A. Mukherjee, and O. Gnawali, “Scaling and
",False,targeted_phishing,False,False,False
22223,"effectiveness of email masquerade attacks: Exploiting nat ural language
",False,targeted_phishing,False,False,False
22224,"generation,” in Proceedings of the 2017 ACM on Asia Conference on
",False,targeted_phishing,False,False,False
22225,"Computer and Communications Security , 2017, pp. 469–482.
",False,targeted_phishing,False,False,False
22226,"[2] A. Giaretta and N. Dragoni, “Community targeted phishin g,” in Inter-
",False,targeted_phishing,False,False,False
22227,"national Conference in Software Engineering for Defence Ap plications .
",False,targeted_phishing,False,False,False
22228,"Springer, 2018, pp. 86–93.
",False,targeted_phishing,False,False,False
22229,"[3] K. Shu, S. Wang, D. Lee, and H. Liu, “Mining disinformatio n and fake
",False,targeted_phishing,False,False,False
22230,"news: Concepts, methods, and recent advancements,” in Disinformation,
",False,targeted_phishing,False,False,False
22231,"misinformation, and fake news in social media . Springer, 2020, pp. 1–
",False,targeted_phishing,False,False,False
22232,"19.
",False,targeted_phishing,False,False,False
22233,"[4] H. Stiff and F. Johansson, “Detecting computer-generat ed disinforma-
",False,targeted_phishing,False,False,False
22234,"tion,” International Journal of Data Science and Analytics , vol. 13, no. 4,
",False,targeted_phishing,False,False,False
22235,"pp. 363–383, 2022.
",False,targeted_phishing,False,False,False
22236,"[5] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner,
",False,targeted_phishing,False,False,False
22237,"and Y . Choi, “Defending against neural fake news,” Advances in neural
",False,targeted_phishing,False,False,False
22238,"information processing systems , vol. 32, 2019.
",False,targeted_phishing,False,False,False
22239,"[6] Y . Yao, B. Viswanath, J. Cryan, H. Zheng, and B. Y . Zhao, “A uto-
",False,targeted_phishing,False,False,False
22240,"mated crowdturﬁng attacks and defenses in online review sys tems,” in
",False,targeted_phishing,False,False,False
22241,"Proceedings of the 2017 ACM SIGSAC conference on computer an d
",False,targeted_phishing,False,False,False
22242,"communications security , 2017, pp. 1143–1158.
",False,targeted_phishing,False,False,False
22243,"[7] P. Sidhaye and J. C. K. Cheung, “Indicative tweet generat ion: An extrac-
",False,targeted_phishing,False,False,False
22244,"tive summarization problem?” in Proceedings of the 2015 Conference on
",False,targeted_phishing,False,False,False
22245,"Empirical Methods in Natural Language Processing , 2015, pp. 138–147.
",False,targeted_phishing,False,False,False
22246,"[8] M. Ghazvininejad, X. Shi, Y . Choi, and K. Knight, “Genera ting topical
",False,targeted_phishing,False,False,False
22247,"poetry,” in Proceedings of the 2016 Conference on Empirical Methods
",False,targeted_phishing,False,False,False
22248,"in Natural Language Processing , 2016, pp. 1183–1191.
",False,targeted_phishing,False,False,False
22249,"[9] C. E. Drake, J. J. Oliver, and E. J. Koontz, “Anatomy of a ph ishing
",False,targeted_phishing,False,False,False
22250,"email.” in CEAS , 2004.
",False,targeted_phishing,False,False,False
22251,"[10] M. A. Covington, “Building natural language generatio n systems,”
",False,targeted_phishing,False,False,False
22252,"Language , vol. 77, no. 3, pp. 611–612, 2001.
",False,targeted_phishing,False,False,False
22253,"[11] R. Turner, S. Sripada, and E. Reiter, “Generating appro ximate geo-
",False,targeted_phishing,False,False,False
22254,"graphic descriptions,” in Empirical methods in natural language gen-
",False,targeted_phishing,False,False,False
22255,"eration . Springer, 2009, pp. 121–140.
",False,targeted_phishing,False,False,False
22256,"[12] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Che n, C. Dewan,
",False,targeted_phishing,False,False,False
22257,"M. Diab, X. Li, X. V . Lin et al. , “Opt: Open pre-trained transformer
",False,targeted_phishing,False,False,False
22258,"language models,” arXiv preprint arXiv:2205.01068 , 2022.
",False,targeted_phishing,False,False,False
22259,"[13] R. Zhang and J. Tetreault, “This email could save your li fe: In-
",False,targeted_phishing,False,False,False
22260,"troducing the task of email subject line generation,” arXiv preprint
",False,targeted_phishing,False,False,False
22261,"arXiv:1906.03497 , 2019.
",False,targeted_phishing,False,False,False
22262,"[14] (2022) clean-text · pypi. [Online]. Available:
",False,targeted_phishing,False,False,False
22263,"https://pypi.org/project/clean-text/
",False,targeted_phishing,False,False,False
22264,"[15] H. Gonzalez, K. Nance, and J. Nazario, “Phishing by form : The abuse
",False,targeted_phishing,False,False,False
22265,"of form sites,” in 2011 6th International Conference on Malicious and
",False,targeted_phishing,False,False,False
22266,"Unwanted Software . IEEE, 2011, pp. 95–101.
",False,targeted_phishing,False,False,False
22267,"[16] (2000) Jose malicious email dataset:
",False,targeted_phishing,False,False,False
22268,"https://monkey.org/ jose/wiki/doku.php Link Deprecated ,
",False,targeted_phishing,False,False,False
22269,"Uploaded to my own github. [Online]. Available:
",False,targeted_phishing,False,False,False
22270,"https://monkey.org/ ∼jose/wiki/doku.php
",False,targeted_phishing,False,False,False
22271,"[17] J. Shetty and J. Adibi, “The enron email dataset databas e schema and
",False,targeted_phishing,False,False,False
22272,"brief statistical report,” Information sciences institute technical report,
",False,targeted_phishing,False,False,False
22273,"University of Southern California , vol. 4, no. 1, pp. 120–128, 2004.[18] (2022) email-blog — kaggle:
",False,targeted_phishing,False,False,False
22274,"https://www.kaggle.com/datasets/mikeschmidtavemac/e mailblog. [On-
",False,targeted_phishing,False,False,False
22275,"line]. Available: https://www.kaggle.com/datasets/mik eschmidtavemac/emailblog
",False,targeted_phishing,False,False,False
22276,"[19] Z. Xie, “Neural text generation: A practical guide,” arXiv preprint
",False,targeted_phishing,False,False,False
22277,"arXiv:1711.09534 , 2017.
",False,targeted_phishing,False,False,False
22278,"[20] A. Das and R. Verma, “Automated email generation for tar geted attacks
",False,targeted_phishing,False,False,False
22279,"using natural language,” arXiv preprint arXiv:1908.06893 , 2019.
",False,targeted_phishing,False,False,False
22280,"[21] (2022) rabimba/email-gen-nlg: https://github.com/ rabimba/email-gen-
",False,targeted_phishing,False,False,False
22281,"nlg. [Online]. Available: https://github.com/rabimba/e mail-gen-nlg
",False,targeted_phishing,False,False,False
22282,"[22] E. Crothers, N. Japkowicz, H. Viktor, and P. Branco, “Ad versarial
",False,targeted_phishing,False,False,False
22283,"robustness of neural-statistical features in detection of generative trans-
",False,targeted_phishing,False,False,False
22284,"formers,” arXiv preprint arXiv:2203.07983 , 2022.
",False,targeted_phishing,False,False,False
22285,"[23] J. Rodriguez, T. Hay, D. Gros, Z. Shamsi, and R. Srinivas an, “Cross-
",False,targeted_phishing,False,False,False
22286,"domain detection of gpt-2-generated technical text,” in Proceedings of
",False,targeted_phishing,False,False,False
22287,"the 2022 Conference of the North American Chapter of the Asso ciation
",False,targeted_phishing,False,False,False
22288,"for Computational Linguistics: Human Language Technologi es, 2022,
",False,targeted_phishing,False,False,False
22289,"pp. 1213–1233.
",False,targeted_phishing,False,False,False
22290,"[24] M. M. Bhat and S. Parthasarathy, “How effectively can ma chines
",False,targeted_phishing,False,False,False
22291,"defend against machine-generated fake news? an empirical s tudy,” in
",False,targeted_phishing,False,False,False
22292,"Proceedings of the First Workshop on Insights from Negative Results in
",False,targeted_phishing,False,False,False
22293,"NLP, 2020, pp. 48–53.
",False,targeted_phishing,False,False,False
22294,"[25] S. Gehrmann, H. Strobelt, and A. M. Rush, “GLTR: statist ical detection
",False,targeted_phishing,False,False,False
22295,"and visualization of generated text,” CoRR , vol. abs/1906.04043, 2019.
",False,targeted_phishing,False,False,False
22296,"[Online]. Available: http://arxiv.org/abs/1906.04043
",False,targeted_phishing,False,False,False
22297,"[26] (2022) dionach/phemail: Phemail is a python open sourc e phishing
",False,targeted_phishing,False,False,False
22298,"email tool that automates the process of sending phishing
",False,targeted_phishing,False,False,False
22299,"emails as part of a social engineering test. [Online]. Avail able:
",False,targeted_phishing,False,False,False
22300,https://github.com/Dionach/PhEmail,False,targeted_phishing,False,False,False
22301,https://github.com/Dionach/PhEmail,False,targeted_phishing,True,False,False
22302,https://github.com/Dionach/PhEmail,False,targeted_phishing,True,False,False
22303,"Abstract. The nonverbal content of speech carries information about
",True,VoiceStress_Analysis,False,False,True
22304,"1 Introduction
",True,VoiceStress_Analysis,False,False,True
22305,"2 Related Work
",True,VoiceStress_Analysis,False,False,True
22306,"3 Experimental Design
",True,VoiceStress_Analysis,False,False,True
22307,"3.2 Jitter
",True,VoiceStress_Analysis,False,False,True
22308,"4 Experimental Results
",True,VoiceStress_Analysis,False,False,True
22309,"Voice Stress Analysis
",False,VoiceStress_Analysis,False,False,True
22310,"L.J.M. Rothkrantz, P. Wiggers, J.W.A. van Wees, R.J. van Vark
",False,VoiceStress_Analysis,False,False,True
22311,"Data and Knowledge Systems Group
",False,VoiceStress_Analysis,False,False,True
22312,"Delft University of Technology
",False,VoiceStress_Analysis,False,False,True
22313,"Mekelweg 4, 2628 CD Delft, The Netherlands
",False,VoiceStress_Analysis,False,False,True
22314,"l.j.m.rothkrantz@ewi.tudelft.nl
",False,VoiceStress_Analysis,False,False,True
22315,"p.wiggers@ewi.tudelft.nl
",False,VoiceStress_Analysis,False,False,True
22316,"the physiological and psychological condition of the speaker. Psycholog-
",False,VoiceStress_Analysis,False,False,True
22317,"ical stress is a pathological element of this condition, of which the cause
",False,VoiceStress_Analysis,False,False,True
22318,"is accepted to be “workload”. Objective, quantiﬁable correlates of stress
",False,VoiceStress_Analysis,False,False,True
22319,"are searched for by means of measuring the acoustic modiﬁcations of the
",False,VoiceStress_Analysis,False,False,True
22320,"voice brought about by workload. Diﬀerent voice features from the speech
",False,VoiceStress_Analysis,False,False,True
22321,"signal to be inﬂuenced by stress are: loudness, fundamental frequency,
",False,VoiceStress_Analysis,False,False,True
22322,"jitter, zero-crossing rate, speech rate and high-energy frequency ratio. To
",False,VoiceStress_Analysis,False,False,True
22323,"examine the eﬀect of workload on speech production an experiment was
",False,VoiceStress_Analysis,False,False,True
22324,"designed. 108 native speakers of Dutch were recruited to participate in
",False,VoiceStress_Analysis,False,False,True
22325,"a stress test (Stroop test). The experiment and the analysis of the test
",False,VoiceStress_Analysis,False,False,True
22326,"results will be reported in this paper.
",False,VoiceStress_Analysis,False,False,True
22327,"1 Introduction
",False,VoiceStress_Analysis,False,False,True
22328,"Although speech is a vocal activity of which much is verbal, there are a number
",False,VoiceStress_Analysis,False,False,True
22329,"of human vocalizations that are essentially non-linguistic. Nonverbal aspects of
",False,VoiceStress_Analysis,False,False,True
22330,"speech are intonation, voice quality, prosody, rhythm and pausing. These phe-
",False,VoiceStress_Analysis,False,False,True
22331,"nomena stand for a non-verbal signaling system, which intertwines with the
",False,VoiceStress_Analysis,False,False,True
22332,"verbal or linguistic system. The non-verbal content of the voice carries, among
",False,VoiceStress_Analysis,False,False,True
22333,"other things, information about the physiological and psychological state of the
",False,VoiceStress_Analysis,False,False,True
22334,"speaker. Human beings are able to identify diﬀerent emotional states, because
",False,VoiceStress_Analysis,False,False,True
22335,"these are characterized by clearly perceptible (non-verbal) behavior. Part of this
",False,VoiceStress_Analysis,False,False,True
22336,"non-verbal communication takes place via other modalities like body movements
",False,VoiceStress_Analysis,False,False,True
22337,"and facial expressions [9]. The question that remains is how much of this infor-
",False,VoiceStress_Analysis,False,False,True
22338,"mation can be recovered from non-verbal vocalizations only.
",False,VoiceStress_Analysis,False,False,True
22339,"One of the most interesting research areas concerning non-verbal communi-
",False,VoiceStress_Analysis,False,False,True
22340,"cation in relation to a person’s psychological state is the search for objective,
",False,VoiceStress_Analysis,False,False,True
22341,"quantiﬁable correlates of stress. In the past this search focused primarily on
",False,VoiceStress_Analysis,False,False,True
22342,"physiological measures, but over the last years a broader range of behaviors has
",False,VoiceStress_Analysis,False,False,True
22343,"been examined, especially non-verbal behavior. The advantage being that stress
",False,VoiceStress_Analysis,False,False,True
22344,"indexes from non-verbal vocalizations can be obtained non-intrusively. From a
",False,VoiceStress_Analysis,False,False,True
22345,"practical point of view this is critical in a situation in which co-operation for
",False,VoiceStress_Analysis,False,False,True
22346,"physiological measurement is precluded, for example in the case of negotiatingwith terrorists. However, even when co-operation is possible, the presence of
",False,VoiceStress_Analysis,False,False,True
22347,"monitoring devices needed for physiological measurement can be stressful and
",False,VoiceStress_Analysis,False,False,True
22348,"anxiety arousing or simply not practical.
",False,VoiceStress_Analysis,False,False,True
22349,"Psychological stress is a pathological element of the physiological and psycho-
",False,VoiceStress_Analysis,False,False,True
22350,"logical condition of the speaker, of which the cause is accepted to be “workload”
",False,VoiceStress_Analysis,False,False,True
22351,"[14]. Objective, quantiﬁable correlates of stress are searched for by means of
",False,VoiceStress_Analysis,False,False,True
22352,"measuring the acoustic modiﬁcations of the voice brought about by workload.
",False,VoiceStress_Analysis,False,False,True
22353,"These changes in the acoustic speech signal due to stress are mainly caused
",False,VoiceStress_Analysis,False,False,True
22354,"by the physiological changes that accompany the stress-reaction. These changes
",False,VoiceStress_Analysis,False,False,True
22355,"also aﬀect the organs of speech, such as the respiration and muscle tension (vocal
",False,VoiceStress_Analysis,False,False,True
22356,"cords) and therefore the speech signal. Hence, it should be possible to establish
",False,VoiceStress_Analysis,False,False,True
22357,"whether a person is stressed just by analyzing his voice.
",False,VoiceStress_Analysis,False,False,True
22358,"2 Related Work
",False,VoiceStress_Analysis,False,False,True
22359,"Much work on stress analysis in real life situations concentrates on air-ground
",False,VoiceStress_Analysis,False,False,True
22360,"communication in aviation and space ﬂight under dangerous conditions. In many
",False,VoiceStress_Analysis,False,False,True
22361,"of these studies [8, 21] an increase of the fundamental frequency (F0) of the voice
",False,VoiceStress_Analysis,False,False,True
22362,"in situations of increasing danger is reported. Williams and Stevens [21] also re-
",False,VoiceStress_Analysis,False,False,True
22363,"ported an increase in F0 range and abrupt ﬂuctuations of F0 contour, with
",False,VoiceStress_Analysis,False,False,True
22364,"increasing stress. In a Russian study [16] the voices of astronauts are examined
",False,VoiceStress_Analysis,False,False,True
22365,"and changes in spectral energy distribution (spectral centroid moving to higher
",False,VoiceStress_Analysis,False,False,True
22366,"frequency) are reported. Increase of the energy of high frequency components,
",False,VoiceStress_Analysis,False,False,True
22367,"has also been reported by [11] in a study involving pilot communication. Scherer
",False,VoiceStress_Analysis,False,False,True
22368,"et al. found depressive patients speak with higher F0 and a larger proportion of
",False,VoiceStress_Analysis,False,False,True
22369,"high frequency components, just before the admission at a psychiatric hospital
",False,VoiceStress_Analysis,False,False,True
22370,"[13]. Jones [7] found increases in fundamental frequency and statistically signif-
",False,VoiceStress_Analysis,False,False,True
22371,"icant decreases of the vocal jitter in recordings obtained from pilots training in
",False,VoiceStress_Analysis,False,False,True
22372,"a simulated AWACS environment.
",False,VoiceStress_Analysis,False,False,True
22373,"In many laboratory studies, stress is brought about by showing unpleasant
",False,VoiceStress_Analysis,False,False,True
22374,"or disgusting slides or ﬁlms, or by placing the subject in situations that pro-
",False,VoiceStress_Analysis,False,False,True
22375,"duce unpleasant emotions, such as stage fright. The degree of stress perceived
",False,VoiceStress_Analysis,False,False,True
22376,"will vary from person to person depending on the persons experience and arous-
",False,VoiceStress_Analysis,False,False,True
22377,"ability. Apart from these individual diﬀerences, some studies show an increase
",False,VoiceStress_Analysis,False,False,True
22378,"in intensity, increased fundamental frequency [17, 12], stronger concentration of
",False,VoiceStress_Analysis,False,False,True
22379,"energy above 500 Hz [12] and an increase in speech rate [15].
",False,VoiceStress_Analysis,False,False,True
22380,"More recently, many experiments were conducted in which cognitive or achieve-
",False,VoiceStress_Analysis,False,False,True
22381,"ment tasks were used to induce stress on a subject [6, 2]. When persons were sub-
",False,VoiceStress_Analysis,False,False,True
22382,"jected to a psychomotor task [5], the speaking fundamental frequency showed
",False,VoiceStress_Analysis,False,False,True
22383,"an increase when the task became more diﬃcult. In addition, word duration
",False,VoiceStress_Analysis,False,False,True
22384,"increased during the task, but decreased again when the task became more com-
",False,VoiceStress_Analysis,False,False,True
22385,"plex. Brenner [3] also found an increase in average amplitude when subjects
",False,VoiceStress_Analysis,False,False,True
22386,"where performing a tracking task.
",False,VoiceStress_Analysis,False,False,True
22387,"Table 1 summarizes the parameters that have been shown to be indicators
",False,VoiceStress_Analysis,False,False,True
22388,"of the vocal expression of emotion, emotional disturbance or stress.Table 1. Overview of major acoustic parameters [13]
",False,VoiceStress_Analysis,False,False,True
22389,"Parameter Description
",False,VoiceStress_Analysis,False,False,True
22390,"F0 mean Fundamental frequency (vibration of the vocal folds as
",False,VoiceStress_Analysis,False,False,True
22391,"averaged over a speech utterance)
",False,VoiceStress_Analysis,False,False,True
22392,"F0 range Diﬀerence between highest and lowest F0 in an utterance
",False,VoiceStress_Analysis,False,False,True
22393,"F0 variability Measure of dispersion of F0
",False,VoiceStress_Analysis,False,False,True
22394,"F0 perturbation or jitter Slight variations in the duration of glottal cycles
",False,VoiceStress_Analysis,False,False,True
22395,"F0 contour Fundamental frequency values plotted over time
",False,VoiceStress_Analysis,False,False,True
22396,"F1 mean Frequency of ﬁrst formant averaged over an utterance
",False,VoiceStress_Analysis,False,False,True
22397,"F2 mean Mean frequency of the second formant
",False,VoiceStress_Analysis,False,False,True
22398,"Intensity mean Energy values for a speech sound wave averaged over an
",False,VoiceStress_Analysis,False,False,True
22399,"utterance
",False,VoiceStress_Analysis,False,False,True
22400,"Intensity range Diﬀerence between highest and lowest intensity value in
",False,VoiceStress_Analysis,False,False,True
22401,"an utterance
",False,VoiceStress_Analysis,False,False,True
22402,"Intensity variability Measure of dispersion of the intensity values
",False,VoiceStress_Analysis,False,False,True
22403,"High frequency energy Relative proportion of energy in the upper region
",False,VoiceStress_Analysis,False,False,True
22404,"Speech rate Length of an utterance
",False,VoiceStress_Analysis,False,False,True
22405,"Spectral noise A-periodic energy components in the spectrum
",False,VoiceStress_Analysis,False,False,True
22406,"Zero crossings Number of times a sound wave graph crosses the zero
",False,VoiceStress_Analysis,False,False,True
22407,"line
",False,VoiceStress_Analysis,False,False,True
22408,"3 Experimental Design
",False,VoiceStress_Analysis,False,False,True
22409,"To study the correspondence between human stress levels and speech produc-
",False,VoiceStress_Analysis,False,False,True
22410,"tion and to assess the relevance of the features listed in Table 1, an exploring
",False,VoiceStress_Analysis,False,False,True
22411,"experiment has been conducted. 108 native speakers of Dutch were subjected
",False,VoiceStress_Analysis,False,False,True
22412,"to several tasks that have been designed to place a cognitive workload on the
",False,VoiceStress_Analysis,False,False,True
22413,"subject. Cognitive workload is deﬁned as the information-processing load placed
",False,VoiceStress_Analysis,False,False,True
22414,"on the human operator while performing a particular task [14]. This information
",False,VoiceStress_Analysis,False,False,True
22415,"processing load is considered to be correlated with the amount of attention that
",False,VoiceStress_Analysis,False,False,True
22416,"must be directed to a task. It is assumed that cognitive workload increases with
",False,VoiceStress_Analysis,False,False,True
22417,"the diﬃculty of the task. In the present investigation subjects performed three
",False,VoiceStress_Analysis,False,False,True
22418,"tasks. In the ﬁrst test subjects had to play a computer game that gradually
",False,VoiceStress_Analysis,False,False,True
22419,"became more diﬃcult. The second task required to simultaneously engage in
",False,VoiceStress_Analysis,False,False,True
22420,"two attention-demanding activities. Finally, the participants were subjected to a
",False,VoiceStress_Analysis,False,False,True
22421,"psychological stress test. During all tasks and during a controlled rest-condition
",False,VoiceStress_Analysis,False,False,True
22422,"before the tasks, the subjects produced utterances. Acoustical analyses of all
",False,VoiceStress_Analysis,False,False,True
22423,"utterances were made and compared with the control condition and with the
",False,VoiceStress_Analysis,False,False,True
22424,"acoustical analyses of the other utterances produced during the same task.
",False,VoiceStress_Analysis,False,False,True
22425,"The psychological stress test, an instance of the Stroop test, proved to be
",False,VoiceStress_Analysis,False,False,True
22426,"the most demanding task for the subjects thus providing the clearest results.
",False,VoiceStress_Analysis,False,False,True
22427,"Therefore we will concentrate on the results of this task for the remainder of the
",False,VoiceStress_Analysis,False,False,True
22428,"paper.3.1 Stroop Test
",False,VoiceStress_Analysis,False,False,True
22429,"The Stroop test is a well-known psychological test [18] that exploits the fact
",False,VoiceStress_Analysis,False,False,True
22430,"that for experienced readers, the reading of a word has become an automatism.
",False,VoiceStress_Analysis,False,False,True
22431,"In its native form this test consisted of three cards: on the ﬁrst card a great
",False,VoiceStress_Analysis,False,False,True
22432,"number of little squares are drawn in the colors red, blue, green and yellow. On
",False,VoiceStress_Analysis,False,False,True
22433,"the second card the words red, blue, green and yellow in black ink are placed on
",False,VoiceStress_Analysis,False,False,True
22434,"the corresponding positions. On the third card, the conﬂict card, the same words
",False,VoiceStress_Analysis,False,False,True
22435,"as on the second card are placed, but now using a non-corresponding ink-color.
",False,VoiceStress_Analysis,False,False,True
22436,"It turns out that the time needed to name the colors on the conﬂict card is much
",False,VoiceStress_Analysis,False,False,True
22437,"higher than the time taken for naming them on the ﬁrst card. Furthermore, the
",False,VoiceStress_Analysis,False,False,True
22438,"subjects tend to make more mistakes reading the third card and show signs of
",False,VoiceStress_Analysis,False,False,True
22439,"tension (movement, sudden laughs).
",False,VoiceStress_Analysis,False,False,True
22440,"In the current experiment a variation on the Stroop-test was used, in which
",False,VoiceStress_Analysis,False,False,True
22441,"a gradual increase of the level of diﬃculty is incorporated. The names of the
",False,VoiceStress_Analysis,False,False,True
22442,"colors (printed in diﬀerent colored ink) were put on a computer screen one by
",False,VoiceStress_Analysis,False,False,True
22443,"one. The diﬃculty of the task increased as the time between the appearances of
",False,VoiceStress_Analysis,False,False,True
22444,"the colors was shortened every minute with half a second, thus decreasing from
",False,VoiceStress_Analysis,False,False,True
22445,"two and a half seconds at the start to half a second in the ﬁnal minute.
",False,VoiceStress_Analysis,False,False,True
22446,"3.2 Jitter
",False,VoiceStress_Analysis,False,False,True
22447,"During the experiments fundamental frequency, variation of fundamental fre-
",False,VoiceStress_Analysis,False,False,True
22448,"quency, jitter, energy, high frequency energy ratio, duration and the number
",False,VoiceStress_Analysis,False,False,True
22449,"of zero crossings were monitored as candidate vocal stress correlates. Of those
",False,VoiceStress_Analysis,False,False,True
22450,"features, jitter may require some explanation.
",False,VoiceStress_Analysis,False,False,True
22451,"Jitter is the perturbation in the vibration of the vocal chords. This results
",False,VoiceStress_Analysis,False,False,True
22452,"in a cycle-to-cycle variation of the fundamental frequency. [19] reported that
",False,VoiceStress_Analysis,False,False,True
22453,"about 20 cycles are enough for jitter analysis. Formally the term perturbation
",False,VoiceStress_Analysis,False,False,True
22454,"implies a deviation from steadiness or regularity [10]. Let aibe any cyclic pa-
",False,VoiceStress_Analysis,False,False,True
22455,"rameter (amplitude, pitch period, etc.) in the ithcycle of the waveform. Then
",False,VoiceStress_Analysis,False,False,True
22456,"the steady value of this parameter over a span of Ncycles can be estimated from
",False,VoiceStress_Analysis,False,False,True
22457,"its arithmetic mean:
",False,VoiceStress_Analysis,False,False,True
22458,"a=1
",False,VoiceStress_Analysis,False,False,True
22459,"NN/summationdisplay
",False,VoiceStress_Analysis,False,False,True
22460,"i=1ai (1)
",False,VoiceStress_Analysis,False,False,True
22461,"And the zeroth-order perturbation function as the arithmetic diﬀerence:
",False,VoiceStress_Analysis,False,False,True
22462,"p0
",False,VoiceStress_Analysis,False,False,True
22463,"i=ai−a, i = 1, . . . , N (2)
",False,VoiceStress_Analysis,False,False,True
22464,"Where the superscript gives the order of the perturbation function. Higher-order
",False,VoiceStress_Analysis,False,False,True
22465,"perturbation functions can be obtained by alternately taking backward and for-
",False,VoiceStress_Analysis,False,False,True
22466,"ward diﬀerences of lower order functions. We will consider the ﬁrst-order per-
",False,VoiceStress_Analysis,False,False,True
22467,"turbation function:
",False,VoiceStress_Analysis,False,False,True
22468,"p1
",False,VoiceStress_Analysis,False,False,True
22469,"i=p0
",False,VoiceStress_Analysis,False,False,True
22470,"i−p0
",False,VoiceStress_Analysis,False,False,True
22471,"i−1=ai−ai−1, i = 1, . . . , N (3)The ﬁrst order perturbation function can be used to determine the fundamen-
",False,VoiceStress_Analysis,False,False,True
22472,"tal frequency perturbation if in Equation 3 aiis taken to be the fundamental
",False,VoiceStress_Analysis,False,False,True
22473,"frequency. The fundamental frequency is computed only for the voiced parts of
",False,VoiceStress_Analysis,False,False,True
22474,"speech. The fundamental frequency perturbation is deﬁned as the average of the
",False,VoiceStress_Analysis,False,False,True
22475,"absolute values of all these diﬀerences normalised to percentage:
",False,VoiceStress_Analysis,False,False,True
22476,"jitter =100
",False,VoiceStress_Analysis,False,False,True
22477,"(N−1)aN/summationdisplay
",False,VoiceStress_Analysis,False,False,True
22478,"i=2|ai−ai−1| (4)
",False,VoiceStress_Analysis,False,False,True
22479,"4 Experimental Results
",False,VoiceStress_Analysis,False,False,True
22480,"In this section the results of statistical analysis of the acoustical data are de-
",False,VoiceStress_Analysis,False,False,True
22481,"scribed. Table 2 reports the averages and standard deviations of the data col-
",False,VoiceStress_Analysis,False,False,True
22482,"lected during the Stroop test. The conditional eﬀects in relation to the observed
",False,VoiceStress_Analysis,False,False,True
22483,"eﬀects in the features will now be discussed from condition to condition. The ﬁrst
",False,VoiceStress_Analysis,False,False,True
22484,"minute is considered to represent normal conditions and is used for comparison.
",False,VoiceStress_Analysis,False,False,True
22485,"Table 2. Results of the Stroop Test
",False,VoiceStress_Analysis,False,False,True
22486,"Feature Time Mean Std. dev.
",False,VoiceStress_Analysis,False,False,True
22487,"Duration 1 454.74 136.16
",False,VoiceStress_Analysis,False,False,True
22488,"2 438.05 106.89
",False,VoiceStress_Analysis,False,False,True
22489,"2 438.05 106.89
",False,VoiceStress_Analysis,False,False,True
22490,"2 438.05 106.89
",False,VoiceStress_Analysis,True,False,True
22491,"2 438.05 106.89
",False,VoiceStress_Analysis,True,False,True
22492,"Abstract
",True,W18-0537,False,False,True
22493,"1 Introduction
",True,W18-0537,False,False,True
22494,"2 Data
",True,W18-0537,False,False,True
22495,"10 non-native speakers. For the other languages,
",True,W18-0537,False,False,True
22496,"3 Features
",True,W18-0537,False,False,True
22497,"Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 315–321
",False,W18-0537,False,False,True
22498,"New Orleans, Louisiana, June 5, 2018. c
",False,W18-0537,False,False,True
22499,"2018 Association for Computational Linguistics
",False,W18-0537,False,False,True
22500,"SB@GU at the Complex Word Identiﬁcation 2018 Shared Task
",False,W18-0537,False,False,True
22501,"David Alfter
",False,W18-0537,False,False,True
22502,"Spr˚akbanken
",False,W18-0537,False,False,True
22503,"University of Gothenburg
",False,W18-0537,False,False,True
22504,"Sweden
",False,W18-0537,False,False,True
22505,"david.alfter@gu.seIldik ´o Pil ´an
",False,W18-0537,False,False,True
22506,"Spr˚akbanken
",False,W18-0537,False,False,True
22507,"University of Gothenburg
",False,W18-0537,False,False,True
22508,"Sweden
",False,W18-0537,False,False,True
22509,"ildiko.pilan@gu.se
",False,W18-0537,False,False,True
22510,"In this paper, we describe our experiments
",False,W18-0537,False,False,True
22511,"for the Shared Task on Complex Word Iden-
",False,W18-0537,False,False,True
22512,"tiﬁcation (CWI) 2018 (Yimam et al., 2018),
",False,W18-0537,False,False,True
22513,"hosted by the 13thWorkshop on Innovative
",False,W18-0537,False,False,True
22514,"Use of NLP for Building Educational Appli-
",False,W18-0537,False,False,True
22515,"cations (BEA) at NAACL 2018. Our sys-
",False,W18-0537,False,False,True
22516,"tem for English builds on previous work for
",False,W18-0537,False,False,True
22517,"Swedish concerning the classiﬁcation of words
",False,W18-0537,False,False,True
22518,"into proﬁciency levels. We investigate dif-
",False,W18-0537,False,False,True
22519,"ferent features for English and compare their
",False,W18-0537,False,False,True
22520,"usefulness using feature selection methods.
",False,W18-0537,False,False,True
22521,"For the German, Spanish and French data we
",False,W18-0537,False,False,True
22522,"use simple systems based on character n-gram
",False,W18-0537,False,False,True
22523,"models and show that sometimes simple mod-
",False,W18-0537,False,False,True
22524,"els achieve comparable results to fully feature-
",False,W18-0537,False,False,True
22525,"engineered systems.
",False,W18-0537,False,False,True
22526,"1 Introduction
",False,W18-0537,False,False,True
22527,"The task of identifying complex words consists of
",False,W18-0537,False,False,True
22528,"automatically detecting lexical items that might be
",False,W18-0537,False,False,True
22529,"hard to understand for a certain audience. Once
",False,W18-0537,False,False,True
22530,"identiﬁed, text simpliﬁcation systems can substi-
",False,W18-0537,False,False,True
22531,"tute these complex words by simpler equivalents
",False,W18-0537,False,False,True
22532,"to increase the comprehensibility ( readability ) of
",False,W18-0537,False,False,True
22533,"a text. Readable texts can facilitate information
",False,W18-0537,False,False,True
22534,"processing for language learners and people with
",False,W18-0537,False,False,True
22535,"reading difﬁculties (Vajjala and Meurers, 2014;
",False,W18-0537,False,False,True
22536,"Heimann M ¨uhlenbock, 2013; Yaneva et al., 2016).
",False,W18-0537,False,False,True
22537,"Building on previous work for classifying
",False,W18-0537,False,False,True
22538,"Swedish words into different language proﬁciency
",False,W18-0537,False,False,True
22539,"levels (Alfter and V olodina, 2018), we extend our
",False,W18-0537,False,False,True
22540,"pipeline with English resources. We explore a
",False,W18-0537,False,False,True
22541,"large number of features for English based on,
",False,W18-0537,False,False,True
22542,"among others, length information, parts of speech,
",False,W18-0537,False,False,True
22543,"word embeddings and language model probabil-
",False,W18-0537,False,False,True
22544,"ities. In contrast to this feature-engineered ap-
",False,W18-0537,False,False,True
22545,"proach, we use a word-length and n-gram proba-
",False,W18-0537,False,False,True
22546,"bility based approach for the German, Spanish and
",False,W18-0537,False,False,True
22547,"French data.Our interest for participation in this shared task
",False,W18-0537,False,False,True
22548,"is connected to the ongoing development of a com-
",False,W18-0537,False,False,True
22549,"plexity prediction system for Swedish (Alfter and
",False,W18-0537,False,False,True
22550,"V olodina, 2018). In contrast to this shared task,
",False,W18-0537,False,False,True
22551,"we perform a ﬁve-way classiﬁcation correspond-
",False,W18-0537,False,False,True
22552,"ing to the ﬁrst ﬁve levels of the CEFR scale of lan-
",False,W18-0537,False,False,True
22553,"guage proﬁciency (Council of Europe, 2001). We
",False,W18-0537,False,False,True
22554,"adapted the pipeline to English, and included some
",False,W18-0537,False,False,True
22555,"freely available English resources to see how well
",False,W18-0537,False,False,True
22556,"these would perform on the CWI 2018 task and to
",False,W18-0537,False,False,True
22557,"gain insights into how we could improve our own
",False,W18-0537,False,False,True
22558,"system.
",False,W18-0537,False,False,True
22559,"2 Data
",False,W18-0537,False,False,True
22560,"There were four different tracks at the shared task.
",False,W18-0537,False,False,True
22561,"Table 1 shows the number of annotated instances
",False,W18-0537,False,False,True
22562,"per language. For the French sub-task, no training
",False,W18-0537,False,False,True
22563,"data was provided. Each instance in the English
",False,W18-0537,False,False,True
22564,"dataset was annotated by 10 native speakers and
",False,W18-0537,False,False,True
22565,"10 non-native speakers. For the other languages,
",False,W18-0537,False,False,True
22566,"10 annotators (native and non-native speakers) an-
",False,W18-0537,False,False,True
22567,"notated the data. An item is considered complex if
",False,W18-0537,False,False,True
22568,"at least one annotator annotates the item as com-
",False,W18-0537,False,False,True
22569,"plex.
",False,W18-0537,False,False,True
22570,"Language Training Development
",False,W18-0537,False,False,True
22571,"English 27299 3328
",False,W18-0537,False,False,True
22572,"Spanish 13750 1622
",False,W18-0537,False,False,True
22573,"German 6151 795
",False,W18-0537,False,False,True
22574,"French / /
",False,W18-0537,False,False,True
22575,"Table 1: Number of instances per language
",False,W18-0537,False,False,True
22576,"In the dataset, information about the total num-
",False,W18-0537,False,False,True
22577,"ber of native and non-native annotators and how
",False,W18-0537,False,False,True
22578,"many of each category considered a word complex
",False,W18-0537,False,False,True
22579,"is also available.
",False,W18-0537,False,False,True
22580,"A surprising aspect of the 2018 dataset was
",False,W18-0537,False,False,True
22581,"the presence of multi-word expressions (MWE),
",False,W18-0537,False,False,True
22582,"which were not part of the 2016 shared task. For315the 2018 task, the training data contains 14% of
",False,W18-0537,False,False,True
22583,"MWEs while the development data contains 13%.
",False,W18-0537,False,False,True
22584,"3 Features
",False,W18-0537,False,False,True
22585,"We extract a number of features from each target
",False,W18-0537,False,False,True
22586,"item, either a single word or a multi-word expres-
",False,W18-0537,False,False,True
22587,"sion. The features can be grouped into: (i) count
",False,W18-0537,False,False,True
22588,"and word form based features, (ii) morphological
",False,W18-0537,False,False,True
22589,"features, (iii) semantic features and (iv) context
",False,W18-0537,False,False,True
22590,"features. In addition, we use psycholinguistic fea-
",False,W18-0537,False,False,True
22591,"tures extracted by N-Watch (Davis, 2005). In Ta-
",False,W18-0537,False,False,True
22592,"ble 2, we list the complete set of features used for
",False,W18-0537,False,False,True
22593,"English.
",False,W18-0537,False,False,True
22594,"Count features
",False,W18-0537,False,False,True
22595,"Length (number of characters)
",False,W18-0537,False,False,True
22596,"Syllable count (S1)
",False,W18-0537,False,False,True
22597,"Contains non-alphanumeric character
",False,W18-0537,False,False,True
22598,"Is number
",False,W18-0537,False,False,True
22599,"Is MWE
",False,W18-0537,False,False,True
22600,"Character bigrams (B1)
",False,W18-0537,False,False,True
22601,"N-gram probabilities (Wikipedia)
",False,W18-0537,False,False,True
22602,"In Ogden list
",False,W18-0537,False,False,True
22603,"AWL distribution
",False,W18-0537,False,False,True
22604,"CEFRLex distribution
",False,W18-0537,False,False,True
22605,"Morphological features
",False,W18-0537,False,False,True
22606,"Part-of-speech
",False,W18-0537,False,False,True
22607,"Sufﬁx length
",False,W18-0537,False,False,True
22608,"Semantic features
",False,W18-0537,False,False,True
22609,"Number of synsets
",False,W18-0537,False,False,True
22610,"Number of hypernyms
",False,W18-0537,False,False,True
22611,"Number of hyponyms
",False,W18-0537,False,False,True
22612,"Sense id
",False,W18-0537,False,False,True
22613,"Context features
",False,W18-0537,False,False,True
22614,"Topic distributions
",False,W18-0537,False,False,True
22615,"Word embeddings
",False,W18-0537,False,False,True
22616,"N-Watch features
",False,W18-0537,False,False,True
22617,"British National Corpus frequency (BNC)
",False,W18-0537,False,False,True
22618,"CELEX frequency (total, written, spoken)
",False,W18-0537,False,False,True
22619,"In Ku ˇcera Francis (KF) list
",False,W18-0537,False,False,True
22620,"Sydney Morning Herald frequency (SMH)
",False,W18-0537,False,False,True
22621,"Reaction time
",False,W18-0537,False,False,True
22622,"Bigram frequency (B2)
",False,W18-0537,False,False,True
22623,"Trigram frequency (T2)
",False,W18-0537,False,False,True
22624,"Syllable count (S2)
",False,W18-0537,False,False,True
22625,"Table 2: Overview of featuresWord length in terms of number of characters
",False,W18-0537,False,False,True
22626,"has been shown to correlate well with complexity
",False,W18-0537,False,False,True
22627,"in a number of studies (Smith, 1961; Bj ¨ornsson,
",False,W18-0537,False,False,True
22628,"1968; O’Regan and Jacobs, 1992).
",False,W18-0537,False,False,True
22629,"1968; O’Regan and Jacobs, 1992).
",False,W18-0537,False,False,True
22630,"Abstract. In the human machine interaction domain adaptive life-like agents
",True,_TSD my_eliza,False,False,True
22631,"1 Introduction
",True,_TSD my_eliza,False,False,True
22632,"2 System Architecture
",True,_TSD my_eliza,False,False,True
22633,"3 Natural Language Processing
",True,_TSD my_eliza,False,False,True
22634,"4 Emotion Recognition
",True,_TSD my_eliza,False,False,True
22635,"4.1 Emotion Eliciting Factor Extraction
",True,_TSD my_eliza,False,False,True
22636,"4.2 Facial Display Selection
",True,_TSD my_eliza,False,False,True
22637,"5 Implementation and Future Work
",True,_TSD my_eliza,False,False,True
22638,"A Multi-modal Eliza Using Natural Language
",False,_TSD my_eliza,False,False,True
22639,"Processing and Emotion Recognition
",False,_TSD my_eliza,False,False,True
22640,"Siska Fitrianie, Pascal Wiggers, and Leon J.M. Rothkrantz
",False,_TSD my_eliza,False,False,True
22641,"Data and Knowledge Systems
",False,_TSD my_eliza,False,False,True
22642,"Delft University of Technology
",False,_TSD my_eliza,False,False,True
22643,"Mekelweg 4, 2628 CD Delft, The Netherlands
",False,_TSD my_eliza,False,False,True
22644,"p.wiggers@its.tudelft.nl
",False,_TSD my_eliza,False,False,True
22645,"l.j.m.rothkrantz@its.tudelft.nl
",False,_TSD my_eliza,False,False,True
22646,"are becoming a popular interface. In order to provide a natural conversation such
",False,_TSD my_eliza,False,False,True
22647,"agents should be able to display emotion and to recognize the user’s emotions. This
",False,_TSD my_eliza,False,False,True
22648,"paper describes a computer model for a multi-modal communication system based
",False,_TSD my_eliza,False,False,True
22649,"on the famous Eliza question-answering system. A human user can communicatewith the developed system using typed natural language. The system will replywith text-prompts and appropriate facial-expressions.
",False,_TSD my_eliza,False,False,True
22650,"1 Introduction
",False,_TSD my_eliza,False,False,True
22651,"Recently, there has been a lot of interest in adaptive life-like agents in the area of
",False,_TSD my_eliza,False,False,True
22652,"human-computer interaction. Examples include the German Smartkom project [1] andthe CSLR reading tutor [2]. The advantages of such systems are obvious, they offera much more natural conversation with a machine than traditional user interfaces takinghuman face-to-face communication as their source of inspiration. This is especiallythe case when interaction through multiple modalities including speech and pointing issupported. However, in human-to-human communication emotions play an importantrole. As indicated by Mehrebian [3] about 55 percent of the emotional meaning ofa message is communicated through the non-verbal channel, which includes gestures,postures and facial expressions. Thus, in order to offer a natural interface software agentsshould also show the proper non-verbal reactions, like facial expressions. On the inputside this requires of course recognition of the user’s emotions.
",False,_TSD my_eliza,False,False,True
22653,"In this paper we describe the design and implementation of a multi-modal question-
",False,_TSD my_eliza,False,False,True
22654,"answering system based on the famous Eliza program [4], which simulates a psychoan-alyst who talks to a client using natural language. Our system recognizes emotion fromnatural language. It will show a facial expression for each sentence typed by the user.Subsequently, it will give a natural language reply together with an appropriate facialexpression to convey emotional content.
",False,_TSD my_eliza,False,False,True
22655,"2 System Architecture
",False,_TSD my_eliza,False,False,True
22656,"The overall system architecture is based on the idea of message passing on a blackboard,
",False,_TSD my_eliza,False,False,True
22657,"as is illustrated in Fig. 1. All processing modules act as independent experts by taking
",False,_TSD my_eliza,False,False,True
22658,"V . Matouˇ sek and P. Mautner (Eds.): TSD 2003, LNAI 2807, pp. 394–399, 2003.
",False,_TSD my_eliza,False,False,True
22659,"c/circlecopyrtSpringer-Verlag Berlin Heidelberg 2003A Multi-modal Eliza Using Natural Language Processing and Emotion Recognition 395
",False,_TSD my_eliza,False,False,True
22660,"their input from and writing their output back to the blackboard. This ensures a ﬂexible
",False,_TSD my_eliza,False,False,True
22661,"data-controlled architecture where modules may be executed in parallel. Modules caneasily be added or changed, which also opens up the possibility of adding more modalitiesto the system. The input to the system is a user’s text string and the results are the replysentences and facial displays. The processing modules in the system can be subdividedinto two layers, the ﬁrst layer consists of modules for natural language processing andthe second layer performs emotional recognition in order to construct facial displays.
",False,_TSD my_eliza,False,False,True
22662,"The next two sections will describe these layers in more detail.
",False,_TSD my_eliza,False,False,True
22663,"Blackboard system
",False,_TSD my_eliza,False,False,True
22664,"Input
",False,_TSD my_eliza,False,False,True
22665,"stringEmotive Lexicon
",False,_TSD my_eliza,False,False,True
22666,"Dictionary
",False,_TSD my_eliza,False,False,True
22667,"Reply with
",False,_TSD my_eliza,False,False,True
22668,"facial displayFacial
",False,_TSD my_eliza,False,False,True
22669,"Expression
",False,_TSD my_eliza,False,False,True
22670,"Dictionary
",False,_TSD my_eliza,False,False,True
22671,"Facial
",False,_TSD my_eliza,False,False,True
22672,"SelectionIntensities
",False,_TSD my_eliza,False,False,True
22673,"Analysis
",False,_TSD my_eliza,False,False,True
22674,"(Thermometer)Facial Display Construction Layers - FDC Layers
",False,_TSD my_eliza,False,False,True
22675,"First reaction
",False,_TSD my_eliza,False,False,True
22676,"facial displayEmotive Lexicon
",False,_TSD my_eliza,False,False,True
22677,"Dictionary ParserUser Affective State Analysis
",False,_TSD my_eliza,False,False,True
22678,"Emotive Labeled Memory
",False,_TSD my_eliza,False,False,True
22679,"Structure ExtractionConcern of the Other
",False,_TSD my_eliza,False,False,True
22680,"AnalysisCognitive Reasoning
",False,_TSD my_eliza,False,False,True
22681,"Based System's GSPAffective Attributing AnalysisAffective Knowledge Bases
",False,_TSD my_eliza,False,False,True
22682,"Natural Language Processing Layers - NLP LayersParserLexical
",False,_TSD my_eliza,False,False,True
22683,"Analysis
",False,_TSD my_eliza,False,False,True
22684,"Words/
",False,_TSD my_eliza,False,False,True
22685,"Phrases List
",False,_TSD my_eliza,False,False,True
22686,"Conversation
",False,_TSD my_eliza,False,False,True
22687,"HistoryPattern Rules List
",False,_TSD my_eliza,False,False,True
22688,"(system's memory structure)User Personal
",False,_TSD my_eliza,False,False,True
22689,"DatabaseSystem' GSP
",False,_TSD my_eliza,False,False,True
22690,"DatabaseWrap
",False,_TSD my_eliza,False,False,True
22691,"ProcessPragmatic AnalysisUser Preference
",False,_TSD my_eliza,False,False,True
22692,"AnalysisCognitive Reasoning
",False,_TSD my_eliza,False,False,True
22693,"Based System's GSP
",False,_TSD my_eliza,False,False,True
22694," Syntactic - Sematic AnalysisAnaphoric
",False,_TSD my_eliza,False,False,True
22695,"analysisTopic
",False,_TSD my_eliza,False,False,True
22696,"MaintenanceRepetition
",False,_TSD my_eliza,False,False,True
22697,"RecognitionPattern
",False,_TSD my_eliza,False,False,True
22698,"Matching
",False,_TSD my_eliza,False,False,True
22699,"Fig. 1. System architecture.
",False,_TSD my_eliza,False,False,True
22700,"3 Natural Language Processing
",False,_TSD my_eliza,False,False,True
22701,"The natural language processing layer analyses a user’s input string in order to construct
",False,_TSD my_eliza,False,False,True
22702,"a reply sentence. First, the P arser subdivides the string in a list of words. This list is the
",False,_TSD my_eliza,False,False,True
22703,"input to the Lexical Analysis Module , which checks the spelling, replaces abbreviations,
",False,_TSD my_eliza,False,False,True
22704,"slang words and codes with full words and uses a thesaurus to replace certain words bytheir synonyms to reduce the amount of variation subsequent modules have to deal with.
",False,_TSD my_eliza,False,False,True
22705,"The syntactic-semantic analyzer performs shallow parsing by matching the input
",False,_TSD my_eliza,False,False,True
22706,"words with predeﬁned patterns (that may contain wildcards) and composes a reply fromthe keywords found using reassemble rules attached to these patterns. For a given promptthe longest matching pattern containing the smallest number of wild cards is chosen.Preceding the pattern matching step the module performs a number of actions to ensurea more natural conversation. Anaphoric analysis guarantees that the system respondsconsistently on prompts by referring to the preceding conversation content. Repetitionrecognition makes sure that the dialog never gets into a loop and the system tries to staywithin the current topic of conversation. The rules used are written in an extended-XMLscript speciﬁcation called AIML (Artiﬁcial Intelligence Markup Language), deﬁned by
",False,_TSD my_eliza,False,False,True
22707,"Wallace [5].396 S. Fitrianie, P . Wiggers, and L.J.M. Rothkrantz
",False,_TSD my_eliza,False,False,True
22708,"The ﬁnal natural language module is the Pragmatic Analysis , which checks the re-
",False,_TSD my_eliza,False,False,True
22709,"ply composed by the previous module against the user preferences that are collectedduring the conversation and against the goals, states and preferences of the system. Asthe system simulates a psychoanalyst its main goal is to keep the conversation going.By distinguishing a dialog state as a certain dialog act like a question, statement, ac-knowledgment, or pause, the system determines which intermediate goal to pursue, forexample answering a question, asking a user for explanation or reﬂecting a feeling. Ifa reply does not comply with the system goals it is rejected and the syntactic-semanticmodule is invoked again to formulate a new reply.
",False,_TSD my_eliza,False,False,True
22710,"4 Emotion Recognition
",False,_TSD my_eliza,False,False,True
22711,"How many and what kind of emotional expressions are to be used poses a non-trivialquestion. In this work we adopted the twenty-four categories of emotions deﬁned byOrtany, Clore and Colling (OCC’s theory [6, 7]). They are based on grouping humanemotions by their eliciting conditions events, the consequences of their action, and theselections of computational implementation. Since classiﬁcations of some emotion elic-iting factors are in a gray area, in this research, we add one emotion type: uncertainty.However, for emotion recognition our current prototype uses only the 7 universal emo-tions deﬁned by Ekman [9]. These are shown in Table 1, together with the correspondingOCC emotions.
",False,_TSD my_eliza,False,False,True
22712,"T able 1. Emotions.
",False,_TSD my_eliza,False,False,True
22713,"Universal OCC theory
",False,_TSD my_eliza,False,False,True
22714,"Neutrality Normal
",False,_TSD my_eliza,False,False,True
22715,"Happiness Joy, Happy-for, Gloating, Satisfaction, Relief, Pride, Admiration, Liking,
",False,_TSD my_eliza,False,False,True
22716,"Gratitude, Gratiﬁcation
",False,_TSD my_eliza,False,False,True
22717,"Sadness Distress, Resentment, Sorry-for, Disappointment, Shame, Remorse
",False,_TSD my_eliza,False,False,True
22718,"Disgust Disliking, Hate
",False,_TSD my_eliza,False,False,True
22719,"Surprise Hope
",False,_TSD my_eliza,False,False,True
22720,"Fear Fear, Fears-conﬁrmed
",False,_TSD my_eliza,False,False,True
22721,"Anger Reproach, Anger
",False,_TSD my_eliza,False,False,True
22722,"Uncertainty Uncertainty
",False,_TSD my_eliza,False,False,True
22723,"4.1 Emotion Eliciting Factor Extraction
",False,_TSD my_eliza,False,False,True
22724,"To extract emotion eliciting factors from the text prompts in the conversation both of
",False,_TSD my_eliza,False,False,True
22725,"the parsed user’s string input and the systems reply sentence a shallow word matchingparser, called Emotive Lexicon Look-up P arser , is used that utilizes a lexicon of words
",False,_TSD my_eliza,False,False,True
22726,"having ’emotional content’ for each of the seven universal emotions. In total a list of247 words was used compiled from three sets of emotional words described by [10–12].For each of these words a natural number intensity value is given. To get the overallA Multi-modal Eliza Using Natural Language Processing and Emotion Recognition 397
",False,_TSD my_eliza,False,False,True
22727,"emotional content of the string a thermometer is deﬁned for each of the seven emotions.
",False,_TSD my_eliza,False,False,True
22728,"When an emotionally rich word is found the thermometers are updated by:
",False,_TSD my_eliza,False,False,True
22729,"Ti(t)=Ti(t−1)+Ii.s
",False,_TSD my_eliza,False,False,True
22730,"∀j/negationslash=i·Tj(t)=Tj(t−1)−distance [j, i](1)
",False,_TSD my_eliza,False,False,True
22731,"Where, iis the active emotion type, sis a summation factor; Iis the emotion intensity
",False,_TSD my_eliza,False,False,True
22732,"and jranges over all universal emotions deﬁned in Table 1. The distance between two
",False,_TSD my_eliza,False,False,True
22733,"emotions follows from the work of Hendrix and Ruttkay [13] who deﬁned the distancevalues shown in Table 2.
",False,_TSD my_eliza,False,False,True
22734,"T able 2. Distance values between emotions.
",False,_TSD my_eliza,False,False,True
22735,"Happiness Surprise Anger Disgust Sadness
",False,_TSD my_eliza,False,False,True
22736,"Happiness 0 3.195 2.637 1.926 2.554
",False,_TSD my_eliza,False,False,True
22737,"Surprise 0 3.436 2.298 2.084
",False,_TSD my_eliza,False,False,True
22738,"Anger 0 1.506 1.645
",False,_TSD my_eliza,False,False,True
22739,"Disgust 0 1.040
",False,_TSD my_eliza,False,False,True
22740,"Sadness 0
",False,_TSD my_eliza,False,False,True
22741,"Each of the memory structures, that is a pattern and the corresponding rules, used by
",False,_TSD my_eliza,False,False,True
22742,"the syntactic-semantic modules is labeled with one or more emotion types in the Emotive
",False,_TSD my_eliza,False,False,True
22743,"Labeled Memory Structure Extraction . We achieved this by adding two additional tags
",False,_TSD my_eliza,False,False,True
22744,"in the AIML scheme. The <affect> tag that labels the user’s affective situation and
",False,_TSD my_eliza,False,False,True
22745,"the<concern> tag that labels the system’s reaction situation. Inside those two new
",False,_TSD my_eliza,False,False,True
22746,"tags, we deﬁne four emotive situation types: positive, negative, joking and normal/any.
",False,_TSD my_eliza,False,False,True
22747,"Whether a certain goal, found during pragmatic analysis is appealing inﬂuences the
",False,_TSD my_eliza,False,False,True
22748,"system’s affective state. As do the preferences deﬁned for the system. The Goal-Based
",False,_TSD my_eliza,False,False,True
22749,"Emotion Reasoning also stores the user’s personal data during conversation, e.g. name,
",False,_TSD my_eliza,False,False,True
22750,"birthday, favorite things and so on.
",False,_TSD my_eliza,False,False,True
22751,"To determine the system’s affective state two knowledge bases are used. One to
",False,_TSD my_eliza,False,False,True
22752,"determine the system’s reaction affective state as stimulus response to the user’s inputstring and on to determine the system’s reaction affective state as the result of thecognitive process of the conversation content to convey its reply sentence. We have
",False,_TSD my_eliza,False,False,True
22753,"deﬁned a set of so-called preference rules that specify the emotion recognition processof the system. Every rule in the set deﬁnes conditions of emotion eliciting factors andthe affective thermometers to activate the rule and a preference that is expressed uponactivation. The result from each knowledge-based system is one of twenty-four OCC’stheory emotion types with addition of two emotion types: normal and uncertainty.
",False,_TSD my_eliza,False,False,True
22754,"Fig. 2 shows two example preference rules. The ﬁrst rule is a stimulus response
",False,_TSD my_eliza,False,False,True
22755,"preference rule for the reaction of joy. In this case the system will answer any questionsfrom the user joyfully, because she enjoys the situation and she met the goal: making theuser feel happy. The second rule is part of the cognitive process knowledge base. Herethe system does not like the user making a joke while it feels sad.398 S. Fitrianie, P. Wiggers, and L.J.M. Rothkrantz
",False,_TSD my_eliza,False,False,True
22756," IF (user is happy) AND (user asks question) AND (systems reply is sad) AND 
",False,_TSD my_eliza,False,False,True
22757,"(situation type of user is not negative) AND (highest thermo is happy) THEN 
",False,_TSD my_eliza,False,False,True
22758,"reaction is joy. 
",False,_TSD my_eliza,False,False,True
22759,"IF (user is sad) AND (systems reply is sad) AND (situation type of user is 
",False,_TSD my_eliza,False,False,True
22760,"joking) AND (situation type of the system is negative) AND (maximum affective 
",False,_TSD my_eliza,False,False,True
22761,"thermo is sad) THEN reply is resentment 
",False,_TSD my_eliza,False,False,True
22762,"Fig. 2. Preference rules.
",False,_TSD my_eliza,False,False,True
22763,"4.2 Facial Display Selection
",False,_TSD my_eliza,False,False,True
22764,"For the activation of an emotion, [6, 8] proposed the use of threshold values by counting
",False,_TSD my_eliza,False,False,True
22765,"all associated elicitation factors, excitatory (positive) and inhibitory (negative), fromother emotions. They used an activation level range [0,m a x ]where max is an integer
",False,_TSD my_eliza,False,False,True
22766,"value determined empirically. All emotions are always active, but their intensity mustexceed a threshold level before they are expressed externally. The activation process iscontrolled by a knowledge-based system that synthesizes and generates cognitive-relatedemotions in the system. To determine the intensity of the systems emotions as a reactionto the user’s string input and the dialog content we deﬁne six affective thermometersclassiﬁed by six Ekman’s universal emotion types (neutrality is not considered here) aswe did for the user emotions in the emotive lexicon. If an emotion is active, the systemcalculates all of thermometers T
",False,_TSD my_eliza,False,False,True
22767,"iaccording to equation (1) given in the previous section.
",False,_TSD my_eliza,False,False,True
22768,"The thermometer having the highest value is chosen as the systems emotion and
",False,_TSD my_eliza,False,False,True
22769,"depending on the intensity a facial display is chosen. Currently, the mapping from emo-
",False,_TSD my_eliza,False,False,True
22770,"tions to facial expressions is one-to-one where the emotions correspond to the 24 OCC
",False,_TSD my_eliza,False,False,True
22771,"emotions, uncertainty or neutrality.
",False,_TSD my_eliza,False,False,True
22772,"5 Implementation and Future Work
",False,_TSD my_eliza,False,False,True
22773,"Currently a web-based client server prototype of the model has been implemented forexperimental purposes. The server provides the blackboard architecture, implemented inJESS, which is accessible over TCP/IP by the client application. Currently, the emotivelexicon contains: 48 lexemes for happiness, 170 lexemes for sadness, 34 lexemes forsurprise, 33 lexemes for fear, 93 lexemes for disgust, and 69 lexemes for anger. Thisprototype has 1953 categories in its list of pattern rules. Its affective knowledge basecontains 77 preference rules of stimulus response and 151 preference rules for the systemsaffective state. We can add new rules to these databases and knowledge bases while theserver is still running.
",False,_TSD my_eliza,False,False,True
22774,"The next step will be to extend the system with a speech interface instead of the typed
",False,_TSD my_eliza,False,False,True
22775,"text interface currently used. The emotions can then be extracted by shallow parsing fromthe spoken words and be combined with emotions deduced from prosodic cues to get amore accurate indication. Furthermore, the static facial displays used in the prototypewill be replaced by a 3D animated talking face that is currently under development withinour group [14].A Multi-modal Eliza Using Natural Language Processing and Emotion Recognition 399
",False,_TSD my_eliza,False,False,True
22776,"References
",False,_TSD my_eliza,False,False,True
22777,"1. Wahlster, W., Reithinger, N., Blocher, A.: SmartKom: Multimodal Communication with
",False,_TSD my_eliza,False,False,True
22778,"a Life-Like Character, Proceedings of Eurospeech 2001, Aalborg Denmark, 2001
",False,_TSD my_eliza,False,False,True
22779,"2. Ma, J., Yan, J., Cole, R.: CU Animate Tools for Enabling Conversations with Animated
",False,_TSD my_eliza,False,False,True
22780,"Characters, Proceeding of ICSLP 2002, Denver, CO USA, September 2002.
",False,_TSD my_eliza,False,False,True
22781,"3. King, Donnel: Nonverbal communication, http://www2.pstcc.cc.tn.us/ dking, 1997.4. Weizenbaum, J.: ELIZA - A Computer Program for the Study of Natural Language Commu-
",False,_TSD my_eliza,False,False,True
22782,"nication between Man and Machine, Communication of the ACM 9(1): p36-p45, 1966.
",False,_TSD my_eliza,False,False,True
22783,"5. Wallace, Richard: Alicebot, http://www.Alicebot.org, 1995.6. Bazzan, Bordini: A Framework for the Simulation of Agents with Emotions, Report on Ex-
",False,_TSD my_eliza,False,False,True
22784,"periments with the Iterated Prisoner’s Dilemma, AGENT’01, Communication of ACM, p292-
",False,_TSD my_eliza,False,False,True
22785,"p299, Canada, 2001.
",False,_TSD my_eliza,False,False,True
22786,"7. Elliott, Siegle: V ariables Inﬂuencing the Intensity of Simulated Affective States, In AAAI
",False,_TSD my_eliza,False,False,True
22787,"Technical Report for Spring Symposium on Reasoning about Mental States: Formal Theories
",False,_TSD my_eliza,False,False,True
22788,"and Applications, 58-67, American Association for Artiﬁcial Intelligence, 1993.
",False,_TSD my_eliza,False,False,True
22789,"8. Elliott: Using the Affective Reasoner to Support Social Simulations, In Proceeding of the
",False,_TSD my_eliza,False,False,True
22790,"13th International Joint Conference on Artiﬁcial Intelligence, 194-200, Chambery, 1993.
",False,_TSD my_eliza,False,False,True
22791,"9. Ekman, Friesen: Unmasking the Face, Prentice Hall, New Jersey, USA, 1975.
",False,_TSD my_eliza,False,False,True
22792,"10. Davitz: the language of emotions, New Y ork, Academic Press, 1969.11. Frijda, N.H: The emotions, Cambridge University Press, Cambridge.12. Fehr, B., Russel, J.A: Concept of emotion viewed from a prototype pespective, In journal of
",False,_TSD my_eliza,False,False,True
22793,"experimental psychology, 113, 464-486.
",False,_TSD my_eliza,False,False,True
22794,"13. Hendix, Ruttkay: Exploring the Space of Emotional Faces of Subjects without Acting Expe-
",False,_TSD my_eliza,False,False,True
22795,"rience, ACM Computing Classiﬁcation System: H.5.2, I.5.3, J.4, 1998.
",False,_TSD my_eliza,False,False,True
22796,"14. Wojdel, A., Rothkrantz, L.J.M.: A text based talking face, in proceedings of TSD 2000.",False,_TSD my_eliza,False,False,True
