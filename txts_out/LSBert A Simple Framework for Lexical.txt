JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 1
LSBert: A Simple Framework for Lexical
SimpliÔ¨Åcation
Jipeng Qiang, Yun Li, Yi Zhu, Yunhao Yuan, and Xindong Wu, Fellow, IEEE,
Abstract ‚ÄîLexical simpliÔ¨Åcation (LS) aims to replace complex words in a given sentence with their simpler alternatives of equivalent
meaning, to simplify the sentence. Recently unsupervised lexical simpliÔ¨Åcation approaches only rely on the complex word itself
regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious
candidates. In this paper, we propose a lexical simpliÔ¨Åcation framework LSBert based on pretrained representation model Bert, that is
capable of (1) making use of the wider context when both detecting the words in need of simpliÔ¨Åcation and generating substitue
candidates, and (2) taking Ô¨Åve high-quality features into account for ranking candidates, including Berts prediction order, Bert-based
language model, and the paraphrase database PPDB, in addition to the word frequency and word similarity commonly used in other LS
methods. We show that our system outputs lexical simpliÔ¨Åcations that are grammatically correct and semantically appropriate, and
obtains obvious improvement compared with these baselines, outperforming the state-of-the-art by 29.8 Accuracy points on three
well-known benchmarks.
Index Terms ‚ÄîLexical simpliÔ¨Åcation, BERT, Unsupervised, Pretrained language model.
F
1 I NTRODUCTION
Lexical SimpliÔ¨Åcation (LS) aims at replacing complex words
with simpler alternatives, which can help various groups of peo-
ple, including children [1], non-native speakers [2], people with
cognitive disabilities [3], [4], to understand text better. LS is an
effective way of simplifying a text because some work shows that
those who are familiar with the vocabulary of a text can often
understand its meaning even if the grammatical constructs used
are confusing to them. The LS framework is commonly framed
as a pipeline of three steps: complex word identiÔ¨Åcation (CWI),
substitute generation (SG) of complex words, and Ô¨Åltering and
substitute ranking (SR). CWI is often treated as an independent
task [5]. Existing LS systems mainly focused on the two steps
(SG and SR) [6].
The popular LS systems still predominantly use a set of rules
for substituting complex words with their frequent synonyms
from carefully handcrafted databases (e.g., WordNet) [7] or au-
tomatically induced from comparable corpora [1] or paraphrase
database [8]. Recent work utilizes word embedding models to
extract substitute candidates for complex words. Given a complex
word, they extracted the top 10 words as substitute candidates from
the word embedding model whose vectors are closer in terms of
cosine similarity with the complex word [2], [5], [9]. Recently,
the LS system REC-LS attempts to generate substitute candidates
by combining linguistic databases and word embedding models.
However, they generated substitute candidates for the complex
word regardless of the context of the complex word, which
will inevitably produce a large number of spurious candidates
J. Qiang, Y. Li, Y. Zhu, and Y. Yuan are with the Department of Computer
Science, Yangzhou, Jiangsu, China.
E-mail:fjpqiang,liyun, zhuyi, yhyuan g@yzu.edu.cn
X. Wu is with Key Laboratory of Knowledge Engineering with Big Data
(Hefei University of Technology), Ministry of Education, Hefei, Anhui,
China, and Mininglamp Academy of Sciences, Minininglamp, Beijing,
China.
E-mail: xwu@hfut.edu.cn
Fig. 1. Comparison of substitute candidates of complex words. Given
one sentence ‚ÄùJohn composed these verses.‚Äù and complex words ‚Äôcom-
posed‚Äô and ‚Äôverses‚Äô, the top three simpliÔ¨Åcation candidates for each
complex word are generated by our method LSBert and the state-of-the-
art two baselines (Glava Àás [9] and REC-LS [6]). The simpliÔ¨Åed sentences
by the three LS methods are shown at the bottom.
that confuse the systems employed in the subsequent steps. For
example, if simpler alternatives of the complex word do not exist
in substitute candidates, the Ô¨Åltering and substitute ranking step of
LS is meaningless.
Context plays a central role in fulÔ¨Ålling substitute generation.
Here, we give a simple example shown in Figure 1. For complex
words ‚Äôcomposed‚Äô and ‚Äôverses‚Äô in the sentence ‚ÄùJohn composed
these verses.‚Äù, the top three substitute candidates of the two
complex words generated by the state-of-the-art LS systems [6],arXiv:2006.14939v1  [cs.CL]  25 Jun 2020JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 2
[9] are only related with the complex words itself regardless of
the context. For example, the candidates ‚Äùconsisting, consists,
comprised‚Äù is generated by Glava Àás [9] for the complex word
‚Äùcomposed‚Äù, and the candidates ‚Äùframed, quieted, planned‚Äù is
produced by REC-LS [6].
In contrast to the existing LS methods that only considered
the context in the last step (substitute ranking), we present a novel
LS framework LSBert, which takes the context into account in all
three steps of LS. As word complexity depends on context, LSBert
uses a novel approach to identify complex words using a sequence
labeling method [10] based on bi-directional long short-term
memory units (BiLSTM). For producing suitable simpliÔ¨Åcations
for the complex word, we exploit recent advances in pretrained
unsupervised deep bidirectional representations Bert [11] . More
speciÔ¨Åcally, we mask the complex word wof the original sentence
Sas a new sentence S0, and concatenate the original sequence
SandS0for feeding into the Bert to obtain the probability
distribution of the vocabulary corresponding to the masked word.
Then we choose the top probability words as substitute candidates.
For ranking the substitutions, we adopt Ô¨Åve high-quality
features including word frequency and word similarity, Berts
prediction order, Bert-based language model, and the paraphrase
database PPDB, to ensure grammaticality and meaning equiva-
lence to the original sentence in the output. LSBert simpliÔ¨Åes one
word at a time and is recursively applied to simplify the sentence
by taking word complexity in context into account. As shown
in Figure 1, the meaning of the original sentence using Glava Àás
is changed, and REC-LS does not make the right simpliÔ¨Åcation.
LSBert generates the appropriate substitutes and achieves its aim
that replaces complex words with simpler alternatives.
This paper has the following two contributions:
(1) LSBert is a novel Bert-based method for LS, which can
take full advantages of Bert to generate and rank substitute
candidates. To our best knowledge, this is the Ô¨Årst attempt to
apply pretrained transformer language models for LS. In contrast
to existing methods without considering the context in complex
word identiÔ¨Åcation and substitute generations, LSBert is easier to
hold cohesion and coherence of a sentence, since LSBert takes the
context into count for each step of LS
(2) LSBert is a simple, effective and complete LS method.
1)Simple: many steps used in existing LS systems have been
eliminated from our method, e.g., morphological transformation.
2) Effective: it obtains new state-of-the-art results on three bench-
marks. 3) Complete: LSBert recursively simpliÔ¨Åes all complex
words in a sentence without requiring additional steps.
To facilitate reproducibility, the code of LSBert framework is
available at https://github.com/BERT-LS.
The rest of this paper is organized as follows. In Section 2,
we introduce the related work of text simpliÔ¨Åcation. Section 3
describes the framework LSBert. In Section 4, we describe the
experimental setup and evaluate the proposed method LSBert.
Finally, we draw our conclusions in Section 5.
2 R ELATED WORK
Textual simpliÔ¨Åcation (TS) is the process of simplifying the
content of the original text as much as possible, while retaining
the meaning and grammaticality so that it can be more easily
read and understood by a wider audience. Textual simpliÔ¨Åcation
focuses on simplifying the vocabulary and syntax of the text. Early
systems of TS often used standard statistical machine translationapproaches to learn the simpliÔ¨Åcation of a complex sentence into a
simpliÔ¨Åed sentence [12]. Recently, TS methods adopted encoder-
decoder model to simplify the text based on parallel corpora [13]‚Äì
[15]. All of the above work belong to the supervised TS systems,
whose performance strongly relies on the availability of large
amounts of parallel sentences. Two public parallel benchmarks
WikiSmall [16] and WikiLarge [17] contain a large proportion of:
inaccurate simpliÔ¨Åcations (not aligned or only partially aligned) ;
inadequate simpliÔ¨Åcations (not much simpler) [18], [19]. These
problems is mainly because designing a good alignment algorithm
for extracting parallel sentences from EW and SEW is very
difÔ¨Åcult [20]. Therefore, a number of approaches focusing on the
generation and assessment of lexical simpliÔ¨Åcation were proposed.
Lexical simpliÔ¨Åcation (LS) only focuses to simplify complex
words of one sentence. LS needs to identify complex words and
Ô¨Ånd the best candidate substitution for these complex words [21],
[22]. The best substitution needs to be more simplistic while
preserving the sentence grammatically and keeping its meaning as
much as possible, which is a very challenging task. The popular
lexical simpliÔ¨Åcation approaches were rule-based, in which each
rule contains a complex word and its simple synonyms [8], [23],
[24]. Rule-based systems usually identiÔ¨Åed synonyms from Word-
Net or other linguistic databases for a predeÔ¨Åned set of complex
words and selected the ‚Äùsimplest‚Äù from these synonyms based on
the frequency of word or length of word [1], [7]. However, there is
a major limitation for the rule-based systems that it is impossible
to give all possible simpliÔ¨Åcation rules for each word.
As complex and simpliÔ¨Åed parallel corpora are available, LS
systems tried to extract rules from parallel corpora [25]‚Äì[27].
Yatskar et al. (2010) identiÔ¨Åed lexical simpliÔ¨Åcations from the edit
history of simple English Wikipedia (SEW). They utilized a prob-
abilistic method to recognize simpliÔ¨Åcation edits distinguishing
from other types of content changes. Biran et al. (2011) considered
every pair of distinct word in the English Wikipedia (EW) and
SEW to be a possible simpliÔ¨Åcation pair, and Ô¨Åltered part of
them based on morphological variants and WordNet. Horn et al.
(2014) also generated the candidate rules from the EW and SEW,
and adopted a context-aware binary classiÔ¨Åer to decide whether a
candidate rule should be adopted or not in a certain context. The
main limitation of the type of methods relies heavily on parallel
corpora.
To entirely avoid the requirement of lexical resources or paral-
lel corpora, LS systems based on word embeddings were proposed
[9]. They extracted the top 10 words as candidate substitutions
whose vectors are closer in terms of cosine similarity with the
complex word. Instead of a traditional word embedding model,
Paetzold and Specia (2016) adopted context-aware word embed-
dings trained on a large dataset where each word is annotated with
the POS tag. Afterward, they further extracted candidates for the
complex word by combining word embeddings with WordNet and
parallel corpora [5]. REC-LS [6] attempted to generate substitutes
from multiple sources, e.g, WordNet, Big Huge Thesaurus1and
word embeddings.
After examining existing LS methods ranging from rules-
based to embedding-based, the major challenge is that they gen-
erated simpliÔ¨Åcation candidates for the complex word regardless
of the context of the complex word, which will inevitably produce
a large number of spurious candidates that confuse the systems
employed in the subsequent steps.
1. https://words.bighugelabs.comJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 3
Fig. 2. Overview of the lexical simpliÔ¨Åcation framework LSBert.
In this paper, we will Ô¨Årst present a LS approach LSBert
that requires only a sufÔ¨Åciently large corpus of raw text without
any manual efforts. Pre-training language models [11], [28], [29]
have attracted wide attention and has shown to be effective for
improving many downstream natural language processing tasks.
Our method exploits recent advances in Bert to generate suitable
simpliÔ¨Åcations for complex words. Our method generates the
candidates of the complex word by considering the whole sentence
that is easier to hold cohesion and coherence of a sentence. In
this case, many steps used in existing LS methods have been
eliminated from our method, e.g., morphological transformation.
The previous version LSBert was published in artiÔ¨Åcial intelli-
gence conference (AAAI) [30], which only focused on substitute
generations given the sentence and its complex word using Bert.
In this paper, we propose an LS framework including complex
word identiÔ¨Åcation, substitute generations, substitute ranking. The
framework can simplify one sentence recursively. One recent
work for LS based Bert [31] was almost simultaneously proposed
with our previous version, which also only focused on substitute
generations.
3 L EXICAL SIMPLIFICATION FRAMEWORK
In this section, we outline each step of our lexical simpliÔ¨Åca-
tion framework LSBert as presented in Figure 2, which includes
the following three steps: complex word identiÔ¨Åcation, substitute
generation, Ô¨Åltering and substitute ranking. LSBert simliÔ¨Åes one
complex word at a time, and is recursively applied to simplify the
sentence. We will give the details of each step below.
3.1 Complex Word IdentiÔ¨Åcation (CWI)
Identifying complex words from one sentence has been studied
for years, whose goal is to select the words in a given sentence
which should be simpliÔ¨Åed [32], [33].CWI was framed as a sequence labeling task [10] and an
approach SEQ based on bi-directional long short-term memory
units (BiLSTM) is trained to predict the binary complexity of
words as annotated in the dataset of [34]. In contrast to the other
CWI models, the SEQ model has the following two advantages:
takes word context into account and helps avoid the necessity of
extensive feature engineering, because SEQ only relies on word
embeddings as the only input information.
The SEQ approach labels each word with a lexical complexity
score ( p) which represents the likelihood of each word belonging
to the complex class. Giving a predeÔ¨Åned threshold p, if the
lexical complexity of one word is greater than the threshold, it
will be treated as a complex word. For example, the example
‚ÄùJohn composed 0:55these verses 0:76‚Äù is showed in Figure 2. If
the complexity threshold is set to 0.5, the two words ‚Äùcomposed‚Äù
and ‚Äùverses‚Äù will be the complex words to be simpliÔ¨Åed.
LSBert starts with the word ‚Äùverses‚Äù with the highest pvalue
above the predeÔ¨Åned threshold to simplify. After completing the
simpliÔ¨Åcation process, we will recalculate the complexity of each
word in the sentence, excluding words that have been simpliÔ¨Åed.
In addition, we exclude the simpliÔ¨Åcation of entity words by
performing named entity identiÔ¨Åcation on the sentence.
3.2 Substitute Generation (SG)
Given a sentence Sand the complex word w, the aim of
substitution generation (SG) is to produce the substitute candi-
dates for the complex word w. LSBert produces the substitute
candidates for the complex word using pretrained language model
Bert. we brieÔ¨Çy summarize the Bert model, and then describe how
we extend it to do lexical simpliÔ¨Åcation.
Bert [11] is a self-supervised method for pretrained a deep
transformer encoder, which is optimized by two training ob-
jectives: masked language modeling (MLM) and next sentence
prediction (NSP). Unlike a traditional language modeling ob-
jective of predicting the next word in a sequence given the
history, MLM predicts missing tokens in a sequence given its
left and right context. Bert accomplishes NSP task by prepending
every sentence with a special classiÔ¨Åcation token, [CLS], and by
combining sentences with a special separator token, [SEP]. The
Ô¨Ånal hidden state corresponding to the [CLS] token is used as the
total sequence representation from which we predict a label for
classiÔ¨Åcation tasks, or which may otherwise be overlooked.
Due to the fundamental nature of MLM, we mask the complex
word wof the sentence Sand get the probability distribution of
the vocabulary p(jSnfwg)corresponding to the masked word w.
Therefore, we can try to use MLM for substitute generation.
For the complex word win a sentence S, we mask the word
wofSusing special symbol ‚Äù[MASK]‚Äù as a new sequence S0. If
we directly feed S0into MLM, the probability of the vocabulary
p(jS0nftig)corresponding to the complex word wonly considers
the context regardless of the inÔ¨Çuence of the complex word w.
Considering that Bert is adept at dealing with sentence pairs due
to the NSP task adopted by Bert. We concatenate the original
sequence SandS0as a sentence pair, and feed the sentence
pair (S; S0)into the Bert to obtain the probability distribution of
the vocabulary p(jS; S0nfwg)corresponding to the mask word.
Thus, the higher probability words in p(jS; S0nfwg)correspond-
ing to the mask word not only consider the complex word itself,
but also Ô¨Åt the context of the complex word.
Finally, we select the top 10 words from p(jS; S0nfwg)as
substitution candidates, excluding the morphological derivationsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 4
,
Fig. 3. Substitution generation of LSBert for the target complex word prediction, or cloze task. The input text is ‚Äùthe cat perched on the mat‚Äù with
complex word ‚Äùperched‚Äù. [MASK], [CLS] and [SEP] are thress special symbols in Bert, where [MASK] is used to mask the word, [CLS] is added in
front of each input instance and [SEP] is a special separator token.
ofw. In addition, considering that the contextual information of
the complex word is used twice, we randomly mask a certain
percentage of words in Sexcluding wfor appropriately reducing
the impact of contextual information.
See Figure 3 for an illustration. Suppose that there is a sentence
‚Äùthe cat perched on the mat‚Äù and the complex word ‚Äùperched‚Äù, we
get the top three substitute candidates ‚Äùsat, seated, hopped‚Äù. We
can see that the three candidates not only have a strong correlation
with the complex word, but also hold the cohesion and coherence
properties of the sentence. If we adopt the existing state-of-the-art
methods [9] and [6], the top three substitution words are ‚Äùatop,
overlooking, precariously‚Äù and ‚Äùput, lighted, lay‚Äù, respectively.
Very obviously, our method generates better substitute candidates
for the complex word.
3.3 Filtering and Substitute Ranking (SR)
Giving substitute candidates C=fc1; c2; :::; c ng, the substi-
tution ranking of the lexical simpliÔ¨Åcation framework is to decide
which one of the candidate substitutions that Ô¨Åts the context of
complex word is the simplest [22], where nis the number of
substitute candidates. First, threshold-based Ô¨Åltering is performed
by LSBert, which is used to remove some complex substitutes.
Substitutes are removed from consideration if their Zipf values
below 3 using Frequency features. Then, LSBert computes var-
ious rankings according to their scores for each of the features.
After obtaining all rankings for each feature, LSBert scores each
candidate by averaging all its rankings. Finally, we choose the
candidate with the highest ranking as the best substitute.
Previous work for this step is based on the following fea-
tures: word frequency, contextual simplicity and Ngram language
modeling, etc. In contrast to previous work, in addition to the
word frequency and word similarity commonly used in other LS
methods, LSBert considers three additional high-quality features:
two features about Bert and one feature about PPDB (A Paraphrase
Database for SimpliÔ¨Åcation).
Bert prediction order. On this step of substitute generation,
we obtain the probability distribution of the vocabulary corre-
sponding to the mask word. Because LSBert already incorporates
the context information on the step of substitution generation, theword order of Bert prediction is a crucial feature which includes
the information of both the context and the complex word itself.
The higher the probability, the more relevant the candidate for the
original sentence.
Language model feature. A substitution candidate should Ô¨Åt
into the sequence of words preceding and following the original
word. We cannot directly compute the probability of a sentence
or sequence of words using Bert like traditional n-gram language
models. Let W=w m; :::; w 1; w; w 1; :::; w mbe the context
of the original word w. We adopt a new strategy to compute the
likelihood of W. We Ô¨Årst replace the original word wwith the
substitution candidate. We then mask one word of Wfrom front
to back and feed into Bert to compute the cross-entropy loss of
the mask word. Finally, we rank all substitute candidates based on
the average loss of W. The lower the loss, the substitute candidate
is a good substitution for the original word. We use as context a
symmetric window of size Ô¨Åve around the complex word.
Semantic similarity. The similarity between the complex
word and the substitution candidate is widely used as a feature
for SR. In general, word embedding models are used to obtain the
vector representation and the cosine similarity metric is chosen to
compute the similarity. Here, we choose the pretrained fastText
model2as word embedding modeling. The higher the similarity
value, the higher the ranking.
Frequency feature. Frequency-based candidate ranking
strategies are one of the most popular choices by lexical simpliÔ¨Å-
cation and quite effective. In general, the more frequency a word
is used, the most familiar it is to readers. We adopt the Zipf scale
created from the SUBTLEX lists [35], because some experiments
[22] revealed that word frequencies from this corpus correlate with
human judgments on simplicity than many other more widely used
corpora, such as Wikipedia. SUBTLEX3is composed of over six
million sentences extracted from subtitles of assorted movies. The
Zipf frequency of a word is the base-10 logarithm of the number
of times it appears per billion words.
2. https://dl.fbaipublicÔ¨Åles.com/fasttext/vectors-english/crawl-300d-2M-
subword.zip
3. http://subtlexus.lexique.orgJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 5
Algorithm 1 Lexical simpliÔ¨Åcation framework
1:S Input Sentence
2:t Complexity threshold
3:ignore list Named Entity IdentiÔ¨Åcation( S)
4:LSBert( S,t,ignore list)
PPDB feature. Some LS methods generated substitute candi-
dates from PPDB or its subset SimplePPDB [8], [36]. PPDB is a
collection of more than 100 million English paraphrase pairs [37].
These pairs were extracted using a bilingual pivoting technique,
which assumes that two English phrases that translate to the
same foreign phrase have the same meaning. Since LSBert has a
better substitution generation than PPDB and SimplePPDB, they
cannot help improve the performance of substitution generation.
Considering PPDB owns useful information about paraphrase, we
try to use PPDB as a feature to rank the candidate substitutions.
We adopt a simple strategy for PPDB to rank the candidates. For
each candidate ciinCofw, the ranking of ciis 1 if the pair
(w; ci)exists in PPDB. Otherwise, the ranking number of ciis
n=3.
3.4 LSBert Algorithm
Following CWI, substitute generation, Ô¨Åltering and substitute
ranking steps, the overall simpliÔ¨Åcation algorithm LSBert is shown
in Algorithm 1 and Algorithm 2. Given the sentence Sand
complexity threshold t, we Ô¨Årst identify named entity using entity
identiÔ¨Åcation system4. We add entities into ignore list which
means these words do not need to be simpliÔ¨Åed.
In LSBert, we identify all complex words in sentence s
using CWI step excluding ignore list(line 1). If the number of
complex words in the sentence sis larger than 0 (line 2), LSBert
will try to simplify the top complex word w(line 3). LSBert calls
substitute generation (line 4) and substitute ranking (line 5) in turn.
LSBert chooses the top substitute (line 6). One important thing to
notice is whether LSBert performs the simpliÔ¨Åcation only if the
top candidate tophas a higher frequency (Frequency feature) or
lower loss (Language model feature) than the original word (line
7). When LSBert performs the simpliÔ¨Åcation, it will replace winto
top(line 8) and add the word topintoignore list(line 9). After
completing the simpliÔ¨Åcation of one word, we will iteratively call
LSBert (line 10 and line 12). If the number of complex words in
Sequals to 0, we will stop calling LSBert (line 15).
4 E XPERIMENTS
We design experiments to answer the following questions:
Q1. The effectiveness of substitute candidates and ranking:
Does the simpliÔ¨Åcation candidate generation of LSBert outper-
forms the substitution generation of the state-of-the-art competi-
tors?
Q2. The effectiveness of the LS system: Do the of LSBert
outperforms the full pipeline of the state-of-the-art competitors?
Q3. The factors of affecting the LSBert: Experiments on
different parameters and models verify the impact on the LSBert
system.
Q4. The qualitative study of the LSBert: We do more
experiments to analyze the advantages and the disadvantages of
LSBert.
4. https://spacy.io/Algorithm 2 LSBert ( S,t,ignore list)
1:complex words CWI(S,t)-ignore list
2:ifnumber( complex words )>0then
3:w head(complex words )
4:subs Substitution Generation( S,w)
5:subs Substitute Ranking( subs )
6:top head(subs )
7: iffre(top)>fre (w)orloss (top)<loss (w)then
8: Replace( S,w,top)
9: ignore list.add(w)
10: LSBert( S,t,ignore list)
11: else
12: LSBert( S,t,ignore list)
13: end if
14:else
15: return S
16:end if
Dataset . We choose the following datasets to evaluate our
framework LSBert from lexical simpliÔ¨Åcation datasets and text
simpliÔ¨Åcation dataset.
(1) We use three widely used lexical simpliÔ¨Åcation datasets
(LexMTurk5[27], BenchLS6[2], NNSeval7[22]) to do experi-
ments. The details of the three datasets are illustrated in this paper
[22]. Notice that, because these datasets already offer the target
words regarded complex by human annotators, we do not address
complex word identiÔ¨Åcation task in our evaluations using the three
datasets. These datasets contain instances composed of a sentence,
a target complex word, and a set of suitable substitutions provided
and ranked by humans with respect to their simplicity.
(2) We use one widely used text simpliÔ¨Åcation dataset (Wik-
iLarge) to do experiments [17]. The training/development/test set
in WikiLarge have 296,402/2000/359 sentence pairs, respectively.
WikiLarge is a set of automatically aligned complex-simple sen-
tence pairs from English Wikipedia (EW) and Simple English
Wikipedia (SEW). Its validation and test sets are taken from Turk-
corpus, where each original sentence has 8 human simpliÔ¨Åcations
created by Amazon Mechanical Turk workers.
4.1 Experiment Setup
We choose the following baselines to comparison:.
(1) Linguistic databases. Devlin [7] extracts synonyms of
complex words from WordNet. Yamamoto [38] is proposed for
Japanese based on dictionary deÔ¨Ånitions to extract substitute
candidates. Here, Yamamoto is adapted for English by using the
Merriam Dictionary to extract deÔ¨Ånitions of complex words.
(2) Parallel corpus. Biran [25] and Horn [27] perform substi-
tute generation (SG) through parallel corpora EW and SEW.
(3) Paraphrase database. SimplePPDB [8] performs SG with
a Ô¨Åltered paraphrase database (PPDB).
(4) Word embeddings. Glava Àás[9] performs SG with typical
word embeddings. Paetzold-CA [2] performs SG with context-
aware word embeddigns.
(5) Multipe source. PaetzoldNE [5] performs SG with parallel
corpora and context-aware word embeddigns. REC-LS [6] per-
forms SG with typical word embeedings and linguistic databases.
5. http://www.cs.pomona.edu/ dkauchak/simpliÔ¨Åcation/lex.mturk.14
6. http://ghpaetzold.github.io/data/BenchLS.zip
7. http://ghpaetzold.github.io/data/NNSeval.zipJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 6
LexMTurk BenchLS NNSeval
PRE RE F1 PRE RE F1 PRE RE F1
Yamamoto 0.056 0.079 0.065 0.032 0.087 0.047 0.026 0.061 0.037
Devlin 0.164 0.092 0.118 0.133 0.153 0.143 0.092 0.093 0.092
Biran 0.153 0.098 0.119 0.130 0.144 0.136 0.084 0.079 0.081
Horn 0.153 0.134 0.143 0.235 0.131 0.168 0.134 0.088 0.106
Glava Àás 0.151 0.122 0.135 0.142 0.191 0.163 0.105 0.141 0.121
Paetzold-CA 0.177 0.140 0.156 0.180 0.252 0.210 0.118 0.161 0.136
Paetzold-NE 0.310 0.142 0.195 0.270 0.209 0.236 0.186 0.136 0.157
REC-LS 0.151 0.154 0.152 0.129 0.246 0.170 0.103 0.155 0.124
Bert-mask 0.254 0.197 0.222 0.176 0.239 0.203 0.138 0.185 0.158
Bert 0.256 0.199 0.224 0.210 0.285 0.242 0.154 0.205 0.176
Bert-dropout 0.255 0.198 0.223 0.204 0.277 0.235 0.153 0.204 0.175
LSBert pre 0.287 0.223 0.251 0.231 0.314 0.267 0.185 0.246 0.211
LSBert 0.306 0.238 0.268 0.244 0.331 0.281 0.194 0.260 0.222
TABLE 1
Evaluation results of substitute generation on three datasets.
(6) Methods based on Bert. Here, we give multiple strategies to
perform SG using Bert. Bert-mask : we directly mask the complex
word of the sentence and feed it into Bert. Bert : we directly
feed the sentence into Bert to generate substitute generates. Bert-
dropout [31] applied dropout to the complex word‚Äôs embeddings
for partially masking the word. These Bert-based baselines are
based on the single sentence that uses to feed into Bert.
(7) Our proposed methods. LSBert preis our previous version.
LSBert is the proposed method in this paper. Our two methods
LSBert preand LSBert feed two sentences for Bert.
The experimental results of Devlin, Yamamoto, Biran, Horn,
and SimplePPDB, Glava Àás, Paetzold-CA, and Paetzold-NE are
from these two papers [5], [22]. For REC-LS method, we use the
code proposed by the authors. Bert-dropout was re-implemented
based on the original paper. In all experiments for methods based
on Bert, we use BERT-Large, Uncased (Whole Word Masking)
pre-trained on BooksCorpus and English Wikipedia8.
4.2 Quantitative Evaluation
(1) Evaluation of Substitute Candidates
The following three widely used metrics are used for evalua-
tion [2], [22], [39].
Precision (PRE) : The proportion of generated candidates that
are in the gold standard.
Recall (RE) : The proportion of gold-standard substitutions
that are included in the generated substitutions.
F1: The harmonic mean between Precision and Recall.
The results are shown in Table 1. As can be seen, our model
LSBert obtains the highest Recall and F1 scores on three datasets,
largely outperforming the previous best baseline Paetzold-NE,
increasing 37.4%, 19.1% and 41.4% using F1 metric. The base-
line Paetzold-NE by combining the Newsela parallel corpus and
context-aware word embeddings obtains better results on PRE
than LSBert, because it uses a different calculation method. If
one candidate exists in the gold standard, different morphological
derivations of the candidate in substitute candidates are all counted
into the PRE metric. Because of considering the context, the
substitute candidates of Bert based methods are normally different
words.
We note that the Bert based model is not only able to out-
perform other systems on all datasets using F1, but it also has
two clear practical advantages: (1) the only input information it
uses at run time is Bert without requiring linguistic database and
8. https://github.com/google-research/bertLexMTurk BenchLS NNSeval
PRE ACC PRE ACC PRE ACC
Yamamoto 0.066 0.066 0.044 0.041 0.444 0.025
Biran 0.714 0.034 0.124 0.123 0.121 0.121
Devlin 0.368 0.366 0.309 0.307 0.335 0.117
PaetzoldCA 0.578 0.396 0.423 0.423 0.297 0.297
Horn 0.761 0.663 0.546 0.341 0.364 0.172
Glava Àás 0.710 0.682 0.480 0.252 0.456 0.197
PaetzoldNE 0.676 0.676 0.642 0.434 0.544 0.335
REC-LS 0.786 0.256 0.734 0.335 0.665 0.218
LSBert pre 0.770 0.770 0.604 0.604 0.420 0.420
LSBert 0.864 0.792 0.697 0.616 0.526 0.436
TABLE 2
The evaluation results using Precision (PRE) and Accuracy (ACC) on
three datasets.
comparable corpus, (2) the substitute candidates using Bert do not
require morphological transformation.
For these baselines based on a single sentence (Bert-mask, Bert
and Bert-dropout), the gap between them is very small. Compared
with Bert based on a single sentence, our method LSBert preand
LSBert have better results, which verify that our strategy based
on sentence pairs Ô¨Åts for lexical simpliÔ¨Åcation. In conclusion, the
results clearly show that LSBert provides a good balance precision
and recall using only Bert.
(2) Evaluation of SG and SR
In this section, we evaluate the performance of various LS
systems that combines SG and SR. We adopt the following two
well-known metrics used by these work [22], [27].
Precision (PRE) : The proportion with which the replacement
of the original word is either the original word itself or is in the
gold standard.
Accuracy (ACC) : The proportion with which the replacement
of the original word is not the original word and is in the gold
standard.
It can be seen from these two metrices that if no simpliÔ¨Åcation
is carried out, the PRE value is 1 and the ACC value is 0. If all
complex words are replaced by the substitutions, the PRE and
ACC vaule have the same value.
The results are shown in Table 2. We can see that our method
LSBert attains the highest Accuracy on three datasets, which
has an average increase of 29.8% over the former state-of-the-
art baseline (Paetzold-NE). It suggests that LSBert is the most
proÔ¨Åcient in promoting simplicity. Paetzold-NE obtains higher
than LSBert on Precision on NNSeval, which also means that
many complex words are replaced by the original word itself, due
to the shortage of simpliÔ¨Åcation rules in parallel corpora. REC-LSJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 7
Methods SARI FRES
TS methodsDRESS-LS (2017) 37.27 75.33
EditNTS (2019) 38.22 73.81
PBMT (2020) 39.08 76.50
Access (2019) 41.87 81.55
LS methodsGlava Àás 30.70 81.82
REC-LS 37.11 69.58
LSBert 39.37 77.07
TABLE 3
Comparison of text simpliÔ¨Åcation methods on WikiLarge dataset.
obtains the best PRE and poor ACC, because it prefers the original
word as the substitute word.
In conclusion, although LSBert only uses raw text for pre-
trained Bert without using any resources, LSBert remains the best
lexical simpliÔ¨Åcation method. The results are in accordance with
the conclusions of the Substitute Generation.
(3)Evaluation of LS system for sentence simpliÔ¨Åcation
Lexical simpliÔ¨Åcation evaluation needs to be provided with
the sentence and the speciÔ¨Åed complex word. Here, we try to
simplify one sentence instead of one word, and choose a sentence
simpliÔ¨Åcation dataset (WikiLarge) for evaluation.
Since most LS methods only focused on one or two steps (SR
or SR) of LS, they cannot directly simplify one setence. Here,
we choose two complete LS systems (Glava Àás [9] and REC-LS
[6] ) to comparison. In additional, we choose four state-of-the-art
text simpliÔ¨Åcation (TS) methods DRESS-LS [17], EditNTS [15],
PBMT [40], and Access [41]. The Ô¨Årst three TS methods except
PBMT are sequence-to-sequence modelings and all need training
data sets to learn. PBMT is an unsupervised text simpliÔ¨Åcation
system based on phrase-based machine translation system. For
LS methods, they only use the testset to output the simpliÔ¨Åed
sentences. For LSBert and Rec-LS, the complexity threshold of
CWI is 0.5. For Glava Àás method, it tries to simplify all content
words (noun, verb, adjective, or adverb) of one sentence.
Following previous work, two widely used metrics (SARI and
FRES) in text simpliÔ¨Åcation are chosen in this paper [42], [43].
SARI [17] is a text-simpliÔ¨Åcation metric by comparing the output
against the simple and complex simpliÔ¨Åcations9. Flesch reading
ease score (FRES) measures the readability of the output [44]. A
higher FRES represents simpler output.
Table 3 shows the results of all models on WikiLarge dataset.
Our model LSBert obtains a SARI score of 39.37 and a FRES
score of 77.07, even outperforming these three supervised TS
systems (DRESS-LS, EditNTS and PBMT), which indicates that
the model has indeed learned to simplify the complex sentences.
Compared with LS methods Glava Àás and REC-LS, LSBert achieves
the best results. The two methods go to two different extremes, in
which Glava Àás simpliÔ¨Åes almost all content words of one sentence
and REC-LS prefers to save the original word. On the FRES
metric, we can see that Glava Àás outperforms LSBert, which is also
because it simpliÔ¨Åes almost all content words without caring for
the equivalent meaning with the original sentence. Compared with
Access, our model is highly competitive, because LSBert does not
need a parallel dataset to learn and only focuses on simplifying
the words. In conclusion, we can see that LSBert outperforms
previous LS baselines, even some supervised TS baselines, which
indicate that our method is effective at creating simpler output.
9. We used the implementation of SARI in [43].LexMTurk BenchLS NNSeval
PRE ACC PRE ACC PRE ACC
LSBert 0.864 0.792 0.697 0.616 0.526 0.436
w/o Bert 0.828 0.774 0.680 0.629 0.456 0.406
w/o Language 0.828 0.744 0.670 0.610 0.527 0.418
w/o Similarity 0.820 0.768 0.659 0.607 0.452 0.397
w/o Frequency 0.842 0.694 0.713 0.554 0.556 0.393
w/o PPDB 0.852 0.784 0.698 0.622 0.502 0.422
TABLE 4
Ablation study results of the ranking features.
SG SR
PRE RE F1 PRE ACC
LexMTurkBase 0.317 0.246 0.277 0.744 0.704
Large 0.333 0.259 0.291 0.792 0.750
WWM 0.306 0.238 0.268 0.864 0.792
BenchLSBase 0.233 0.317 0.269 0.586 0.537
Large 0.252 0.342 0.290 0.636 0.589
WWM 0.244 0.331 0.281 0.697 0.616
NNSevalBase 0.172 0.230 0.197 0.393 0.347
Large 0.185 0.247 0.211 0.402 0.360
WWM 0.194 0.260 0.222 0.526 0.436
TABLE 5
InÔ¨Çuence of different Bert models.
4.3 Ablation Study of LSBert
To further analyze the advantages and the disadvantages of
LSBert, we do more experiments in this section.
(1) InÔ¨Çuence of Ranking Features
To determine the importance of each ranking feature, we make
an ablation study by removing one feature in turn. The results are
presented in Table 4. We can see that LSBert combining all Ô¨Åve
features achieves the best results, which means all features have a
positive effect. LSBert removing frequency feature achieves better
results on PRE metric, but it decreases the values of ACC. These
features have different contributions for LSBert‚Äôs performance, for
example, PPDB feature brings the least impact on the performance
of LSBert compared with the other features. In this paper, LSBert
thinks all features are equally important, that may not be the best
option. In the future, we can improve LSBert by combining these
features using different weights.
(2) InÔ¨Çuence of Different Bert Modeling for Substitute
Generation
Pretrained Bert plays one vital role in LSBert. Bert has differ-
ent versions based on the parameter scale and training strategy.
Here, we attempt to investigate the inÔ¨Çuence of different Bert
versions on the performance of LSBert. We choose the following
three Bert models:
(1) Bert-based, uncased (Base): 12-layer, 768-hidden, 12-
heads, 110M parameters.
(2) Bert-large, uncased (Large): 24-layer, 1024-hidden, 16-
heads, 340M parameters.
(3) Bert-large, uncased, Whole Word Masking (WWM): 24-
layer, 1024-hidden, 16-heads, 340M parameters. The above two
Bert models randomly select WordPiece tokens to mask. Whole
Word Masking always masks all of the tokens corresponding to a
word at once.
Table 5 shows the results of the experiments using different
Bert models on three datasets. From Table 5, we can see that the
WWM model obtains the highest accuracy and precision over the
two other models. Besides, the Large model outperforms the Base
model. It can be concluded that a better Bert model can help to
improve the performance of LSBert system. If in the future a betterJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 8
Bert model is available, one can try to replace the Bert model in
this paper to further improve the performance of LS system.
(3) InÔ¨Çuence of the Number of Substitute Candidates
In this part, we try to investigate the inÔ¨Çuence of the number
of simpliÔ¨Åcation candidates to the performance of LSBert. The
number of candidates ranges from 5 to 60, respectively. Figure 4
shows the performance of substitute candidates (Precision, Recall
and F1), SR and SR (Precision, Accuracy) varying the number of
candidates on three benchmarks. When increasing the number of
candidates, the score of precision decreases and the score of recall
increases. When increasing the number of candidates, the score of
F1 Ô¨Årst increases, and declines Ô¨Ånally. The best performance of
LSBert through the experiments is achieved by setting the number
of candidates equals 10 for a good trade-off between precision and
recall. The score of the accuracy and precison of the SG and SR
(full) Ô¨Årst increase and converge Ô¨Ånally, which means that the SG
and SR is less sensitive to the number of candidates.
4.4 Qualitative Study
All of the above experiments are quantitative analyses of
LSBert. Here, we also qualitatively evaluate our model from three
aspects: substitute generation, substitute ranking and sentence
simpliÔ¨Åcation.
(1) The analysis of substitute generation results
When the number of substitute candidates is set to 10, the
proportion of LSBert that generates at least one valid substitute
candidate is 98.6% on Lexmturk dataset, namely, LSBert only
produces no effective substitute word in only 8 sentences. When
the number of generated candidates is 15, LSBert cannot generate
any valid candidates on only 4 sentences. When the number of
generated candidates is 30, only one sentence cannot be generated
valid candidate by LSBert. In this section, we will analyze the 8
sentences on Table 6.
We can see that LSBert can generate one or two valid substitute
candidates on these sentences (sent4, sent5, sent7 and sent8),
e.g, ‚Äùsenior- >powerful‚Äù, ‚Äùfull-Ô¨Çedged- >development‚Äù, ‚Äùkinetic-
>dynamic‚Äù, and ‚Äùedited- >altered‚Äù. Since the labels are provided
by humans, it is impossible to provide all suitable substitutes
in labels. LSBert fail to produce any valid candidate word on
the other sentences. When we analyze these wrong substitute
candidates, we can Ô¨Ånd that they can Ô¨Åt the context. We can guess
that LSBert mainly focuses more on the context and ignores the
meaning of the original word on these wrong examples.
(2) The analysis of substitute ranking results
LSBert can Ô¨Ånd one or more suitable alternatives for almost all
samples, but the Ô¨Ånal system results do not always select the most
suitable candidate as the Ô¨Ånal substitute. In this section, we will
analyze the possible reasons for this question. In Table 7, we give
some examples that LSBert cannot produce the right substitute
ranking.
From sent1 and sent3, we can see that the substitute ranking
(SR) chooses the best substitute, but LSBert still chooses the
original word. This is because the Zipf value of ‚Äùdivided‚Äù is
3.65 and the Zipf value of ‚ÄùclassiÔ¨Åed‚Äù is 3.83, LSBert considers
‚ÄùclassiÔ¨Åed‚Äù to be simpler than ‚Äùdivided‚Äù. It is the same reason for
sent3 in which the Zipf value of ‚Äùnoted‚Äù is 3.68 and the Zipf value
of ‚Äùreported‚Äù is 4.18. Consequently, in sent1 and sent3, the best
substitutes of SR cannot be used as the Ô¨Ånal substitutes.
The second case is that the best substitution of the SR step
is not from the labels provided by humans. In sent2 and sent4,LSBert chooses ‚Äùmaintained‚Äù as a simpler for ‚Äùretained‚Äù and
‚Äùnever‚Äù as a simpler for ‚Äùrarely‚Äù. We can Ô¨Ånd that these words
‚Äùmaintained‚Äù and ‚Äùnever‚Äù are also suitable substitutes, but do not
appear in the labels.
(3) The analysis of sentence simpliÔ¨Åcation results
The above qualitative study for LSBert need to provide the
complex word by humans. In this experiment, we try to verify
the results of LS methods on sentence simpliÔ¨Åcation. We also
choose the two methods Glava Àás and REC-LS to comparison.
Table 8 shows some examples from the WikiLarge dataset to be
simpliÔ¨Åed. We note that we draw the same conclusions from these
examples with LS system for sentence simpliÔ¨Åcation. Glava Àás tries
to simplify every content word in the sentence ignoring the aim
of LS. LS aims to replace complex words in a given sentence
with simpler alternatives of equivalent meaning. Rec-LS can make
the right simpliÔ¨Åcations, e.g., sentence 2. But, for sentence 1 and
sentence 3, Rec-LS outputs the original sentence. LSBert replaces
complex words with simpler alternatives and makes the most
reasonable simpliÔ¨Åcation. This veriÔ¨Åes that our framework LSBert
Ô¨Åts for lexical simpliÔ¨Åcation.
5 C ONCLUSION
We propose a simple BERT-based framework LSBert for
lexical simpliÔ¨Åcation (LS) by leveraging the idea of masking
language model of Bert. The existing LS methods only consider
the context of the complex word on the last step (substitute
ranking) of LS. LSBert focuses on the context of the complex
word on all steps of lexical simpliÔ¨Åcation without relying on the
parallel corpus or linguistic databases. Experiment results have
shown that our approach LSBert achieves the best performance
on three well-known benchmarks. Since Bert can be trained in
raw text, our method can be applied to many languages for
lexical simpliÔ¨Åcation. One limitation of our method is that it only
generates a single-word replacement for the complex word, but
we plan to extend it to support multi-word expressions. In the
future, the pretrained Bert model can be Ô¨Åne-tuned with just simple
English corpus (e.g., Newsela), and then we will use Ô¨Åne-tuned
Bert for lexical simpliÔ¨Åcation.
ACKNOWLEDGEMENT
This research is partially supported by the National Natu-
ral Science Foundation of China under grants 61703362 and
91746209; the National Key Research and Development Pro-
gram of China under grant 2016YFB1000900; the Program for
Changjiang Scholars and Innovative Research Team in Uni-
versity (PCSIRT) of the Ministry of Education, China, under
grant IRT17R32; and the Natural Science Foundation of Jiangsu
Province of China under grant BK20170513. This manuscript
is an extended version of the conference paper, titled Lexical
SimpliÔ¨Åcation with Pretrained Encoders, published in the Thirty-
Fourth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI), New
York, February 7-12, 2020.
REFERENCES
[1] J. De Belder, M.-F. Moens, Text simpliÔ¨Åcation for children, In Proceed-
ings of the 2010 SIGIR Workshop on Accessible Search Systems (2010)
19‚Äì26.
[2] G. H. Paetzold, L. Specia, Unsupervised lexical simpliÔ¨Åcation for non-
native speakers., in: AAAI, 2016, pp. 3761‚Äì3767.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 9
Fig. 4. InÔ¨Çuence of number of substitute candidates.
Sent1 Much of the water carried by these streams is diverted .
Labels drawn away, redirected, changed, turned, moved, rerouted, led away, sent away, separated, switched, split, ...
LSBert reclaimed, displaced, transferred, derived, pumped, routed, converted, recycled, discarded, drained
Sent2 ... , every person born into the world is enslaved to the service of sin and , apart from the efÔ¨Åcacious or prevenient grace of God, ...
Labels ever, present, showy, useful, effective, capable, strong, valuable, powerful, active, efÔ¨Åcient, helpful, generous, power, kindness, effect, ...
LSBert benevolent, exemplary, abundant, extraordinary, essential, inspired, ubiquitous, irresistible, exclusive, inclusive
Sent3 The Amazon Basin is the part of South America drained by the Amazon River and its tributaries .
Labels streams, branches, riverlets, adjacent, smaller rivers, channels, rivers, brooks, ditches, children creeks, offshoots, creeks
LSBert basins, drains, derivatives, headwaters, components, subsidiaries, minions, rays, sources, forks
Sent4 He held several senior positions in the Royal Flying Corps during World War I, ...
Labels high-level, older, upper, top, higher, high, superior, important, veteran, head, advance, top-level, advanced, leader, chief, principal, big
LSBert junior, signiÔ¨Åcant, prestigious, leadership, civil, command, formal, prominent, subordinate, powerful
Sent5 On 1 October 1983 the pilot project began operations as a full-Ô¨Çedged bank and was renamed the Grameen Bank to ...
Labels real, developed, fully operating, legitimate, total, complete, full, qualiÔ¨Åed, whole, major, working, full-service, ...
LSBert commercial, development, national, community, central, formal, private, public, bangladeshi, chartered
Sent6 The principal greenhouse , in an art nouveau style with ... , resembles the mid-19th century Crystal Palace in London .
Labels is similar to, looks like, looks-like, mimics, represents, matches, shows, mirrors, echos, look like, favors, appears like, ...
LSBert recalls, suggests, approaches, echoes, references, parallels, appears, depicts, incorporates, follows
Sent7 A perfectly elastic collision is deÔ¨Åned as one in which there is no loss of kinetic energy in the collision .
Labels active, moving, movement, motion, static, motive, innate, kinetic, real, strong, driving, motion related, motion-, living, powerful, ...
LSBert mechanical, rotational, dynamic, total, thermal, momentum, physical, the, potential, energetic
Sent8 None of your watched items were edited in the time period displayed .
Labels changed, looked at, reÔ¨Åned, revise, Ô¨Ånished, Ô¨Åxed, revised, revised, scanned, shortened
LSBert altered, incorporated, appropriate, modiÔ¨Åed, organized, Ô¨Åltered, included, blended, amended, enhanced
TABLE 6
The examples of substitute candidates that do not contain one valid substitution provided by humans on LexMTurk. The complex word of each
sentence are shown in bold.
Sent1 Triangles can also be classiÔ¨Åed according to their internal angles, measured here in degrees.
Labels grouped, categorized, arranged, labeled, divided, organized, separated, deÔ¨Åned, described, ...
LSBert SR divided ,described , separated, designated, ...
LSBert Substitute classiÔ¨Åed
Sent2 ...; he retained the conductorship of the Vienna Philharmonic until 1927.
Labels kept, held, had, got
LSBert SR maintained, held,kept , remained, continued, shared, ...
LSBert Substitute maintained
Sent3 ..., and a Venetian in Paris in 1528 also reported that she was said to be beautiful
Labels said, told, stated, wrote, declared, indicated, noted, claimed, announced, mentioned
LSBert SR noted , conÔ¨Årmed, described, claimed, recorded, said, ...
LSBert Substitute reported
Sent4 ..., the king will rarely play an active role in the development of an offensive or ....
Labels infrequently, hardly, uncommonly, barely, seldom, unlikely, sometimes, not, seldomly, ...
LSBert SR never, usually, seldom ,not,barely ,hardly , ...
LSBert Substitute never
TABLE 7
The examples that the Ô¨Ånal substitute generated by LSBert is not from the labels. The words in the substitute ranking belonging to the labels are
shown in bold.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 10
1Sentence Admission to Tsinghua is exceedingly competitive.
Label Entrance to Tsinghua is very very difÔ¨Åcult.
Glava Àás Offers toQinghua isvery exciting .
REC-LS Admission to Tsinghua is exceedingly competitive.
LSBert Entrance to Tsinghua is very tough .
2Sentence Many species had vanished by the end of the nineteenth century, with European settlement.
Label With Euopean settlement many species have been vanished.
Glava Àás Some birds wasgone by the time of the twentieth history , with world land .
REC-LS Many species had disappeared by the end of the 19th century, with European settlement.
LSBert Many animals haddisappeared by the end of the nineteenth century, with European settlement.
3Sentence In 1987 Wexler was inducted into the Rock and Roll Hall of Fame.
Label In 1987 Wexler was inducted into the Rock and Roll Hall of Fame.
Glava Àás In 1987 Livingston wasfame into the rock and youhall of hall.
REC-LS In 1987 Wexler was inducted into the Rock and Roll Hall of Fame.
LSBert In 1987 Wexler was elected into the Rock and Roll Hall of Honor .
4Sentence Oregano is an indispensable ingredient in Greek cuisine.
Label Oregano is a necessary ingredient in Greek cuisine.
Glava Àás Garlic is an essential ingredient in Greek cooking .
REC-LS Oregano is an essential element in Greek cuisine.
LSBert Oregano is an important element in Greek food.
5Sentence Their eyes are quite small, and their visual acuity is poor.
Label Their eyes are quite small, and their visual acuity is poor.
Glava Àás Their eyes have very little , and their musical visual isbad.
REC-LS Their eyes are quite small, and their ocular acuteness is poor.
LSBert Their eyes are quite small, and their visual ability isbad.
TABLE 8
The simpliÔ¨Åed sentences are shown using three different LS methods on WikiLarge dataset. Substitutions are shown in bold.
[3] L. Feng, Automatic readability assessment for people with intellectual
disabilities, ACM SIGACCESS accessibility and computing (93) (2009)
84‚Äì91.
[4] H. Saggion, Automatic text simpliÔ¨Åcation, Synthesis Lectures on Human
Language Technologies 10 (1) (2017) 1‚Äì137.
[5] G. Paetzold, L. Specia, Lexical simpliÔ¨Åcation with neural ranking, in:
ACL: V olume 2, Short Papers, 2017, pp. 34‚Äì40.
[6] S. Gooding, E. Kochmar, Recursive context-aware lexical simpliÔ¨Åcation,
in: Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 4855‚Äì4865.
[7] S. Devlin, J. Tait, The use of a psycholinguistic database in the simpli
cation of text for aphasic readers, Linguistic Databases 1 (1998) 161173.
[8] E. Pavlick, C. Callison-Burch, Simple ppdb: A paraphrase database for
simpliÔ¨Åcation, in: ACL: V olume 2, Short Papers, 2016, pp. 143‚Äì148.
[9] G. Glava Àás, S. ÀáStajner, Simplifying lexical simpliÔ¨Åcation: do we need
simpliÔ¨Åed corpora?, in: ACL, 2015, pp. 63‚Äì68.
[10] S. Gooding, E. Kochmar, Complex word identiÔ¨Åcation as a sequence
labelling task, in: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, 2019, pp. 1148‚Äì1153.
[11] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
bidirectional transformers for language understanding, arXiv preprint
arXiv:1810.04805.
[12] W. Coster, D. Kauchak, Simple english wikipedia: a new text simpliÔ¨Åca-
tion task, in: ACL, 2011, pp. 665‚Äì669.
[13] T. Wang, P. Chen, K. Amaral, J. Qiang, An experimental study of
lstm encoder-decoder model for text simpliÔ¨Åcation, arXiv preprint
arXiv:1609.03663.
[14] S. Nisioi, S. ÀáStajner, S. P. Ponzetto, L. P. Dinu, Exploring neural text
simpliÔ¨Åcation models, in: ACL, V ol. 2, 2017, pp. 85‚Äì91.
[15] Y . Dong, Z. Li, M. Rezagholizadeh, J. C. K. Cheung, Editnts: An
neural programmer-interpreter model for sentence simpliÔ¨Åcation through
explicit editing, in: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, 2019, pp. 3393‚Äì3402.
[16] Z. Zhu, D. Bernhard, I. Gurevych, A monolingual tree-based translation
model for sentence simpliÔ¨Åcation, in: Proceedings of the 23rd interna-
tional conference on computational linguistics, 2010, pp. 1353‚Äì1361.
[17] X. Zhang, M. Lapata, Sentence simpliÔ¨Åcation with deep reinforcement
learning, arXiv preprint arXiv:1703.10931.
[18] W. Xu, C. Callison-Burch, C. Napoles, Problems in current text simpliÔ¨Å-
cation research: New data can help, TACL 3 (1) (2015) 283‚Äì297.
[19] S. ÀáStajner, H. B ¬¥echara, H. Saggion, A deeper exploration of the standard
pb-smt approach to text simpliÔ¨Åcation and its evaluation, in: ACL, 2015,
pp. 823‚Äì828.
[20] W. Hwang, H. Hajishirzi, M. Ostendorf, W. Wu, Aligning sentences from
standard wikipedia to simple wikipedia, in: ACL, 2015, pp. 211‚Äì217.[21] M. Shardlow, A survey of automated text simpliÔ¨Åcation, International
Journal of Advanced Computer Science and Applications 4 (1) (2014)
58‚Äì70.
[22] G. H. Paetzold, L. Specia, A survey on lexical simpliÔ¨Åcation, in: Journal
of ArtiÔ¨Åcial Intelligence Research, V ol. 60, 2017, pp. 549‚Äì593.
[23] M. Lesk, Automatic sense disambiguation using machine readable dictio-
naries: How to tell a pine cone from an ice cream cone, in: Proceedings
of the 5th Annual International Conference on Systems Documentation,
1986, pp. 24‚Äì26.
[24] M. Maddela, W. Xu, A word-complexity lexicon and a neural readability
ranking model for lexical simpliÔ¨Åcation, in: EMNLP, 2018, pp. 3749‚Äì
3760.
[25] O. Biran, S. Brody, N. Elhadad, Putting it simply: a context-aware
approach to lexical simpliÔ¨Åcation, in: ACL, 2011, pp. 496‚Äì501.
[26] M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, L. Lee, For the sake
of simplicity: Unsupervised extraction of lexical simpliÔ¨Åcations from
wikipedia, in: NACACL, Association for Computational Linguistics,
2010, pp. 365‚Äì368.
[27] C. Horn, C. Manduca, D. Kauchak, Learning a lexical simpliÔ¨Åer using
wikipedia, in: ACL, V olume 2: Short Papers, 2014, pp. 458‚Äì463.
[28] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, J. Kang, Biobert:
pre-trained biomedical language representation model for biomedical text
mining, arXiv preprint arXiv:1901.08746.
[29] G. Lample, A. Conneau, Cross-lingual language model pretraining, arXiv
preprint arXiv:1901.07291.
[30] J. Qiang, Y . Li, Y . Zhu, Y . Yuan, X. Wu, Lexical simpliÔ¨Åcation with
pretrained encoders, Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intel-
ligence.
[31] W. Zhou, T. Ge, K. Xu, F. Wei, M. Zhou, Bert-based lexical substitution,
in: Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 2019, pp. 3368‚Äì3373.
[32] M. Shardlow, A comparison of techniques to automatically identify
complex words., in: 51st Annual Meeting of the Association for Com-
putational Linguistics Proceedings of the Student Research Workshop,
2013, pp. 103‚Äì109.
[33] S. M. Yimam, C. Biemann, S. Malmasi, G. H. Paetzold, L. Specia,
S.ÀáStajner, A. Tack, M. Zampieri, A report on the complex word
identiÔ¨Åcation shared task 2018 (2018) 66‚Äì78.
[34] S. M. Yimam, S. ÀáStajner, M. Riedl, C. Biemann, Cwig3g2-complex
word identiÔ¨Åcation task across three text genres and two user groups,
in: Proceedings of the Eighth International Joint Conference on Natural
Language Processing (V olume 2: Short Papers), 2017, pp. 401‚Äì407.
[35] M. Brysbaert, B. New, Moving beyond kucera and francis: A critical
evaluation of current word frequency norms and the introduction of a new
and improved word frequency measure for american english, Behavior
Research Methods 41 (4) (2009) 977‚Äì990.JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 11
[36] R. Kriz, E. Miltsakaki, M. Apidianaki, C. Callisonburch, SimpliÔ¨Åcation
using paraphrases and context-based lexical substitution, in: Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, 2018, pp. 207‚Äì217.
[37] J. Ganitkevitch, B. V . Durme, C. Callison-Burch, Ppdb: The paraphrase
database, in: NAACL-HLT, 2013, pp. 758‚Äì764.
[38] T. Kajiwara, H. Matsumoto, K. Yamamoto, Selecting proper lexical
paraphrase for children, in: ROCLING, 2013, pp. 59‚Äì73.
[39] G. Paetzold, L. Specia, Lexenstein: A framework for lexical simpliÔ¨Å-
cation, in: Proceedings of ACL-IJCNLP 2015 System Demonstrations,
2015, pp. 85‚Äì90.
[40] J. Qiang, X. Wu, Unsupervised statistical text simpliÔ¨Åcation, IEEE
Transactions on Knowledge and Data Engineering.
[41] L. Martin, B. Sagot, ¬¥E. de la Clergerie, A. Bordes, Controllable sentence
simpliÔ¨Åcation, arXiv preprint arXiv:1910.02677.
[42] K. Woodsend, M. Lapata, Learning to simplify sentences with quasi-
synchronous grammar and integer programming, in: EMNLP, 2011, pp.
409‚Äì420.
[43] W. Xu, C. Napoles, E. Pavlick, Q. Chen, C. Callison-Burch, Optimizing
statistical machine translation for text simpliÔ¨Åcation, TACL 4 (2016)
401‚Äì415.
[44] P. J. Kincaid, R. P. Fishburne, R. J. L. Richard, B. S. Chissom, Derivation
of new readability formulas (automated readability index, fog count
and Ô¨Çesch reading ease formula) for navy enlisted personnel, Technical
report, DTIC Document.
Jipeng Qiang is an assistant professor and the
group leader of Computational Linguistics and
Data Mining Group at Y anghou University. He
received his Ph.D. degree in computer science
and technology from Hefei university of Tech-
nology in 2016. He was a Ph.D. visiting student
in ArtiÔ¨Åcial Intelligence Lab at the University of
Massachusetts Boston from 2014 to 2016. His
research interests mainly include data mining
and computational linguistics. He has received
one grant from National Natural Science Foun-
dation of China, one grant from Natural Science Foundation of Jiangsu
Province of China, one grant from Natural Science Foundation of the
Higher Education Institutions of Jiangsu Province of China. He has
published more than 40 papers in data mining, artiÔ¨Åcial intelligence, and
computational linguistics conferences and journals.
Yun Li is currently a professor in the School
of Information Engineering, Y angzhou University,
China. He received the M.S. degree in computer
science and technology from Hefei University
of Technology, China, in 1991, and the Ph.D.
degree in control theory and control engineering
from Shanghai University, China, in 2005. He
has published more than 100 scientiÔ¨Åc papers.
His research interests include data mining and
cloud computing.
Yi Zhu is currently an assistant professor in the
School of information Engineering, Y angzhou
University, China. He received the BS degree
from Anhui University, the MS degree from Uni-
versity of Science and Technology of China, and
the PhD degree from Hefei University of Tech-
nology. His research interests are in data mining
and knowledge engineering. His research inter-
ests include data mining, knowledge engineer-
ing, and recommendation systems..
Yunhao Yuan is currently an associate profes-
sor in the School of information Engineering,
Y angzhou University, China. He received the M.
Eng. degree in computer science and technol-
ogy from Y angzhou University, China, in 2009,
and the Ph.D. degree in pattern recognition and
intelligence system from Nanjing University of
Science and Technology, China, in 2013. His
research interests include pattern recognition,
data mining, and image processing.
Xindong Wu is a Y angtze River Scholar in the
School of Computer Science and Information
Engineering at the Hefei University of Technol-
ogy, China, and the president of Mininglamp
Academy of Sciences, Minininglamp, Beijing,
China, and a fellow of IEEE and AAAS. He re-
ceived his B.S. and M.S. degrees in computer
science from the Hefei University of Technology,
China, and his Ph.D. degree in artiÔ¨Åcial intelli-
gence from the University of Edinburgh, Britain.
His research interests include data mining, big
data analytics, knowledge-based systems, and Web information ex-
ploration. He is currently the steering committee chair of the IEEE
International Conference on Data Mining (ICDM), the editor-in-chief of
Knowledge and Information Systems (KAIS, by Springer), and a series
editor-in-chief of the Springer Book Series on Advanced Information and
Knowledge Processing (AI&KP). He was the editor-in-chief of the IEEE
Transactions on Knowledge and Data Engineering (TKDE, by the IEEE
Computer Society) between 2005 and 2008. He served as program
committee chair/co-chair for the 2003 IEEE International Conference
on Data Mining, the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, and the 19th ACM Conference
on Information and Knowledge Management.