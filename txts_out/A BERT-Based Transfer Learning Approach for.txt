A BERT-Based Transfer Learning Approach for
Hate Speech Detection in Online Social Media
Marzieh Mozafari1, Reza Farahbakhsh1, and No el Crespi1
CNRS UMR5157, T el ecom SudParis, Institut Polytechnique de Paris, Evry, France
fmarzieh.mozafari, reza.farahbakhsh, noel.crespi g@telecom-sudparis.eu
Abstract. Generated hateful and toxic content by a portion of users
in social media is a rising phenomenon that motivated researchers to
dedicate substantial eorts to the challenging direction of hateful con-
tent identication. We not only need an ecient automatic hate speech
detection model based on advanced machine learning and natural lan-
guage processing, but also a suciently large amount of annotated data
to train a model. The lack of a sucient amount of labelled hate speech
data, along with the existing biases, has been the main issue in this do-
main of research. To address these needs, in this study we introduce a
novel transfer learning approach based on an existing pre-trained lan-
guage model called BERT (Bidirectional Encoder Representations from
Transformers). More specically, we investigate the ability of BERT at
capturing hateful context within social media content by using new ne-
tuning methods based on transfer learning. To evaluate our proposed ap-
proach, we use two publicly available datasets that have been annotated
for racism, sexism, hate, or oensive content on Twitter. The results show
that our solution obtains considerable performance on these datasets in
terms of precision and recall in comparison to existing approaches. Con-
sequently, our model can capture some biases in data annotation and
collection process and can potentially lead us to a more accurate model.
Keywords: hate speech detection, transfer learning, language model-
ing, BERT, ne-tuning, NLP, social media.
1 Introduction
People are increasingly using social networking platforms such as Twitter, Face-
book, YouTube, etc. to communicate their opinions and share information. Al-
though the interactions among users on these platforms can lead to constructive
conversations, they have been increasingly exploited for the propagation of abu-
sive language and the organization of hate-based activities [1,16], especially due
to the mobility and anonymous environment of these online platforms. Violence
attributed to online hate speech has increased worldwide. For example, in the
UK, there has been a signicant increase in hate speech towards the immigrant
and Muslim communities following the UK's leaving the EU and the Manch-
ester and London attacks1. The US also has been a marked increase in hate
1Anti-muslim hate crime surges after Manchester and London Bridge attacks (2017): https://www.
theguardian.comarXiv:1910.12574v1  [cs.SI]  28 Oct 20192 Marzieh Mozafari et al.
speech and related crime following the Trump election2. Therefore, governments
and social network platforms confronting the trend must have tools to detect
aggressive behavior in general, and hate speech in particular, as these forms of
online aggression not only poison the social climate of the online communities
that experience it, but can also provoke physical violence and serious harm [16].
Recently, the problem of online abusive detection has attracted scientic at-
tention. Proof of this is the creation of the third Workshop on Abusive Language
Online3or Kaggles Toxic Comment Classication Challenge that gathered 4,551
teams4in 2018 to detect dierent types of toxicities (threats, obscenity, etc.).
In the scope of this work, we mainly focus on the term hate speech as abusive
content in social media, since it can be considered a broad umbrella term for
numerous kinds of insulting user-generated content. Hate speech is commonly
dened as any communication criticizing a person or a group based on some char-
acteristics such as gender, sexual orientation, nationality, religion, race, etc. Hate
speech detection is not a stable or simple target because misclassication of reg-
ular conversation as hate speech can severely aect users freedom of expression
and reputation, while misclassication of hateful conversations as unproblematic
would maintain the status of online communities as unsafe environments [2].
To detect online hate speech, a large number of scientic studies have been
dedicated by using Natural Language Processing (NLP) in combination with
Machine Learning (ML) and Deep Learning (DL) methods [1, 8, 11, 13, 22, 25].
Although supervised machine learning-based approaches have used dierent text
mining-based features such as surface features, sentiment analysis, lexical re-
sources, linguistic features, knowledge-based features or user-based and platform-
based metadata [3, 6, 23], they necessitate a well-dened feature extraction ap-
proach. The trend now seems to be changing direction, with deep learning mod-
els being used for both feature extraction and the training of classiers. These
newer models are applying deep learning approaches such as Convolutional Neu-
ral Networks (CNNs), Long Short-Term Memory Networks (LSTMs), etc. [1,8]
to enhance the performance of hate speech detection models, however, they still
suer from lack of labelled data or inability to improve generalization property.
Here, we propose a transfer learning approach for hate speech understanding
using a combination of the unsupervised pre-trained model BERT [4] and some
new supervised ne-tuning strategies. As far as we know, it is the rst time
that such exhaustive ne-tuning strategies are proposed along with a genera-
tive pre-trained language model to transfer learning to low-resource hate speech
languages and improve performance of the task. In summary:
We propose a transfer learning approach using the pre-trained language
model BERT learned on English Wikipedia and BookCorpus to enhance
hate speech detection on publicly available benchmark datasets. Toward that
end, for the rst time, we introduce new ne-tuning strategies to examine
the eect of dierent embedding layers of BERT in hate speech detection.
2A.: Hate on the rise after Trumps election: http://www.newyorker.com
3https://sites.google.com/view/alw3/home
4https://www.kaggle.com/c/jigsaw-toxic-comment-classication-challenge/A BERT-based hate speech detection approach in social media 3
Our experiment results show that using the pre-trained BERT model and
ne-tuning it on the downstream task by leveraging syntactical and contex-
tual information of all BERT's transformers outperforms previous works in
terms of precision, recall, and F1-score. Furthermore, examining the results
shows the ability of our model to detect some biases in the process of col-
lecting or annotating datasets. It can be a valuable clue in using pre-trained
BERT model for debiasing hate speech datasets in future studies.
2 Previous Works
Here, the existing body of knowledge on online hate speech and oensive lan-
guage and transfer learning is presented.
Online Hate Speech and Oensive Language: Researchers have been study-
ing hate speech on social media platforms such as Twitter [3], Reddit [12, 14],
and YouTube [15] in the past few years. The features used in traditional ma-
chine learning approaches are the main aspects distinguishing dierent methods,
and surface-level features such as bag of words, word-level and character-level
n-grams, etc. have proven to be the most predictive features [11, 13, 22]. Apart
from features, dierent algorithms such as Support Vector Machines [10], Naive
Baye [16], and Logistic Regression [3,22], etc. have been applied for classication
purposes. Waseem et al. [22] provided a test with a list of criteria based on the
work in Gender Studies and Critical Race Theory (CRT) that can annotate a
corpus of more than 16 ktweets as racism, sexism, or neither. To classify tweets,
they used a logistic regression model with dierent sets of features, such as word
and character n-grams up to 4, gender, length, and location. They found that
their best model produces character n-gram as the most indicative features, and
using location or length is detrimental. Davidson et al. [3] collected a 24 Kcor-
pus of tweets containing hate speech keywords and labelled the corpus as hate
speech, oensive language, or neither by using crowd-sourcing and extracted
dierent features such as n-grams, some tweet-level metadata such as the num-
ber of hashtags, mentions, retweets, and URLs, Part Of Speech (POS) tagging,
etc. Their experiments on dierent multi-class classiers showed that the Logis-
tic Regression with L2 regularization performs the best at this task. Malmasi et
al. [10] proposed an ensemble-based system that uses some linear SVM classiers
in parallel to distinguish hate speech from general profanity in social media.
As one of the rst attempts in neural network models, Djuric et al. [5] pro-
posed a two-step method including a continuous bag of words model to extract
paragraph2vec embeddings and a binary classier trained along with the embed-
dings to distinguish between hate speech and clean content. Badjatiya et al. [1]
investigated three deep learning architectures, FastText, CNN, and LSTM, in
which they initialized the word embeddings with either random or GloVe em-
beddings. Gambck et al. [8] proposed a hate speech classier based on CNN
model trained on dierent feature embeddings such as word embeddings and
character n-grams. Zhang et al. [25] used a CNN+GRU (Gated Recurrent Unit
network) neural network model initialized with pre-trained word2vec embed-
dings to capture both word/character combinations (e. g., n-grams, phrases) and4 Marzieh Mozafari et al.
word/character dependencies (order information). Waseem et al. [23] brought a
new insight to hate speech and abusive language detection tasks by proposing
a multi-task learning framework to deal with datasets across dierent annota-
tion schemes, labels, or geographic and cultural inuences from data sampling.
Founta et al. [7] built a unied classication model that can eciently han-
dle dierent types of abusive language such as cyberbullying, hate, sarcasm,
etc. using raw text and domain-specic metadata from Twitter. Furthermore,
researchers have recently focused on the bias derived from the hate speech train-
ing datasets [2,21,24]. Davidson et al. [2] showed that there were systematic and
substantial racial biases in ve benchmark Twitter datasets annotated for oen-
sive language detection. Wiegand et al. [24] also found that classiers trained
on datasets containing more implicit abuse (tweets with some abusive words)
are more aected by biases rather than once trained on datasets with a high
proportion of explicit abuse samples (tweets containing sarcasm, jokes, etc.).
Transfer Learning: Pre-trained vector representations of words, embeddings,
extracted from vast amounts of text data have been encountered in almost every
language-based tasks with promising results. Two of the most frequently used
context-independent neural embeddings are word2vec and Glove extracted from
shallow neural networks. The year 2018 has been an inection point for dier-
ent NLP tasks thanks to remarkable breakthroughs: Universal Language Model
Fine-Tuning (ULMFiT) [9], Embedding from Language Models (ELMO) [17],
OpenAI s Generative Pre-trained Transformer (GPT) [18], and Googles BERT
model [4]. Howard et al. [9] proposed ULMFiT which can be applied to any
NLP task by pre-training a universal language model on a general-domain cor-
pus and then ne-tuning the model on target task data using discriminative
ne-tuning. Peters et al. [17] used a bi-directional LSTM trained on a specic
task to present context-sensitive representations of words in word embeddings by
looking at the entire sentence. Radford et al. [18] and Devlin et al. [4] generated
two transformer-based language models, OpenAI GPT and BERT respectively.
OpenAI GPT [18] is an unidirectional language model while BERT [4] is the
rst deeply bidirectional, unsupervised language representation, pre-trained us-
ing only a plain text corpus. BERT has two novel prediction tasks: Masked LM
and Next Sentence Prediction. The pre-trained BERT model signicantly out-
performed ELMo and OpenAI GPT in a series of downstream tasks in NLP [4].
Identifying hate speech and oensive language is a complicated task due to the
lack of undisputed labelled data [10] and the inability of surface features to cap-
ture the subtle semantics in text. To address this issue, we use the pre-trained
language model BERT for hate speech classication and try to ne-tune specic
task by leveraging information from dierent transformer encoders.
3 Methodology
Here, we analyze the BERT transformer model on the hate speech detection
task. BERT is a multi-layer bidirectional transformer encoder trained on the
English Wikipedia and the Book Corpus containing 2,500M and 800M tokens,
respectively, and has two models named BERT baseand BERT large. BERT baseA BERT-based hate speech detection approach in social media 5
contains an encoder with 12 layers (transformer blocks), 12 self-attention heads,
and 110 million parameters whereas BERT largehas 24 layers, 16 attention heads,
and 340 million parameters. Extracted embeddings from BERT basehave 768 hid-
den dimensions [4]. As the BERT model is pre-trained on general corpora, and
for our hate speech detection task we are dealing with social media content,
therefore as a crucial step, we have to analyze the contextual information ex-
tracted from BERT' s pre-trained layers and then ne-tune it using annotated
datasets. By ne-tuning we update weights using a labelled dataset that is new
to an already trained model. As an input and output, BERT takes a sequence
of tokens in maximum length 512 and produces a representation of the sequence
in a 768-dimensional vector. BERT inserts at most two segments to each input
sequence, [CLS] and [SEP]. [CLS] embedding is the rst token of the input se-
quence and contains the special classication embedding which we take the rst
token [CLS] in the nal hidden layer as the representation of the whole sequence
in hate speech classication task. The [SEP] separates segments and we will not
use it in our classication task. To perform the hate speech detection task, we
use BERT basemodel to classify each tweet as Racism, Sexism, Neither or Hate,
Oensive, Neither in our datasets. In order to do that, we focus on ne-tuning
the pre-trained BERT baseparameters. By ne-tuning, we mean training a clas-
sier with dierent layers of 768 dimensions on top of the pre-trained BERT base
transformer to minimize task-specic parameters.
3.1 Fine-Tuning Strategies
Dierent layers of a neural network can capture dierent levels of syntactic and
semantic information. The lower layer of the BERT model may contain more gen-
eral information whereas the higher layers contain task-specic information [4],
and we can ne-tune them with dierent learning rates. Here, four dierent ne-
tuning approaches are implemented that exploit pre-trained BERT basetrans-
former encoders for our classication task. More information about these trans-
former encoders' architectures are presented in [4]. In the ne-tuning phase, the
model is initialized with the pre-trained parameters and then are ne-tuned us-
ing the labelled datasets. Dierent ne-tuning approaches on the hate speech
detection task are depicted in Figure 1, in which Xiis the vector representation
of token iin a tweet sample, and are explained in more detail as follows:
1. BERT based ne-tuning: In the rst approach, which is shown in
Figure 1a, very few changes are applied to the BERT base. In this architecture,
only the [CLS] token output provided by BERT is used. The [CLS] output, which
is equivalent to the [CLS] token output of the 12th transformer encoder, a vector
of size 768, is given as input to a fully connected network without hidden layer.
The softmax activation function is applied to the hidden layer to classify.
2. Insert nonlinear layers: Here, the rst architecture is upgraded and an
architecture with a more robust classier is provided in which instead of using
a fully connected network without hidden layer, a fully connected network with
two hidden layers in size 768 is used. The rst two layers use the Leaky Relu6 Marzieh Mozafari et al.
(a) BERT basene-tuning
 (b) Insert nonlinear layers
 (c) Insert Bi-LSTM layer
 (d) Insert CNN layer
Fig. 1: Fine-tuning strategies
activation function with negative slope = 0.01, but the nal layer, as the rst
architecture, uses softmax activation function as shown in Figure 1b.
3. Insert Bi-LSTM layer: Unlike previous architectures that only use
[CLS] as the input for the classier, in this architecture all outputs of the latest
transformer encoder are used in such a way that they are given as inputs to a
bidirectional recurrent neural network (Bi-LSTM) as shown in Figure 1c. After
processing the input, the network sends the nal hidden state to a fully connected
network that performs classication using the softmax activation function.
4. Insert CNN layer: In this architecture shown in Figure 1d, the outputs
of all transformer encoders are used instead of using the output of the latest
transformer encoder. So that the output vectors of each transformer encoder
are concatenated, and a matrix is produced. The convolutional operation is per-
formed with a window of size (3, hidden size of BERT which is 768 in BERT base
model) and the maximum value is generated for each transformer encoder by
applying max pooling on the convolution output. By concatenating these values,
a vector is generated which is given as input to a fully connected network. By
applying softmax on the input, the classication operation is performed.
4 Experiments and Results
We rst introduce datasets used in our study and then investigate the dierent
ne-tuning strategies for hate speech detection task. We also include the details
of our implementation and error analysis in the respective subsections.
4.1 Dataset Description
We evaluate our method on two widely-studied datasets provided by Waseem
and Hovey [22] and Davidson et al. [3]. Waseem and Hovy [22] collected 16 k
of tweets based on an initial ad-hoc approach that searched common slurs and
terms related to religious, sexual, gender, and ethnic minorities. They annotated
their dataset manually as racism, sexism, or neither. To extend this dataset,
Waseem [20] also provided another dataset containing 6 :9kof tweets annotated
with both expert and crowdsourcing users as racism, sexism, neither, or both.
Since both datasets are overlapped partially and they used the same strategyA BERT-based hate speech detection approach in social media 7
in denition of hateful content, we merged these two datasets following Waseem
et al. [23] to make our imbalance data a bit larger. Davidson et al. [3] used
the Twitter API to accumulate 84.4 million tweets from 33,458 twitter users
containing particular terms from a pre-dened lexicon of hate speech words and
phrases, called Hatebased.org. To annotate collected tweets as Hate, Oensive,
or Neither, they randomly sampled 25 ktweets and asked users of CrowdFlower
crowdsourcing platform to label them. In detail, the distribution of dierent
classes in both datasets will be provided in Subsection 4.3.
4.2 Pre-Processing
We nd mentions of users, numbers, hashtags, URLs and common emoticons and
replace them with the tokens <user>,<number >,<hashtag >,<url>,<emoticon >.
We also nd elongated words and convert them into short and standard format;
for example, converting yeeeessss to yes. With hashtags that include some to-
kens without any with space between them, we replace them by their textual
counterparts; for example, we convert hashtag \#notsexist" to \not sexist". All
punctuation marks, unknown uni-codes and extra delimiting characters are re-
moved, but we keep all stop words because our model trains the sequence of
words in a text directly. We also convert all tweets to lower case.
4.3 Implementation and Results Analysis
For the implementation of our neural network, we used pytorch-pretrained-bert
library containing the pre-trained BERT model, text tokenizer, and pre-trained
WordPiece. As the implementation environment, we use Google Colaboratory
tool which is a free research tool with a Tesla K80 GPU and 12G RAM. Based
on our experiments, we trained our classier with a batch size of 32 for 3 epochs.
The dropout probability is set to 0.1 for all layers. Adam optimizer is used
with a learning rate of 2e-5. As an input, we tokenized each tweet with the
BERT tokenizer. It contains invalid characters removal, punctuation splitting,
and lowercasing the words. Based on the original BERT [4], we split words to
subword units using WordPiece tokenization. As tweets are short texts, we set
the maximum sequence length to 64 and in any shorter or longer length case it
will be padded with zero values or truncated to the maximum length.
We consider 80% of each dataset as training data to update the weights
in the ne-tuning phase, 10% as validation data to measure the out-of-sample
performance of the model during training, and 10% as test data to measure
the out-of-sample performance after training. To prevent overtting, we use
stratied sampling to select 0.8, 0.1, and 0.1 portions of tweets from each class
(racism/sexism/neither or hate/oensive/neither) for train, validation, and test.
Classes' distribution of train, validation, and test datasets are shown in Table 1.
As it is understandable from Tables 1(a) and 1(b), we are dealing with im-
balance datasets with various classes distribution. Since hate speech and oen-
sive languages are real phenomena, we did not perform oversampling or under-
sampling techniques to adjust the classes distribution and tried to supply the8 Marzieh Mozafari et al.
Table 1: Dataset statistics of the both Waseem-dataset (a) and Davidson-dataset (b).
Splits are produced using stratied sampling to select 0.8, 0.1, and 0.1 portions of tweets
from each class (racism/sexism/neither or hate/oensive/neither) for train, validation,
and test samples, respectively.
Racism Sexism Neither Total
Train 1693 3337 10787 15817
Validation 210 415 1315 1940
Test 210 415 1315 1940
Total 2113 4167 13417
(a) Waseem-dataset.Hate Oensive Neither Total
Train 1146 15354 3333 19832
Validation 142 1918 415 2475
Test 142 1918 415 2475
Total 1430 19190 4163
(b) Davidson-dataset.
datasets as realistic as possible. We evaluate the eect of dierent ne-tuning
strategies on the performance of our model. Table 2 summarized the obtained re-
sults for ne-tuning strategies along with the ocial baselines. We use Waseem
and Hovy [22], Davidson et al. [3], and Waseem et al. [23] as baselines and
compare the results with our dierent ne-tuning strategies using pre-trained
BERT basemodel. The evaluation results are reported on the test dataset and
on three dierent metrics: precision, recall, and weighted-average F1-score. We
consider weighted-average F1-score as the most robust metric versus class imbal-
ance, which gives insight into the performance of our proposed models. According
to Table 2, F1-scores of all BERT based ne-tuning strategies except BERT +
nonlinear classier on top of BERT are higher than the baselines. Using the
pre-trained BERT model as initial embeddings and ne-tuning the model with a
fully connected linear classier (BERT base) outperforms previous baselines yield-
ing F1-score of 81% and 91% for datasets of Waseem and Davidson respectively.
Inserting a CNN to pre-trained BERT model for ne-tuning on downstream task
provides the best results as F1- score of 88% and 92% for datasets of Waseem and
Davidson and it clearly exceeds the baselines. Intuitively, this makes sense that
combining all pre-trained BERT layers with a CNN yields better results in which
our model uses all the information included in dierent layers of pre-trained
BERT during the ne-tuning phase. This information contains both syntactical
and contextual features coming from lower layers to higher layers of BERT.
Table 2: Results on the trial data using pre-trained BERT model with dierent ne-
tuning strategies and comparison with results in the literature.
Method Datasets Precision(%) Recall(%) F1-Score(%)
Waseem and Hovy [22] Waseem 72.87 77.75 73.89
Davidson et al. [3] Davidson 91 90 90
Waseem et al. [23]Waseem - - 80
Davidson - - 89
BERT baseWaseem 81 81 81
Davidson 91 91 91
BERT base+ Nonlinear LayersWaseem 73 85 76
Davidson 76 78 77
BERT base+ LSTMWaseem 87 86 86
Davidson 91 92 92
BERT base+ CNNWaseem 89 87 88
Davidson 92 92 92A BERT-based hate speech detection approach in social media 9
4.4 Error Analysis
Although we have very interesting results in term of recall, the precision of the
model shows the portion of false detection we have. To understand better this
phenomenon, in this section we perform a deep analysis on the error of the
model. We investigate the test datasets and their confusion matrices resulted
from the BERT base+ CNN model as the best ne-tuning approach; depicted in
Figures 2 and 3. According to Figure 2 for Waseem-dataset, it is obvious that
the model can separate sexism from racism content properly. Only two samples
belonging to racism class are misclassied as sexism and none of the sexism
samples are misclassied as racism. A large majority of the errors come from
misclassifying hateful categories (racism and sexism) as hatless (neither) and
vice versa. 0.9% and 18.5% of all racism samples are misclassied as sexism and
neither respectively whereas it is 0% and 12.7% for sexism samples. Almost 12%
of neither samples are misclassied as racism or sexism. As Figure 3 makes clear
for Davidson-dataset, the majority of errors are related to hate class where the
model misclassied hate content as oensive in 63% of the cases. However, 2.6%
and 7.9% of oensive and neither samples are misclassied respectively.
Fig. 2: Waseem-datase's confusion matrix
 Fig. 3: Davidson-dataset's confusion matrix
To understand better the mislabeled items by our model, we did a manual
inspection on a subset of the data and record some of them in Tables 3 and 4.
Considering the words such as \daughters", \women", and \burka" in tweets
with IDs 1 and 2 in Table 3, it can be understood that our BERT based classi-
er is confused with the contextual semantic between these words in the samples
and misclassied them as sexism because they are mainly associated to feminin-
ity. In some cases containing implicit abuse (like subtle insults) such as tweets
with IDs 5 and 7, our model cannot capture the hateful/oensive content and
therefore misclassies. It should be noticed that even for a human it is dicult
to discriminate against this kind of implicit abuses.
By examining more samples and with respect to recently studies [2, 19, 24],
it is clear that many errors are due to biases from data collection [24] and rules
of annotation [19] and not the classier itself. Since Waseem et al. [22] created a
small ad-hoc set of keywords and Davidson et al. [3] used a large crowdsourced10 Marzieh Mozafari et al.
Table 3: Misclassied samples from Waseem-dataset.
ID Tweet Annotated Predicted
1 @user Good tweet. But they actually start selling their daughters at 9. Racism Sexism
2RT @user: Are we going to continue seeing the oppression of women or are we
going to make a stand? #BanTheBurka http://t.co/hZDx8mlvTv.Racism Sexism
3RT @user: @user my comment was sexist, but I'm not personally, always a sexist. Sexism Neither
4RT @user: @user Ah, you're a #feminist? Seeing #sexism everywhere then, do
check my tweets before you call me #sexistSexism Neither
5 @user By hating the ideology that enables it, that is what I'm doing. Racism Neither
Table 4: Misclassied samples from Davidson-dataset.
ID Tweet Annotated Predicted
6@user: If you claim Macklemore is your favorite rapper I'm also assuming you
watch the WNBA on your free time faggotHate Oensive
7@user: Some black guy at my school asked if there were colored printers in the
library. "It's 2014 man you can use any printer you want I said.Hate Neither
8 RT @user: @user typical coon activity. Hate Neither
9@user: @user @user White people need those weapons to defend themselves
from the subhuman trash your sort unleashes on us.Neither Hate
10RT @user: Finally! Warner Bros. making superhero lms starring a woman,
person of color and actor who identies as ""queer"";Neither Oensive
dictionary of keywords (Hatebase lexicon) to sample tweets for training, they in-
cluded some biases in the collected data. Especially for Davidson-dataset, some
tweets with specic language (written within the African American Vernacular
English) and geographic restriction (United States of America) are oversam-
pled such as tweets containing disparage words \nigga", \faggot", \coon", or
\queer", result in high rates of misclassication. However, these misclassica-
tions do not conrm the low performance of our classier because annotators
tended to annotate many samples containing disrespectful words as hate or of-
fensive without any presumption about the social context of tweeters such as
the speakers identity or dialect, whereas they were just oensive or even neither
tweets. Tweets IDs 6, 8, and 10 are some samples containing oensive words and
slurs which arenot hate or oensive in all cases and writers of them used this
type of language in their daily communications. Given these pieces of evidence,
by considering the content of tweets, we can see in tweets IDs 3, 4, and 9 that
our BERT-based classier can discriminate tweets in which neither and implicit
hatred content exist. One explanation of this observation may be the pre-trained
general knowledge that exists in our model. Since the pre-trained BERT model
is trained on general corpora, it has learned general knowledge from normal tex-
tual data without any purposely hateful or oensive language. Therefore, despite
the bias in the data, our model can dierentiate hate and oensive samples ac-
curately by leveraging knowledge-aware language understanding that it has and
it can be the main reason for high misclassications of hate samples as oensive
(in reality they are more similar to oensive rather than hate by considering
social context, geolocation, and dialect of tweeters).
5 Conclusion
Conating hatred content with oensive or harmless language causes online au-
tomatic hate speech detection tools to ag user-generated content incorrectly.
Not addressing this problem may bring about severe negative consequences forA BERT-based hate speech detection approach in social media 11
both platforms and users such as decreasement of platforms' reputation or users
abandonment. Here, we propose a transfer learning approach advantaging the
pre-trained language model BERT to enhance the performance of a hate speech
detection system and to generalize it to new datasets. To that end, we introduce
new ne-tuning strategies to examine the eect of dierent layers of BERT in
hate speech detection task. The evaluation results indicate that our model out-
performs previous works by proting the syntactical and contextual information
embedded in dierent transformer encoder layers of the BERT model using a
CNN-based ne-tuning strategy. Furthermore, examining the results shows the
ability of our model to detect some biases in the process of collecting or anno-
tating datasets. It can be a valuable clue in using the pre-trained BERT model
to alleviate bias in hate speech datasets in future studies, by investigating a
mixture of contextual information embedded in the BERTs layers and a set of
features associated to the dierent type of biases in data.
References
1. Badjatiya, P., Gupta, S., Gupta, M., et al.: Deep learning for hate speech detection
in tweets. CoRR abs/1706.00188 (2017). URL http://arxiv.org/abs/1706.00188
2. Davidson, T., Bhattacharya, D., Weber, I.: Racial bias in hate speech and abusive
language detection datasets. CoRR abs/1905.12516 (2019). URL http://arxiv.
org/abs/1905.12516
3. Davidson, T., Warmsley, D., Macy, M.W., et al.: Automated hate speech detection
and the problem of oensive language. CoRR abs/1703.04009 (2017). URL
http://arxiv.org/abs/1703.04009
4. Devlin, J., Chang, M., Lee, K., et al.: BERT: pre-training of deep bidirectional
transformers for language understanding. CoRR abs/1810.04805 (2018). URL
http://arxiv.org/abs/1810.04805
5. Djuric, N., Zhou, J., Morris, R., et al.: Hate speech detection with comment em-
beddings. In: Proceedings of the 24th International Conference on World Wide
Web, WWW '15 Companion, pp. 29{30. ACM, New York, NY, USA (2015). DOI
10.1145/2740908.2742760
6. Fortuna, P., Nunes, S.: A survey on automatic detection of hate speech in text.
ACM Comput. Surv. 51(4), 85:1{85:30 (2018). DOI 10.1145/3232676
7. Founta, A.M., Chatzakou, D., Kourtellis, N., et al.: A unied deep learning archi-
tecture for abuse detection. In: Proceedings of the 10th ACM Conference on Web
Science, WebSci '19, pp. 105{114. ACM, New York, NY, USA (2019)
8. Gamb ack, B., Sikdar, U.K.: Using convolutional neural networks to classify hate-
speech. In: Proceedings of the First Workshop on Abusive Language Online, pp.
85{90. Association for Computational Linguistics, Vancouver, BC, Canada (2017).
DOI 10.18653/v1/W17-3013
9. Howard, J., Ruder, S.: Fine-tuned language models for text classication. CoRR
abs/1801.06146 (2018). URL http://arxiv.org/abs/1801.06146
10. Malmasi, S., Zampieri, M.: Challenges in discriminating profanity from hate speech.
CoRR abs/1803.05495 (2018). URL http://arxiv.org/abs/1803.05495
11. Mehdad, Y., Tetreault, J.: Do characters abuse more than words? In: Proceedings
of the 17th Annual Meeting of the Special Interest Group on Discourse and Dia-
logue, pp. 299{303. Association for Computational Linguistics, Los Angeles (2016).
DOI 10.18653/v1/W16-3638
12. Mittos, A., Zannettou, S., Blackburn, J., et al.: "And We Will Fight For Our
Race!" A measurement Study of Genetic Testing Conversations on Reddit and
4chan. CoRR abs/1901.09735 (2019). URL http://arxiv.org/abs/1901.0973512 Marzieh Mozafari et al.
13. Nobata, C., Tetreault, J., Thomas, A., et al.: Abusive language detection in online
user content. In: Proceedings of the 25th International Conference on World Wide
Web, WWW '16, pp. 145{153. International World Wide Web Conferences Steering
Committee, Republic and Canton of Geneva, Switzerland (2016). DOI 10.1145/
2872427.2883062
14. Olteanu, A., Castillo, C., Boy, J., et al.: The eect of extremist violence on hateful
speech online. CoRR abs/1804.05704 (2018). URL http://arxiv.org/abs/1804.
05704
15. Ottoni, R., Cunha, E., Magno, G., et al.: Analyzing right-wing youtube channels:
Hate, violence and discrimination. In: Proceedings of the 10th ACM Conference
on Web Science, WebSci '18, pp. 323{332. ACM, New York, NY, USA (2018).
DOI 10.1145/3201064.3201081
16. Pete, B., L., W.M.: Cyber hate speech on twitter: An application of machine clas-
sication and statistical modeling for policy and decision making. Policy and
Internet 7(2), 223242 (2015). DOI 10.1002/poi3.85
17. Peters, M.E., Neumann, M., Iyyer, M., et al.: Deep contextualized word represen-
tations. CoRR abs/1802.05365 (2018). URL http://arxiv.org/abs/1802.05365
18. Radford, A.: Improving language understanding by generative pre-training (2018)
19. Sap, M., Card, D., Gabriel, S., et al.: The risk of racial bias in hate speech detection.
In: Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pp. 1668{1678. Association for Computational Linguistics, Florence,
Italy (2019). DOI 10.18653/v1/P19-1163
20. Waseem, Z.: Are you a racist or am I seeing things? annotator inuence on hate
speech detection on twitter. In: Proceedings of the First Workshop on NLP and
Computational Social Science, pp. 138{142. Association for Computational Lin-
guistics, Austin, Texas (2016). DOI 10.18653/v1/W16-5618
21. Waseem, Z., Davidson, T., Warmsley, D., et al.: Understanding abuse: A typology
of abusive language detection subtasks. In: Proceedings of the First Workshop on
Abusive Language Online, pp. 78{84. Association for Computational Linguistics,
Vancouver, BC, Canada (2017). DOI 10.18653/v1/W17-3012. URL https://www.
aclweb.org/anthology/W17-3012
22. Waseem, Z., Hovy, D.: Hateful symbols or hateful people? predictive features for
hate speech detection on twitter. In: Proceedings of the NAACL Student Research
Workshop, pp. 88{93. Association for Computational Linguistics, San Diego, Cal-
ifornia (2016). DOI 10.18653/v1/N16-2013
23. Waseem, Z., Thorne, J., Bingel, J.: Bridging the Gaps: Multi Task Learning for
Domain Transfer of Hate Speech Detection, pp. 29{55. Springer International
Publishing, Cham (2018). DOI 10.1007/978-3-319-78583-7 3
24. Wiegand, M., Ruppenhofer, J., Kleinbauer, T.: Detection of Abusive Language:
the Problem of Biased Datasets. In: Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and Short Papers), pp. 602{608.
Association for Computational Linguistics, Minneapolis, Minnesota (2019). DOI
10.18653/v1/N19-1060
25. Zhang, Z., Robinson, D., Tepper, J.: Detecting hate speech on twitter using a
convolution-gru based deep neural network. In: The Semantic Web, pp. 745{760.
Springer International Publishing, Cham (2018)