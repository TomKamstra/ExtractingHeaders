Proceedings of SemEval-2016 , pages 996–1000,
San Diego, California, June 16-17, 2016. c2016 Association for Computational Linguistics
LTG at SemEval-2016 Task 11: Complex Word Identiﬁcation with Classiﬁer
Ensembles
Shervin Malmasi1Mark Dras1Marcos Zampieri2,3
1Macquarie University, Sydney, NSW, Australia
2Saarland University, Germany
3German Research Center for Artiﬁcial Intelligence, Germany
{first.last}@mq.edu.au, marcos.zampieri@dfki.de
Abstract
We present the description of the LTG entry
in the SemEval-2016 Complex Word Identi-
ﬁcation (CWI) task, which aimed to develop
systems for identifying complex words in En-
glish sentences. Our entry focused on the use
of contextual language model features and the
application of ensemble classiﬁcation meth-
ods. Both of our systems achieved good per-
formance, ranking in 2ndand 3rdplace overall
in terms of F-Score.
1 Introduction
Complex Word Identiﬁcation (CWI) is the task of
identifying complex words in texts using computa-
tional methods (Shardlow, 2013). The task is usu-
ally carried out as part of lexical and text simpli-
ﬁcation systems. Shardlow (2014) considers CWI
as the ﬁrst processing step in lexical simpliﬁcation
pipelines. Complex or difﬁcult words should ﬁrst be
identiﬁed so they can be later substituted by simpler
ones to improve text readability.
CWI has gained more importance in the last
decade as lexical and text simpliﬁcation systems
have been developed or tailored for a number of pur-
poses. They have been applied to make texts more
accessible to language learners (Petersen and Os-
tendorf, 2007); other researchers have explored text
simpliﬁcation strategies targeted at populations with
low literacy skills (Alu ´ısio et al., 2008). Finally, an-
other relevant application of text simpliﬁcation are
people with dyslexia (Rello et al., 2013).
The SemEval 2016 Task 11: Complex Word Iden-
tiﬁcation (CWI) provides an interesting opportunityto evaluate methods and approaches for this task.
The organizers proposed a binary text classiﬁcation
task in which participants were required label words
in English sentences as either complex ( 1) or simple
(0). The task organizers provided participants with a
training set containing sentences annotated with this
information, followed by an unlabeled test set for
evaluation. The assessment of whether words in a
sentence are complex or simple was performed by
human annotators required to label the data.1
2 Data
Based on the information available at the shared
task’s website2: “400 annotators were presented
with several sentences and asked to select which
words they did not understand their meaning.”
The CWI task dataset was divided as follows:
•Training set: 2,237 judgments by 20 annota-
tors over 200 sentences. A word is considered
complex if at least one of the 20 annotators as-
signed it as so.
•Test set: 88,221 judgments made over 9,000
sentences (1 annotator per sentence).
1Here the term complex is used as a synonym for difﬁcult.
Unlike the Morphology term complex (antonym of simplex )
that deﬁnes compound words or words composed of multiple
morphs (Adams, 2001).
2http://alt.qcri.org/semeval2016/task11/9963 Methodology
The primary focus of our team’s entry was the use of
judgements from different annotators to create train-
ing data. We looked at how adjusting the thresh-
old for inter-annotator agreement would affect the
results and whether the combination of data created
using different threshold values could improve per-
formance.
Initially, the training data released by the organiz-
ers was labeled in a way that a word was marked as
complex if any annotator judged it so. During the
course of the shared task the organizers released ad-
ditional information about the training data, chieﬂy
the individual judgements of the 20 annotators that
were used to derive the ﬁnal labels for each word.
We attempted to use this data in our system. Dur-
ing development we noted that by increasing this
threshold to two, the performance of our system un-
der cross-validation improved by a small amount.
Accordingly, we pursued this direction as the main
focus of our experiments.
3.1 Classiﬁers
We utilize a decision tree classiﬁer, which we found
to perform better than Support Vector Machine
(SVM) and Na ¨ıve Bayes classiﬁers for this data.
3.2 Features
Our core set of features are based on estimating n-
gram probabilities using web-scale language mod-
els. More speciﬁcally, this data was sourced from
the Microsoft Web N-Gram Service3, although we
should note that this service has been deprecated
and replaced since the shared task.4These language
models are trained on web-scale corpora collected
by Microsoft’s Bing search engine from crawling
English web pages.
Given a target word wt, we extract several prob-
ability estimates to use as classiﬁcation features.
These estimates, which we describe below, use the
target word as well its preceding and following
words, as shown in Figure 1.
3http://weblm.research.microsoft.com/
4It has been replaced by Microsoft’s Project Oxford:
https://www.projectoxford.ai/weblm3.2.1 Word Probability
This is an estimate of how likely the target word
is to occur in the language model:
P(wt)
Rarer words would be assigned lower values and
thus this feature can help quantify word frequency
for the classiﬁer.
3.2.2 Conditional Probability
We calculate the bigram probability of wt:
P(wt|wt−1)
Similarly, we estimate the trigram probability:
P(wt|wt−1, w t−2)
These values estimate the likelihood of the target
word occurring given the previous one or two words.
They can help quantify if the word is being used in
a common or less frequent context.
3.2.3 Joint Probability
We also use the following joint probability esti-
mates of the target word and its surrounding words:
P(wt−1, w t−2, w t)
P(wt−1, w t)
P(wt−1, w t, w t+1)
P(wt, w t+1)
P(wt, w t+1, w t+2)
The intuition underlying the use of all of these
n-gram language model features is that the under-
standing of certain words depends on the context
they appear in. A large number of English words
are polysemous and their classiﬁcation, without tak-
ing into account the speciﬁc sense being use, could
lead to misclassiﬁcations. This can occur in scenar-
ios where a learner knows the most frequently used
sense of a polysemous word, but is confronted with
a different sense that they have not encountered be-
fore. Additionally, even if a known word is used in
an unusual context, it could be a cause of confusion
for learners.997This cavity is formed by the mantle skirt, a double fold of mantle which [...]
themantle skirt adouble
𝑤𝑤𝑡𝑡−2𝑤𝑤𝑡𝑡−1𝑤𝑤𝑡𝑡𝑤𝑤𝑡𝑡+1𝑤𝑤𝑡𝑡+2Figure 1: An example of the context extracted for a target word, which is “skirt” in this example.
3.2.4 Word Length
Guided by the intuition that the most frequent
words in a language are usually shorter, we use the
length of a word as a classiﬁcation feature.
4 Ensemble Classiﬁers
Classiﬁer ensembles are a way of combining differ-
ent classiﬁers or experts with the goal of improving
accuracy through enhanced decision making. They
have been applied to a wide range of real-world
problems and shown to achieve better results com-
pared to single-classiﬁer methods (Oza and Tumer,
2008). Through aggregating the outputs of multi-
ple classiﬁers in some way, their outputs are gener-
ally considered to be more robust. Ensemble meth-
ods continue to receive increasing attention from
researchers and remain a focus of much machine
learning research (Wo ´zniak et al., 2014; Kuncheva
and Rodr ´ıguez, 2014).
Such ensemble-based systems often use a paral-
lel architecture, as illustrated in Figure 2, where the
classiﬁers are run independently and their outputs
are aggregated using a fusion method. For exam-
ple,Bagging (bootstrap aggregating) is a commonly
used method for ensemble generation (Breiman,
1996) that can create multiple base classiﬁers.
Input 
Classifier 1  
Classifier 2  
Combiner  
 Decision  
… 
Classifier N  Ensemble Architecture 
2 
Figure 2: An example of parallel classiﬁer ensemble architec-
ture where Nindependent classiﬁers provide predictions which
are then fused using an ensemble combination method.It works by creating multiple bootstrap training
sets from the original training data and a separate
classiﬁer is trained from each one of these sets. The
generated classiﬁers are said to be diverse because
each training set is created by sampling with re-
placement and contains a random subset of the orig-
inal data.
Other, more sophisticated, ensemble methods that
rely on meta-learning may employ a stacked archi-
tecture where the output from a ﬁrst set of classiﬁers
is fed into a second level meta-classiﬁer and so on.
The ﬁrst part of creating an ensemble is generat-
ing the individual classiﬁers. Various methods for
creating these ensemble elements have been pro-
posed. These involve using different algorithms, pa-
rameters or feature types; applying different prepro-
cessing or feature scaling methods and varying ( e.g.
distorting or resampling) the training data.
5 Systems
In this section we describe the two systems we cre-
ated and entered in the shared task.
5.1 System 1
Our ﬁrst system was based on decision tree classi-
ﬁer trained on data where the minimum threshold for
inter-annotator agreement was set to 3. Given that
the testing data was only annotated by a single rater,
we did not want to pick a value that was too high,
even though this could improve cross-validation per-
formance on the training data.
Additionally, we converted this setup to an en-
semble by creating 100 randomized decision tree
classiﬁers by using bagging, which we described
earlier. The decisions of these learners were fused
via plurality voting to yield the ﬁnal label for an in-
stance.998Rank Team System Accuracy Precision Recall F-score G-score
1 PLUJAGH SEWDFF 0.922 0 .289 0 .453 0 .353 0 .608
2 LTG System2 0.889 0.220 0.541 0.312 0.672
3 LTG System1 0.933 0.300 0.321 0.310 0.478
4 MAZA B 0.912 0 .243 0 .420 0 .308 0 .575
5 HMC DecisionTree25 0.846 0 .189 0 .698 0 .298 0 .765
6 TALN RandomForest SIM.output 0.847 0 .186 0 .673 0 .292 0 .750
7 HMC RegressionTree05 0.838 0 .182 0 .705 0 .290 0 .766
8 MACSAAR RFC 0.825 0 .168 0 .694 0 .270 0 .754
9 TALN RandomForest WEI.output 0.812 0 .164 0 .736 0 .268 0 .772
10 UWB All 0.803 0 .157 0 .734 0 .258 0 .767
Table 1: The top 10 systems in task, ranked by their F-score.
5.2 System 2
For our second system we extended the threshold-
based approach to an ensemble of decision trees
trained on different data.
We created four individual classiﬁers, each
trained with a different minimum threshold5rang-
ing between 1–4. The outputs of these classiﬁers, a
binary prediction, were then combined using a plu-
rality voting combiner. It should also be noted that
having an even number of base classiﬁers also intro-
duces the possibility of ties occurring.
6 Results
The top 10task submissions, ranked by the F-score,
are shown in Table 1 with our systems highlighted.
Both of our systems achieved very competitive re-
sults, ranking in second and third place overall.
Our second system, an ensemble of classiﬁers
trained on distinct data derived using different lev-
els of inter-rater agreement, performed slightly bet-
ter than the ﬁrst system. This could be interpreted
as this evidence the second approach is slightly
better, and we hypothesize that combining annota-
tions from different combinations of annotators may
help the classiﬁer learn reliable models of the phe-
nomenon, since individual annotations (as well as
the original combined annotation) were noisy. How-
ever, determining this requires further experiments.
This is due to the fact that with four classiﬁers in the
ensemble, voting resulted in a tie for some 6%of the
testing data. These ties were broken arbitrarily, in-
troducing an element of stochasticity to our results.
In hindsight this does not appear to have been the
5Setting the threshold to 1is equivalent to using the original
training data.most intuitive or robust way of dealing with such ties
since the distribution of classes is not balanced. In
fact, this distribution is highly skewed, as we discuss
in the next section.
6.1 Conclusion
We developed two ensemble-based systems for this
task, both of which achieved competitive results in
the ﬁnal rankings. Our results indicate that the use
of contextual features, as well as language models,
are promising for this task.
Analysis of the gold standard labels release af-
ter the task shows that only 4.7%of the 88k sam-
ples belonged to the positive class. This is a very
highly skewed distribution that can make it hard to
train effective classiﬁers. It also means that accuracy
cannot be used as the sole evaluation metric here; a
balanced measure of precision and recall like the F-
score is required. Alternatively, the balanced accu-
racy measure (Brodersen et al., 2010) could also be
used. Such a high data imbalance can result in train-
ing classiﬁers that are biased towards the majority
class. This bias can be more problematic if the distri-
bution of classes is different in the test set. Accord-
ingly, future work in this area could look at the use
of methods for dealing with unbalanced datasets (He
and Garcia, 2009). The application of such methods,
in conjunction with ensembles, could potentially re-
sult in greater performance.
Future work could attempt to integrate additional
language resources for this task. Analyzing the text
produced by learners could provide insight into the
limitations of learner vocabulary. Learner corpora,
widely used in the task of Native Language Identiﬁ-
cation (Malmasi and Dras, 2014; Malmasi and Dras,
2015) could be useful here.999Acknowledgments
We would like to thank the CWI task organizers for
managing the organization of this event. We also
thank the anonymous reviewers for their insightful
comments.
References
Valerie Adams. 2001. Complex words in English . Rout-
ledge.
Sandra M Alu ´ısio, Lucia Specia, Thiago AS Pardo, Er-
ick G Maziero, and Renata PM Fortes. 2008. Towards
brazilian portuguese automatic text simpliﬁcation sys-
tems. In Proceedings of DocEng .
Leo Breiman. 1996. Bagging predictors. In Machine
Learning , pages 123–140.
Kay H Brodersen, Cheng Soon Ong, Klaas E Stephan,
and Joachim M Buhmann. 2010. The balanced accu-
racy and its posterior distribution. In Proceedings of
ICPR .
Haibo He and Edwardo A Garcia. 2009. Learning from
Imbalanced Data. IEEE Transactions on Knowledge
and Data Engineering, , 21(9):1263–1284.
Ludmila I Kuncheva and Juan J Rodr ´ıguez. 2014. A
weighted voting framework for classiﬁers ensembles.
Knowledge and Information Systems , 38(2):259–275.
Shervin Malmasi and Mark Dras. 2014. Chinese Native
Language Identiﬁcation. In Proceedings of EACL .
Shervin Malmasi and Mark Dras. 2015. Multilingual
Native Language Identiﬁcation. In Natural Language
Engineering .
Nikunj C Oza and Kagan Tumer. 2008. Classiﬁer en-
sembles: Select real-world applications. Information
Fusion , 9(1):4–20.
Sarah E Petersen and Mari Ostendorf. 2007. Text sim-
pliﬁcation for language learners: a corpus analysis. In
Proceedings of SLaTE .
Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Hora-
cio Saggion. 2013. Simplify or help?: text simpliﬁ-
cation strategies for people with dyslexia. In Proceed-
ings of W4A .
Matthew Shardlow. 2013. A comparison of techniques to
automatically identify complex words. In Proceedings
of the ACL Student Research Workshop .
Matthew Shardlow. 2014. A survey of automated text
simpliﬁcation. International Journal of Advanced
Computer Science and Applications (IJACSA) , (Spe-
cial Issue on Natural Language Processing).
Michał Wo ´zniak, Manuel Gra ˜na, and Emilio Corchado.
2014. A survey of multiple classiﬁer systems as hybrid
systems. Information Fusion , 16:3–17.1000