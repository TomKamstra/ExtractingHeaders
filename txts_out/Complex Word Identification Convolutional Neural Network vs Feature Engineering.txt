Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 322–327
New Orleans, Louisiana, June 5, 2018. c2018 Association for Computational Linguistics
Complex Word Identiﬁcation:
Convolutional Neural Network vs. Feature Engineering
Segun Taofeek Aroyehun
CIC, Instituto Polit ´ecnico Nacional
Mexico City, Mexico
aroyehun.segun@gmail.comJason Angel
CIC, Instituto Polit ´ecnico Nacional
Mexico City, Mexico
ajason08@gmail.com
Daniel Alejandro P ´erez Alvarez
CIC, Instituto Polit ´ecnico Nacional
Mexico City, Mexico
daperezalvarez@gmail.comAlexander Gelbukh
CIC, Instituto Polit ´ecnico Nacional
Mexico City, Mexico
www.gelbukh.com
Abstract
We describe the systems of NLP-CIC team
that participated in the Complex Word Iden-
tiﬁcation (CWI) 2018 shared task. The shared
task aimed to benchmark approaches for iden-
tifying complex words in English and other
languages from the perspective of non-native
speakers. Our goal is to compare two ap-
proaches: feature engineering and a deep neu-
ral network. Both approaches achieved com-
parable performance on the English test set.
We demonstrated the ﬂexibility of the deep-
learning approach by using the same deep neu-
ral network setup in the Spanish track. Our
systems achieved competitive results: all our
systems were within 0.01 of the system with
the best macro-F1 score on the test sets except
on Wikipedia test set, on which our best sys-
tem is 0.04 below the best macro-F1 score.
1 Introduction
Complex Word Identiﬁcation (CWI) is an impor-
tant step in text simpliﬁcation. The ability to accu-
rately identify word(s) as complex or not in a given
context for a given target population signiﬁcantly
impacts subsequent processing steps such as lexi-
cal substitution in the simpliﬁcation pipeline.
The organizers of the 2018 CWI shared task
(Yimam et al., 2018) provided participants with
multilingual human-annotated datasets (Yimam
et al., 2017a,b) for the identiﬁcation of complex
words. Training and development data were pro-
vided for three languages: English, Spanish, and
German. In the case of English, three genres were
covered: news, Wikinews, and Wikipedia.
Some of the participants of the previous
CWI shared task used neural network-based ap-
proaches. For instance, Bingel et al. (2016) useda simple feed-forward neural network, while Nat
(2016) used an ensemble of recurrent neural net-
works (RNN). The performance of the neural net-
work approaches was not impressive. The RNN
achieved the best result among all the submissions
that used neural networks (Paetzold and Specia,
2016b).
In this paper, we report further experiments with
the efﬁcacy of deep neural networks for CWI, us-
ing another deep neural network architecture—
Convolutional Neural Network (CNN). Namely,
we compare two approaches for the task of CWI:
one based on an extensive feature engineering
and the tree ensembles classiﬁer, and another one
based on deep neural network using the word em-
bedding representation. The latter approach is,
to the best of our knowledge, the ﬁrst attempt
to apply CNNs to the task of CWI. Apart from
comparing the performance of the two approaches
on the classiﬁcation subtask of CWI on English,
we demonstrate the ﬂexibility of the CNN-based
approach by applying it to another language—
Spanish in our case.
The remainder of the paper is organized as fol-
lows. Section 2 outlines relevant work. Sections 3
and 4 present our two approaches. Section 5 gives
some details on the datasets used. Results of our
experiments are in Section 6. Section 7 presents
error analysis. Finally, Section 8 concludes the pa-
per and outlines future work directions.
2 Related Work
The majority of works on CWI are related to fea-
ture engineering at various linguistic levels. Sec-
tion 2.1 below discusses existing approaches to
feature engineering for machine-learning models
used for CWI. On the other hand, Section 2.2 men-322tions some relevant applications of CNNs to natu-
ral language processing (NLP).
2.1 Feature Engineering for he CWI Task
Participants of the ﬁrst edition of CWI shared
task have experimented with various linguistic fea-
tures. These linguistic features span various lin-
guistic levels: morphological, syntactic, semantic,
and psycholinguistic. Paetzold and Specia (2016c)
used morphological, lexical, and semantic fea-
tures to train frequency-based, lexicon-based, and
machine-learning models for CWI. Konkol (2016)
used only frequency of occurrence of a word in
Wikipedia as the only feature to train a Max en-
tropy classiﬁer. Davoodi and Kosseim (2016) ex-
perimented with the degree of abstractness of a
word as a psycholinguistic feature for CWI.
In this work, we used some of these features
and experimented with some new features, such as
contextual and entity information and additional
psycholinguistic scores.
2.2 CNNs in NLP
Convolutional neural networks have shown no-
table results in the ﬁelds of computer vision,
speech recognition and recently in NLP.
CNN models achieve state-of-the-art results
in NLP tasks such as clause coherence (Yin
and Sch ¨utze, 2015b), paraphrase identiﬁcation
(Yin and Sch ¨utze, 2015b,a) and Twitter sentiment
analysis (Severyn and Moschitti, 2015).
Kim (2014) presents a CNN fed with word2vec
word embedding vectors (Mikolov et al., 2013)
used for detection of positive and negative re-
views, as well as sentence classiﬁcation. Their
results suggest that pre-trained vectors encode
generic semantic features, which can beneﬁt var-
ious NLP classiﬁcation tasks. In our work, we
used a similar model with slight additions to the
architecture of the network and a different prepro-
cessing step.
3 Feature-Engineering Approach
In this section, we present the set of features used
to build one of our CWI systems.
Morphological Features Most of the morpho-
logical features we used consist of frequency
count of target text on large corpora such as
Wikipedia and Simple Wikipedia. We computed
term frequency, inverse term frequency, document
frequency and term document entropy. Also, thetf-idf values were calculated. In addition, we used
characteristics of each target text such as number
of characters, vowels, and syllables.
Syntactic and Lexical Features We used
OpenNLP1part-of-speech (POS) tagger to deter-
mine the target word’s POS in the context. We
used the POS as a parameter to ﬁlter the possible
meanings of the target word. With this, we ob-
tained the number of senses, lemmas, hyponyms,
and hypernyms given by WordNet.2
Psycholinguistic and Entity Features We in-
cluded some psycholinguistic scores provided by
the improved MRC psycholinguistics database
(Paetzold and Specia, 2016a) as features. The
database provides familiarity, age of acquisition,
concreteness, and imagery scores for each word.
We hypothesized that these scores would be use-
ful to identify complex word. Unfortunately, many
target words were absent in this database. We used
OpenNLP and Stanford CoreNLP3to tag target
words as organization, person, location, date, and
time. The resulting tag was used as an entity fea-
ture.
Word Embedding Distances as Features Be-
yond these classic linguistic features, we used
word embeddings. Namely, we downloaded the
pre-trained Word2vec (Mikolov et al., 2013) vec-
tors of 300 dimensions to measure the distance be-
tween the sentence and the target word. The dis-
tance was computed using cosine similarity and
Euclidean similarity over the average of the vec-
tor representation of the words in the sentence and
the target text.
Classical Machine Learning Models We no-
ticed that for this task (with our features), the
tree learner offered better performance than other
models. Thus, we tried several settings for the
tree learner model provided by KNIME (Berthold
et al., 2009), as well as more complex variations
such as random forest, gradient boosted, and tree
ensembles. The best obtained result was given by
the tree ensembles with 600 models.
4 Deep-Learning Approach
In this section, we present our deep-learning ap-
proach. It is based on 2D convolution and word-
1http://opennlp.apache.org/
2https://wordnet.princeton.edu/
3https://stanfordnlp.github.io/CoreNLP323Figure 1: The architecture of our network
embedding representation of the target text frag-
ment and its context.
Since text is one-dimensional, we applied a
preprocessing step described in Section 4.1 prior
to the application of convolution layer. Sec-
tion 4.2 describes our network architecture, and
Section 4.3 presents the training procedure.
4.1 Preprocessing
As a ﬁrst step, we removed punctuation marks,
digits, and special characters. Each word was then
replaced by its vector representation using the pre-
trained word vectors from the Word2vec model
(Mikolov et al., 2013) for English and fastText
model (Grave et al., 2018) for Spanish. A min-
max normalization was applied to every vector to
convert the values from the range [−1,1]to[0,1].
We assigned a zero vector to the words missing in
the pre-trained embeddings.
We deﬁned the left context (LC) and the right
context (RC) as those words that appear to the left
and the right of the target text, respectively. As a
compact representation of the left or right context,
we used one 300-dimensional vector calculated as
the average of the vectors of all the words in the
LC and RC, respectively (if the target text was lo-
cated at the beginning or the end of the sentence,
we used a zero vector as the respective context rep-
resentation). Next, we generated a matrix where
the ﬁrst row corresponds to the LC vector, the next
krows are given by the embedding vectors of the
words contained in the target text, where kis the
number of words in the target text, and the last row
corresponds to the RC vector. In order to have a
regular representation, we padded the matrix withp=m−kzero vectors, where mis the maximum
value of kin the training set.
Figure 1 illustrates the preprocessing step on the
sentence of an example in the English training set.
The output of the preprocessing step is the input
of the network.
We believe that the averaging operation on the
words in the contexts allowed differentiating be-
tween cases where the same sentence has distinct
target texts. Those words included or excluded in
the context will slightly modify the representation
of the context, which will help the model to learn
some relationships between the target text and the
rest of the sentence. We could have compressed
the representation matrix by combining the vec-
tor representation of the words in the target text
instead of stacking them. However, this could dra-
matically reduce the valuable information pertain-
ing to the target text.
4.2 Architecture of our Network
In our architecture, we used an input, convolution,
pooling, and fully-connected layers; see Figure 1.
Below we describe each of these layers except the
input layer, which was described in Section 4.1.
Convolution The number of ﬁlters in this layer
varied from 16 to 256 with a convolution stride of
1 and kernel size in the range of 2 to 4. We ap-
plied the rectiﬁed linear unit activation function to
the output of this layer in order to introduce non-
linearity. This layer is central to the idea of CNN,
which enables the network to identify the most im-
portant features in the input. The output of this
layer is often referred to as feature maps. Our net-324Source Training Set Test Set
Avg. Length Avg. Sentence Examples Positive Avg. Length Avg. Sentence Examples
Target Text Length Target Text Length
Wikinews 8 168 7746 42% 8 166 1287
News 8 174 14002 40% 8 153 2095
Wikipedia 8 175 5551 45% 9 194 870
Spanish 9 193 13750 40% 9 190 2233
Table 1: English and Spanish datasets
work included four convolutional layers.
Pooling Max pooling was applied to the output
of the convolution layer to downsample the feature
maps. The feature maps of the last pooling layer
were ﬂattened.
Fully-Connected Layer We used three fully-
connected layers (FC). The ﬁrst FC took as in-
put the ﬂattened output of the last pooling layer.
The ﬁrst two FCs used a linear activation func-
tion and the third applied the sigmoid activation
function. The last FC gave a number in the range
[0,1], which was the ﬁnal output of the network.
By a threshold (we found 0.5 to be optimal), we
determined whether the output on a given exam-
ple implied a label of 0 (simple) or 1 (complex).
4.3 Training
We used the binary cross-entropy as our objec-
tive function for training the network. We ex-
perimented with various types of optimizers. We
chose optimizers with static learning rate and
those with adaptive learning rate schedules. Based
on the performance of the model on the validation
set, we found RMSprop to be the best on updating
the network parameters and minimizing the loss
function while using 100 epochs.
The dataset is imbalanced: it contains unequal
proportion of examples by class labels, roughly
60% negative examples and 40% positive exam-
ples. So, we introduced class weights in our train-
ing procedure, which resulted in performance im-
provement. We computed class weights using
scikit-learn (Pedregosa et al., 2011).
To mitigate overﬁtting, we tried several regu-
larization alternatives (Goodfellow et al., 2016)
including kernel and weight regularization, batch
normalization, dropout, and early stopping. We
found dropout and early stopping useful. Our ﬁnal
model included dropout (Srivastava et al., 2014)
after every layer with dropout probability of 0.25.5 Datasets
Table 1 shows some statistics on the corpora we
used: the average length of the target text and sen-
tences and the number of examples in the train-
ing and test sets, with the percentage of positive
examples (target texts labeled as complex) in the
training set. The table shows that the datasets are
skewed towards negative examples: the percent-
age of positive examples on the datasets did not
exceed 45%. The Wikipedia dataset has the small-
est number of training examples, 5551. The aver-
age length of target text in the training examples
and test examples are comparable. One can see
some variations in the average length of sentences
in the training and test sets. These variations are
remarkable for the Wikipedia and News datasets.
6 Results
This section presents the performance of both
models on the English test set and that of the CNN
model on the Spanish test set.
Table 2 shows the macro-F1 and accuracy
scores as well as the respective ranks of both
CNN and TreeE models on the English test set.
The performance measures are given per genre in
the English test set. Out of 11 teams, our best
model places ﬁfth on News; second on Wikinews,
and seventh on Wikipedia. All our systems were
within 0.01 of the system with the best macro-F1
score on the test sets except on Wikipedia test set.
On the Wikipedia test set, our best system was
0.04 below the best macro-F1 score.
On the Spanish test set, we submitted only the
CNN-based system. Table 3 shows its macro-
precision, macro-recall, macro-F1, and accuracy
scores. Our best submission ranks third among
seven teams that participated in the Spanish track.
The main advantage of the CNN model is that it
can be applied to any language for which an em-
bedding can be easily created given the availability
of sufﬁcient electronic textual resources.325News Wikinews Wikipedia
Models Macro-F1 Accuracy Rank Macro-F1 Accuracy Rank Macro-F1 Accuracy Rank
NLP-CIC-TreeE 0.851 0.859 9 0.831 0.837 3 0.772 0.774 11
NLP-CIC-CNN 0.855 0.863 8 0.824 0.828 7 0.772 0.772 12
Table 2: Accuracy and macro-F1 scores by genres on the English test set
Model Macro-Recall Macro-Precision Macro-F1 Accuracy Rank
NLP-CIC-CNN 0.765 0.772 0.767 0.772 3
Table 3: CNN performance scores on the Spanish test set
Source NLP-CIC-TreeE Model NLP-CIC-CNN Model
Correct Wrong Correct Wrong
Wikinews 0.94±0.53 1 .10±0.650.94±0.51 1 .12±0.72
News 0.97±0.55 1 .21±0.750.97±0.55 1 .17±0.75
Wikipedia 1.05±0.65 1 .04±0.681.04±0.66 1 .08±0.65
Table 4: Target text Normalized character count BY model performance on English test set
7 Discussion
We observed a relationship between the length of
the target text—character count—and the perfor-
mance of our models.
On the News genre dataset of the English test
set, our CNN model tends to show better perfor-
mance on target texts with fewer words compared
to Tree Ensembles. When the target text contains
more than three words, Tree ensembles perform
better than CNN. Similarly, both models tend to
make mistakes when the average character count
in the target text is higher. Table 4 shows the nor-
malized mean character count of the target text in
the English test set when each of our models made
correct and wrong predictions.
We believe that this behavior is a reﬂection of
the training examples: there are fewer examples
with longer target texts.
8 Conclusion and Future Work
We have described two approaches for the classi-
ﬁcation subtask of the CWI 2018 shared task: one
using feature engineering with Tree Ensembles
and one using CNN. We compared them on the test
set provided for the CWI 2018 shared task. On the
English test set, the two approaches showed com-
parable performance: the difference between the
performance scores was within 0.01. On the En-
glish test set, our best model placed ﬁfth on News,
second on Wikinews, and seventh on Wikipedia.
On the Spanish test set, the CNN model ranked
third. This result demonstrates the ﬂexibility ofapplying CNN to CWI on any language for which
pre-trained embeddings are available.
Our models behaved differently depending on
the length of the target text: they tend to make
mistakes on longer target texts. We attribute this
behavior to the skewness of the training set.
In the future, it would be interesting to evalu-
ate the impact of domain-speciﬁc features, as well
as of different vector operations used to generate
context vectors, on the performance of our models.
Acknowledgements
Thanks to the CWI 2018 task organizers. We ap-
preciate the anonymous reviewers for their useful
comments on this paper. The last author acknowl-
edges the support of the Mexican government via
CONACYT (SNI) and the Instituto Polit ´ecnico
Nacional grant SIP-20181792.
References
Michael R Berthold, Nicolas Cebron, Fabian Dill,
Thomas R Gabriel, Tobias K ¨otter, Thorsten Meinl,
Peter Ohl, Kilian Thiel, and Bernd Wiswedel. 2009.
KNIME-the Konstanz information miner: version
2.0 and beyond. ACM SIGKDD explorations
Newsletter , 11(1):26–31.
Joachim Bingel, Natalie Schluter, and H ´ector
Mart ´ınez Alonso. 2016. CoastalCPH at SemEval-
2016 Task 11: The importance of designing your
Neural Networks right. In Proceedings of the
10th International Workshop on Semantic Eval-
uation (SemEval-2016) , pages 1028–1033, San
Diego, California. Association for Computational
Linguistics.326Elnaz Davoodi and Leila Kosseim. 2016. CLaC at
SemEval-2016 Task 11: Exploring linguistic and
psycho-linguistic Features for Complex Word Iden-
tiﬁcation. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval-2016) ,
pages 982–985, San Diego, California. Association
for Computational Linguistics.
Ian Goodfellow, Yoshua Bengio, and Aaron
Courville. 2016. Deep Learning . MIT
Press, Cambridge, Massachusetts. http:
//www.deeplearningbook.org .
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learn-
ing word vectors for 157 languages. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC 2018) , Miyazaki,
Japan.
Yoon Kim. 2014. Convolutional neural networks
for sentence classiﬁcation. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 1746–1751,
Doha, Qatar.
Michal Konkol. 2016. UWB at SemEval-2016 Task
11: Exploring Features for Complex Word Identi-
ﬁcation. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval-2016) ,
pages 1038–1041, San Diego, California. Associa-
tion for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efﬁcient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .
Gillin Nat. 2016. Sensible at SemEval-2016 Task 11:
Neural Nonsense Mangled in Ensemble Mess. In
Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016) , pages 963–
968, San Diego, California. Association for Com-
putational Linguistics.
Gustavo Paetzold and Lucia Specia. 2016a. Inferring
psycholinguistic properties of words. In Proceed-
ings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
435–440, San Diego, California. Association for
Computational Linguistics.
Gustavo Paetzold and Lucia Specia. 2016b. SemEval
2016 Task 11: Complex Word Identiﬁcation. In
Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016) , pages 560–
569, San Diego, California. Association for Com-
putational Linguistics.
Gustavo Paetzold and Lucia Specia. 2016c. SV000gg
at SemEval-2016 Task 11: Heavy Gauge Complex
Word Identiﬁcation with System V oting. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation (SemEval-2016) , pages 969–974,San Diego, California. Association for Computa-
tional Linguistics.
Fabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of machine
learning research , 12(Oct):2825–2830.
Aliaksei Severyn and Alessandro Moschitti. 2015.
Twitter sentiment analysis with deep convolutional
neural networks. In Proceedings of the 38th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval , pages 959–
962, Santiago, Chile. ACM.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overﬁtting. The Journal of Machine Learning
Research , 15(1):1929–1958.
Seid Muhie Yimam, Chris Biemann, Shervin Mal-
masi, Gustavo Paetzold, Lucia Specia, Sanja ˇStajner,
Ana¨ıs Tack, and Marcos Zampieri. 2018. A Report
on the Complex Word Identiﬁcation Shared Task
2018. In Proceedings of the 13th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications , New Orleans, United States. Association
for Computational Linguistics.
Seid Muhie Yimam, Sanja ˇStajner, Martin Riedl, and
Chris Biemann. 2017a. CWIG3G2-Complex Word
Identiﬁcation Task across Three Text Genres and
Two User Groups. In Proceedings of the Eighth In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers) , volume 2,
pages 401–407, Taipei, Taiwan. Asian Federation of
Natural Language Processing.
Seid Muhie Yimam, Sanja ˇStajner, Martin Riedl, and
Chris Biemann. 2017b. Multilingual and cross-
lingual complex word identiﬁcation. In Proceed-
ings of RANLP , pages 813–822, Varna, Bulgaria. IN-
COMA Ltd.
Wenpeng Yin and Hinrich Sch ¨utze. 2015a. Convolu-
tional neural network for paraphrase identiﬁcation.
InProceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 901–911, Denver, Colorado. Association for
Computational Linguistics.
Wenpeng Yin and Hinrich Sch ¨utze. 2015b. Multi-
grancnn: An architecture for general matching of
text chunks on multiple levels of granularity. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 63–73,
Beijing, China. Association for Computational Lin-
guistics.327