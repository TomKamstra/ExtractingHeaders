24
AdversarialAttackson Deep-learningModels in Natural
Language Processing: A Survey
WEIEMMA ZHANG, The Universityof Adelaide, Australia
QUAN Z.SHENG and AHOUD ALHAZMI, Macquarie University,Australia
CHENLIANG LI, Wuhan University,China
With the development of high computational devices, deep neural networks (DNNs), in recent years, have
gainedsignificantpopularityinmanyArtificialIntelligence(AI)applications.However,previouseffortshave
shown that DNNs are vulnerable to strategically modified samples, named adversarial examples . These sam-
ples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions.
Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), re-
searcheffortsonattackingDNNsforNaturalLanguageProcessing(NLP)applicationshaveemergedinrecent
years.However,theintrinsicdifferencebetweenimage(CV)andtext(NLP)renderschallengestodirectlyap-
ply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a
widerangeofNLPapplications.Inthisarticle,wepresentasystematicsurveyontheseworks.Wecollectall
related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze
40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary
knowledgeofNLPanddiscussrelatedseminalworksincomputervision.Weconcludeoursurveywithadis-
cussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks
on NLP DNNs.
CCSConcepts:• Computingmethodologies →Natural languageprocessing ;
Additional Key Words and Phrases: Deep neural networks, adversarial examples, textual data, natural lan-
guage processing
ACM Reference format:
Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial Attacks on Deep-
learning Models in Natural Language Processing: A Survey. ACM Trans. Intell. Syst. Technol. 11, 3, Article 24
(March 2020), 41 pages.
https://doi.org/10.1145/3374217
1 INTRODUCTION
Deepneuralnetworks(DNNs)are large neuralnetworkswhosearchitectureis organized as a se-
ries of layers of neurons, each of which serves as the individual computing units. Neurons are
connected by links with different weights and biases and transmit the results of its activation
Authors’ addresses: W. E. Zhang, School of Computer Science, The University of Adelaide, Sydney, Australia, SA 5005;
email: wei.e.zhang@adelaide.edu.au; Q. Z. Sheng and A. Alhazmi, Department of Computing, Macquarie University, Aus-
tralia, NSW 2109; emails: michael.sheng@mq.edu.au, ahoud.alhazmi@hdr.mq.edu.au; C. Li, School of Cyber Science and
Engineering,Wuhan University, Wuhan,China, 430072;email:cllee@whu.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
prior specific permission and/or a fee.Request permissions from permissions@acm.org .
© 2020Association forComputingMachinery.
2157-6904/2020/03-ART24$15.00
https://doi.org/10.1145/3374217
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:2 W.E.Zhanget al.
functiononitsinputstotheneuronsofthenextlayer.Deepneuralnetworkstrytomimicthebio-
logicalneuralnetworksofhumanbrainstolearnandbuildknowledgefromexamples.Thus,they
demonstrate the strengths in dealing with complicated tasks that are not easily to be modelled as
linear or non-linear problems. Furthermore, empowered by continuous real-valued vector repre-
sentations (i.e., embeddings) they are good at handling data with various modalities, e.g., image,
text, video,andaudio.
With the development of high computational devices, deep neural networks in recent years
have gained tremendous attention in many Artificial Intelligence (AI) communities such as Com-
puter Vision [ 65,126], Natural Language Processing [ 16,66], Web Mining [ 101,149], and Game
theory [119]. However, the interpretability of deep neural networks is still unsatisfactory as they
work as black boxes, which means it is difficult to get intuitions from what each neuron exactly
has learned. One of the problems of the poor interpretability is evaluating the robustness of deep
neural networks. In recent years, researchers [ 40,132] used small unperceivable perturbations
to evaluate the robustness of deep neural networks and found that they are not robust to these
perturbations. Szegedy et al. [ 132] first evaluated the state-of-the-art deep neural networks used
for image classification with small generated perturbations on the input images. They found that
the image classifiers were fooled with high probability, but human judgment is not affected. The
perturbedimagepixelswerenamed adversarialexamples andthisnotationislaterusedtodenote
all kinds of perturbed samples in a general manner. As the generation of adversarial examples is
costlyandimpracticalinReference[ 132],Goodfellowetal.[ 40]proposedafastgenerationmethod
that popularized this research topic (Section 3.1provides further discussion on these works). Fol-
lowedtheirworks,manyresearcheffortshavebeenmadeandthepurposesoftheseworkscanbe
summarized as: (i) evaluating the deep neural networks by fooling them with unperceivable per-
turbations; (ii) intentionally changing the output of the deep neural networks; and (iii) detecting
the oversensitivity and over-stability points of the deep neural networks and finding solutions to
defensetheattack.
JiaandLiang[ 54]arethefirsttoconsideradversarialexamplegeneration(or adversarialattack ,
we will use these two expressions interchangeably hereafter) on deep neural networks for text-
basedtasks(namely, textualdeepneuralnetworks ).Theirworkquicklygainedresearchattentionin
theNaturalLanguageProcessing(NLP)community.However,duetointrinsicdifferencesbetween
imagesandtextualdata,theadversarialattackmethodsonimagescannotbedirectlyappliedtothe
latter.Firstofall,imagedata(e.g.,pixelvalues)iscontinuous,buttextualdataisdiscreteinnature.
Conventionally, we vectorizethe texts before inputtingtheminto the deep neuralnetworks.Tra-
ditional vectoring methods include leveraging term frequency and inverse document frequency,
and one-hot representation (details in Section 3.3). When applying gradient-based adversarial at-
tacks adopted from images on these representations, the generated adversarial examples are in-
validcharactersorwordsequences[ 157].Onesolutionistousewordembeddingsastheinputof
deep neural networks. However, this will also generate words that can not be matched with any
words in the word embedding space [ 38]. Second, the perturbation of images are small change of
pixel values that are hard to be perceived by human eyes, thus humans can correctly classify the
images, showing the poor robustness of deep neural models. But for adversarial attack on texts,
smallperturbationsareeasilyperceptible.Forexample,replacementofcharactersorwordswould
generate invalid words or syntactically incorrect sentences. Further, it would alter the semantics
of the sentence drastically. Therefore, the perturbations are easily to be perceived–in this case,
even human beingcannotprovide correctpredictions.
To address the aforementioned differences and challenges, many attacking methods have been
proposed since the pioneer work of Jia and Liang [ 54]. Despite the popularity of the topic in the
NLPcommunity,thereisnocomprehensivereviewpaperthatcollectsandsummarizestheefforts
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:3
in this research direction. There is a need for this kind of work that helps successive researchers
andpractitionersto haveanoverview ofthesemethods.
Related surveys and the differences to this survey. In Reference [ 9], the authors presented
a comprehensive review on different classes of attacks and defenses against machine learning
systems. Specifically, they proposed a taxonomy for identifying and analyzing these attacks and
applied the attacks on a machine learning–based application, i.e., a statistical spam filter, to illus-
tratetheeffectivenessoftheattackanddefense.Thisworktargetedmachinelearningalgorithms
rather than neural models. Inspired by Reference [ 9], the authors of Reference [ 35]r e v i e w e dt h e
defences of adversarial attack in the security point of view. The work is not limited to machine
learning algorithms or neuralmodels, but a generic reportaboutadversarialdefenses on security
related applications. The authors found that existing security related defense works lack of clear
motivations and explanations on how the attacks are related to the real security problems and
howtheattackanddefensearemeaningfullyevaluated.Thus,theyestablishedataxonomyofmo-
tivations, constraints, and abilities for more plausible adversaries. In Reference [ 13], the authors
provided a thorough overview of the evolution of the adversarial attack research over a ten-year
period from 2008 to 2018, and focused on the research works from computer vision and cyber
security. The paper covers the works from pioneering non-deep-learning algorithms to recent
deep-learningalgorithms.Itisalsofromthesecuritypointofviewtoprovidedetailedanalysison
the effect of the attacks and defenses. The authors of Reference [ 79] reviewed the same problem
fromadata-drivenperspective.Theyanalyzedtheattacksanddefensesaccordingtothelearning
phases,i.e.,thetrainingphaseandtestphase.
Unlikepreviousworksthatdiscussgenerallyontheattackmethodsonmachinelearningalgo-
rithms, Reference [ 154] focuses on the adversarial examples on deep-learning models. It reviews
current research efforts on attacking various deep neural networks in different applications. The
defensemethodsarealsoextensivelysurveyed.However,itmainlydiscussesadversarialexamples
for image classification and object recognition tasks. The work in Reference [ 2] provides a com-
prehensive review on the adversarial attacks on deep-learning models used in computer vision
tasks.Itisanapplication-drivensurveythatgroupstheattackmethodsaccordingtothesub-tasks
under computer vision area. The article also comprehensively reports the works on the defense
side,themethodsofwhicharemainly groupedinto threecategories.
Allthementionedworkseithertargetageneraloverviewoftheattacksanddefensesonmachine
learningmodelsorfocusonspecificdomainssuchascomputervisionandcybersecurity.Ourwork
differswiththeminthatwespecificallyfocusontheattacksanddefensesontextualdeep-learning
models. Furthermore, we provide a comprehensive review that covers information from different
aspectsto make thissurveyself-contained.
Papersselection. Thepaperswereviewedinthisarticlearehigh-qualitypapersselectedfromtop
NLPandAIconferences,includingACL,1COLING,2NAACL,3EMNLP,4ICLR,5AAAI,6andIJCAI.7
Other than accepted papers in aforementioned conferences, we also considered good papers in
1Annual Meetingof the Association for ComputationalLinguistics.
2InternationalConferenceon ComputationalLinguistics.
3Annual Conferenceof the North American Chapterof the Association for ComputationalLinguistics.
4EmpiricalMethods inNaturalLanguageProcessing.
5InternationalConferenceon LearningRepresentations.
6AAAI Conferenceon Artificial Intelligence.
7InternationalJoint Conferenceon ArtificialIntelligence.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:4 W.E.Zhanget al.
e-Printarchive,8asitreflectsthelatestresearchoutputs.Weselectedpapersfromthearchivewith
threemetrics:paperquality,methodnovelty,and thenumberof citations(optional9).
Contributions of this survey. The aim of this survey is to provide a comprehensive review
on the research efforts on generating adversarial examples on textual deep neural networks. It is
motivatedbythedrasticallyincreasingattentionsonthistopic.Thissurveywillserveresearchers
and practitioners who are interested in attacking textual deep neural models. We expect that the
readers have some basic knowledge of the deep neural networks architectures, which are not the
focusin thisarticle.To summarize,thekey contributionsof thissurveyare:
•We conduct a comprehensive review for adversarial attacks on textual deep neural models
and propose different classification schemes to organize the reviewed literature; this is the
firstwork of thiskind;
•Weprovideallrelatedinformationtomakethesurveyself-containedandthusitiseasyfor
readerswhohave limited NLPknowledge to understand;
•Wediscusssomeopenissues,andidentifysomepossibleresearchdirectionsinthisresearch
fieldaimingtobuildmorerobusttextualdeep-learningmodelswiththehelpofadversarial
examples.
Theremainderofthisarticleisorganizedasfollows.Weintroducethepreliminariesforadver-
sarialattacksondeep-learningmodelsinSection 2,includingthetaxonomyofadversarialattacks
and deep-learning models used in NLP. In Section 3, we address the difference between attack-
ing image data and textual data and briefly reviewe exemplary works for attacking image DNN
that inspired their follow-ups in NLP. Section 4first presents our classification on the literature
and then gives a detailed introduction to the state of the art. We discuss the defense strategies in
Section5and pointouttheopenissuesin Section 6.Finally, thearticleisconcludedin Section 7.
2 OVERVIEW OF ADVERSARIAL ATTACKS ANDDEEP-LEARNINGTECHNIQUES
IN NATURAL LANGUAGEPROCESSING
Beforewediveintothedetailsofthissurvey,westartwithanintroductiontothegeneraltaxonomy
ofadversarialattackondeep-learningmodels.Wealsointroducethedeep-learningtechniquesand
theirapplicationsin naturallanguage processing.
2.1 Adversarial Attacks onDeep-learning Models:The General Taxonomy
In this section, we provide the definitions of adversarial attacks and introduce different aspects
of the attacks, followed by the measurement of perturbations and the evaluation metrics of the
effectivenessoftheattacksina generalmannerthatappliesto any datamodality.
2.1.1 Definitions. We presentthemain conceptsof adversarialattacksas following:
•DeepNeuralNetwork(DNN) .Adeepneuralnetwork(weuseDNNanddeep-learningmodel
interchangeably hereafter) can be simply presented as a nonlinear function fθ:X→Y,
where Xis the input features/attributes, Yis the output predictions that can be a discrete
set of classes or a sequence of objects. θrepresents the DNN parameters and are learned
via gradient-based back-propagation during the model training. Best parameters would be
8arXiv.org.
9As the research topic only emerges from 2017, we relax the citation number to over five if it is published more than one
year. If the paper has less than five citations, but is very recent and satisfies the other two metrics, then we also include it
in this survey.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:5
obtained by minimizing the the gap between the model’s prediction fθ(X)and the correct
label Y,wherethegapis measuredbyloss function J(fθ(X),Y).
•Perturbations .Perturbationsareintentlycreatedsmallnoisesthattobeaddedtotheoriginal
inputdataexamples in teststage,aimingto foolthedeep-learningmodels.
•AdversarialExamples .Anadversarialexample x/primeisanexamplecreatedviaworst-caseper-
turbation of the input to a deep-learning model. An ideal DNN would still assign correct
class y(inthecaseofclassificationtask)to x/prime,whileavictimDNNwouldhavehighconfi-
denceon wrongpredictionof x/prime.x/primecan beformalized as
x/prime=x+η,f(x)=y,x∈X, (1)
f(x/prime)/nequaly,
orf(x/prime)=y/prime,y/prime/nequaly,
whereηis the worst-case perturbation. The goal of the adversarial attack can be deviating
thelabeltoincorrectone ( f(x/prime)/nequaly) orspecifiedone( f(x/prime)=y/prime).
2.1.2 Threat Model. We adopt the definition of Threat Model for attacking DNN from Refer-
ence[154].Inthefollowing, we discussseveralaspectsof thethreatmodel:
•ModelKnowledge .Theadversarialexamplescanbegeneratedusingblack-boxorwhite-box
strategies in terms of the knowledge of the attacked DNN. Black-box attack is performed
whenthearchitectures,parameters,lossfunction,activationfunctionsandtrainingdataof
the DNN are not accessible. Adversarial examples are generated by directly accessing the
test dataset, or by querying the DNN and checking the output change. On the contrary,
white-boxattackis basedon theknowledge oftechnicaldetailsof DNN.
•Target.Thegeneratedadversarialexamplescanchangetheoutputpredictiontobeincorrect
ortospecificresultasshowninEquation( 1).Comparedtotheun-targetedattack( f(x/prime)/nequal
y), targeted attack ( f(x/prime)=y/prime) is more strict as it not only changes the prediction but also
enforces constraint on the output to generate specified prediction. For binary tasks, e.g.,
binaryclassification,un-targetedattackequalstothetargetedattack.
•Granularity . The attack granularity refers to the level of data from which the adversarial
examplesaregenerated.Forexample,itisusuallytheimagepixelsforimagedata.Regarding
thetextualdata,itcouldbecharacter,word,andsentence-levelembedding.Section 3.3will
give furtherintroductionon attackgranularityfor textualDNN.
•Motivation .Generatingadversarialexamplesismotivatedbytwogoals:attackanddefense.
TheattackaimstoexaminetherobustnessofthetargetDNN,whilethedefensetakesastep
furtherutilizinggeneratedadversarialexamplestorobustifythetargetDNN.Section 5will
give moredetails.
2.1.3 Measurements. Two groups of measurements are required in the adversarial attack for
(i)controllingtheperturbationsand (ii)evaluatingtheeffectivenessoftheattack,respectively.
•Perturbation Constraint . As aforementioned, the perturbation ηshould not change the true
class label of the input—that is, an ideal DNN classifier, if we take classification as the ex-
ample,willprovidethesamepredictionontheadversarialexampletotheoriginalexample.
ηcannotbetoosmallaswell,toavoidendingupwithnoeffectontargetDNNs.Ideally,ef-
fectiveperturbationisthemostimpactfulnoiseinaconstrainedrange.Reference[ 132]first
put a constraint that (x+η)∈[0,1]nfor image adversarial examples, ensuring the adver-
sarialexamplehasthesamerangeofpixelvaluesastheoriginaldata[ 143].Reference[ 40]
simplifies the solution and uses max norm to constrain η:||η||∞≤ϵ. This was inspired by
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:6 W.E.Zhanget al.
theintuitionthataperturbationthatdoesnotchangeanyspecificpixelbymorethansome
amountϵcannot change the output class [ 143]. Using max-norm is sufficient enough for
image classification/object recognition tasks. Later on, other norms, e.g., L2andL0,w e r e
used to control the perturbation in attacking DNN in computer vision. Constraining ηfor
textualadversarialattackissomehow different.Section 3.3will give more details.
•Attack Evaluation . Adversarial attacks are designed to degrade the performance of DNNs.
Therefore,evaluatingtheeffectivenessoftheattackisbasedontheperformancemetricsof
differenttasks.Forexample,classificationtaskshavemetricssuchasaccuracy,F1scoreand
AUC score. We leave the metrics for different NLP as out-of-scope content in this article
andsuggest readersrefertospecifictasksforinformation.
2.2 Deep-learning in NLP
Neuralnetworkshavebeengaining increasingpopularityin NLP community in recentyearsand
variousDNNmodelshavebeenadoptedindifferentNLPtasks.Apartfromthefeedforwardneu-
ral networks and Convolutional Neural Networks (CNN), Recurrent/Recursive Neural Networks
(RNN) and their variants are the most common neural networks used in NLP, because of their
naturalabilityofhandlingsequences.Inrecentyears,twoimportantbreakthroughsindeeplearn-
ing are brought into NLP. They are sequence-to-sequence learning [131]a n dattention mechanism
[8]. Reinforcementlearning and generative models are also gained much popularity[ 152].In this
section, we will briefly overview the DNN architectures and techniques applied in NLP that are
closely related to this survey. We suggest readers refer to detailed reviews of neural networks in
NLP in References[ 100,152].
2.2.1 Feed-Forward Networks. A feed-forward network has several forward layers and each
nodeinalayerconnectstoeachnodeinthefollowinglayer,makingthenetworkfullyconnected.
The network utilizes nonlinear transformation to distinguish data that is not linearly separable.
The major drawback of its application in NLP is that it cannot handle well the text sequences in
which the word order matters as it do not record the order of the elements. To evaluate the ro-
bustnessoffeedforwardnetworkinNLP,adversarialexamplesareoftengeneratedforspecifically
designedfeed-forwardnetworks.Forexample,theauthorsofReferences[ 3,43,44]workedonthe
specifiedmalware detectionmodels.
2.2.2 ConvolutionalNeuralNetwork(CNN). AConvolutionalNeuralNetworkcontainsconvo-
lutional layers and pooling (down-sampling) layers and final fully connectedlayer. The Convolu-
tionallayer uses convolutionoperationto extract meaningful localpatternsof input.Specifically,
CNN identifies local predictors and combines them together to generate a fixed-sized vector for
theinputs,whichcontainsthemostorimportantinformativeaspectsofthedata.Inaddition,itis
order-sensitive. Therefore, it excels in computer vision tasks and later is widely adopted in NLP
applications.
Yoon Kim [ 59]adoptedCNNforsentenceclassificationandusedWord2Vectorepresentwords
as input. Then the convolutional operation is restricted to the direction of word sequence, rather
thanthewordembeddings.Themodeldemonstratesexcellentperformanceonseveralbenchmark
datasets and has become a benchmark work of adopting CNN in NLP applications. Zhang et al.
[156]presentedCNNfortextclassificationatcharacterlevel.Theyusedone-hotrepresentationfor
each character of alphabet. These two representative textual CNNs are evaluated via adversarial
examples in many applications[ 12,29,30,34,76].
2.2.3 Recurrent Neural Networks/Recursive Neural Networks. Recurrent Neural Networks are
neural models adapted from feed-forward neural networks for learning mappings between
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:7
sequential inputs and outputs [ 116]. RNNs allows data with arbitrary length and it introduces
cyclesintheircomputationalgraphtomodelefficientlytheinfluenceoftime[ 48].Themodeldoes
notsufferfromstatisticalestimationproblemsstemmingfromdatasparsityandthusleadstoim-
pressiveperformanceindealingwithsequentialdata[ 36].Recursiveneuralnetworks[ 37]extends
recurrentneuralnetworksfromsequencestotree,whichrespectsthehierarchicaldependencyof
thelanguage.Insomesituations,backwardsdependenciesexist,whichisinneedforthebackward
analysis.Bi-directionalRNNthuswasproposedtoprocesseachsentencesinbothdirections,for-
wardsand backwards,usingtwoparallelRNNnetworks,andcombine theiroutputs.
RNN has many variants, among which the Long Short-Term Memory (LSTM) network [ 50]
gains the most popularity. LSTM is a specific RNN that was designed to capture the long-term
dependencies. In LSTM, the hidden state are computed through combination of three gates, i.e.,
inputgate ,forgetgate andoutputgate ,thatcontrolinformation.LSTMnetworkshavesubsequently
proved to be more effective than conventional RNNs [ 42]. GRUs is a simplified version of LSTM
that it only consists of two gates, thus it is more efficient in terms of computational cost. Some
popularLSTMvariantsareproposedtosolvevariousNLPtasks[ 21,50,112,133,141,146].These
representativeworkshavereceivedtheinterestsofevaluationwithadversarialexamplesrecently
[34,53,54,91,103,112,118,130,157].
2.2.4 Sequence-to-sequence Learning (Seq2Seq) Models. Sequence-to-sequence learning
(Seq2Seq) [ 131] is one of the important breakthroughs in deep learning and is now widely used
for NLP applications. Seq2Seq model has the superior capacity to generate another sequence
informationforagivensequenceinformationwithanencoder-decoderarchitecture[ 27].Usually,
a Seq2Seq model consists of two recurrent neural networks: an encoder that processes the
input and compresses it into a vector representation and a decoder that predicts the output.
Latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED) model [ 122] is a recently
popular Seq2Seq model that generates sequences leveraging the complex dependencies between
subsequences. Reference [ 24] is one of the first neural machine translation (NMT) model that
adopts the Seq2Seq model. OpenNMT [ 63], a Seq2Seq NMT model proposed recently, becomes
one of the benchmark works in NMT. As they are adopted and applied widely, attack works also
emerge [22,30,98,127].
2.2.5 Attention Models. Attention mechanism[ 8] is anotherbreakthroughin deeplearning. It
wasinitiallydevelopedtoovercomethedifficultyofencodingalongsequencerequiredinSeq2Seq
models[27].Attentionallowsthedecodertolookbackonthehiddenstatesofthesourcesequence.
Thehiddenstatesthenprovideaweightedaverageasadditionalinputtothedecoder.Thismecha-
nismpays attentiononinformativepartsofthesequence.Ratherthanlookingattheinputsequence
in vanilla attention models, self-attention [ 136] in NLP is used to look at the surrounding words
in a sequence to obtain more contextually sensitive word representations [ 152]. BiDAF [ 121]i s
a bidirectional attention flow mechanism for machine comprehension and achieved outstanding
performance when proposed. References [ 54,127] evaluated the robustness of this model via ad-
versarialexamplesandbecamethefirstfewworksusingadversarialexamplesforattackingtextual
DNNs.Otherattention-basedDNNs[ 25,107]alsoreceivedadversarialattacksrecently[ 29,91].
2.2.6 Reinforcement Learning Models. Reinforcement learning trains an agent by giving a re-
ward after agents perform discrete actions. In NLP, a reinforcement learning framework usually
consists of an agent (a DNN), a policy (guiding action) and a reward. The agent picks an action
(e.g.,predictingnextwordinasequence)basedonapolicy,thenupdatesitsinternalstateaccord-
ingly,untilarrivingtheendofthesequencewherearewardiscalculated.Reinforcementlearning
requires proper handling of the action and the states, which may limit the expressive power and
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:8 W.E.Zhanget al.
learning capacity of the models [ 152]. But it gains much interests in task-oriented dialogue sys-
tems[74]astheysharethefundamentalprincipleasdecisionmakingprocesses.Limitedworksso
far canbe foundto attackthereinforcementlearningmodel in NLP [ 98].
2.2.7 DeepGenerativeModels. Inrecentyears,twopowerfuldeepgenerativemodels,Genera-
tive Adversarial Networks (GANs) [ 39] and Variational Auto-Encoders (VAEs) [ 62] are proposed
and gain much research attention. Generative models are able to generate realistic data instances
thatareverysimilartogroundtruthdatainalatentspace.IntheNLPfield,theyareusedtogen-
erate text. GANs [ 39] consists of two adversarial networks: a generator and adiscriminator .T h e
discriminatoristodiscriminatetherealandgeneratedsamples,whilethegeneratoristogenerate
realisticsamplesthataimtofoolthediscriminator.GANusesamin-maxlossfunctiontotraintwo
neural networks simultaneously. VAEs consist of encoder and generator networks. The encoder
encodes an input into a latent space and the generator generates samples from the latent space.
Deep generative models are not easy to train and evaluate. Hence, these deficiencies hinder their
wideusageinmanyreal-worldapplications[ 152].Althoughtheyhavebeenadoptedingenerating
texts,so farno workexamines theirrobustnessusingadversarialexamples.
3 FROM IMAGETO TEXT
Adversarialattacksareoriginatedfromthecomputervisioncommunity.Inthissection,weintro-
ducerepresentativeworks,discussdifferencesbetweenattackingimagedataandtextualdata,and
presentpreliminaryknowledgewhenperformingadversarialattackson textualDNNs.
3.1 CraftingAdversarial Examples: Inspiring Works in ComputerVision
Since adversarial examples were first proposed for attacking DNNs for object recognition in
the computer vision community [ 17,40,95,104,105,132,157], this research direction has been
receiving sustained attentions. We briefly introduce some works that inspired their followers in
NLP community in this section, allowing the reader to better understand the adversarial attacks
on textual DNNs. For comprehensive review of attack works in computer vision, please refer to
Reference[ 2].
L-BFGS. Szegedy et al. invented the adversarial examples notation [ 132]. They proposed an ex-
plicitly designed method to cause the model to give wrong prediction of adversarial input ( x+η)
for image classificationtask.Itcame tosolve theoptimizationproblem:
η=argmin
ηλ||η||2
2+J(x+η,y/prime)s.t.(x+η)∈[0,1]n, (2)
wherey/primeis the target output of ( x/prime+η), but incorrect given an ideal classifier. Jdenotes the cost
function of the DNN and λis a hyperparameter to balance the two parts of the equation. This
minimizationwasinitiallyperformedwithabox-constrained Limitedmemory Broyden-Fletcher-
Goldfarb-Shanno(L-BFGS)algorithmandthuswasnamedafterit.Theoptimizationwasrepeated
multiple timesuntil reachinga minimum λthatsatisfyEquation( 2).
FastGradientSignMethod(FGSM) .L-BFGSisveryeffective,buthighlyexpensive—thisinspired
Goodfellow et al. [ 40] to find a simplified solution. Instead of fixing y/primeand identifying the most
effectiveηin L-BFGS, FGSM fixed l∞norm ofηand minimized the cost (Equation ( 3)). Then they
linearized the problem with a first-order Taylor series approximation (Equation ( 4)), and got the
closed-formsolutionof η(Equation( 5))[143]:
η=argmin ηJ(x+η,y)s.t.||η||∞≤ϵ, (3)
η=argmin ηJ(x,y)+ηT∇xJ(x,y)s.t.||η||∞≤ϵ, (4)
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey 24:9
η=ϵ·sign(∇xJ(x,y)), (5)
whereϵisaparametersetbyattacker,controllingtheperturbation’smagnitude.sign(x)isthesign
functionthatreturns1when x>0,and−1whenx<0,otherwisereturns0. ∇xJ(x,y)denotesthe
gradient of loss function respect to the input, and can be calculated via back-propagation. FGSM
attractsthemost follow-upworksin NLP.
JacobianSaliencyMapAdversary(JSMA) .UnlikeFGSMusinggradientstoattack,Papernotetal.
[105]generatedadversarialexamplesusingforwardderivatives(i.e.,modelJacobian).Thismethod
evaluatestheneuralmodel’soutputsensitivitytoeachinputcomponentusingits JacobianMatrix .
Jacobianmatricesformtheadversarialsaliencymapsthatrankeachinputcomponent’scontribu-
tion to the target attack. A perturbation is then selected from the maps. Thus, the method was
namedJacobian-basedSaliencyMapAttack .TheJacobian matrixof agiven xisgiven by
JacbF[i,j]=∂Fi
∂xj, (6)
where xiis theith component of the input and Fjis thejth component of the output. Here F
denotesthe logits(i.e.,theinputstothesoftmaxfunction)layer. JF[i,j]measuresthesensitivityof
Fjwithrespectto xi.
C&WAttack .CarliniandWagner[ 17]aimedtoevaluatethedefensivedistillationstrategy[ 49]
for mitigating the adversarial attacks. They restricted the perturbations with lpnorms where p
equalsto0,2,and∞and proposedseven versionsof Jfor thefollowing optimizationproblem:
η=argmin
η||η||p+λJ(x+η,y/prime)s.t.(x+η)∈[0,1]n, (7)
and theformulationsharesthesame notationwithaforementionedworks.
DeepFool.DeepFool[ 95]isaniterative L2-regularizedalgorithm.Theauthorsfirstassumedthe
neural network is linear, thus they can separate the classes with a hyperplane. They simplified
the problem and found optimal solution based on this assumption and constructed adversarial
examples.Toaddressthenon-linearityfactoftheneuralnetwork,theyrepeatedtheprocessuntil
a trueadversarialexample isfound.
SubstituteAttack .Theabove-mentionedrepresentativeworksareallwhite-boxmethods,which
requirethefullknowledgeoftheneuralmodel’sparametersandstructures.However,inpractice,
it is not always possible for attackers to craft adversaries in white-box manner due to the limited
access to the model. The limitation was addressed by Papernot et al. [ 104] and they introduced a
black-boxattackstrategy.Theytrainedasubstitutemodeltoapproximatethedecisionboundaries
ofthetargetmodelwiththelabelsobtainedbyqueryingthetargetmodel.Thentheyconducteda
white-box attack on this substitute one and generated adversarial examples accordingly. Specifi-
cally,theyadoptedFSGMand JSMAin generatingadversarialexamples forthesubstituteDNN.
GAN-like Attack . There are another branch of black-box attacks that leverages the Generative
Adversarial Neural (GAN) models. Zhao et al. [ 157] first trained a generative model, WGAN, on
the training dataset X. WGAN could generate data points that follow the same distribution with
X.Thentheyseparatelytrainedaninvertertomapdatasample xtozinthelatentdensespaceby
minimizing the reconstruction error. Instead of perturbing x, they searched for adversaries z∗in
theneighbourof zinthelatentspaceandmapped z∗backto x∗andcheckif x∗wouldchangethe
prediction.Theyintroducedtwosearchalgorithms: iterativestochasticsearch andhybridshrinking
search. The former used expanding strategy that gradually expands the search space, while the
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:10 W.E.Zhanget al.
later used shrinking strategy that starts from a wide range and recursively tightens the upper
bound ofthesearchrange.
3.2 Attacking Image DNNsversus Attacking Textual DNNs
To attack a textual DNN model, we cannot directly apply the approaches from the image DNN
attackersas thereare threemain differencesbetweenthem:
•Discrete vs. Continuous Inputs . Image inputs are continuous, typically the methods use Lp
norm to measure the distance between clean data point with the perturbed data point.
However, textual data is symbolic, thus discrete. It is hard to define the perturbations on
texts. Carefully designed variants or distance measurements for textual perturbations are
required. Another choice is to first map the textual data to continuous data, then adopt
the attack method from computer vision. We will give further discussion for this aspect in
Section3.3.
•Perceivable vs. Unperceivable . Small change of the image pixels usually can not be easily
perceived by human beings, hence the adversarial examples will not change the human
judgment, but can only fool the DNN models. However, small changes on texts, e.g., char-
acterorwordchange,willeasilybeperceived,renderingthepossibilityofattackfailure.For
example,thechangescouldbeidentifiedorcorrectedbyspelling-checkandgrammarcheck
before inputting into textual DNN models. Therefore, it is nontrivial to find unperceivable
textualadversaries.
•Semantic vs. Semantic-less . In the case of images, small changes usually do not change the
semanticsoftheimageastheyaretrivialandunperceivable.However,perturbationontexts
would easily change the semantics of a word and a sentence, thus can be easily detected
and heavily affect the model output. For example, deleting a negation word would change
the sentiment of a sentence. But this is not the case in computer vision where perturbing
individual pixels does not turn the image from a cat to another animal. Changing seman-
tics of the input is against the goal of adversarial attack that keeps the correct prediction
unchangedwhile foolinganvictimDNN.
Due to these differences, current state-of-the-art textual DNN attackers either carefully adjust
the methods from image DNN attackers by enforcing additional constraints, or propose novel
methodsusingdifferenttechniques.
3.3 Vectorizing Textual Inputs and Perturbation Measurements
Vectorizing Textual Input. DNN models require vectors as input, for image tasks, the normal
way is to use the pixel value to form the vectors/matrices as DNN input. But for textual models,
specialoperationsareneededtotransformthetextintovectors.Therearethreemainbranchesof
methods: word-count-based encoding, one-hot encoding and dense encoding (or feature embed-
ding) and thelattertwo aremostly usedin DNNmodels oftextualapplications.
•Word-Count-based Encoding. The Bag-of-words (BOW) method has the longest history in
vectorizingtext.IntheBOWmodel,zero-encodedvectorwithlengthofthevocabularysize
isinitialized.Thenthedimensioninvectorisreplacedbythecountofcorrespondingword’s
appearanceinthegivensentence.Anotherword-count-basedencodingistoutilizetheterm
frequency-inversedocumentfrequency(TF-IDF)ofaword(term),andthedimensioninthe
vectoris theTF-IDFvalue oftheword.
•One-hot Encoding. In one-hot encoding, a vector feature represents a token–a token could
be a character (character-level model) or a word (word-level model). For character-level
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:11
one-hotencoding,therepresentationcanbeformulated as[ 30]
x=[(x11,...x1n);...(xm1,...xmn)], (8)
where xbeatextofsequence, xij∈{0,1}|A|and|A|isthealphabet(insomeworks, |A|also
include symbols). In Equation ( 8),mis the number of words, nis the maximum number
of characters for a word in sequence x. Thus, each word has the same-fixed length of vec-
tor representation and the length is decided by the maximum number of characters of the
words. For word-level one-hot encoding, following the above notations, the text xcan be
representedas
x=[(x1,...,xm,xm+1...xk)], (9)
wherexm∈{0,1}|V|and|V|is the vocabulary, which contains all words in a corpus. kis
themaximumnumberofwordsallowedforatext,sothat[ (xm+1...xk)]iszero-paddingsif
m+1<k.One-hotencodingproducesvectorswithonly0and1values,where1indicates
the corresponding character/word appears in the sentence/paragraph, and 0 indicates it
does not appear. Thus, one-hot encoding usually generates sparse vectors/matrices. DNNs
have proven to be very successful in learning values from the sparse representations as
theycanlearnmoredensedistributedrepresentationsfromtheone-hotvectorsduringthe
trainingprocedure.
•DenseEncoding. Comparingtotheone-hotencoding,denseencodinggenerateslowdimen-
sionalanddistributedrepresentationsfortextualdata.Word2Vec[ 90]usescontinuousbag-
of-words (CBOW) and skip-gram models to generate dense representation for words, i.e.,
wordembeddings.Theunderlyingassumptionisthatwordsappearingwithinsimilarcon-
text possess similar meaning. Word embeddings, to some extend, alleviates the discrete-
ness and data-sparsity problems for vectorizing textual data [ 36]. Extensions of word em-
beddings such as doc2vec and paragraph2vec [ 69] encode sentences/paragraphs to dense
vectors.
PerturbationMeasurement. AsdescribedinSection 2.1.3,thereneedsawaytomeasurethesize
oftheperturbation,sothatitcanbecontrolledtoensuretheabilityoffoolingthevictimDNNwhile
remainsunperceivable.However,themeasurementintextualperturbationsisdrasticallydifferent
with the perturbations in image. Usually, the size of the perturbation is measured by the distance
between clean data xand its adversarial example x/prime. But in texts, the distance measurement also
needstoconsiderthegrammarcorrectness,syntaxcorrectnessandsemantic-preservance.Wehere
listthemeasurementsusedin therestofthissurvey.
•Norm-based measurement. Directly adopting norms such as Lp,p∈0,1,2,∞requires the
inputdatabecontinuous.Onesolutionistousecontinuousanddenserepresentations(e.g.,
embedding)torepresentthetexts.Butthisusuallyresultsininvalidandincomprehensible
texts,whichinturn needto involve otherconstraints.
•Grammar and syntax related measurement. Ensuring the grammar or syntactic correctness
makes theadversarialexamplesnot easilyperceived.
—Grammar and syntax checker are used in some works to ensure the textual adversarial
examples generatedarevalid.
—Perplexity is usually used to measure the quality of a language model. In one reviewed
literature[ 91],theauthorsusedperplexitytoensurethegeneratedadversarialexamples
(sentences)arevalid.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:12 W.E.Zhanget al.
—Paraphrase is controlled and can be regarded as a type of adversarial example (Sec-
tion4.3.3). When perturbing, the validity of paraphrases is ensured in the generation
process.
•Semantic-preserving measurement. Measuring semantic similarity/distance is often per-
formedonwordvectorsbyadoptingvectors’similarity/distancemeasurements.Giventwo
n-dimensionalword vectors p=(p1,p2,...,pn)andq=(q1,q2,...,qn):
—EuclideanDistance is adistanceof twovectorsin the Euclidean space:
d(p,q)=/radicalbig
(p1−q1)2+(p2−q2)2+···+(pn−qn)2. (10)
—CosineSimilarity computescosinevalueof theangle betweenthetwo vectors:
cos(p,q)=/summationtextn
i=1pi×qi/radicalBig/summationtextn
i=1(pi)2×/radicalBig/summationtextn
i=1(qi)2. (11)
•Edit-based measurement. Edit distance is a way of quantifying the minimum changes from
one string to the other. Different definitions of edit distance use different sets of string
operations[ 73].
—LevenshteinDistance usesinsertion,removaland substitutionoperations.
—Word Mover’s Distance (WMD) [68] is an edit distance operated on word embeddings. It
measurestheminimumamountofdistancethatthewordsofonedocumentneedtotravel
toreachthewordsoftheotherdocumentintheembeddingspace[ 38].Thisdistancecan
be measuredwithaminimization problemformulated asfollows:
min/summationtextn
i,j=1Tij||ei−ej||2, (12)
s.t.,/summationtextn
j=1Tij=di,∀i∈{i,...,n},/summationtextn
i=1Tij=d/prime
i,∀j∈{i,...,n},
where eiandejarewordembeddingsofword iandword j,respectively. nisthenumber
of words. T∈Rn×nis a flow matrix, where Tij≤0 denotes how much of word iind
travels to word jind/prime.A n d dandd/primeare normalized bag-of-words vectors of the two
documents,respectively.
—Number of changes is a straightforward yet simple way to measure the edits and it is
adoptedin somerelevant literature.
•Jaccardsimilaritycoefficient isusedformeasuringsimilarityoffinitesamplesetsbyutilizing
intersectionandunionofthesets:
J(A,B)=|A∩B|
|A∪B|. (13)
In texts,A,Bare two documents (or sentences). |A∩B|denotes the number of words ap-
pearingin bothdocuments, |A∪B|referstothenumberof uniquewords intotal.
4 ATTACKING NEURALMODELS IN NLP:THE STATE-OF-THE-ART
Inthissection,wefirstintroducethecategoriesofattackmethodsontextualdeep-learningmod-
els and then highlight the state-of-the-art research works, aiming to identify the most promising
advancesin recentyears.
4.1 Categoriesof Attack MethodsonTextual Deep-learning Models
We categorize existing adversarial attack methods from different viewpoints. Figure 1illustrates
theseviewpointsin detail.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:13
Fig.1. Categories of adversarialattack methodson textualdeep-learning models.
Inthisarticle,fiveviewpointsareusedto categorizetheattackmethods:
•Model Access refers to the knowledge of attacked model when the attack is performed. In
thefollowing section,wefocuson thediscussionfromthisviewpoint.
•Semantic Application refers to the methods via different NLP applications. More detailed
discussionwillbeprovidedin Section 4.5.
•Target Type refers to the goal of the attack is enforcing incorrect prediction or targeting
specificresults.
•SemanticGranularity considersonwhatgranularity levelthemodelis attacked.
•attackedDNNs are victimDNNsthatwe havediscussedinSection 2.2.
In the following sections, we will continuously provide information about different categories
that the methods belong to. One important category of methods need to be noted is the cross-
modalattacks,inwhichtheattackedmodelconsidersthetasksdealingwithmulti-modaldata,e.g.,
image and text data. They are not attacks for pure textual DNNs, hence we discuss this category
of methods separately in Section 4.4in addition to white-box attacks (Section 4.2) and black-box
attacks(Section 4.3).
4.2 White-boxAttack
In a white-box attack, the attack requires the access to the model’s full information, including
architecture, parameters, loss functions, activation functions, input and output data. White-box
attackstypicallyapproximatetheworst-caseattackforaparticularmodelandinput,incorporating
asetofperturbations.Thisattackstrategyisoftenveryeffective.Inthissection,wegroupwhite-
boxattackson textualDNNsintoseven categories.
4.2.1 FGSM-based. FGSM is one of the first attack methods on images (Section 3.1). It attracts
many follow-up efforts in attacking textual DNNs. TextFool [ 76] adopts the concept of FGSM.
Specifically, it approximates the contributions of text items and identifies the ones that possess
significant contribution to the text classification task. Instead of using sign of the cost gradient
in FGSM, this work considers the magnitude of the cost gradient. The authors proposed three
types of attacks: insertion,modification ,a n dremoval. Specifically, they computed cost gradient
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:14 W.E.Zhanget al.
ΔxJ(f,x,c/prime)of each training sample x, employing back propagation, where fis the model func-
tion,xis the original data sample, and c/primeis the target output class. Then they identified the char-
actersthatcontainthedimensionswiththehighestgradientmagnitudeandnamedthem hotchar-
acters.Phrasesthatcontainenough hotcharacters andoccurthemostfrequentlyarechosenas Hot
Training Phrases (HTPs). In the insertionstrategy, adversarial examples are crafted by inserting a
few HTPs of the target class c/primenearby the phrases with significant contribution to the original
classc. The authors further leveraged external sources like Wikipedia and forged fact to select
the valid and plausible sentences. In the modification strategy, the authors identified Hot Sample
Phrase(HSP) usingthesamewayofidentifyingHTPs.ThentheyreplacedthecharactersinHTPs
bycommonmisspellingsorcharactersvisuallysimilar.SimilartoidentifyingtheHTPs,theyiden-
tify phrases with significant contribution to the current classification by locating hot characters
with the highest gradient magnitude. In the removalstrategy, the inessential adjective or adverb
in HSPs are removed. The three strategies and their combinations are evaluated on a CNN text
classifier[ 156].However, thesemethodsareperformedmanually, asdiscussedby theauthors.
The work in Reference [ 117] adopted the same idea as TextFool, but it provides a removal-
addition-replacement strategythatfirsttriesto removetheadverb( wi),whichcontributesthemost
to the text classification task (measured using loss gradient). If the output sentences in this step
have incorrect grammar, then the method will insertaw o r dpjbeforewi.pjis selected from a
candidate pool, in which the synonyms and typos and genre specific keywords (identified via
term frequency) are candidate words. If the output cannot satisfy the highest cost gradient for
all thepj, then the method replaceswiwithpj. The authors showed that their method is more
effective than TextFool. As the method ordered the words with their contribution ranking and
crafted adversarial samples according to the order, it is a greedy method that always gets the
minimummanipulationuntiltheoutputchanges.Toavoidbeingperceivedbyhuman,theauthors
constrained the replaced/added words to not affect the grammar and part-of-Speech (POS) of the
original words.
Inmalwaredetection,anportableexecutable(PE)isrepresentedbybinaryvector {x1,...,xm},
xi∈{0,1}that uses 1 and 0 to indicate the PE is present or not where mis the number of PEs.
UsingPEs’vectorsasfeatures,malwaredetectionDNNscanidentifythemalicioussoftware.Mal-
ware detection is not a typical textual application. As it also works with discrete data, methods
for attacking textual DNNs can be applied to attacking malware detection DNNs. The authors of
Reference[ 3]investigatedthemethodstogeneratebinary-encodedadversarialexamples.Topre-
serve the functionality of the adversarial examples, they incorporated four bounding methods to
craftperturbations.ThefirsttwomethodsadoptFSGMk[67],themulti-stepvariantofFGSM.This
FGSM variant restricts the perturbations in a binary domain by introducing deterministic round-
ing (dFGSMk) and randomized rounding (rFGSMk). These two bounding methods are similar to
L∞-ballconstraintsonimages[ 40].Thethirdmethod(multi-stepBitGradientAscent(BGAk))sets
thebitofthe jthfeatureifthecorrespondingpartialderivativeofthelossisgreaterthanorequalto
thelossgradient’s L2-normdividedby√m.Thefourthmethod(multi-stepBitCoordinateAscent
(BCAk)) updates one bit in each step by considering the feature with the maximum correspond-
ing partial derivative of the loss. These two last methods actually visit multiple feasible vertices.
The work also proposes an adversarial learning framework that aims to robustify the malware
detectionmodel.
Reference [ 114] also attacks malware detection DNNs. The authors made perturbations on the
embedding presentation of the binary sequences and reconstructed the perturbedexamples to its
binary representation. Particularly, they appended a uniformly random sequence of bytes (pay-
load) to the original binary sequence. Then they embedded the new binary to its embedding and
performedFGSMonlyontheembeddingofthepayload.Theperturbationisperformediteratively
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:15
until the detector outputs incorrect prediction. Since the perturbation is only performed on pay-
load,insteadoftheinput,thismethodwillpreservethefunctionalityofthemalware.Finally,they
reconstructed adverse embedding to valid binary file by mapping the adversary embedding to its
closestneighbourin thevalid embeddingspace.
Gradient-based methods leverage the gradient of the loss function as FGSM, but differs from
FGSM as it uses the gradient itself instead of using the sign or magnitude of gradient. AdvGen
[23] is a gradient-based method for attacking neural machine translation (NMT) models. It first
generatesadversarialexamplesbyconsideringthesimilaritybetweenthelossfunction’sgradient,
and the distance between a word and its replacing word (i.e., adversarial word). It uses language
model to identify the most possible replacing words given the word, because language model en-
forcesthesemantic-preservanceoftheadversarialword.ThenAdvGenincorporatesthegenerated
adversarialexamples intothedecoderof NMTmodeltodefend theattacks.
ManyworksdirectlyadoptFGSMforadversarialtraining,i.e.,putitasregularizerwhentraining
themodel.We willdiscusssomerepresentativesin Section 5.
4.2.2 JSMA-based. JSMAisanotherpioneerworkonattackingneuralmodelsforimageappli-
cations (refers to Section 3.1). Reference [ 103] uses forward derivative as JSMA to find the most
contributable sequence towards the adversary direction. The network’s Jacobian had been calcu-
latedbyleveragingcomputationalgraphunfolding[ 96].Theauthorscraftedadversarialsequences
fortwotypesofRNNmodelswhoseoutputiscategoricalandsequentialdata,respectively.Forcat-
egorical RNN, the adversarial examples are generated by considering the Jacobian JacbF[:,j]c o l -
umncorrespondingtooneoftheoutputcomponents j.Specifically,foreachword i,theyidentified
thedirectionofperturbationby
sign(JacbF(x/prime)[i,д(x/prime)]), (14)
д(x/prime)=argmax 0,1(pj), (15)
wherepjis the output probability of the target class. As in JSMA, they chose logit to replace
probabilityinthisequation.Theyfurtherprojectedtheperturbedexamplesontotheclosestvector
intheembeddingspacetogetvalidembedding.ForsequentialRNN,aftercomputingtheJacobian
matrix, they altered the subset of input setps {i}with high Jacobian values JacbF[i,j] and low
Jacobianvalues JacbF[i,k]fork/nequaljtoachievemodificationon a subsetofoutputsteps {j}.
Reference [ 43] (and Reference [ 44]) is the first work to attack neural malware detector. The
authors first performed feature engineering and obtained more than 545K static features for soft-
ware applications. They used binary indicator feature vector to represent an application. Then
they crafted adversarialexamples on the input feature vectorsby adoptingJSMA: they computed
gradient of model Jacobian to estimate the perturbation direction. Later, the method chooses a
perturbation ηgiven input sample that with maximal positive gradient into the target class. In
particular,theperturbationsare chosenvia index i,satisfying
i=argmax j∈[1,m],Xj=y/primef/prime
y(Xj), (16)
wherey/primeisthetargetclass, misthenumberoffeatures.Onthebinaryfeaturevectors,theperturba-
tionsarebinaryvalueflipoperations(i.e.,0 →1or1→0).Thismethodpreservesthefunctionality
oftheapplications.Toensurethatthemodificationscausedbytheperturbationsdonotchangethe
application much, which will keep the malware application’s functionality complete, the authors
usedtheL1normtoboundtheoverallnumberoffeaturesmodified,andfurtherboundedthenum-
ber of altered features to 20. In addition, the authors provided three methods to defense against
theattacks,namely, featurereduction ,distillation ,andadversarialtraining .Theyfoundadversarial
trainingisthemost effectivedefensemethod.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:16 W.E.Zhanget al.
4.2.3 C&W-based. The work in Reference [ 130] adopted C&W method (refers to Section 3.1)
for attacking predictive models of medical records. The aim is to detect susceptible events and
measurementsineachpatient’smedicalrecords,whichprovideguidancefortheclinicalusage.The
authorsusedstandardLSTMasthepredictivemodel.GiventhepatientEHRdatabeingpresented
by a matrix Xi∈Rd×ti(dis the number of medical features and tiis the time index of medical
check),thegenerationof theadversarialexample isformulated as
min
ˆXmax{−ϵ,[loдit(x/prime)]y−[loдit(x)]y/prime}+λ||x/prime−x||1, (17)
whereloдit(·)denotesthelogitlayeroutput, λistheregularizationparameterthatcontrolsthe L1
norm regularization, y/primeis the targeted label, while yis the original label. After generating adver-
sarial examples, the authors picked the optimal example according to their proposed evaluation
scheme that considers both the perturbation magnitude and the structure of the attacks. Finally,
they used the adversarial example to compute the susceptibility score for the EHR as well as the
cumulative susceptibilityscorefor differentmeasurements.
Seq2Sick [ 22] attacked the seq2seq models using two targeted attacks: non-overlapping attack
andkeywords attack . For non-overlapping attack, the authors aimed to generate adversarial se-
quences that are entirely different from the original outputs. They proposed a hinge-like loss
functionthatoptimizeson thelogit layer of theneuralnetwork:
|K|/summationdisplay
i=1min
t∈[M]/braceleftbigg
mt/parenleftbigg
max/braceleftbigg
−ϵ,max
y/nequalki/braceleftBig
z(y)
t/bracerightBig
−z(ki)
t/bracerightbigg/parenrightbigg/bracerightbigg
, (18)
where{zt}indicates the logit layer outputs of the adversarial example. For the keyword attack,
targeted keywords are expected to appear in the output sequence. The authors also put the op-
timization on the logit layer and tried to ensure that the targeted keyword’s logit be the largest
amongallwords.Furthermore,theydefinedmaskfunction mtosolvethekeywordcollisionprob-
lem. Thelossfunctionthenbecomes
Lkeywords =|K|/summationdisplay
i=1min
t∈[M]/braceleftbigg
mt/parenleftbigg
max/braceleftbigg
−ϵ,max
y/nequalki/braceleftBig
z(y)
t/bracerightBig
−z(ki)
t/bracerightbigg/parenrightbigg/bracerightbigg
, (19)
wherekidenotes the ith word in output vocabulary. To ensure the generated word embedding is
valid, this work also considers two regularization methods: group lasso regularization to enforce
the group sparsity, and group gradient regularization to make adversaries within the permissible
region of theembeddingspace.
4.2.4 Direction-based. HotFlip[30]performsatomicflipoperationstogenerateadversarialex-
amples.Insteadofleveraginggradientofloss,HotFlipusesthedirectionalderivatives.Specifically,
HotFlip represents character-level operations, i.e., swap, insert and delete, as vectors in the input
space and estimates the change in loss by directional derivatives with respect to these vectors.
Specifically, given one-hot representation of the input, a character flip in the jth character of the
ithword(a→b)canberepresentedby thefollowing vector:
−→vijb=(0,..;(0,..(0,..−1,0,..,1,0)j,..0)i;0,..), (20)
where−1and1areinthecorrespondingpositionsforthe athandthe bthcharactersofthealpha-
bet,respectively.Thenthebestcharacterswapcanbefoundbymaximizingafirst-orderapproxi-
mation of losschangevia directionalderivativealong theoperationvector:
max∇xJ(x,y)T·−→vijb=max
ijv∂J(b)
∂xij−∂J(a)
∂xij, (21)
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:17
whereJ(x,y)is the model’s loss function with input xand true output y. Similarly, insertion at
thejth position of the ith word can also be treated as a character flip, followed by more flips,
since the characters are shifted to the right until the end of the word. The character deletion is
a number of character flips, since the characters are shifted to the left. Using the beam search,
HotFlipefficientlyfinds thebestdirectionsfor multipleflips.
Reference [ 29] extended HotFlip by adding targeted attacks. Besides the swap, insertion and
deletion as provided in HotFlip, the authors proposed a controlled attack , which is to remove a
specific word from the output, and a targeted attack , which is to replace a specific word by a
chosen one. To achieve these attacks, they maximized the loss function J(x,yt)and minimized
J(x,y/prime
t),wheretisthetargetwordforthecontrolledattack,and t/primeisthewordtoreplace t.Further,
they proposed three types of attacks that provide multiple modifications. In one-hotattack, they
manipulated all the words in the text with the optimal operation. In Greedyattack, they made
anotherforwardandbackwardpass,inadditiontopickingthebestoperationfromthewholetext.
InBeamsearch attack,theyreplacedthesearchmethodingreedywiththebeamsearch.Inallthe
attacksproposedinthiswork,theauthorssetthresholdforthemaximumnumberofchanges,e.g.,
20%of charactersareallowed tobe changed.
4.2.5 Attention-based. The authors of Reference [ 14] proposed two white-box attacks for the
purpose of comparing the robustness of CNN verses RNN. They leveraged the model’s internal
attentiondistributiontofindthepivotalsentence,whichisassignedalargerweightbythemodel
to derive the correct answer. Then they exchanged the words that received the most attention
withtherandomlychosenwordsinaknownvocabulary.Theyalsoperformedanotherwhite-box
attackbyremovingthewholesentencethatgetsthehighestattention.Althoughtheyfocusedon
attention-based models, their attacks do not examine the attention mechanism itself, but solely
leverage theoutputsof theattentioncomponent(i.e.,attentionscore).
4.2.6 Reprogramming. Reference [ 97] adopts adversarial reprogramming (AP) to attack se-
quence neural classifiers. AP [ 31] is a recently proposed adversarial attack where an adversarial
reprogramming function дθis trained to re-purpose the attacked DNN to perform an alternate
task (e.g., question classification to name classification) without modifying the DNN’s parame-
ters. AP adopts idea from transfer learning, but keeps the parameters unchanged. The authors of
Reference [ 97] proposed both white-box and black-box attacks. In white-box, Gumbel-Softmax is
appliedtotrain дθwhocanworkondiscretedata.Wediscusstheblack-boxmethodlater.Theau-
thorsevaluatedtheirmethodsonvarioustextclassificationtasksandconfirmedtheeffectiveness
oftheirmethods.
4.2.7 Hybrid. Reference [ 38] perturbs the input text on word embeddings against the CNN
model. This is a general method that is applicable to most of the attack methods developed for
computer vision DNNs. The authors specifically applied FGSM and DeepFool. Directly applying
methodsfromcomputervisionwouldgeneratemeaninglessadversarialexamples.Toaddressthis
issue, the authors rounded the adversarial examples to the nearest meaningful word vectors by
usingWordMover’sDistance(WMD)asthedistancemeasurement.Theevaluationsonsentiment
analysis and text classification datasets show that WMD is a qualified metric for controlling the
perturbations.
SummaryofWhite-boxAttack. Wesummarizethereviewedwhite-boxattackworksinTable 1.
We highlight four aspects, which include granularity—on which level the attack is performed;
target—whether the method is target or untarget; the attacked model, perturbation control—
methodstocontrolthesizeoftheperturbation,andapplications.Itisworthnotingthatinbinary
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:18 W.E.Zhanget al.
Table 1. Summaryof theReviewed White-boxAttack Methods
Strategy Work Granularity Target Attacked Models Perturbation
ControlApplications
FSGM-based[76]character,word YCNN[156] L∞ TC
[117]word NCNN[156] L∞,Grammar and
POS correctnessTC
[114]PE binary CNN[26] Boundaries employ
L∞andL2MAD
[3]PE embedding binary MalConv[ 109] L∞ MAD
[23]word NTransformer [ 136]Language model MT
JSMA-based[103]wordembedding binary LSTM – TC
[43,44]applicationfeatures binary Feedforward L1 MAD
C&W-based[130]medical features YLSTM L1 MSP
[22]wordembedding YOpenNMT-py [ 63]L2+gradient
regularizationTS,MT
Direction-based[30]character NCharCNN-LSTM[ 60]– TC
[29]character YCharCNN-LSTM[ 25]Number of changes MT
Attention-based[14]word,sentence N[80,140], CNN,
LSTM,and ensemblesNumber of changes MRC,QA
Reprogramming [97]word NCNN,LSTM,Bi-LSTM – TC
Hybrid [38]wordembedding NCNN WMD TC, SA
PE: portable executable; TC: text classification; SA: sentiment analysis; TS: text summarisation; MT: machine transla-
tion; MAD: malware detection; MSP: Medical Status Prediction; MRC: machine reading comprehension; QA: question
answering;WMD:Word Mover’s Distance;–: notavailable.
classifications, target and untarget methods show the same effect, so we point out their targetas
“binary”in thetable.
4.3 Black-boxAttack
Black-boxattackdoesnotrequirethedetailsoftheneuralnetworks,butcanaccesstheinputand
output.Thistypeofattacksoftenrelyonheuristicstogenerateadversarialexamples,anditismore
practical as in many real-world applications the details of the DNN is a black box to the attacker.
In thisarticle,we groupblack-boxattackson textualDNNsinto fivecategories.
4.3.1 ConcatenationAdversaries. Reference[ 54]isthefirstworktoattackreadingcomprehen-
sion systems. The authors proposed concatenation adversaries , which is to append distracting but
meaningless sentences at the end of the paragraph. These distracting sentences do not change
the semantics of the paragraph and the question answers, but will fool the neural model. The
distracting sentences are either carefully generated informative sentences or arbitrary sequence
of words using a pool of 20 random common words. Both perturbations were obtained by itera-
tivelyqueryingtheneuralnetworkuntiltheoutputchanges.Figure 2illustratesanexamplefrom
Reference [ 54] that after adding distracting sentences (in blue) the answer changes from correct
one (green) to incorrect one (red). The authors of Reference [ 142]i m p r o v e dt h ew o r kb yv a ry i n g
the locations where the distracting sentences are placed and expanding the set of fake answers
for generating the distracting sentences, rendering new adversarial examples that can help train-
ingmorerobustneuralmodels.Also,Reference[ 14]utilizedthedistractingsentencestoevaluate
the robustnessof their reading comprehensionmodel. Specifically,they use a poolof ten random
common words in conjunction with all question words and the words from all incorrect answer
candidatestogeneratethedistractingsentences.Inthiswork,asimpleword-levelblack-boxattack
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:19
Fig.2. ConcatenationadversarialattackonreadingcomprehensionDNN.Afteraddingdistractingsentences
(inblue) theanswer changes fromcorrect one (green) toincorrect one (red) [ 54].
Fig.3. Generalprincipleofconcatenationadversaries.Correctoutputareoftenutilizedtogeneratedistorted
output,whichlaterwillbeusedtobuilddistractingcontents.Appendingdistractingcontentstotheoriginal
paragraphasadversarialinputtotheattackedDNNandcausetheattackedDNNtoproduceincorrectoutput.
is also performed by replacing the most frequent words via their synonyms. As aforementioned,
the authors also provided two white-box strategies. Figure 3illustrates the general workflow for
concatenation attack. Correct output (i.e., answer in MRC tasks) are often leveraged to generate
distorted output, which later will be used to build distracting contents. Appending distracting
contentstotheoriginalparagraphformstheadversarialinputtotheattackedDNN.Thedistract-
ing contents will not distracthuman being and ideal DNNs, but can make vulunerable DNNs to
produceincorrectoutput.
4.3.2 EditAdversaries. TheworkinReference[ 12]perturbedtheinputdataofneuralmachine
translationapplicationsintwoways: Synthetic,whichperformedthecharacterorderchanges,such
as swap, middle random (i.e., randomly change orders of characters except the first and the last),
fullyrandom(i.e.,randomlychangeordersofallcharacters)andkeyboardtype.Theyalsocollected
typos and misspellings as adversaries, by naturally leveraging the typos from the datasets. Fur-
thermore,theauthorsofReference[ 98]attackedtheneuralmodelsfordialoguegeneration.They
applied various perturbations in dialogue context, namely, Random Swap (randomly transposing
neighboring tokens) and Stopword Dropout (randomly removing stopwords), Paraphrasing (re-
placingwordswiththeirparaphrases),GrammarErrors(e.g.,changingaverbtothewrongtense)
for theShould-Not-Change attacks, and the Add Negation strategy (negates the root verb of the
sourceinput)andAntonymstrategy(changesverbs,adjectives,oradverbstotheirantonyms)for
Should-Change attacks.DeepWordBug[ 34]isasimplemethodthatusescharactertransformations
togenerateadversarialexamples.Theauthorsfirstidentifiedtheimportant“tokens,”i.e.,wordsor
characters that affect the model prediction mostly by scoring functions developed by measuring
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:20 W.E.Zhanget al.
Fig.4. EditadversarialattackonsentimentanalysisDNN.Aftereditingwords(red),thepredictionchanges
from 100% of Negative to89% of Positive[ 73].
the DNN classifier’s output. Then they modified the identified tokens using four strategies: re-
place, delete, add and swap. The authors evaluated their method on a variety of NLP tasks, e.g.,
textclassification,sentimentanalysisandspamdetection.ByfollowingtheworkinReference[ 34],
the work in Reference [ 73] refines the scoring function. Moreover, this work provided white-box
attackadoptingJSMA.Onecontributionofthisworkliesontheperturbationrestrictionbyusing
fourtextualsimilaritymeasurement:editdistanceoftext;Jaccardsimilaritycoefficient;Euclidean
distance on word vector; and cosine similarity on word embedding. Their method had been eval-
uated only on sentiment analysis task. Figure 4is an example of edit adversarial example from
Reference[ 73],whereonlya few editoperationsmislead theclassifierto give wrongpredictions.
Whenattackingthetextclassificationmodels,theworkinReference[ 110]providesaprobability
method to select the word to replace. It first collects all synonyms of the words in the existing
corpus. Then it selects a proposed substitute words from the synonyms by measuring its impact
ontheclassificationprobability.Theonesleadtothemostsignificantchangeswillbeselected.The
authors also incorporate the word saliency to determine the replacement order. Word saliency is
thedegree ofchange intheoutputclassificationprobabilityif a word issetto unknown.
The authors of Reference [ 91] proposed a method for automatically generating adversarial ex-
amplesthatviolateasetofgivenFirst-OrderLogicconstraintsinnaturallanguageinference(NLI).
They proposed an inconsistency loss to measure the degree to which a set of sentences causes a
model to violate a rule. The adversarial example generation is the process for finding the map-
ping between variables in rules to sentences that maximize the inconsistency loss. The sentences
are composed by sentences with a low perplexity (defined by a language model). To generate
low-perplexity adversarial sentence examples, they used three edit perturbations: (i) change one
wordinoneoftheinputsentences;(ii)removeoneparsesubtreefromoneoftheinputsentences;
(iii) insert one parse sub-tree from one sentence in the corpus in the parse tree of the another
sentence.
The work in Reference [ 5] uses genetic algorithm (GA) for minimising the number of word
replacement from the original text, but at the same time can change the result of the attacked
model. The authors adopted crossover andmutation operations in GA to generate perturbations.
They measured the effectiveness of the word replacement accoding to the impact on attacked
DNNs.Theirattackfocusedonsentimentanalysis andtextualentailment DNNs.
In Reference [ 19], the authors proposed a framerwork for adversarial attack on Differentiable
Neural Computer (DNC). DNC is a computing machine with DNN as its central controller oper-
atingonanexternalmemorymodulefordataprocessing.Theirmethodusestwonewautomated
andscalablestrategiestogenerategrammaticallycorrectadversarialattacksinquestionanswering
domain, utilizing metamorphic transformation. The first strategy, Pick-n-Plug , consists of a pick
operator to draw adversarial sentences from a particular task (source task) and the plug operator
injects these sentences into a story from another task (target task), without changing its correct
answers.Anotherstrategy, Pick-Permute-Plug ,extendstheadversarialcapabilityof Pick-n-Plug by
an additional permute operator after picking sentences (gpick) from a source task. Words in a
particular adversarial sentence can be permuted with its synonyms to generate a wider range of
possibleattacks.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:21
Fig.5. Generalprincipleofeditadversaries.Perturbationsareperformedonsentences,words,orcharacters
byedit strategies such asreplace, delete, add,and swap.
An edit-based approach MHA (Metropolis-Hastings Attack) was proposed aiming to provide
fluent and effective adversarial attacks [ 155]. MHA is based on language model and Metropolis-
Hastings (M-H) sampling. The authors used M-H sampling to generate the word to replace the
oldword(forreplaceoperation)andtherandomword(forinsertoperation).Languagemodelsare
used to enforce the fluency of the sentence after replace/insert/delete operations. They proposed
both black-box and white-box versions. The only difference between the two is the definition of
thepre-selectionfunctionwhenchoosingthemost possiblewords tooperate.
Figure5illustrates the general workflow for edit adversaries. Perturbations are performed on
sentences, words or characters by edit strategies, such as replacement, deletion, insertion, and
swap.
4.3.3 Paraphrase-based Adversaries. SCPNs [53] produces a paraphrase of the given sentence
with desired syntax by inputting the sentence and a targeted syntactic form into an encoder-
decoder architecture. Specifically, the method first encodes the original sentence, then inputs the
paraphrasesgeneratedbyback-translationandthetargetedsyntactictreeintothedecoder,whose
output is the targeted paraphrase of the original sentence. One major contribution lies on the se-
lection and processing of the parse templates. The authors trained a parse generator separately
fromSCPNsandselected20mostfrequenttemplatesinthePARANMT-50Mdataset.Aftergener-
atingparaphrasesusingtheselectedparsetemplates,theyfurtherprunednon-sensiblesentences
bycheckingn-gramoverlapandparaphrase-basedsimilarity.Theattackedclassifiercancorrectly
predict the label of the original sentence but fails on its paraphrase, which is regarded as the ad-
versarialexample.SCPNshasbeenevaluatedonsentimentanalysisandtextualentailmentDNNs
and showed significant impact on the attacked models. Although this method uses target strat-
egy to generate adversarial examples, it does not specify targeted output. Therefore, we group
it as untargeted attack. Furthermore, the work in Reference [ 127] uses the idea of paraphrase
generation techniques that create semantically equivalent adversaries (SEA). The authors gener-
ated paraphrases of an input sentence x, and got predictions from funtil the original prediction
is changed. At the same time, they considered the semantically equivalent to x/primethat is 1 if xis
semantically equivalent to x/primeand 0 otherwise as shown in Equation ( 22). After that, this work
proposes a semantic-equivalent rule-based method for generalizing these generated adversaries
intosemantically equivalentrulestounderstandand fixthemostimpactfulbug:
SEA(x,x/prime)=1[SemEq(x,x/prime)∧f(x)/nequalf(x/prime)]. (22)
Figure6depictsthegeneralprincipleofparaphrase-basedadversaries.Theparaphrasesarecon-
sidered as adversarial examples. When generating the paraphrases, controlled perturbations are
incorporated.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:22 W.E.Zhanget al.
Fig. 6. General principle of paraphrase-based adversaries. Carefully designed (controlled) paraphrases are
regarded as adversarialexamples, which fool DNN toproduceincorrect output.
4.3.4 GAN-based Adversaries. Some works propose to leverage Generative Adversarial Net-
work (GAN) [ 39] to generate adversaries [ 157]. The purpose of adopting GAN is to make the
adversarial examples more natural. In Reference [ 157], the model proposed to generate adversar-
ial examples consists of two key components: a GAN, which generate fake data samples, and an
inverter that maps input xto its latent representation z/prime. The two components are trained on the
original input by minimizing reconstruction error between original input and the adversarial ex-
amples. Perturbation is performed in the latent dense space by identifying the perturbed sample
ˆzintheneighborhoodof z/prime.Twosearchapproaches,namely, iterativestochasticsearch andhybrid
shrinkingsearch ,areproposedtoidentifytheproper ˆz.However,itrequiresqueryingtheattacked
model each time to find the ˆzthat can make the model produce incorrect prediction. Therefore,
this method is quite time-consuming. The work is applicable to both image and textual data as it
intrinsically eliminates the problem raised by the discrete attribute of textual data. The authors
evaluated their method on three applications, namely: textual entailment, machine translation,
and image classification.
4.3.5 Substitution. The work in Reference [ 52] proposes a black-box framework that attacks
RNNmodelformalwaredetection.Theframeworkconsistsoftwomodels:agenerativeRNNand
a substitute RNN. The generative RNN aims to generate adversarial API sequence from the mal-
ware’sAPIsequence.Itisbasedontheseq2seqmodelproposedinReference[ 131].Itparticularly
generates a small piece of API sequence and inserts the sequence after the input sequence. The
substitute RNN, which is a bi-directional RNN with attention mechanism, is to mimic the behav-
ior of the attacked RNN. Therefore, generating adversarial examples will not query the original
attacked RNN, but its substitution. The substitute RNN is trained on both malware and benign
sequences,aswellastheGumbel-SoftmaxoutputsofthegenerativeRNN.Here,Gumbel-softmax
isusedtoenablethejointtrainingofthetwoRNNmodels,becausetheoriginaloutputofthegen-
erativeRNNisdiscrete.Specifically,itenablesthegradienttobeback-propagatedfromgenerative
RNN to substitute RNN. This method performs attack on API, which is represented as a one-hot
vector,i.e.,given MAPIs,thevectorforthe ithAPIisanM-dimensionalbinaryvectorthatthe ith
dimension is1 whileotherdimensionsare 0s.
4.3.6 Reprogramming. As aforementioned,Reference[ 97]provides bothwhite-boxandblack-
box attacks. We describe black-box attack here. In a black-box attack, the authors formulated the
sequence generation as a reinforcement learning problem, and the adversarial reprogramming
function дθis the policy network. Then they applied REINFORCE-based optimisation to train
дθ.
SummaryofBlack-boxAttack. Wesummarizethereviewedblack-boxattackworksinTable 2.
We highlight four aspects include granularity on which level the attack is performed; whether
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:23
Table 2. Summaryof theReviewed Black-box AttackMethods
Strategy Work Granularity Target Attacked Models Perturbation
ControlApplications
Concatenation[54]word NBiDAF,Match-LSTM – MRC
[142]word, character NBiDAF+Self-
Attn+ELMo
[108]– MRC
[14]word, sentence N[80,140],CNN, LSTM
andensemblesNumber ofchanges MRC,QA
Edit[12]character, word NNematus [ 120],
char2char [ 70],
charCNN [ 60]– MT
[98]word, phrase NVHRED[ 123]+attn,
RLin[74],DynoNet
[47]– DA
[34]character, word NWord-level LSTM,
Character-level CNN– SA,TC
[73]character, word NWord-level LSTM,
Character-level CNNEdDist,JSC,EuDistV,
CSESA
[91]word, phrase NcBiLSTM,DAM,ESIM Perplexity NLI
[5]word NLSTM EuDistV SA,TE
[19]word, sentence NDNC – QA
[155]word YLSTM,BiDAF M-H SA,NLI
[110]word NCNN,LSTM synonyms TC
Paraphrase-
based[53]word NLSTM Syntax-ctrl
paraphraseSAandTE
[127]word NBiDAF,Visual7W
[158],fastText[ 41]Self-defined
semantic-equivalencyMRC,SA,VQA
GAN-based[157]word NLSTM,TreeLSTM,
GoogleTranslate
(En-to-Ge)GAN-constraints TE,MT
Substitution[52]API NLSTM,BiLSTMand
variants– MD
Reprogramming [97]word NCNN,LSTM,Bi-LSTM – TC
MRC: machine reading comprehension; QA: question answering; VQA: visual question answering; DA: dialogue gen-
eration; TC: text classification; MT: machine translation; SA: sentiment analysis; NLI: natural language inference; TE:
textual entailment; MD: malware detection; EdDist: edit distance of text; JSC: Jaccard similarity coeffcient; EuDistV:
Euclideandistance onword vector; CSE: cosine similarityon word embedding;“–”: notavailable.
the method is target driven or not; the attacked neural model, perturbation control, and NLP
applications.
4.4 Multi-modalAttacks
Some works attack DNNs that are dealing with cross-modal data. For example, the neural mod-
els contain an internal component that performs image-to-text or speech-to-text conversion. Al-
thoughtheseattacksarenotforpuretextualdata,webrieflyintroducetherepresentativeonesfor
thepurposeof a comprehensivereview.
4.4.1 Image-to-text. Image-to-textmodelsisaclassoftechniquesthatgeneratetextualdescrip-
tionfor an image basedonthesemanticcontentof thelatter.
Optical Character Recognition (OCR). Recognizing characters from images is a task named
Optical Character Recognition (OCR). OCR is a multimodal learning task that takes an image as
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:24 W.E.Zhanget al.
inputandoutputtherecognizedtext.TheauthorsinReference[ 129]proposedawhite-boxattack
on OCR and follow-up NLP applications.Theyfirstusedthe originaltext torender a cleanimage
(conversionDNNs).ThentheyfoundwordsinthetextthathaveantonymsinWordNetandsatisfy
editdistancethreshold.Onlytheantonymsthatarevalidandkeepsemanticinconsistencieswillbe
kept.Later,themethodlocatesthelinesinthecleanimagecontainingtheaforementionedwords,
whichcanbereplacedbytheirselectedantonyms.Themethodthentransformsthetargetwordto
targetsequence.Giventheinput/targetimagesandsequences,theauthorsformedthegenerating
of adversarialexample as anoptimization problem:
minωc·JCTCf(x/prime,t/prime)+||x−x/prime||2
2, (23)
x/prime=(α·tanh(ω)+β)/2, (24)
α=(xmax−xmin)/2,β=(xmax+xmin)/2,
JCTC(f(x,t))=−logp(t|x), (25)
wheref(x)istheneuralsystemmodel, JCTC(·)istheConnectionistTemporalClassification(CTC)
loss function, xis the input image, tis the ground truth sequence, x/primeis the adversarial example,
t/primeis the target sequence, ω,α,βare parameters controlling adversarial examples to satisfy the
box-constraint of x/prime∈[xmin,xmax]p,w h e r epis the number of pixels ensuring valid x/prime. After
generatingadversarialexamples,themethodreplacestheimagesofthecorrespondinglinesinthe
text image. The authors evaluated this method in three aspects: single word recognition, whole
documentrecognition,andNLPapplications,whichwerebasedontherecognizedtext(sentiment
analysis and document categorisation specifically). They also found that the proposed method
suffersfromlimitations suchas transferabilityand physicalrealizability.
SceneTextRecognition(STR). STRisalsoanimage-to-textapplication.InSTR,theentireimage
is mapped to word strings directly. In contrast, the recognition in OCR is a pipeline process: first
segmentsthewordstocharacters,thenperformstherecognitiononsinglecharacters.AdaptiveAt-
tack[153]evaluatesthepossibilityofperformingadversarialattackforscenetextrecognition.The
authors proposed two attacks, namely, basic attack and adaptive attack. Basic attack is similar to
the work in Reference [ 129] and it also formulates theadversarial example generation as an opti-
mization problem:
minωJCTCf(x/prime,t/prime)+λD(x,x/prime), (26)
x/prime=tanh(ω), (27)
whereD(·)isEuclideandistance.Thedifferencesto[ 129]lieonthedefinitionof x/prime(Equation( 24)
vs. Equation ( 27)), and the distance measurement between x,x/prime(L2norm vs. Euclidean distance),
andtheparameter λ,whichbalancestheimportanceofbeingadversarialexampleandclosetothe
original image. As searching for proper λis quite time-consuming, the authors proposed another
method to adaptively find λ. They named this method Adaptive Attack, in which they defined
thelikelihoodofasequentialclassificationtaskfollowingaGaussiandistributionandderivedthe
adaptiveoptimization forsequentialadversarialexamples as
min||x−x/prime||2
2
λ2
1+JCTCf(x/prime,t/prime)
λ2
2+logλ2
1+Tlogλ2
2+1
λ2
2, (28)
whereλ1andλ2aretwoparameterstobalanceperturbationandCTCloss, Tisthenumberofvalid
paths given targeted sequential output. Adaptive Attack can be applied to generate adversarial
examples on both non-sequential and sequential classification problems. Here, we only highlight
theequationforsequentialdata(Equation( 28)).Theauthorsevaluatedtheirproposedmethodson
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:25
tasks targeting the text insertion, deletion and substitution in output. The results demonstrated
thatAdaptiveAttackismuch fasterthanbasicattack.
Image Captioning. Image captioning is another multimodal learning task that takes an image
asinputandgeneratesatextualcaptiondescribingitsvisualcontents.Show-and-Fool[ 20]gener-
atesadversarialexamplestoattacktheCNN-RNN–basedimagecaptioningmodel.TheCNN-RNN
model attacked uses a CNN as encoder for image feature extraction and a RNN as decoder for
caption generation. Show-and-Fool has two attack strategies: targeted caption (i.e., the generated
captionmatchesthetargetcaption)and targetedkeywords (i.e.,thegeneratedcaptioncontainsthe
targetedkeywords).In general,theyformulated thetwotasksusingthefollowing formulation:
minωc·J(x/prime)+||x/prime−x||2
2, (29)
x/prime=x+η,
x=tanh(y),x/prime=tanh(ω+y),
wherec>0 is a pre-specified regularization constant, ηis the perturbation, ω,yare param-
eters controlling x/prime∈[−1,1]n. The difference between these two strategies is the definition
of the loss function J(·). For targeted caption strategy, provided the targeted caption as S=
(S1,S2,...St,...SN),w h e r eStrefers to the index of the tth word in the vocabulary and Nis
thelengthof thecaption,thelossis formulatedas
JS,loдit(x/prime)=N−1/summationdisplay
t=2max/braceleftBig
−ϵ,max
k/nequalSt/braceleftBig
z(k)
t/bracerightBig
−z(St)
t/bracerightBig
, (30)
whereStisthetargetw or d, z(St)
tisthelogitofthetargetword.Infact,thismethodmininizesthe
difference between the maximumn logitexceptSt, and the logit of St. For the targeted keywords
strategy,given thetargetedkeywords K:=K1,...,KM,thelossfunctionis
JK,loдit(x/prime)=M/summationdisplay
j=1min
t∈[N]/braceleftbigg
max/braceleftbigg
−ϵ,max
k/nequalKj/braceleftBig
z(k)
t/bracerightBig
−z(Kj)
t/bracerightbigg/bracerightbigg
. (31)
The authors performed extensive experiments on Show-and-Tell [ 137] and varied the parameters
in the attacking loss. They found that Show-and-Fool is not only effective on attacking Show-
and-Tell,theCNN-RNN–basedimagecaptioningmodel,butisalsohighlytransferabletoanother
model Show-Attend-and-Tell [ 147].
VisualQuestionAnswering(VQA). Givenanimageandanaturallanguagequestionaboutthe
image, VQA is to provide an accurate answer in natural language. The work in Reference [ 148]
proposesaniterativeoptimizationmethodtoattacktwoVQAmodels.Theobjectivefunctionpro-
posedmaximizestheprobabilityofthetargetanswerandunweightsthepreferenceofadversarial
examples with smaller distance to the original image when this distance is below a threshold.
Specifically,theobjectivecontainsthreecomponents.ThefirstoneissimilartoEquation( 26),that
replacesthelossfunctiontothelossoftheVQAmodelanduses ||x−x/prime||2/√
Nasdistancebetween
x/primeandx. The second component maximizes the difference between the softmax output and the
prediction when it is different with the target answer. The third component ensures the distance
between x/primeandxis under a lower bound. The attacks are evaluated by checking whether better
success rate is obtained over the previous attacks, and the confidence score of the model to pre-
dict the target answer. Based on the evaluations, the authors concluded that attention, bounding
box localization and compositional internal structures are vulnerable to adversarial attacks. This
work also attacks an image captioning neural model. We refer to the original paper for further
information.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:26 W.E.Zhanget al.
Table 3. Summaryof theReviewed Cross-modalAttacks
Multi-modal Applications Work Target Access Attacked Models Perturbation
Control
Image-to-TextOpticalCharacterRecognition [129]Ywhite-box Tesseract [ 135] L2,EdDist
SceneTextRecognition [153]Ywhite-box CRNN[124] L2
Image Captioning [20]Ywhite-box Show-and-Tell[ 137] L2
Visual Question Answering [148]Ywhite-box MCB[33], N2NMN[ 51]L2
Visual-Semantic Embeddings [125]Nblack-box VSE++ [32] –
Speech-to-Text Speech Recognition [18]Ywhite-box DeepSpeech[ 46] L2
EdDist: edit distanceof text;-:not available.
Visual-semantic Embeddings(VSE). TheaimofVSEistobridgenaturallanguage andtheun-
derlyingvisualworld.InVSE,theembeddingspacesofbothimagesanddescriptivetexts(captions)
arejointlyoptimizedandaligned.Reference[ 125]attacksthelatestVSEmodelbygeneratingad-
versarialexamplesinthetestsetandevaluatestherobustnessoftheVSEmodels.Theyperformed
the attack on textual part by introducing three method: (i) replace nouns in the image captions
utilizing the hypernymy/hyponymy relations in WordNet; (ii) change the numerals to different
ones and singularize or pluralize the corresponding nouns when necessary; (iii) detect the rela-
tions and shuffle the non-interchangeable noun phrases or replace the prepositions. This method
can beconsideredasa black-boxeditadversary.
4.4.2 Speech-to-text. Speech-to-text is also known as speech recognition. The task is to rec-
ognize and translate the spoken language into text automatically. Reference [ 18] attacks Deep
Speech, a state-of-the-art speech-to-text transcription neural network based on LSTM. Given a
naturalwaveform,theauthorsconstructedanaudioperturbationthatisalmostinaudiblebutcan
berecognizedbyaddingintotheoriginalwaveform.Theperturbationisconstructedbyadopting
the idea from C&W method (refer to Section 3.1), which measures the image distortion by the
maximum amount of changed pixels. Adapting this idea, they measured the audio distortion by
calculating relative loudness of an audio and proposed to use Connectionist Temporal Classifica-
tion lossfor theoptimizationtask.Thentheysolved thistaskwithAdamoptimizer[ 61].
Summary of Multi-modal Attack. We summarize the reviewed black-box attack works in Ta-
ble3. We list the representative works and highlight key aspects, including target or untarget,
accessDNNornot,controlofperturbation,andtheattackedneuralmodels.
4.5 Benchmark Datasets byApplications
In recent years, neural networks gain success in different NLP domains and the popular applica-
tionsincludetextclassification,readingcomprehension,machinetranslation,textsummarization,
question answering, dialogue generation, to name a few. In this section, we review the current
works on generating adversarial examples on the neural networks in the perspective of NLP ap-
plications. Table 4summarizes the works we reviewed in this article according to their applica-
tion domain. We further list the benchmark datasets used in these works in the table as auxiliary
information—thus,wereferreaderstothelinks/referenceswecollectforthedetaileddescriptions
ofthedatasets.Notethattheauxiliarydatasetsthathelptogenerateadversarialexamplesarenot
included.Instead,we only presentthedatasetsusedtoevaluatetheattackedneuralnetworks.
Text Classification. Majority of the surveyed works attack the deep neural networks for text
classification, since these tasks can be framed as a classification problem. Sentiment analysis
aims to classify the sentiment to several groups (e.g., in three-group scheme: neural, positive,
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:27
Table 4. AttackedApplications and BenchmarkDatasets
Applications Representative Works Benchmark Datasets
ClassificationText Classification [30,34,38,76,97,118]DBpedia,ReutersNewswires, AG’s
news, Sogou News, Yahoo!Answers,
RCV1, SurnameClassification Dataset
SentimentAnalysis[30,34,53,97,103,117,
118,127]SST, IMDBReview, Yelp Review, Elec,
RottenTomatoes Review, Amazon
Review, ArabicTweets Sentiment
Spam Detection [34] EnronSpam, Datasetsfrom [ 156]
GenderIdentification [117] Twitter Gender
GrammarErrorDetection [118] FCE-public
MedicalStatusPrediction [130] ElectronicHealth Records(EHR)
Malware Detection [4,43,44,52,114] DREBIN,Microsoft Kaggle
RelationExtraction [10,145]NYT Relation, UWRelation, ACE04,
CoNLL04 EC,Dutch RealEstate
Classifieds, AdverseDrugEvents
MachineTranslation [12,22,29,157]TEDTalks, WMT’16 Multimodal
Translation Task
MachineComprehension [14,19,54,142]SQuAD,MovieQAMultipleChoice,
Logical QA
Text Summarization [22] DUC2003, DUC2004, Gigaword
Text Entailment [53,56,91,157] SNLI, SciTail, MultiNLI,SICK
POSTagging [151] WSJ portionofPTB, Treebanks inUD
DialogueSystem [98] UbuntuDialogue,CoCoA,
Cross-modelOpticalCharacter Recognition [129] Hillary Clinton’s emails
SceneText Recognition [153] Street ViewText, ICDAR2013, IIIT5K
ImageCaptioning [20,148] MSCOCO,VisualGenome
VisualQuestionAnswering [148] Datasets from [ 6], Datasets from [ 158]
Visual-SemanticEmbedding [125] MSCOCO
SpeechTecognition [18] Mozilla CommonVoice
and negative). Gender identification, grammatical error detection, and malware detection can
be framed as binary classification problems. Relation extraction can be formulated as single or
multi-classification problem. Predict medical status is a multi-class problem that the classes are
defined by medical experts. These works usually use multiple datasets to evaluate their attack
strategies to show the generality and robustness of their methods. Reference [ 76] uses the DBpe-
diaontologydataset[ 71]toclassifythedocumentsamplesinto14high-levelclasses.Reference[ 38]
usestheIMDBmoviereviews[ 83]forsentimentanalysis,andReuters-2andReuters-5newswires
datasetprovidedbyNLTKpackage10forcategorization.Reference[ 103]usesaun-specifiedmovie
review dataset for sentiment analysis. Reference [ 117] also uses IMDB movie review dataset for
sentiment analysis. The work also performs gender classification on and a Twitter dataset.11Ref-
erence [34] performs spam detection on Enron Spam Dataset [ 89] and adopts six large datasets
fromReference[ 156],i.e.,AG’snews,12Sogounews[ 138],DBPediaontologydataset,Yahoo!An-
swers13for text categorization and Yelp reviews,14Amazon reviews [ 88] for sentiment analysis.
10https://www.nltk.org/ .
11https://www.kaggle.com/crowdflower/twitter-user-gender-cla2013 .
12https://www.di.unipi.it/Egulli/ .
13Yahoo! Answers Comprehensive Questions andAnswers version 1.0datasetthrough the Yahoo! Webscope program.
14YelpDatasetChallengein2015.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:28 W.E.Zhanget al.
Reference[ 30]alsousesAG’snewsfortextclassification.Further,theauthorsusedStanfordSenti-
mentTreebank(SST)dataset[ 128]forsentimentanalysis.Reference[ 118]conductsevaluationon
threetasks:sentimentanalysis(IMDBmoviereview,Elec[ 55],RottenTomatoes[ 102]),textcatego-
rization (DBpedia Ontology dataset and RCV1 [ 72]) and grammatical error detection (FCE-public
[150]). Reference [ 130] generates adversarial examples on the neural medical status prediction
systemwithreal-worldelectronichealthrecordsdata.
Many works target the malware detection models. References [ 43,44] perform attack on neu-
ral malware detection systems. They use DREBIN dataset, which contains both benigh and mali-
ciousandroidapplications[ 7].Reference[ 114]collectsbenighwindowsapplicationfilesanduses
Microsoft Malware Classification Challenge dataset [ 113] as the malicious part. Reference [ 52]
crawls180programswithcorrespondingbehaviorreportsfromawebsiteformalwareanalysis.15
Seventy percent of the crawled programs are malware. Reference [ 97] targets the text classifica-
tionneuralmodelsandusesfourdatasetstoevaluatetheirattackmethods:SurnameClassification
Dataset,16Experimental Data for Question Classification [ 75], Arabic Tweets Sentiment Classifi-
cation Dataset [ 1], and IMDB movie review dataset. In Reference [ 145], the authors modelled the
relation extraction as a classification problem, where the goal is to predict the relations existing
between entity pairs for the given text mentions. They used two relation datasets: NYT dataset
[111]andUWdataset[ 78].Reference[ 10]targetsatimprovingtheefficacyoftheneuralnetworks
for joint entity and relation extraction. Different to the method in Reference [ 145], the authors
models the relation extraction task as a multi-label head selection problem. The four datasets are
used in their work: ACE04 dataset [ 28], CoNLL04 EC tasks [ 115], Dutch Real Estate Classifieds
(DREC)dataset[ 11],and AdverseDrugEvents(ADE)[ 45].
MachineTranslation. MachineTranslationworksonparalleldatasets,oneofwhichusessource
language and the other one is in the target language. Reference [ 12] uses the TED talks parallel
corpuspreparedforIWSLT2016[ 87]fortestingtheNMTsystems.ItalsocollectsFrench,German,
and Czech corpus for generating natural noises to build a look-up table, which contains possible
lexical replacements.These replacementsare later used for generating adversarialexamples. Ref-
erence [29] also uses the same TED talks corpus and used German to English, Czech to English,
and FrenchtoEnglishpairs.
MachineComprehension. Machinecomprehensiondatasetsusuallyprovidecontextdocuments
or paragraphs to the machines. Based on the comprehension of the contexts, machine compre-
hension models can answer a question. Jia and Liang are one of the first to consider the textual
adversaryandtheytargetedtheneuralmachinecomprehensionmodels[ 54].TheyusedtheStan-
ford Question Answering Dataset (SQuAD) to evaluate the impact of their attack on the neural
machine comprehension models. SQuAD is a widely recognized benchmark dataset for machine
comprehension. Reference [ 142] follows the previous works and also works on SQuAD dataset.
AlthoughthefocusofReference[ 14]istodeveloparobustmachinecomprehensionmodelrather
than attacking MC models, the authors used the adversarial examples to evaluate their proposed
system.TheyusedMovieQAmultiplechoicequestionansweringdataset[ 134]fortheevaluation.
Reference[ 19]targetsattacksondifferentiableneuralcomputer(DNC),whichisanovelcomput-
ingmachinewithDNN.ItevaluatestheattacksonlogicalquestionansweringusingbAbItasks.17
Text Summarization. The goal for text summarization is to summarize the core meaning of
a given document or paragraph with succinct expressions. Reference [ 22] evaluates their attack
15https://malwr.com/ .
16Classifyingnameswithacharacter-levelrnn-pytroch tutorial.
17https://research.fb.com/downloads/babi/ .
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:29
on multiple applications including text summarization and it uses DUC2003,18DUC2004,19and
Gigaword20for evaluatingtheeffectivenessof adversarialexamples.
Text Entailment. The fundamental task of text entailment is to decide whether a premise text
entails a hypothesis, i.e., the truth of one text fragment follows from another text. Reference [ 56]
assesses various models on two entailment datasets: Stanford Natural Lauguage Inference (SNLI)
[15]andSciTail[ 58].Reference[ 91]alsousestheSNLIdataset.Furthermore,itusestheMultiNLI
[144]dataset.
Part-of-Speech(POS)Tagging. ThepurposeforPOStaggingistoresolvethepart-of-speechfor
each word in a sentence, such as noun, verb. It is one of the fundamental NLP tasks to facilitate
other NLP tasks, e.g., syntactic parsing. Neural networks are also adopted for this NLP task. Ref-
erence[151]adoptsthemethodinReference[ 93]tobuildamorerobustneuralnetworkbyintro-
ducingadversarialtraining,butitappliesthestrategy(withminormodifications)inPOStagging.
By training on the mixture of clean and adversarial examples, the authors found that adversarial
examplesnotonlyhelpimprovingthetaggingaccuracybutalsocontributetodownstreamtaskof
dependency parsing and is generally effective in different sequence labelling tasks. The datasets
usedintheirevaluationinclude:theWallStreetJournal(WSJ)portionofthePennTreebank(PTB)
[85]and treebanksfromUniversalDependencies(UD)v1.2[ 99].
Dialogue Generation. Dialogue generation is a fundamental component for real-world virtual
assistants such as Siri21and Alexa.22It is the text generation task that automatically generates a
response for the post given by the user. Reference [ 98] is one of the first to attack the generative
dialogue models. It uses the Ubuntu Dialogue Corpus [ 82] and Dynamic Knowledge Graph Net-
work with the Collaborative Communicating Agents (CoCoA) dataset [ 47] for the evaluation of
theirtwoattackstrategies.
Cross-model Applications. Reference [ 129] evaluates the OCR systems with adversarial exam-
plesusingHillaryClinton’semails,23whichisintheformofimages.Italsoconductstheattackon
NLPapplicationsusingRottenTomatoesandIMDBreviewdatasets.TheworkinReference[ 153]
attacks the neural networks designed for scene text recognition. The authors conducted experi-
mentsonthreestandardbenchmarksforcroppedwordimagerecognition,namely,theStreetView
Textdataset(SVT)[ 139]theICDAR2013dataset(IC13)[ 57]andtheIIIT5K-worddataset(IIIT5K)
[92].Reference[ 20]attackstheimagecaptioningneuralmodels.ThedatasetusedistheMicrosoft
COCO (MSCOCO) dataset [ 77]. Reference [ 148] works on the problems of attacking neural mod-
els for image captioning and visual question answering. For the first task, it uses Visual Genome
dataset[64].Forthesecondtask,itusestheVQAdatasetscollectedandprocessedinReference[ 6].
Reference [ 125] works on Visual-Semantic Embedding applications, where the MSCOCO dataset
is used. Reference [ 18] targets the speech recognition problem. The datasets used is the Mozilla
Common Voicedataset.24
Multi-Applications. Someworksadapttheirattackmethodsintodifferentapplications,namely,
they evaluate their method’s transferability across applications. Reference [ 22] attacks the
18http://duc.nist.gov/duc2003/tasks.html .
19http://duc.nist.gov/duc2004/ .
20https://catalog.ldc.upenn.edu/LDC2003T05 .
21https://www.apple.com/au/siri/ .
22https://en.wikipedia.org/wiki/Amazon_Echo .
23https://www.kaggle.com/kaggle/hillary-clinton-emails/data .
24https://voice.mozilla.org/en .
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:30 W.E.Zhanget al.
sequence-to-sequence models. Specifically, it evaluates the attack on two applications: text sum-
marization and machine translation. For text summarization, as mentioned before, it uses three
datasetsDUC2003,DUC2004,andGigaword.Forthemachinetranslation,itsamplesasubsetform
WMT’16 Multimodal Translation dataset.25Reference [ 53] proposes to generate syntactically ad-
versarialparaphrasesandevaluatestheattackonsentimentanalysisandtextentailmentapplica-
tions. It uses SST for sentimental analysis and SICK [ 86] for text entailment. Reference [ 157]i sa
genericapproachforgeneratingadversarialexamplesonneuralmodels.Theapplicationsinvesti-
gated include image classification (MINIST digital image dataset), textual entailment (SNLI), and
machine translation. Reference [ 93] evaluates the attacks on five datasets, covering both senti-
mentanalysis(IMDBmoviereview,Elecproductreview,RottenTomatoesmoviereview)andtext
categorization (DBpedia Ontology, RCV1 news articles). Reference [ 127] targets both sentiment
analysis and visual question answering. For sentiment analysis, it uses the Rotten Tomato movie
reviews and IMDB movie reviews datasets. For visual question answering, it tests on the dataset
providedbyZhuetal. [ 158].
5 DEFENSE
Anessentialpurposeforgeneratingadversarialexamplesforneuralnetworksistoutilizethesead-
versarialexamplestoenhancethemodel’srobustness[ 40].Therearetwocommonwaysintextual
DNN to achieve this goal: adversarial training andknowledge distillation . Adversarial training in-
corporatesadversarialexamplesinthemodeltrainingprocess.Knowledgedistillationmanipulates
the neural network model and trains a new model. In this section, we introduce some represen-
tative studies belonging to these two directions. For more comprehensive defense strategies on
machine learningand deep-learningmodels and applications,pleaserefertoReferences[ 2,13].
5.1 Adversarial Training
Szegedyetal.[ 132]invented adversarialtraining ,astrategythatconsistsoftraininganeuralnet-
work to correctly classify both normal examples and adversarial examples. Goodfellow et al. [ 40]
employed explicit training with adversarial examples. In this section, we describe works utiliz-
ingdata augmentation ,model regularization ,a n drobust optimization for the defense purpose on
textualadversarialattacks.
5.1.1 DataAugmentation. Dataaugmentationextendstheoriginaltrainingsetwiththegener-
atedadversarialexamplesandtrytoletthemodelseemoredataduringthetrainingprocess.Data
augmentationiscommonlyusedagainstblack-boxattackswithadditionaltrainingepochsonthe
attackedDNNwithadversarialexamples.
The authors of Reference [ 54] attempted to enhance the reading comprehension model with
trainingontheaugmenteddatasetthatincludestheadversarialexamples.Theyshowedthatdata
augmentation is effective and robust against the attack that uses the same adversarial examples.
However,theirworkalsodemonstratedthatthisaugmentationstrategywould bestillvulnerable
againsttheattackswithotherkindsofadversarialexamples.Reference[ 142]sharedsimilarideato
augment the training dataset, but selected further informative adversarial examples as discussed
in Section 4.3.1.
The work in Reference [ 56] trained the text entailment system augmented with adversarial
examples. The purpose is to make the system more robust. The authors proposed three methods
togeneratemoredatawithdiversecharacteristics:(1) knowledge-based ,whichreplaceswordswith
theirhypernym/hyponymprovidedinseveralgivenknowledgebases;(2) hand-crafted ,whichadds
25http://www.statmt.org/wmt16/translation-task.html .
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:31
Table 5. AnExample of AdversarialTraining
Effectiveness [ 54]
Attacks Original Augmented
AddSent 34.8 70.4
AddSentMod 34.3 39.2
AddSent and AddSentMod are two attacking meth-
ods.Aftertrainingonaugmenteddata,theF1scores
(%)areimprovedagainstthe attackson testdata.
negationstotheexistingentailment;(3) neural-based ,whichleveragesaseq2seqmodeltogenerate
an entailment examples by enforcing the loss function to measure the cross-entropy between the
original hypothesis and the predicted hypothesis. During the training process, they adopted the
ideafromgenerativeadversarialnetworktotrainadiscriminatorandagenerator,andincorporated
theadversarialexamples in thediscriminator’soptimizationstep.
The work in Reference [ 12] explores another way for data augmentation. It takes the average
character embedding as a word representation and incorporates it into the input. This approach
is intrinsically insensitive to character scrambling such as swap,mid,a n dRand, thus can resist to
noisescausedbythesescramblingattacksproposedinthework.However,thisdefenseisineffec-
tive tootherattacksthatdo notperturboncharacters’orders.
Table5gives an example of the effectiveness of adversarial training in Reference [ 54]. The au-
thors pointed out that the adversarial examples need to be carefully designed when training on
adversarial examples to improve the model. This can be shown from the results of two attacking
methodsin thetable.
5.1.2 ModelRegularization. Modelregularizationenforcesthegeneratedadversarialexamples
astheregularizer andfollows theform of
min(J(f(x),y)+λJ(f(x/prime),y)), (32)
whereλisa hyperparameter.
FollowingReference[ 40],theworkinReference[ 93]constructstheadversarialtrainingwitha
linearapproximation asfollows
−logp(y|x+−ϵд/||д||2,;θ), (33)
д=∂xlogp(y|x;ˆθ),
where||д||2is theL2norm regularization, θis the parameter of the neural model, and ˆθis a con-
stant copy of θ. The difference to Reference [ 40] is that the authors performed the adversarial
generation and training in terms of the word embedding. Further, they extended their previous
work on attacking image deep neural model [ 94], where the local distribution smoothness (LDS)
is defined as the negative of the KL divergence of two distributions (original data and the adver-
saries). LDS measures the robustness of the model against the perturbation in local and “virtual”
adversarialdirection.Inthissense,theadversaryiscalculatedasthedirectiontowhichthemodel
distribution is the most sensitive in terms of KL divergence. They also applied this attack strat-
egy on word embeddings and performed adversarial training by adding adversarial examples as
regularizer.
The work in Reference [ 118] follows the idea from Reference [ 93] and extends the adversarial
training on LSTM. The authors followed FGSM to incorporate the adversarial training as a reg-
ularizer. But to enable the interpretability of adversarial examples, i.e., the word embeddings of
the adversaries should be valid word embeddings in the vocabulary, they introduced a direction
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:32 W.E.Zhanget al.
vectorthatassociatestheperturbedembeddingtothevalidwordembedding.Reference[ 145]sim-
plyadoptedtheregularizerutilizedinReference[ 93],butappliedtheperturbationsonpre-trained
word embeddings and in a different task: relation extraction. Other similar works that adopt Ref-
erence[93]areReferences[ 10,118,145,151].Wewillnotcoveralltheseworksinthisarticle,since
they simplyadoptingthesametechnique.
5.1.3 RobustOptimization. Madryetal.[ 84]castedDNNmodellearningasarobustoptimiza-
tion with min-max (saddle point) formulation, which is the composition of an inner non-concave
maximizationproblem(attack)andanouternon-convexminimizationproblem(defense).Accord-
ing to Danskin’s theorem, gradients at inner maximizers correspond to descent directions for the
min-maxproblem,thustheoptimizationcanstillapplyback-propagationtoproceed.Theapproach
successfullydemonstratedrobustnessofDNNsagainstadversarialimagesbytrainingandlearning
universally. Reference [ 3] adopted the idea and applied on malware detection DNN that handles
discretedata.Itslearningobjectiveisformulatedas
θ∗=arдmin
θE(x,y)∼D/bracketleftbigg
max
x/prime∈S(x)L(θ,x/prime,y)/bracketrightbigg
, (34)
whereS(x)isthesetofbinaryindicatorvectorsthatpreservethefunctionalityofmalware x,Lis
thelossfunctionfortheoriginalclassificationmodel, yisthegroundtruthlabel, θisthelearnable
parameters,and Ddenotesthedistributionof datasample x.
Itisworthnotingthattheproposedrobustoptimizationmethodisauniversalframeworkunder
which other adversarial training strategies have natural interpretation. We describe it separately,
keepingin view itspopularityin theliterature.
5.2 Distillation
Papernotetal.[ 106]proposeddistillationasanotherpossibledefenseagainstadversarialexamples.
Theprincipleistousethesoftmaxoutput(e.g.,theclassprobabilitiesinclassficationDNNs)ofthe
original DNN to train the second DNN, which has the same structure with the original one. The
softmaxoutputof theoriginalDNN isalso modifiedby introducinga temperature parameter T:
qi=exp(zi/T)/summationtext
kexp(zk/T), (35)
whereziis input of softmax layer. Tcontrols the level of knowledge distillation. When T=1,
Equation ( 35) turns back to the normal softmax function. If Tis large, then qiis close to a uni-
form distribution; when Tis small, the function will output more extreme values. Reference [ 44]
adopteddistillationdefenseforDNNsondiscretedataandappliedahightemperature T,ashigh-
temperature softmax is proved to reduce the model sensitivity to small perturbations [ 106]. The
authors trained the second DNN with the augmentation of original dataset and the softmax out-
puts from the original DNN. From the evaluations, they found that adversarial training is more
effectivethanusingdistillation.
6 DISCUSSIONS ANDOPEN ISSUES
Generating textual adversarial examples has a relatively shorter history than generating image
adversarial examples on DNNs, because it is more challenging to make perturbation on discrete
data, and meanwhile preserving the valid syntactic, grammar and semantics. We discuss some of
theissuesinthissectionandprovide suggestionsonfuturedirections.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:33
6.1 Perceivability
Perturbations in image pixels are usually hard to be perceived, thus do not affect human judg-
ment, but can fool the deep neural networks. However, the perturbation on text is obvious, no
matter the perturbation is flipping characters or changing words. Invalid words and syntactic er-
rors can be easily identified by human and detected by the grammar check software, hence the
perturbation is hard to attack a real NLP system. However, many research works generate such
typesofadversarialexamples.Itisacceptableonlyifthepurposeisutilizingadversarialexamples
to robustify the attacked DNN models. In semantic-preserving perspective, changing a word in a
sentencesometimeschangesitssemanticsdrasticallyandiseasilydetectedbyhumanbeings.For
NLP applications such as reading comprehension, and sentiment analysis, the adversarial exam-
ples need to be carefully designed in order not to change the should-be output. Otherwise, both
correct output and perturbed output change, violating the purpose of generating adversarial ex-
amples.Thisischallengingand limitedworksreviewed consideredthisconstraint.Therefore,for
practicalattack,weneedtoproposemethodsthatmaketheperturbationsnotonlyunperceivable
butalso preservecorrectgrammar and semantics.
6.2 Transferability
Transferability is a common property for adversarial examples. It reflects the generalization of
the attack methods. Transferability means adversarial examples generated for one deep neural
network on a dataset can also effectively attack another deep neural network (i.e., cross-model
generalization) or dataset (i.e., cross-data generalization). This property is more often exploited
in black-box attacks as the details of the deep neural networks do not affect the attack method
much. It is also shown that untargeted adversarial examples are much more transferable than
targeted ones [ 81]. Transferability can be organized into three levels in deep neural networks:
(i) same architecture with different data; (ii) different architectures with same application; and
(iii) different architectures with different data [ 154]. Although current works on textual attacks
cover both three levels, the performance of the transferred attacks still decrease drastically com-
pared to it on the original architecture and data, i.e., poor generalization ability. More efforts are
expectedto deliverbettergeneralization ability.
6.3 Automation
Some reviewed works are able to generate adversarial examples automatically, while others can-
not. In white-box attacks, leveraging the loss function of the DNN can identify the most affected
points (e.g., character, word) in a text automatically. Then the attacks are performed on these
points by automatically modifying the corresponding texts. In black-box attacks, some attacks,
e.g.,substitution trains substitute DNNs and applies white-box attack strategies on the substitu-
tion. This can be achieved automatically. However, most of the other works craft the adversarial
examplesinamanualmanner.Forexample,Reference[ 54]concatenatesmanually-chosenmean-
inglessparagraphstofoolthereadingcomprehensionsystems,todiscoverthevulnerabilityofthe
victim DNNs. Many research works follow Reference [ 54], not aiming on practical attacks, but
more on examining robustness of the target network. These manual works are time-consuming
andimpractical.Webelievethatmoreeffortsinthislinecouldpassthroughthisbarrierinfuture.
6.4 New Architectures
AlthoughmostofthecommontextualDNNshavegainedattentionfromtheperspectiveofadver-
sarial attack (Section 2.2), many DNNs have not been attacked so far, e.g., the generative neural
models:GenerativeAdversarialNetworks(GANs)andVariationalAuto-Encoders(VAEs).InNLP,
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:34 W.E.Zhanget al.
theyareusedtogeneratetexts.Deepgenerativemodelsrequiremoresophisticatedskillformodel
training. This would explain that these techniques have been mainly overlooked by adversarial
attack so far. Future works may consider generating adversarial examples for these generative
DNNs. Another example is differentiable neural computer (DNC). Only one work attacked DNC
sofar[19].Attentionmechanismsomehowbecomesastandardcomponentinmostofthesequen-
tialmodels.Buttherehasbeennoworkexaminingthemechanismitself.Instead,worksareeither
attackingtheoverallsystemthatcontainattentions,orleveragingattentionscorestoidentifythe
word for perturbation[ 14].
6.5 Iterative versusOne-off
Iterativeattacksiterativelysearchandupdatetheperturbationsbasedonthegradientoftheoutput
oftheattackedDNNmodel.Thus,itshowshighqualityandeffectiveness.Thatis,theperturbations
can be small enough and hard to defense. However, these methods usually require long time to
find the proper perturbations, rendering an obstacle for attacking in real-time. Therefore, one-off
attacksareproposedtotacklethisproblem.FGSM[ 40]isoneexampleofone-offattack.Intuitively,
one-off attack is much faster than iterative attack, but is less effective and easier to be defensed
[153]. When designing attack methods on a real application, attackers need to carefully consider
thetradeoffbetweenefficiencyand effectivenessof theattack.
7 CONCLUSION
Thisarticlepresentsthefirstcomprehensivesurveyinthedirectionofgeneratingtextualadversar-
ialexamplesondeepneuralnetworks.Wereviewrecentresearcheffortsanddevelopclassification
schemes to organize the existing literature. Additionally, we summarize and analyze them from
different aspects. We attempt to provide a good reference for researchers to gain insight of the
challenges,methods,andissuesinthisresearchtopicandshedlightonfuturedirections.Wehope
more robustdeepneuralmodels areproposedbasedon theknowledge oftheadversarialattacks.
REFERENCES
[1] NawafA.Abdulla,NizarA.Ahmed,MohammedA.Shehab,andMahmoudAl-Ayyoub.2013.Arabicsentimentanal-
ysis:Lexicon-basedandcorpus-based.In ProceedingsoftheIEEEJordanConferenceonAppliedElectricalEngineering
andComputing Technologies (AEECT’13) .IEEE, 1–6.
[2] NaveedAkhtarandAjmalS.Mian.2018.Threatofadversarialattacksondeeplearningincomputervision:Asurvey.
IEEEAccess 6(2018),14410–14430.
[3] AbdullahAl-Dujaili,AlexHuang,ErikHemberg,andUna-MayO’Reilly.2018.Adversarialdeeplearningforrobust
detectionof binary encoded malware.In Proceedings oftheIEEESecurity and Privacy Workshops(SPW’18) . 76–82.
[4] AbdullahAl-Dujaili,AlexHuang,ErikHemberg,andUna-MayO’Reilly.2018.Adversarialdeeplearningforrobust
detectionof binary encoded malware.In Proceedings ofthe IEEESecurity and Privacy Workshops (SPWorkshops’18) .
76–82.
[5] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018.
Generatingnaturallanguageadversarialexamples.In ProceedingsoftheConferenceonEmpiricalMethodsinNatural
Language Processing(EMNLP’18) . 2890–2896.
[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi
Parikh.2015.VQA:Visualquestionanswering.In Proceedingsofthe2015IEEEInternationalConferenceonComputer
Vision(ICCV’15) .2425–2433.
[7] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, and Konrad Rieck. 2014. DREBIN: Effective and
explainabledetectionofAndroidmalwareinyourpocket.In Proceedingsofthe21stAnnualNetworkandDistributed
System Security Symposium (NDSS’14) .
[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to
alignand translate.In Proceedings of the2015 International ConferenceonLearning Representations (ICLR’15) .
[9] MarcoBarreno,BlaineNelson,AnthonyD.Joseph,andJ.D.Tygar.2010.Thesecurityofmachinelearning. Machine
Learning 81,2 (2010),121–148.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:35
[10] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. Adversarial training for multi-
context joint entity and relation extraction. In Proceedings of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’18) .2830–2836.
[11] Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. An attentive neural architecture
for jointsegmentationandparsingand itsapplicationto realestateads. ExpertSyst.Appl. 102(2018),100–112.
[12] YonatanBelinkovandYonatanBisk.2018.Syntheticandnaturalnoisebothbreakneuralmachinetranslation. arXiv
preprint arXiv:1711.02173.
[13] BattistaBiggioandFabioRoli.2018.Wildpatterns:Tenyearsaftertheriseofadversarialmachinelearning. Pattern
Recogn.84(2018),317–331.
[14] Matthias Blohm, Glorianna Jagfeld, Ekta Sood, Xiang Yu, and Ngoc Thang Vu. 2018. Comparing attention-based
convolutional and recurrent neural networks: Success and limitations in machine reading comprehension. In Pro-
ceedings ofthe22nd Conferenceon Computational Natural Language Learning (CoNLL’18) .108–118.
[15] SamuelR.Bowman,GaborAngeli,ChristopherPotts,andChristopherD.Manning.2015.Alargeannotatedcorpus
forlearningnaturallanguageinference.In Proceedings oftheConferenceonEmpirical Methodsin Natural Language
Processing (EMNLP’15) .632–642.
[16] SamuelR.Bowman,LukeVilnis,OriolVinyals,AndrewM.Dai,RafalJózefowicz,andSamyBengio.2016.Generating
sentencesfromacontinuousspace.In Proceedingsofthe20thSIGNLLConferenceonComputationalNaturalLanguage
Learning (CoNLL’16) .10–21.
[17] NicholasCarliniandDavidA.Wagner.2017.Towardsevaluatingtherobustnessofneuralnetworks.In Proceedings
of theIEEESymposium on Security and Privacy (SP’17) . 39–57.
[18] Nicholas Carlini and David A. Wagner. 2018. Audio adversarial examples: Targeted attacks on speech-to-text. In
Proceedings ofIEEESecurity andPrivacy Workshops (SPW’18) .1 – 7 .
[19] Alvin Chan, Lei Ma, Felix Juefei-Xu, Xiaofei Xie, Yang Liu, and Yew Soon Ong. 2018. Metamorphic relation based
adversarial attackson differentiableneuralcomputer. CoRRabs/1809.02444(2018).
[20] HonggeChen,HuanZhang,Pin-YuChen,JinfengYi,andCho-JuiHsieh.2018.Attackingvisuallanguagegrounding
with adversarial examples: A case study on neural image captioning. In Proceedings of the 56th Annual Meeting of
theAssociation for Computational Linguistics (ACL’18) .2587–2597.
[21] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natu-
ral language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics
(ACL’17).1657–1668.
[22] MinhaoCheng,JinfengYi,HuanZhang,Pin-YuChen,andCho-JuiHsieh.2018.Seq2Sick:Evaluatingtherobustness
of sequence-to-sequence models withadversarial examples. arXiv preprint arXiv:1803.01128 .
[23] Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019. Robust neural machine translation with doubly adversarial
inputs. In Proceedings ofthe57th Conferenceof theAssociation forComputational Linguistics (ACL’19) .1085–1097.
[24] KyunghyunCho,BartvanMerrienboer,ÇaglarGülçehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,and
Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine transla-
tion. InProceedings oftheConferenceon Empirical Methods in Natural Language Processing (EMNLP’14) .1724–1734.
[25] Marta R. Costa-Jussà and José A. R. Fonollosa. 2016. Character-based neural machine translation. In Proceedings of
the54th AnnualMeeting of theAssociation forComputational Linguistics (ACL’16) .
[26] George E. Dahl, Jack W. Stokes, Li Deng, and Dong Yu. 2013. Large-scale malware classification using random
projections and neuralnetworks. In Proceedings of the 38th International Conferenceon Acoustics, Speech and Signal
Processing (ICASSP’13) .3422–3426.
[27] LiDengandYangLiu.2018. DeepLearning in Natural Language Processing . Springer.
[28] George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel.
2004.Theautomaticcontentextraction(ACE)program—Tasks,data,andevaluation.In Proceedingsofthe4thInter-
national Conferenceon Language Resources andEvaluation (LREC’04) .
[29] Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On adversarial examples for character-level neural machine
translation. In Proceedings ofthe27th International Conferenceon Computational Linguistics (COLING’18) .653–663.
[30] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-box adversarial examples for text
classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL’18) .
31–36.
[31] Gamaleldin F. Elsayed, Ian J. Goodfellow, and Jascha Sohl-Dickstein. 2019. Adversarial reprogramming of neural
networks. In Proceedings of the7th International ConferenceonLearning Representations (ICLR’19). Poster .
[32] Fartash Faghri, David J. Fleet, Ryan Kiros, and Sanja Fidler. 2017. VSE++: Improved visual-semantic embeddings.
CoRRabs/1707.05612.
[33] AkiraFukui,DongHukPark,DaylenYang,AnnaRohrbach,TrevorDarrell,andMarcusRohrbach.2016.Multimodal
compactbilinearpoolingforvisualquestionansweringandvisualgrounding.In Proceedingsofthe2016Conference
on Empirical Methods in Natural Language Processing (EMNLP’16) .457–468.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:36 W.E.Zhanget al.
[34] JiGao,JackLanchantin,MaryLouSoffa,andYanjunQi.2018.Black-boxgenerationofadversarialtextsequencesto
evadedeep-learningclassifiers.In Proceedings oftheIEEESecurityandPrivacyWorkshops(SP’18).Workshop .50–56.
[35] Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David Andersen, and George E. Dahl. 2018. Motivating the rules
of thegameforadversarial exampleresearch. CoRRabs/1807.06732.
[36] Yoav Goldberg. 2017. Neural Network Methods for Natural Language Processing . Morgan & Claypool Publishers.
DOI:https://doi.org/10.2200/S00762ED1V01Y201703HLT037
[37] ChristophGollerandAndreasKuchler.1996.Learningtask-dependentdistributedrepresentationsbybackpropaga-
tionthrough structure. Neural Netw. 1 (1996),347–352.
[38] Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn Ku. 2018. Adversarial texts with gradient methods.
arXiv preprint arXiv:1801.07175 .
[39] IanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,AaronC.Courville,
andYoshuaBengio.2014.Generativeadversarialnets.In ProceedingsoftheAnnualConferenceonNeuralInformation
ProcessingSystems (NIPS’14) . 2672–2680.
[40] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversarial examples.
InProceedings ofthe3rd International ConferenceonLearning Representations (ICLR’15) .
[41] EdouardGrave,TomasMikolov,ArmandJoulin,andPiotrBojanowski.2017.Bagoftricksforefficienttextclassifi-
cation.In Proceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics
(EACL’17) .427–431.
[42] AlexGraves,Abdel-rahmanMohamed,andGeoffreyE.Hinton.2013.Speechrecognitionwithdeeprecurrentneural
networks. In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP’13) .
6645–6649.
[43] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel. 2016. Adversarial
perturbations againstdeep neural networks formalwareclassification. arXiv preprint arXiv:1606.04435 .
[44] KathrinGrosse,NicolasPapernot,PraveenManoharan,MichaelBackes,andPatrickD.McDaniel.2017.Adversarial
examples for malware detection. In Proceedings of the 22nd European Symposium on Research in Computer Security
(ESORICS’17) .62–79.
[45] Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca
Toldo.2012.Developmentofabenchmarkcorpustosupporttheautomaticextractionofdrug-relatedadverseeffects
from medicalcasereports. J.Biomed. Info. 45,5(2012),885–892.
[46] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev
Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep speech: Scaling up end-to-end speech
recognition. CoRRabs/1412.5567.
[47] HeHe,AnushaBalakrishnan,MihailEric,andPercyLiang.2017.Learningsymmetriccollaborativedialogueagents
with dynamic knowledge graph embeddings. In Proceedings of the 55th Annual Meeting of the Association for Com-
putational Linguistics (ACL’17) .1766–1776.
[48] Jeff Heaton. 2018. Ian Goodfellow, Yoshua Bengio, and Aaron Courville: Deep learning—The MIT Press, 2016, 800
pp.,ISBN 0262035618. Genetic Program. Evol.Mach. 19,1–2(2018),305–307.
[49] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. CoRR
abs/1503.02531.Retrievedfrom http://arxiv.org/abs/1503.02531.
[50] Sepp Hochreiter andJürgen Schmidhuber. 1997.Longshort-term memory. Neural Comput. 9,8(1997),1735–1780.
[51] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to reason: End-
to-endmodulenetworksforvisualquestionanswering.In ProceedingsofIEEEInternationalConferenceonComputer
Vision(ICCV’17) .804–813.
[52] WeiweiHuandYingTan.2018.Black-boxattacksagainstRNNbasedmalwaredetectionalgorithms.In Proceedings
oftheWorkshopsof thethe32ndAAAIConferenceonArtificial Intelligence (AAAIworkshops’18) .245–251.
[53] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syn-
tactically controlled paraphrase networks. In Proceedings of the Conference of the North American Chapter of the
Association forComputational Linguistics: Human Language Technologies (NAACL-HLT’18) .1875–1885.
[54] RobinJiaandPercyLiang.2017.Adversarialexamplesforevaluatingreadingcomprehensionsystems.In Proceedings
oftheConferenceon Empirical Methods in Natural Language Processing (EMNLP’17) .2021–2031.
[55] Rie Johnson and Tong Zhang. 2015. Semi-supervised convolutional neural networks for text categorization via re-
gion embedding. In Proceedings of the Annual Conference on Neural Information Processing Systems 2015 (NIPS’15) .
919–927.
[56] Dongyeop Kang, Tushar Khot, Ashish Sabharwal, and Eduard H. Hovy. 2018. AdvEntuRe: Adversarial training for
textual entailment with knowledge-guided examples. In Proceedings of the 56th Annual Meeting of the Association
forComputational Linguistics (ACL’18) .2418–2428.
[57] DimosthenisKaratzas,FaisalShafait,SeiichiUchida,MasakazuIwamura,LluisGomeziBigorda,SergiRoblesMestre,
Joan Mas, David Fernández Mota, Jon Almazán, and Lluís-Pere de las Heras. 2013. ICDAR 2013 robust reading
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:37
competition. In Proceedings of the 12th International Conference on Document Analysis and Recognition (ICDAR’13) .
1484–1493.
[58] TusharKhot,AshishSabharwal,andPeterClark.2018.SciTaiL:Atextualentailmentdatasetfromsciencequestion
answering.In Proceedings of the32nd AAAIConference onArtificial Intelligence(AAAI’18) .5189–5197.
[59] YoonKim.2014.Convolutionalneuralnetworksforsentenceclassification.In ProceedingsoftheConferenceonEm-
pirical Methods in Natural Language Processing (EMNLP’14) .1746–1751.
[60] YoonKim,YacineJernite,DavidSontag,andAlexanderM.Rush.2016.Character-awareneurallanguagemodels.In
Proceedings ofthe13th AAAIConferenceonArtificial Intelligence(AAAI’16) .2741–2749.
[61] Diederik P. Kingma and Jimmy Ba. 2015. DAM: A method for stochastic optimization. In Proceedings of the 3rd
International Conference onLearning Representations (ICLR’15) .
[62] Diederik P. Kingma and Max Welling. 2014. Auto-encoding variational Bayes. In Proceedings of the International
Conference onLearning Representations (ICLR’14) .
[63] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. OpenNMT: Open-source
toolkitforneuralmachinetranslation.In Proceedingsofthe55thAnnualMeetingoftheAssociationforComputational
Linguistics (ACL’17) .67–72.
[64] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,YannisKalan-
tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual genome: Connecting language
and vision usingcrowdsourced dense imageannotations. Int.J.Comput. Vision 123,1(2017),32–73.
[65] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classification with deep convolutional
neural networks. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS’12) .
1106–1114.
[66] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain
Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing.
InProceedings of the33ndInternational Conference onMachine Learning (ICML’16) .1378–1387.
[67] AlexeyKurakin,IanJ.Goodfellow,andSamyBengio.2017.Adversarialmachinelearningatscale.In Proceedingsof
the5th International ConferenceonLearning Representations (ICLR’17) .
[68] Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From word embeddings to document
distances. In Proceedings ofthe32nd International ConferenceonMachine Learning (ICML’15) .957–966.
[69] QuocV.LeandTomasMikolov.2014.Distributedrepresentationsofsentencesanddocuments.In Proceedingsofthe
31th International ConferenceonMachine Learning (ICML’14) .1188–1196.
[70] Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully character-level neural machine translation without
explicitsegmentation. Trans.Assoc.Comput. Linguist. 5 (2017),365–378.
[71] JensLehmann,RobertIsele,MaxJakob,AnjaJentzsch,DimitrisKontokostas,PabloN.Mendes,SebastianHellmann,
Mohamed Morsey, Patrick van Kleef, Sören Auer, and Christian Bizer. 2015. DBpedia—A large-scale, multilingual
knowledgebaseextractedfrom Wikipedia. Semantic Web 6,2 (2015),167–195.
[72] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text catego-
rization research. J.Mach.Learn. Res. 5(2004),361–397.
[73] JinfengLi,ShoulingJi,TianyuDu,BoLi,andTingWang.2019.TextBugger:Generatingadversarialtextagainstreal-
world applications.In Proceedings of26th AnnualNetwork andDistributed System Security Symposium (NDSS’19) .
[74] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley,and JianfengGao. 2016. Deep reinforcement learn-
ing for dialogue generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP2016) . 1192–1202.
[75] Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th International Conference on
Computational Linguistics (COLING’02) .
[76] BinLiang,HongchengLi,MiaoqiangSu,PanBian,XirongLi,andWenchangShi.2018.Deeptextclassificationcan
befooled.In Proceedings ofthe27th International JointConferenceonArtificial Intelligence(IJCAI’18) .4208–4215.
[77] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C.
LawrenceZitnick.2014.MicrosoftCOCO:Commonobjectsincontext.In Proceedingsofthe13thEuropeanConference
onComputer Vision(ECCV’14) .740–755.
[78] Angli Liu, Stephen Soderland, Jonathan Bragg, Christopher H. Lin, Xiao Ling, and Daniel S. Weld. 2016. Effective
crowd annotation for relation extraction. In Proceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL’16) .897–906.
[79] Qiang Liu, Pan Li, Wentao Zhao, Wei Cai, Shui Yu, and Victor C. M. Leung. 2018. A survey on security threats and
defensivetechniques of machinelearning:A datadrivenview. IEEEAccess 6 (2018),12103–12117.
[80] Tzu-Chien Liu,Yu-Hsueh Wu,and Hung-yi Lee.2017.Attention-based CNNmatchingnet. CoRRabs/1709.05036.
[81] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2017. Delving into transferable adversarial examples and
black-boxattacks.In Proceedings of theInternational ConferenceonLearning Representations (ICLR’17) .
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:38 W.E.Zhanget al.
[82] Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for
researchinunstructuredmulti-turndialoguesystems.In Proceedingsofthe16thAnnualMeetingoftheSpecialInterest
GrouponDiscourse and Dialogue (SIGDIAL’15) .285–294.
[83] AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopherPotts.2011.Learning
word vectors for sentimentanalysis. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics (ACL’11) .142–150.
[84] AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu.2018.Towardsdeep-
learning models resistant to adversarial attacks. In Proceedings of the 6th International Conference on Learning Rep-
resentations(ICLR’18) .
[85] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english:The Penntreebank. Comput.Linguist. 19,2(1993),313–330.
[86] Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014.
SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through se-
mantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation
(SemEval@COLING’14) .1– 8.
[87] Cettolo Mauro, Girardi Christian, and Federico Marcello. 2012. Wit3: Web inventory of transcribed and translated
talks. InProceedings of the 16th Annual Conference of the European Association for Machine Translation (EAMT’12) .
261–268.
[88] JulianJ.McAuleyandJureLeskovec.2013.Hiddenfactorsandhiddentopics:Understandingratingdimensionswith
review text. In Proceedings ofthe7th ACMConferenceon Recommender Systems (RecSys’13) . 165–172.
[89] VangelisMetsis,IonAndroutsopoulos,andGeorgiosPaliouras.2006.SpamfilteringwithNaiveBayes—WhichNaive
Bayes?In Proceedings ofthe3rd ConferenceonEmail and Anti-Spam(CEAS’06) .
[90] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations
ofwordsandphrasesandtheircompositionality.In Proceedingsofthe27thAnnualConferenceonNeuralInformation
ProcessingSystems (NIPS’13) . 3111–3119.
[91] Pasquale Minervini and Sebastian Riedel. 2018. Adversarially regularising neural NLI Models to integrate logi-
cal background knowledge. In Proceedings of the 22nd Conference on Computational Natural Language Learning
(CoNLL’18) .65–74.
[92] AnandMishra,KarteekAlahari,andC.V.Jawahar.2012.Scenetextrecognitionusinghigherorderlanguagepriors.
InProceedings ofthe23rd British Machine Vision Conference(BMVC’12) .1–11.
[93] TakeruMiyato,AndrewM.Dai,andIanJ.Goodfellow.2017.Adversarialtrainingmethodsforsemi-supervisedtext
classification.In Proceedings ofthe5th International ConferenceonLearning Representations (ICLR’17) .
[94] TakeruMiyato,Shin-ichiMaeda,MasanoriKoyama,KenNakae,andShinIshii.2016.Distributionalsmoothingwith
virtualadversarialtraining.In Proceedingsofthe4thInternationalConferenceonLearningRepresentations(ICLR’16) .
[95] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: A simple and accurate
method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR’16) . 2574–2582.
[96] Michael C. Mozer. 1995. A focused backpropagation algorithm for temporal. Backprop.: Theory Architect. Appl. 137
(1995).
[97] Paarth Neekhara, Shehzeen Hussain, Shlomo Dubnov, and Farinaz Koushanfar. 2018. Adversarial reprogramming
of sequence classification neuralnetworks. CoRRabs/1809.01829.
[98] Tong Niu and Mohit Bansal. 2018. Adversarial over-sensitivity and over-stability strategies for dialogue models. In
Proceedings of the22ndConferenceon Computational Natural Language Learning (CoNLL’18) .486–496.
[99] Joakim Nivre, Željko Agić, Maria Jesus Aranzabe, Masayuki Asahara, Aitziber Atutxa, Miguel Ballesteros, John
Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Cristina Bosco et al. 2015. Universal dependencies 1.2. https://
universaldependencies.org/ .
[100] Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita. 2018. A survey of the usages of deep learning in natural
languageprocessing. CoRRabs/1807.10854.
[101] HamidPalangi,LiDeng,YelongShen,JianfengGao,XiaodongHe,JianshuChen,XinyingSong,andRababK.Ward.
2016.Deepsentenceembeddingusinglongshort-termmemorynetworks:Analysisandapplicationtoinformation
retrieval. IEEE/ACMTrans.Audio Speech Lang. Process. 24,4(2016),694–707.
[102] BoPangandLillianLee.2005.Seeingstars:Exploitingclassrelationshipsforsentimentcategorizationwithrespect
toratingscales.In Proceedingsofthe43rdAnnualMeetingoftheAssociationforComputationalLinguistics(ACL’05) .
115–124.
[103] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. 2016. Crafting adversarial input se-
quences for recurrent neural networks. In Proceedings of the Military Communications Conference (MILCOM’16) .
IEEE, 49–54.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:39
[104] Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami.
2017.Practicalblack-boxattacksagainstmachinelearning.In ProceedingsoftheACMAsiaConferenceonComputer
and Communications Security (AsiaCCS’17) .506–519.
[105] NicolasPapernot,PatrickD.McDaniel,SomeshJha,MattFredrikson,Z.BerkayCelik,andAnanthramSwami.2016.
The limitations of deep learning in adversarial settings. In Proceedings of the IEEE European Symposium on Security
and Privacy (EuroS&P’16) . 372–387.
[106] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense
to adversarial perturbations against deep neural networks. In Proceedings of the IEEE Symposium on Security and
Privacy (SP’16) . 582–597.
[107] Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for
natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP’16) . 2249–2255.
[108] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-
moyer.2018.Deepcontextualizedwordrepresentations.In ProceedingsoftheConferenceoftheNorthAmericanChap-
ter oftheAssociation forComputational Linguistics: Human Language Technologies (NAACL-HLT’18) .2227–2237.
[109] Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and Charles K. Nicholas. 2018. Mal-
ware detection by eating a whole EXE. In Proceedings of the Workshops of the 32nd AAAI Conference on Artificial
Intelligence . 268–276.
[110] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial examples
through probability weighted word saliency. In Proceedings of the 57th Conference of the Association for Computa-
tional Linguistics (ACL’19) .1085–1097.
[111] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without la-
beled text. In Proceedings of European Conference on Machine Learning and Knowledge Discovery in Databases
(ECML/PKDD’10) .148–163.
[112] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomás Kociský, and Phil Blunsom. 2016. Reasoning
about entailment with neural attention. In Proceedings of the 2016 International Conference on Learning Representa-
tions (ICLR’16) .
[113] Royi Ronen, Marian Radu, Corina Feuerstein, Elad Yom-Tov, and Mansour Ahmadi. 2018. Microsoft malware clas-
sification challenge. CoRRabs/1802.10135.Retrievedfrom http://arxiv.org/abs/1802.10135.
[114] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. 2018. Generic black-box end-to-end attack against
state of the art API call based malware classifiers. In Proceedings of the 21st International Symposium of Research on
Attacks, Intrusions,andDefenses (RAID’18) .490–510.
[115] DanRothandWen-tauYih.2004.Alinearprogrammingformulationforglobalinferenceinnaturallanguagetasks.
InProceedings of the8th Conferenceon Computational Natural Language Learning (CoNLL’04) .1– 8.
[116] DavidE.Rumelhart,GeoffreyE.Hinton,andRonaldJ.Williams.1986.Learningrepresentationsbyback-propagating
errors.Nature323,6088(1986),533.
[117] SuranjanaSamantaandSameepMehta.2018.Generatingadversarialtextsamples.In Proceedingsofthe40thEuropean
Conference onIR Research (ECIR’18) .744–749.
[118] MotokiSato,JunSuzuki,HiroyukiShindo,andYujiMatsumoto.2018.Interpretableadversarialperturbationininput
embeddingspacefortext.In Proceedingsofthe27thInternationalJointConferenceonArtificialIntelligence(IJCAI’18) .
4323–4330.
[119] Dale Schuurmans and Martin Zinkevich. 2016. Deep-learning games. In Proceedings of the Annual Conference on
Neural Information Processing Systems 2016 (NIPS’16) . 1678–1686.
[120] Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler, Marcin Junczys-
Dowmunt,SamuelLäubli,AntonioValerioMiceliBarone,JozefMokry,andMariaNadejde.2017.Nematus:Atoolkit
for neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for
Computational Linguistics (EACL’17) .65–68.
[121] Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for
machine comprehension. In Proceedings of the 5th International Conference on Learning Representations (ICLR’17).
Poster.
[122] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C. Courville, and Joelle Pineau. 2016. Building end-
to-enddialoguesystemsusinggenerativehierarchicalneuralnetworkmodels.In Proceedingsofthe30thAAAICon-
ference on Artificial Intelligence(AAAI’16) .3776–3784.
[123] IulianVladSerban,AlessandroSordoni,RyanLowe,LaurentCharlin,JoellePineau,AaronC.Courville,andYoshua
Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In Proceedings of the
31st AAAIConferenceonArtificial Intelligence(AAAI’17) .3295–3301.
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.24:40 W.E.Zhanget al.
[124] Baoguang Shi, Xiang Bai, and Cong Yao. 2017. An end-to-end trainable neural network for image-based sequence
recognition and its applicationto scene text recognition. IEEE Trans. Pattern Anal. Mach. Intell. 39,11 (2017),2298–
2304.
[125] HaoyueShi,JiayuanMao,TeteXiao,Yuning Jiang,andJianSun.2018.Learningvisually-grounded semanticsfrom
contrastive adversarial samples. In Proceedings of the 27th International Conference on Computational Linguistics
(COLING’18) .3715–3727.
[126] KarenSimonyanandAndrewZisserman.2015.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.
InProceedings ofthe3rd International ConferenceonLearning Representations (ICLR’15) .
[127] SameerSingh,CarlosGuestrin,andMarcoTúlioRibeiro.2018.Semanticallyequivalentadversarialrulesfordebug-
gingNLPmodels.In Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL’18) .
856–865.
[128] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher
Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the
Conferenceon Empirical Methods in Natural Language Processing (EMNLP’13) .1631–1642.
[129] Congzheng Song and Vitaly Shmatikov. 2018. Fooling OCR systems with adversarial text images. CoRR
abs/1802.05385.
[130] Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, and Jiayu Zhou. 2018. Identify susceptible locations in medical
records via adversarial attacks on deep predictive models. In Proceedings of the 24th ACM SIGKDD International
ConferenceonKnowledgeDiscovery &Data Mining (KDD’18) . 793–801.
[131] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Pro-
ceedings of theAnnualConferenceon Neural Information Processing Systems (NIPS’14) . 2672–2680.
[132] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, and Joan Bruna. 2014. Intriguing properties of neural net-
works. In Proceedings ofthe2nd International ConferenceonLearning Representations (ICLR’14) .
[133] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-
structuredlongshort-termmemorynetworks.In Proceedingsofthe53rdAnnualMeetingoftheAssociationforCom-
putational Linguistics (ACL’15) .1556–1566.
[134] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016.
MovieQA: Understanding stories in movies through question-answering. In Proceedings of the IEEE Conference on
ComputerVision andPattern Recognition (CVPR’16) . 4631–4640.
[135] Tesseract.2016.Retrievedfrom https://github.com/tesseract-ocr/ .
[136] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,andIllia
Polosukhin.2017.Attentionisallyouneed.In ProceedingsoftheAnnualConferenceonNeuralInformationProcessing
Systems (NIPS’17) . 6000–6010.
[137] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption
generator.In Proceedings of IEEEConferenceonComputer Visionand PatternRecognition (CVPR’15) . 3156–3164.
[138] Canhui Wang, Min Zhang, Shaoping Ma, and Liyun Ru. 2008. Automatic online news issue construction in web
environment.In Proceedings ofthe17th International ConferenceonWorldWide Web(WWW’08) . 457–466.
[139] KaiWang,BorisBabenko,andSergeJ.Belongie.2011.End-to-endscenetextrecognition.In ProceedingsoftheIEEE
International ConferenceonComputer Vision(ICCV’11) .1457–1464.
[140] Shuohang Wang and Jing Jiang. 2017. A compare-aggregate model for matching text sequences. In Proceedings of
the5th International ConferenceonLearning Representations (ICLR’17) .
[141] Shuohang WangandJingJiang.2017.Machinecomprehensionusingmatch-LSTMandanswerpointer.In Proceed-
ings of the5th International ConferenceonLearning Representations (ICLR’17) .
[142] Yicheng Wang and Mohit Bansal. 2018. Robust machine comprehension models via adversarial training. In Pro-
ceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT’18) .575–581.
[143] DavidWarde-FarleyandIanGoodfellow.2016.Adversarialperturbationsofdeepneuralnetworks. Perturb.Optimiz.
Statist.311(2016).
[144] AdinaWilliams,NikitaNangia,andSamuelR.Bowman.2018.Abroad-coveragechallengecorpusforsentenceun-
derstandingthroughinference.In Proceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociation
forComputational Linguistics (NAACL’18) .1112–1122.
[145] Yi Wu, David Bamman, and Stuart Russell. 2017. Adversarial training for relation extraction. In Proceedings of the
Conferenceon Empirical Methods in Natural Language Processing .1778–1783.
[146] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,
Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei
Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No. 3,Article 24.Publicationdate:March2020.Adversarial Attacks on Deep-learningModelsin Natural Language Processing: A Survey 24:41
Jeffrey Dean. 2016. Google’s neural machine translation system: Bridging the gap between human and machine
translation. CoRRabs/1609.08144Retrievedfrom http://arxiv.org/abs/1609.08144.
[147] KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronC.Courville,RuslanSalakhutdinov,RichardS.Zemel,and
Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings
of the32ndInternational ConferenceonMachine Learning(ICML’15) . 2048–2057.
[148] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrell, and Dawn Song. 2018. Fooling vision and
languagemodelsdespitelocalizationandattentionmechanism.In ProceedingsofIEEEConferenceonComputerVision
and Pattern Recognition (CVPR’18) . 4951–4961.
[149] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for
learningandinferenceinknowledgebases.In Proceedings ofthe3rdInternational ConferenceonLearningRepresen-
tations (ICLR’15) .
[150] Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading
ESOL texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11) .
180–189.
[151] MichihiroYasunaga,JungoKasai,andDragomirR.Radev.2018.Robustmultilingualpart-of-speechtaggingviaad-
versarialtraining.In ProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics (NAACL’18) .976–986.
[152] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018. Recent trends in deep-learning-based
naturallanguageprocessing. IEEEComput.Intell.Mag. 13,3(2018),55–75.
[153] Xiaoyong Yuan, Pan He, and Xiaolin Andy Li. 2018. Adaptive adversarial attack on scene text recognition. CoRR
abs/1807.03326.Retrievedfrom http://arxiv.org/abs/1807.03326.
[154] XiaoyongYuan,PanHe,QileZhu,andXiaolinLi.2019.Adversarialexamples:Attacksanddefensesfordeeplearn-
ing.IEEETrans.Neural Netw.Learn. Syst. 30,9(2019),2805–2824.
[155] Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. 2019. Generating fluent adversarial examples for natural
languages. In Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL’19) . 1085–
1097.
[156] XiangZhang,JunboJakeZhao,andYannLeCun.2015.Character-levelconvolutionalnetworksfortextclassification.
InProceedings in Annual Conferenceon Neural Information Processing Systems (NIPS’15) . 649–657.
[157] ZhengliZhao,DheeruDua,andSameerSingh.2018.Generatingnaturaladversarialexamples.In Proceedingsofthe
6th International ConferenceonLearning Representations (ICLR’18) .
[158] Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual7W: Grounded question answering in
images.In Proceedings oftheIEEEConferenceonComputer VisionandPattern Recognition (CVPR’16) . 4995–5004.
Received May 2019; revised October 2019; accepted November 2019
ACM Transactions on IntelligentSystems andTechnology, Vol. 11,No.3,Article 24.Publicationdate:March 2020.